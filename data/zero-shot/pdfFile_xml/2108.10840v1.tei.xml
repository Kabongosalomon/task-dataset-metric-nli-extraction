<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Qiu</surname></persName>
							<email>qiushuhao@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
							<email>czhu@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* the corresponding author: Chuang Zhu (czhu@bupt.edu.cn)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>https://bupt-ai-cz.github.io/Meta-SelfLearning Figure 1. Dataset Overview: We address a multi-source domain adaptation dataset for text recognition, which contains more than 5 million images from five different domains, which are synthetic domain, document domain, street view domain, handwritten domain, and car license domain respectively. Some examples of data are shown in the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In recent years, deep learning-based methods have shown promising results in computer vision area. However, a common deep learning model requires a large amount of labeled data, which is labor-intensive to collect and label. What's more, the model can be ruined due to the domain shift between training data and testing data. Text recognition is a broadly studied field in computer vision and suffers from the same problems noted above due to the diversity of fonts and complicated backgrounds. In this paper, we focus on the text recognition problem and mainly make three contributions toward these problems. First, we collect a multi-source domain adaptation dataset for text recognition, including five different domains with over five million images, which is the first multi-domain text recognition dataset to our best knowledge. Secondly, we propose a new method called Meta Self-Learning, which combines the self-learning method with the meta-learning paradigm and achieves a better recognition result under the scene of multidomain adaptation. Thirdly, extensive experiments are conducted on the dataset to provide a benchmark and also show the effectiveness of our method. The code of our work and dataset are available soon at https://bupt-ai-cz. github.io/Meta-SelfLearning/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In recent years, deep learning-based methods have shown promising results in computer vision area. However, a common deep learning model requires a large amount of labeled data, which is labor-intensive to collect and label. What's more, the model can be ruined due to the domain shift between training data and testing data. Text recognition is a broadly studied field in computer vision and suffers from the same problems noted above due to the diversity of fonts and complicated backgrounds. In this paper, we focus on the text recognition problem and mainly make three contributions toward these problems. First, we collect a multi-source domain adaptation dataset for text recognition, including five different domains with over five million images, which is the first multi-domain text recognition dataset to our best knowledge. Secondly, we propose a new method called Meta Self-Learning, which combines the self-learning method with the meta-learning paradigm and achieves a better recognition result under the scene of multidomain adaptation. Thirdly, extensive experiments are conducted on the dataset to provide a benchmark and also show the effectiveness of our method. The code of our work and dataset are available soon at https://bupt-ai-cz. github.io/Meta-SelfLearning/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the booming development of deep learning leads to great progress in computer vision field. Text recognition has always been an important field in computer vision, for texts are everywhere in daily life and the understanding of them can be very meaningful. However, to realize accurate recognition in the real scene (which is known as scene text recognition) is still a challenging field because of the diversity of fonts and complicated environment (distortion, variation of font, occlude, etc.). Many deep-learningbased methods are proposed over the past few years to solve this problem <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>As a data-driven method, the performance of the deep learning model highly relies on the amount of training data. The common way for addressing the above problem is to build publicly available large-scale datasets. However, collecting and labeling a large amount of real scene text data can be a time-consuming and labor-intensive work. What's more, the direct use of these datasets can not produce good results sometimes because of the distribution shift between the training data (source domain) and the testing data (target domain). Domain adaptation is a research field that focuses on aligning the source domain and target domain and thus obtaining better results. In recent years, many domain adaptation methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref> have been proposed to solve the domain shift problem in image classification problems. Some researches in the text recognition area are also proposed that have good results in different domains <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>While single-source domain adaptation is widely researched, multi-source domain adaptation is actually more suitable for the scene text recognition problem, for the training data of scene texts are always collected from many different sources. As a generalization form of single-domain, multi-source domain adaptation universally gets a better result than single-source domain adaption for the larger training corpus. Most works in this area focus on image classification problems <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b23">24]</ref>, and some datasets are proposed for this field <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref>. However, to our best knowledge, there are no publicly available datasets for text recognition in this area, and therefore, there is almost no related research work. In this paper, we collect a multi-source domain adaption dataset and provide a benchmark to fill this gap.</p><p>Some recent studies combined multi-source domain adaptation methods and meta-learning together. Hieu et al. <ref type="bibr" target="#b24">[25]</ref> proposed a self-learning method combined with metalearning, which achieved SOTA on many image classification tasks. Li et al. <ref type="bibr" target="#b14">[15]</ref> proposed a framework to fit any domain adaptation methods and got better results for the good initialization provided by the meta-learning method. However, during the meta-update, this method didn't make use of the information from the target, which is very important for unsupervised domain adaptation problems. Inspired by this work, we proposed a method called meta self-learning in this paper. Our method adequately utilizes the information of the target domain by adding the target domain data to the meta-update process and get pseudo-labels with higher quality. The main contributions of our work are summarized as follows:</p><p>? We collect a multi-source domain adaptation dataset for text recognition with over 5 million images from 5 different domains. To the best of our knowledge, this is the first multi-domain adaptation dataset for text recognition.</p><p>? We propose a new self-learning framework for multisource domain adaptation, which is effective and can be easily fit into any MDA and self-learning problem.</p><p>? Experiments are conducted on our dataset, which provide a benchmark and show the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Text Recognition</head><p>A common text recognition system can be divided into four stages: image preprocessing stage, feature extraction stage, sequence modeling stage, and prediction stage. The preprocessing stage mainly focuses on normalizing the image and rotating the text images into an appropriate position; STN <ref type="bibr" target="#b9">[10]</ref> is a commonly used method in the computer vision area. Recent works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref> proposed methods like thin-plate spline (TPS) transformation to get a better result. In the feature extraction stage, CNN is used to extract the generate feature maps for images. In the field of text recognition, images are always be transformed into a feature map with a height of 1, therefore can be processed as a sequence in the following stages. In the sequence modeling stage, models like Bi-LSTM or GRU are used to learn the sequence information from the feature extracted in the last stage. Due to the variable length of the data, connectionist temporal classification (CTC) <ref type="bibr" target="#b5">[6]</ref> or attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> are commonly used in the prediction stage to make the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Domain adaptation</head><p>The original domain adaptation only focuses on the single-source domain problem, and the main idea is to align the distribution between the source domain and target domain. Multi-source domain adaptation problem is also getting popular in recent years for it is more closer to the real scene and can get a better result than single-source domain adaptation generally. The main domain adaptation methods can be mainly classified into three types.</p><p>Discrepancy based domain adaptation: Tzeng et al. <ref type="bibr" target="#b32">[33]</ref> proposed a domain confusion loss by calculating the maximum mean discrepancy (MMD) between the source domain data and the target domain data. Long et al. <ref type="bibr" target="#b19">[20]</ref> proposed to calculate the MMD of more than one layer and used a multi-kernel MMD (MK-MMD) to achieve a better alignment. Sun et al. <ref type="bibr" target="#b30">[31]</ref> proposed CORAL loss to align the second-order statistics of the source and target distributions. Peng et al. <ref type="bibr" target="#b23">[24]</ref> proposed a multi-source domain adaptation method to calculate the moment distance not only between the source domain and target domain, but also among source domains.</p><p>Adversarial training based domain adaptation: Ganin et al. <ref type="bibr" target="#b4">[5]</ref> used a domain discriminator and proposed a gradient reversal layer to separate the feature extractor and the domain discriminator, which forces the feature extractor to extract the domain-invariant feature. Zhao et al. <ref type="bibr" target="#b39">[40]</ref> proposed an adversarial method to solve the multi-source domain adaptation problem using the gradient reversal layer and provided a thorough analysis.</p><p>Self-training-based domain adaptation: Self-training method has been widely used in image classification and segmentation problems. The method trains the model iteratively by generating pseudo-label of target data and adding them into the training data <ref type="bibr" target="#b31">[32]</ref>. However, the direct use of this mechanism may only be helpful for the easy class and lead to a bias among classes in classification problems. Zou et al. <ref type="bibr" target="#b41">[42]</ref> proposed a confidence regularized self-training method by adding regularizers to the network and achieve a better result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Meta-Learning</head><p>Meta-learning, also known as learning-to-learn, is a broadly studied field in recent years. Different from traditional learning methods focusing on a specific task, metalearning methods aim to learn "how to learn" from multiple tasks and achieve fast adaptation on a new task with a few samples.</p><p>MAML <ref type="bibr" target="#b3">[4]</ref> is a very famous meta-learning algorithm, which aims to learn a good initialization of parameters and can guarantee a fast convergence to local minimal with a small amount of data on a new task. However, the computation overhead of MAML during training is very high due to the calculation of second-order derivatives. To reduce the computational cost of MAML, Reptile <ref type="bibr" target="#b22">[23]</ref> provides a family of first-order meta-learning algorithms to approximate the original MAML. Some meta-learning methods are designed based on metric learning, such as matching network and prototypical network <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>, which also make great influence in the field of few-shot learning.</p><p>Due to the inherent "bi-level update" property <ref type="bibr" target="#b6">[7]</ref>, some meta-learning-based domain adaptation and domain generalization methods were proposed in past few years. Li et al. <ref type="bibr" target="#b15">[16]</ref> proposed a domain generalization method by dividing the source domains into meta-train domains and metatest domains to simulate the real training process, which achieves better results on the real target domain. An online meta-learning method was proposed to enhance the effectiveness of any domain adaptation method <ref type="bibr" target="#b14">[15]</ref>. The online meta-learning paradigm also enables the long-term effect of meta-learning, instead of only being effective at the beginning of the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Self-Learning</head><p>Self-learning methods predict labels for the unlabeled data using the model trained on source domains and take them as correct labels if the predict confidence is higher than a threshold <ref type="bibr" target="#b13">[14]</ref>. The self-learning method can always bring considerable improvement because of the direct use of target domain data. However, there also exist some problems. The generated pseudo-labels can be noisy sometimes, and lead the model to a bad local minimal. Therefore, most works focus on how to generate pseudo-labels with high quality. Recent works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> provide methods for balancing different classes or adding regularizers to the model. Hieu et al. <ref type="bibr" target="#b24">[25]</ref> provided a meta-learning paradigm combining with the teacher-student model, thus the parameters of the teacher model can be evaluated by the pseudo-label and get updated with better quality. In this paper, we provide a new way to combine the meta-learning paradigm and the self-learning methods on the basis of <ref type="bibr" target="#b14">[15]</ref>, by utilizing the information of pseudo-label during the meta-update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Domain Text Recognition Dataset</head><p>In this section, we will introduce the details of our dataset.</p><p>Our multi-domain text dataset consists of 5,209,215 images in total, and is divided into five domains, which are synthetic domain, handwritten domain, document domain, street view domain, and car plate domain. The character set size is set to 3,816, with 3,754 common Chinese characters and 62 alphanumeric characters, which can be represented as C. All of the five different domains are split into the training set and the test set with the ratio of 9 : 1. The details of different domains are shown as follows.</p><p>Synthetic Domain. A good deep-learning-based text recognition model requires millions of images, which is very hard to collect and label. To fill up the gap of data size, synthetic texts are widely used during the training procedure. Therefore, a synthetic domain is necessary for the multi-domain dataset.</p><p>Our synthetic dataset contains 1,110,620 images in total. We generate all the training data with 5 different fonts and three different backgrounds. Random blocks and lines are added for the augmentation. When generating corpora, we found it impossible to cover all the characters in the charset only using real corpus. To achieve a better result on uncommon characters, we sacrifice the semantic information of this domain and generated all samples by random sampling from the C. The length of each corpus is between 4 and 10.</p><p>Document Domain. The data of document domain is collected from an open-source project 1 , and the dataset contains about 3 million images. We filtered out the images that contains characters not in C and got 1,710,885 images in total. The corpora in this domain are from documents and news, and have the same length of 10.</p><p>Street View Domain. There are many publicly available street view datasets on the internet, however, most of them only contain street view text images from one region, which means only contains Chinese character or alphanumeric characters. In order to make a better recognition result on both Chinese and alphabetical characters, we merged the images from both Chinese scene text recognition datasets and English scene text recognition datasets, including SVT <ref type="bibr" target="#b34">[35]</ref>, SVT perspective <ref type="bibr" target="#b8">[9]</ref>, ICDAR2013 <ref type="bibr" target="#b12">[13]</ref>, ICDAR2015 <ref type="bibr" target="#b11">[12]</ref>, RCTW17 <ref type="bibr" target="#b21">[22]</ref>, ICDAR-2019 <ref type="bibr" target="#b20">[21]</ref>, and CUTE80 <ref type="bibr" target="#b25">[26]</ref>. After the same filtering operation with the document domain, we got 199,346 images in this domain.</p><p>Handwritten Domain. The data of the handwritten domain is generated using the images in CASIA Online and Offline Chinese Handwriting Databases <ref type="bibr" target="#b17">[18]</ref>. Out of the same consideration in the synthetic domain, we think that a better coverage of the charset is more important. Therefore, we use the same corpora with the synthetic domain. What's more, we also use some corpora from the street view domain to balance between the semantic information and the coverage of the charset. Images are generated by concatenating single-char images together according to the corpora. We get 1,897,021 images in total for handwritten images.</p><p>Car License Domain. The car license domain is composed of two parts. The first part is the largest Chinese car license dataset CCPD <ref type="bibr" target="#b36">[37]</ref>, and we only use the base part of this dataset, which contains 199,996 images. Although the CCPD contains a large amount of data, there exists a severe problem in this dataset. A Chinese license plate consists of 7 characters, the first one is the Chinese abbreviation of the province, and the remaining six are letters or numbers. Among the 7 characters, the abbreviation of the province is the most difficult to recognize for the Chinese characters are more complicated than alphabetic characters. However, most images in this dataset are collected from the same city, which means most of the images have the same province identity. This situation leads to a severe imbalance of the dataset, and the model being trained on this dataset can not get good performance in recognizing the province identity of other provinces. To solve this problem, we provide extra 7,932 images collected from surveillance cameras in 26 different provinces and alleviate this problem. We finally got 207,928 images in total, including 31 Chinese characters and 34 alphabetic characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In this paper, we focus on the problem of multi-source domain adaptation and provide a new method combining the self-learning method with the online meta-learning method. The overview of our method is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Given D S = {S 1 , S 2 , . . . , S N } as the data with labels from multiple source domains, D T as the target domain without labels, our goal is to get a model f (?) with parameters ? that achieves good results on the target domain using D S and D T . By using the self-learning method, pseudo-label will be generated using D T , the data with pseudo-label can be represented asD T . While previous work using online metalearning method didn't take pseudo-label into account during the meat-update, we addD T into the meta-train set and the model will be updated with both D S andD T during the meta-update. This setting brings great gain for the model, for the information of the target domain can be very valuable. What's more, pseudo-labels with higher quality can be acquired under this paradigm. In the following section, we will first introduce the detail of our proposed method, and then introduce the text recognition model we used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Meta Self-Learning</head><p>The whole procedure of our meta self-learning method is described in Algorithm 1.</p><p>Warm-Up and Generate Pseudo-Label. The model will first be trained on D S as the warm-up phase. Warm-up is a necessary process for the self-learning method, and this process will greatly improve the quality of the generated pseudo-label and lead to a better result. Without warm-up process, the generated pseudo-labels will either have low confidence or wrong content, which will greatly jeopardize the predict accuracy on the target domain. After the warmup, the target data with pseudo-labelD T will be generated.</p><p>Random Split. The usage of the pseudo-label is one of the most important issue. As the raw pseudo-label can be noisy, a meta-update is used in our method. During the meta-update, both D S andD T will be used, and are divided randomly into meta-train set M and meta-test set M , which corresponds to the support set and query set in vanilla MAML.</p><p>Meta-Train. The network will first update on the metatrain set M using the text recognition loss l in Eq. 12</p><formula xml:id="formula_0">l a = 1 ||M || ||M || i=0 l(?;? i , y i ),<label>(1)</label></formula><p>where ||M || is the size of meta-train set,? i and y i are the predicted label and the ground-truth label of text image, respectively. Then, the gradient of the model parameter ? is calculated as ? ? , where</p><formula xml:id="formula_1">? ? = ?l a (?) ?? .<label>(2)</label></formula><p>Then, the parameter will be updated as</p><formula xml:id="formula_2">? = ? ? ?? ? ,<label>(3)</label></formula><p>where ? is the learning rate for the meta-train phase.</p><p>Meta-test. The meta-test phase is used to evaluate the model using meta-test set M . In this phase, the loss function l b is calculated with parameter updated in meta-train phase ? .</p><formula xml:id="formula_3">l b = 1 || M || || M || i=0 l(? ;? i , y i ),<label>(4)</label></formula><p>where || M || is the size of meta-test set. Following the vanilla MAML, we need to calculate the gradient of the original parameter ? using l b , which is</p><formula xml:id="formula_4">?l b (? ) ?? = ?l b (? ) ?? ? ?? ?? = ?l b (? ) ?? ? ? ? ? ?ls(?) ?? ?? = ?l b (? ) ?? ? (1 ? ? ? 2 l s (?) ?? 2 )<label>(5)</label></formula><p>It can be seen that a second-order derivative needs to be calculated. However, calculating the second-order derivative can be prohibitively expensive for a deep learning framework, especially for a large model with a long computation graph. Therefore, a first-order approximation of MAML is widely used, we can simply neglect the secondorder entry in Eq. 5, and the gradient ? ? can be approximated as</p><formula xml:id="formula_5">?l b (? ) ?? ? ?l b (? ) ?? .<label>(6)</label></formula><p>After the approximation, the gradient we need to update the original parameter ? can be replaced by ? ? , where</p><formula xml:id="formula_6">? ? = ?l b (? ) ?? .<label>(7)</label></formula><p>Therefore, the initial parameter ? can be directly updated using ? ? as ? = ? ? ?? ? , where ? is the learning rate for the meta-test phase.</p><p>Outer optimization As the meta-update uses the pseudo-label which can be noisy, an outer optimization is added additionally after it. In this phase, only the data from D S with real label is used to update the model with learning rate ?, as ? = ? ? ?? ? .</p><p>As MAML learns the initialization of network parameters only, it can not be applied to a normal network training with a consecutive update every iteration, for the influence of the initialization can be very trivial after times of iteration. In this paper, we use the online meta-learning method <ref type="bibr" target="#b14">[15]</ref>, and implements the procedure above in each iteration, therefore the network can benefit from the meta-learning paradigm throughout the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Text Recognition Model</head><p>In this section, we will introduce the text recognition model we used following the flow in Section 2. For most of our images don't have severe deformation, the TPS module is not used in our model. During the feature extraction stage, the raw input image X is fed into the CNN (we use ResNet-50 in our experiment) and generates the output feature map F (X) = x, where x has the shape of D ? 1 ? T and can be represented as x = {x 1 , x 2 , . . . , x t }, x i ? R d . Note that D and T represent the channel number and the length of the feature map respectively, and the height of the feature map is set to 1. During the sequence modeling stage, we use a BiLSTM, and the hidden state of each time step can be represented as h = {h 1 , h 2 , . . . , h t }, h i ? R h . The hidden state h is then used for the final prediction. In our model, we use an attention mechanism in <ref type="bibr" target="#b1">[2]</ref>. During the prediction of each time step, a context vector c t is calculated by weighting the importance of different time steps</p><formula xml:id="formula_7">c t = T i=0 ? t,i h i ,<label>(8)</label></formula><p>where the weight ? t,i is</p><formula xml:id="formula_8">? t,i = exp(c t,i ) T j=0 exp(c t,j ) .<label>(9)</label></formula><p>The c t,i in Eq. 9 is the importance of the i-th time step to the t-th time step, calculated with</p><formula xml:id="formula_9">c t,i = tanh(W S s t?1 + W h h i ),<label>(10)</label></formula><p>where W s , W h are the learnable parameters and s t?1 is the hidden state of the decoder. The hidden state s t is calculated using the hidden state and the ground truth of the last time step g t?1 (which is teacher forcing method), together with the context vector of current time step c t using a LSTM</p><formula xml:id="formula_10">s t = LST M (g t?1 , s t?1 , c t ).<label>(11)</label></formula><p>Finally, a cross-entropy loss is used to calculate the classification loss on each time step</p><formula xml:id="formula_11">L = T i=1 k j=1 ?y ik log? ik ,<label>(12)</label></formula><p>where k is the size of the charset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>In this section, we provide a benchmark on the dataset we proposed, and also show the experimental results and ablation studies to demonstrate the effectiveness of our method. The base text recognition model is modified on the best model in <ref type="bibr" target="#b0">[1]</ref>. We implement our model using PyTorch on an NVIDIA Tesla T4. Adam is used as the outer optimizer, and SGD is used as the meta optimizer. ? is set to 1e ? 3, ? and ? are changed during the training process. During training, we pick one domain as the target domain while the other four domains as source domains. We set the batch size to 24 per domain, which is 96 for 4 source domains and all the images are resized into 100 ? 32. When using pseudolabel, we will use the training set of the target domain to generate the pseudo-label and the result is tested on the test set, which is unavailable during training.</p><p>The size of the character set in these experiments is set to 3818, which includes 3756 common Chinese characters and 62 alphanumeric characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Results</head><p>Baseline. The baseline model is trained with only source domains without any multi-source domain adaptation methods. The test accuracy of each domain is shown in <ref type="table" target="#tab_0">Table  1</ref>. It can be seen that, directly using the source domain data performs badly on the target domain, which indicates that there are non-negligible domain gaps among different domains. The average accuracy among the 5 domains is 17.86%.</p><p>MLDG <ref type="bibr" target="#b15">[16]</ref>. As described in the last section, our algorithm is a combination of meta-learning paradigm and pseudo-label method. In order to figure out the effectiveness of each part, we conduct experiments with two methods respectively. Li et al. <ref type="bibr" target="#b15">[16]</ref> provide a training method using the meta-learning paradigm only. During the training, the source domains are divided into meta-train set and meta-test set. The model will first update one step using the meta-train set and then validate on the meta-test set. The final model converged on source domains will be deployed on the truly held-out target domain. According to the experiment results shown in table, the MLDG is not very effective for text recognition tasks for there is only a 1.16% improvement on average. We think the reason is that the difference between source domains and target domain is not only on appearance but also on the semantic level. For example, the document domain has a fixed length of 10 characters per sample, while other domains only contain few samples of the same length. What's more, the training data are sampled from different corpora, making it hard to learn the target domain's distribution with source domain data only.</p><p>Pseudo-Label. The experiment results using pseudolabel methods are shown in <ref type="table" target="#tab_0">Table 1</ref>. As the warm-up is a necessary step for the pseudo-label method, we use the baseline model as the pre-trained model and start training using pseudo-label directly on it. For car, street, synthetic, and document domains, we set the threshold of pseudo- Meta Self-Learning. Using the same setting with the pseudo-label method, the same experiments are conducted with our meta self-learning method, and the results are shown in <ref type="table" target="#tab_0">Table 1</ref>. It can be seen that our method brings up to 13.67% gain and 8.11% gain on average accuracy compared with the vanilla pseudo-label method, and achieves best result on every target domain. It's worth noting that, we actually used different settings for the different target domains. It is based on the finding that for each target domain, the selection of domains for meta-update and outer optimization may affect the result greatly, and we will discuss the detail in the next section. The result shown in the table is the result corresponds to the best setting for each target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Discussion on Different Target Domains</head><p>During training, we found that the training procedure shown in Algorithm ?? not always performs best, therefore, we did experiments on three different settings as shown in <ref type="table">Table 2</ref>. The main difference between these three settings is mainly reflected on the usage of pseudo-label images.</p><p>IAOA represents the training procedure shown in algorithm 1, which use all 5 domains during the meta-update, and only use source domains during the outer optimization. Under this setting, we got the best results on the car license domain and the street domain. However, this setting didn't get good results on document domain and synthetic domain, and even worse than using the vanilla pseudo-label method. This may indicate that, in some domains, the use of source domain data may jeopardize the effectiveness of the pseudolabel image. Therefore, we tried to make pseudo-label play a more important role during training in some domains.</p><p>IPOA represents using the pseudo-label domain as metatest set only during meta-update and use all five domains during outer optimization. In this setting, images with pseudo-label are added into the outer optimization and therefore influence the result more. It can be seen that, this setting achieves a better result than the pseudo-label method on synthetic, document and handwritten domains, which verifies our inference above. Therefore, we exploit the pseudo-label further in the next setting.</p><p>IPOP has the same setting with IPOA during metaupdate, while only use images with pseudo-label during the outer optimization. This setting achieves great improvements on synthetic and document domains, which is 64.09% on the document domain and 65.33% on the synthetic domain.</p><p>From the experiment results, we can see that the different usage of pseudo-label images can produce great gaps in the final accuracy, however, the impact is also different among different domains, but why this happens? Here we provide an intuitive explanation by the following experiments.</p><p>During training, we record the number of generated pseudo-label images and images that have the correct pseudo-label. The results from the car plate, document and synthetic domain using vanilla pseudo-label method are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. In all three domains, the number of the generated pseudo-labels keep increasing during the training process and finally reach nearly 50,000, which is the maximum number of images we set that allowed to be used as pseudo-label. Meanwhile, the accuracy of generated pseudo-labels keep decreasing and finally converge to a value. It can be seen that the pseudo-label accuracy of the car plate domain finally converges to about 0.4 while the in document and synthetic domain, this value is between 0.5 and 0.6, which means that, the pseudo-label quality of the car plate domain is relatively low. Therefore, it is reasonable to see that the accuracy of car license domain gets lower when rely more on the pseudo-label domain, while the accuracy of document and synthetic domain get better results.</p><p>The effectiveness of our method can also be shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. We demonstrate the number of pseudo-label and pseudo-label accuracy in both vanilla pseudo-label method and our methods on car, synthetic, and document domains during training. The results of synthetic domain and document domain are similar. The pseudo-label number will converge to nearly 50,000 in both two methods, while our method stably gets a higher accuracy on the generated pseudo-label, which is on average 10% higher than the vanilla pseudo-label method. For the car plate domain, the number of pseudo-label in our method is controlled to about 30,000, and get a 20% promotion on the accuracy. These indicate that our method can produce pseudo-label with higher quality and get better training results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Application Analysis</head><p>Our method provides a self-learning method for multisource domain adaptation problems, but it can also be transferred into the single-source domain adaptation problem, where D S contains only one domain. Actually, our method is a self-learning framework and is model-agnostic, therefore can be easily applied to any task using self-learning methods. However, as discussed in the last section, the paradigm for different tasks may need to be changed according to the quality of pseudo-labels. In future work, we will try to find a theoretical explanation and a unified framework for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we collect and generate a multi-source domain adaptation dataset for text recognition. To our best knowledge, this is the first and the largest publicly available dataset for this area, which is very meaningful for the great significance of both domain adaptation and text recognition problems. We also propose a new meta self-learning method for the multi-source domain adaptation problem, which is model-agnostic and can be easily applied to other tasks. Extensive experiments are done on our dataset to provide a benchmark and demonstrate the effectiveness of our method. However, our dataset is still very challenging because of the large scale of charset and notable domain shift among domains, and worth more exploration in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 : 5 6 Update: ? = ? ? ?? ? ; 7 8</head><label>15678</label><figDesc>Meta Self-Learning for Multi-source Domain Adaptation Data: D S = {S 1 , S 2 , . . . , S N }, D T = T 1 Input: Initial model f (?) Input: Meta train learning rate ?, meta test learning rate ?, outer learning rate ? Input: pseudo-label threshold ? Result: Optimized parameter ? 1 Warming up using D s , get ?; 2 while not converge do 3D T = f (?; T 1 ) &gt; ? ; 4 Random Split: D S +D T ? M + M ; Meta train: evaluate ? ? = ?l(?;M ) ?? ; Meta test: evaluate ? ? = ?l(? ; M ) ?? ; Update ? = ? ? ?? ? ; 9 Outer optimization: evaluate ? ? = ?l(?;D S ) ?? ; 10 Update ? = ? ? ?? ? ; 11 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of Meta Self-Learning method: The procedure of our method can be summarized as follow steps: 1. The data from source domains with labels DS are used for warm-up; 2. The model is evaluated on the target domain data without labels DT and generates pseudo-labels; 3. The target domain data with pseudo-labels DS andDT are split randomly as M and M ; 4. Meta train using M ; 5. Meta test using M ; 6. Outer optimization using a subset of DS andDT .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The accuracy of pseudo-label for car plate, document and synthetic domain during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The accuracy of pseudo-label during training for vanilla pseudo-label method and our method from 3 domains. (a) is the synthetic domain, (b) is the document domain, (c) is the car license domain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Experiment results on five different target domains. St represents street domain; Sy represents synthetic domain; D represents documentation domain; H represents Handwritten domain; C represents car license domain St,Sy,D,H?C St,Sy,D,C?H St,Sy,C,H?D C,St,D,H?Sy C,Sy,D,H?St Average</figDesc><table><row><cell>Source Only</cell><cell></cell><cell>22.43%</cell><cell>3.50%</cell><cell>29.39%</cell><cell>24.75%</cell><cell>9.24%</cell><cell>17.86%</cell></row><row><cell>MLDG [16]</cell><cell></cell><cell>23.85%</cell><cell>3.39%</cell><cell>30.31%</cell><cell>25.11%</cell><cell>12.46%</cell><cell>19.02%</cell></row><row><cell>Pseudo-Label [14]</cell><cell></cell><cell>44.97%</cell><cell>3.77%</cell><cell>51.60%</cell><cell>54.11%</cell><cell>15.00%</cell><cell>33.89%</cell></row><row><cell cols="2">Meta Self-Learning (Ours)</cell><cell>58.64%</cell><cell>5.41%</cell><cell>64.09%</cell><cell>65.33%</cell><cell>16.52%</cell><cell>42.00%</cell></row><row><cell></cell><cell cols="6">Table 2. Experiment results of different settings on meta self-learning method.</cell></row><row><cell></cell><cell cols="6">St,Sy,D,H?C St,Sy,D,C?H St,Sy,C,H?D C,St,D,H?Sy C,Sy,D,H?St</cell></row><row><cell>IAOS</cell><cell cols="2">58.64%</cell><cell>4.93%</cell><cell>42.94%</cell><cell>37.06%</cell><cell>16.52%</cell></row><row><cell>IPOA</cell><cell cols="2">44.94%</cell><cell>5.41%</cell><cell>53.35%</cell><cell>56.72%</cell><cell>15.34%</cell></row><row><cell>IPOP</cell><cell cols="2">41.05%</cell><cell>3.41%</cell><cell>64.09%</cell><cell>65.33%</cell><cell>15.02%</cell></row><row><cell cols="4">label confidence as 0.9. The threshold for handwritten do-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">main is set to 0.98, for the pre-trained model performs bad</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">on this domain. For the number of training data is very</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">large in all domains, testing on the whole training set can be</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">very time-consuming. Therefore, we only evaluate 50,000</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">images per domain, and the evaluation is done every 5,000</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">iterations. As shown in table, using pseudo-label can bring</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">an up to 22.54% gain on accuracy for a single domain, and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">the average accuracy increases by 16.03%.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/YCG09/chinese ocr</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by 111 Project of China (B17007), and in part by the National Natural Science Foundation of China (61602011).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seong Joon Oh, and Hwalsuk Lee. What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghun</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geewook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07503</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05439</idno>
		<title level="m">Meta-learning in neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gtc: Guided training of ctc towards efficient and accurate scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11005" to="11012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<title level="m">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02025</idno>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised writer adaptation for synthetic-to-real handwritten word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?al</forename><surname>Rusinol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><surname>Forn?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3502" to="3511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez I Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sergi Robles Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Fernandez</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<meeting><address><addrLine>Jon Almazan Almazan, and Lluis Pere De Las Heras</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online meta-learning for multi-source and semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="382" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Casia online and offline chinese handwriting databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu-Feng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Star-net: a spatial attention residue network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ic-dar2019 robust reading challenge on multi-lingual scene text detection and recognition-rrc-mlt-2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nibal</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinaki</forename><surname>Nath Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wafa</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Christophe</forename><surname>Burie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1582" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nibal</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imen</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580</idno>
		<title level="m">Meta pseudo labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhar</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahankote</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Chee Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study. Knowledge and Information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="245" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04080</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decoupled attention network for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxiang</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12216" to="12224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards end-to-end license plate detection and recognition: A large dataset and baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changchun</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="255" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ga-dan: Geometry-aware domain adaptation network for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9105" to="9115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence domain adaptation network for robust text image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenju</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2740" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="8559" to="8570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
