<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
							<email>usmn@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>4saenko@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avneesh</forename><surname>Sud</surname></persName>
							<email>asud@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the era of deep learning, human pose estimation from multiple cameras with unknown calibration has received little attention to date. We show how to train a neural model to perform this task with high precision and minimal latency overhead. The proposed model takes into account joint location uncertainty due to occlusion from multiple views, and requires only 2D keypoint data for training. Our method outperforms both classical bundle adjustment and weakly-supervised monocular 3D baselines on the well-established Human3.6M dataset, as well as the more challenging in-the-wild Ski-Pose PTZ dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We tackle the problem of estimating 3D coordinates of human joints from RGB images captured using synchronized (potentially moving) cameras with unknown positions, orientations, and intrinsic parameters. We additionally assume having access to a training set with only 2D positions of joints labeled on captured images.</p><p>Historically, real-time capture of the human 3D pose has been undertaken only by large enterprises that could afford expensive specialized motion capture equipment <ref type="bibr" target="#b11">[12]</ref>. In principle, if camera calibrations are available <ref type="bibr" target="#b0">[1]</ref>, human body joints can be triangulated directly from cameraspace observations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>. One scenario in which camera calibration cannot easily be estimated is sports capture, in which close-ups of players are captured in front of low-texture backgrounds, with wide-baseline, moving cameras. Plain backgrounds preclude calibration, as not sufficiently many feature correspondences can be detected across views; see <ref type="figure">Figure 1</ref>.</p><p>In this work, we propose a neural network to simultaneously predict 3D human and relative camera poses from multiple views; see <ref type="figure">Figure 1</ref>. Our approach uses human body joints as a source of information for camera calibration. As joints often become occluded, uncertainty must be carefully accounted for, to avoid bad calibration and con-  <ref type="figure">Figure 1</ref>. We show how to train a neural network that can aggregate outputs of multiple single-view methods, takes prediction uncertainty into consideration, has minimal latency overhead, and requires only 2D supervision for training. Our method mimics the structure of bundle-adjustment solvers, but using the joints of the human body to drive camera calibration, and by implementing a bundle-like solver with a simple feed-forward neural network. sequent erroneous 3D pose predictions. As we assume a synchronized multi-camera setup at test-time, our algorithm should also be able to effectively aggregate information from different viewpoints. Finally, our approach supervised by 2D annotations alone, as ground-truth annotation of 3D data is unwieldy. As summarized in <ref type="figure">Figure 2</ref>, and detailed in what follows, none of the existing approaches fully satisfies these fundamental requirements. Fully-supervised 3D pose estimation approaches yield the lowest estimation error, but make use of known 3D camera specification during either training <ref type="bibr">[45]</ref> or both training and inference <ref type="bibr" target="#b18">[19]</ref>. However, the prohibitively high cost of 3D joint annotation and full camera calibration in-the-wild makes it difficult to acquire large enough labeled datasets representative of specific environments <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">37]</ref>, therefore rendering supervised methods not applicable in this setup. Monocular 3D methods, <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr">44]</ref> as well as 2D-to-3D lifting networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">43]</ref>, relax data constraints to enable 3D pose inference using just multi-view 2D data without calibration at train time. Unfortunately, at inference time, these methods can only be applied to a single view at a time, therefore unable to leverage cross-view information and uncertainty.  <ref type="figure">Figure 2</ref>. Prior work -Existing solutions either require 3D annotations <ref type="bibr" target="#b18">[19]</ref>, perform inference on a single view at a time <ref type="bibr">[44]</ref>, or ignore uncertainty in joint coordinates due to occlusions <ref type="bibr" target="#b24">[25]</ref>.</p><p>Classical SfM (structure from motion) approaches to 3D pose estimation <ref type="bibr" target="#b24">[25]</ref> iteratively refine both the camera and the 3D pose from noisy 2D observations. However, these methods are often much slower than their neural counterparts, since they have to perform several optimization steps during inference. Further, most of them do not consider uncertainty estimates, resulting in sub-par performance.</p><p>To overcome these limitation we propose MetaPose; see <ref type="figure">Figure 1</ref>. Our method for 3D pose estimation aggregates pose predictions and uncertainty estimates across multiple views, requires no 3D joint annotations or camera parameters at both train and inference time, and adds very little latency to the resulting pipeline.</p><p>Overall, we propose the feed-forward neural architecture that can accurately estimate the 3D human pose and the relative cameras configuration from multiple views, taking into account joint occlusions and prediction uncertainties, and uses only 2D joint annotations for training. We employ an off-the-shelf weakly-supervised 3D network to form an initial guess about the pose and the camera setup, and a neural meta-optimizer that iteratively refines this guess using 2D joint location probability heatmaps generated by an off-the-shelf 2D pose estimation network. This modular approach not only yields low estimation error, leading to state-of-the-art results on Human3.6M <ref type="bibr" target="#b16">[17]</ref> and Ski-Pose PTZ <ref type="bibr">[37]</ref>, but also has low latency, as inference within our framework executes as a feed-forward neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review multi-view 3D human pose estimation, while we refer to Joo et al. <ref type="bibr" target="#b21">[22]</ref> for a survey of 3D human pose estimation in the wild. Full supervision. Supervised methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr">42]</ref> yield the lowest 3D pose estimation errors on multi-view single person <ref type="bibr" target="#b16">[17]</ref> and multi-person <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref> datasets, but require precise camera calibration during both training and inference. Other approaches [45] use datasets with full 3D annotations and a large number of annotated cameras to train methods that can adapt to novel camera setups in vi-sually similar environments, therefore somewhat relaxing camera calibration requirements. Martinez et al. <ref type="bibr" target="#b31">[32]</ref> use pre-trained 2D pose networks <ref type="bibr" target="#b33">[34]</ref> to take advantage of existing datasets with 2D pose annotations. Epipolar transformers <ref type="bibr" target="#b15">[16]</ref> use only 2D keypoint supervision, but require camera calibration to incorporate 3D information in the 2D feature extractors.</p><p>Weak and self-supervision. Several approaches exist that do not use paired 3D ground truth data. Many augment limited 3D annotations with 2D labels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b37">49]</ref>. Fittingbased methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr">46]</ref> jointly fit a statistical 3D human body model and 3D human pose to monocular images or videos. Analysis-by-synthesis methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">36]</ref> learn to predict 3D human pose by estimating appearance in a novel view. Most related to our work are approaches that exploit the structure of multi-view image capture. Epipo-larPose <ref type="bibr" target="#b26">[27]</ref> uses epipolar geometry to obtain 3D pose estimates from multi-view 2D predictions, and subsequently uses them to directly supervise 3D pose regression. Iqbal et al. <ref type="bibr" target="#b17">[18]</ref> proposes a weakly-supervised baseline to predict pixel coordinates of joints and their depth in each view and penalized the discrepancy between rigidly aligned predictions for different views during training. The selfsupervised CanonPose [44] further advances state-of-theart by decoupling 3D pose estimation in "canonical" frame. Drover et al. <ref type="bibr" target="#b8">[9]</ref> learn a "dictionary" mapping 2D pose projections into corresponding realistic 3D poses, using a large collection of simulated 3D-to-2D projections. RepNet <ref type="bibr">[43]</ref> and Chen et al. <ref type="bibr" target="#b5">[6]</ref> train similar "2D-to-3D lifting networks" with more realistic data constraints. While all the aforementioned methods use multi-view consistency for training, they do not allow pose inference from multiple images.</p><p>Iterative refinement. Estimating camera and pose simultaneously is a long-standing problem in vision <ref type="bibr">[38]</ref>. One of the more recent successful attempts is the work of Bridgeman et al. <ref type="bibr" target="#b3">[4]</ref> that proposed an end-to-end network that refines the initial calibration guess using center points of multiple players in the field. In the absence of such external calibration signals, Takahashi et al.</p><p>[41] performs bundle adjustment with bone length constraints, but do not report results on a public benchmark. AniPose <ref type="bibr" target="#b24">[25]</ref> performs joint 3D pose and camera refinement using a modified version of the robust 3D registration algorithm of Zhou et al. <ref type="bibr" target="#b36">[48]</ref>. Such methods ignore predicted uncertainty for faster inference, but robustly iteratively estimate outlier 2D observations and ignores them during refinement. In Section 5, we show that these classical approaches struggle in ill-defined settings, such as when we have a small number of cameras. More recently, SPIN <ref type="bibr" target="#b28">[29]</ref>, HUND <ref type="bibr" target="#b35">[47]</ref> and Holopose <ref type="bibr" target="#b13">[14]</ref> incorporate iterative pose refinement for monocular inputs, however, the refinement is tightly integrated into the pose estimation network. MetaPose effectively regularizes the multi-view pose estimation problem with a finite-capacity <ref type="bibr">Figure 3</ref>. Method -We illustrate our method with a simple 2D example of regressing the 3D vertices of an equilateral triangle given multiview observations. (left) AniPose <ref type="bibr" target="#b24">[25]</ref> performs classical bundle adjustment to identify camera positions and 3D vertices that minimize reprojection error to 2D landmarks on the input images. Conversely, our technique emulates classical bundle adjustment in a "neural" fashion by a meta-optimizer: first (middle), the EpipolarPose <ref type="bibr" target="#b26">[27]</ref> neural network obtains a per-frame 3D estimate of the joints, which we co-align via procrustes to estimate an initialization of both 3D cameras and 3D joints; then (right), a neural network meta-optimizer performs a bundle adjustment and refines both joints and cameras, using per-view keypoint localization heatmaps as input. Additional prior information, such as the fact that the triangle is equilateral, can be elegantly integrated in the meta-optimizer training. neural network resulting in both faster inference and higher precision than the classical refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>As illustrated in <ref type="figure">Figure 3</ref>, given a collection of {I c } images, we seek to optimize, up to a global rotation, scale, and shift:</p><formula xml:id="formula_0">? J = {j j ? R 3 } J j=1</formula><p>: the 3D coordinates of 3D body joints, ? C = {c c ? R P } C c=1 : the cameras parameters. Having also observed:</p><formula xml:id="formula_1">? H = {h c ? R J?H?W } C c=1</formula><p>: a set of 2D heatmaps of locations on images {I c } captured using these cameras, And assuming that, at training time, we are provided with:</p><p>? K = {k j,c }: the ground truth 2D locations of the projection of joint j j in camera c c .</p><p>Bayesian model. Formally, assuming that heatmaps depend on camera parameters and joint positions J only through 2D keypoint locations (i.e. p(H|K, J, C) = p(H|K)), the joint distribution can be factorized as:</p><formula xml:id="formula_2">p(J, C, K, H) = p(H|K) p(K|J, C) p(J) p(C)<label>(1)</label></formula><p>Joints and keypoints are assumed to be related by:</p><formula xml:id="formula_3">p(K|J, C) = j,c ?(k j,c ? ?(j j , c c ))<label>(2)</label></formula><p>where ? is the Dirac distribution, and ?(j, c) projects a joint j to the 2D coordinates in camera c. We use a weakprojection camera model , hence, each camera is defined by a tuple of rotation matrix R, pixel shift vector t, and single scale parameter s, i.e. c = [R, t, s], and the projection operator is defined as ?(j, (R, t, s)) = s ? I [0:1] ? R ? j + t where I [0:1] is a truncated identity matrix that discards the third dimension of the multiplied vector. This choice of the camera model simplifies initialization of camera parameters from single-view estimates (Section 3.2) and eliminates the ambiguity in focal length-vs-distance to the camera. In Section 5 we show experimentally what fraction of the final error comes from this camera model. Inference task. Our inference task is then to estimate the J and C from observed heatmaps H. We first introduce a probabilistic bundle adjustment formulation to handle joint position uncertainty, then propose a regression model that models complex interactions between joint positions and observed heatmaps. The overall inference task can be framed as finding the maximum of the posterior probability of the pose and camera parameters given observed heatmaps, marginalized over possible keypoint locations:</p><formula xml:id="formula_4">max J,C p(J, C|H) = p(k|H) p(k|J, C) p(J) p(C) p(k) dk (3)</formula><p>where, assuming that no prior information over camera parameters, keypoint locations, and poses is given (i.e. constant p(C), p(K) and p(J)) and using (2) we get:</p><formula xml:id="formula_5">p(J, C|H) ? c,j p(k j,c = ?(j j , c c )|H)<label>(4)</label></formula><p>Further, assuming that each keypoint k c,j is affected only by a corresponding heatmap h c,j , and more specifically that the conditional probability density is proportional to the corresponding value of the heatmap:</p><formula xml:id="formula_6">p(k j,c |H) = p(k j,c |h j,c ) ? h j,c [k j,c ]<label>(5)</label></formula><p>we get a probabilistic bundle adjustment problem:</p><formula xml:id="formula_7">max J,C c,j h j,c [?(j j , c c )]<label>(6)</label></formula><p>As we will show in Section 5, better estimation accuracy with faster inference time can be archived if assume that each keypoint can be affected by any heatmap via the following functional relation up to a normally distributed residual:</p><formula xml:id="formula_8">p(K|H, ?) = N (K | ?(J ? (H), C ? (H)), I)<label>(7)</label></formula><p>where J ? , C ? are joint and camera regression models (e.g. neural networks) parameterized by an unknown parameter ?, and N is a multivariate normal density. Parameters of this model can be found via maximum likelihood estimation using observations from p(K, H) available during training</p><formula xml:id="formula_9">? MLE = arg max ? p(H, K|?) = arg max ? p(K|H, ?) (8) = arg min ? E K,H K ? ?(J ? (H), C ? (H)) 2 2<label>(9)</label></formula><p>Then the test-time inference reduces to evaluation of the regression model at given heatmaps:</p><formula xml:id="formula_10">arg max J,C p(J, C|H, ?) = J ? (H), C ? (H)<label>(10)</label></formula><p>Intuitively, the parametric objective enables complex interactions between all observed heatmaps and all predicted joint locations. The resulting model outperforms the probabilistic bundle adjustment both in terms of speed and accuracy, as we show in Section 5. Solver. To solve the highly non-convex problem in <ref type="bibr" target="#b8">(9)</ref>, and to do so efficiently, we employ a modular two stages approach; see <ref type="figure">Figure 3</ref>:</p><p>Stage 1 (S1): Initialization -Section 3.2: We first acquire an initial guess (J init , C init ) using single-view 3D pose estimates for the camera configuration and the 3D pose by applying rigid alignment to per-view 3D pose estimates obtained using a pre-trained weakly-supervised single-view 3D network, e.g. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr">44</ref>] Stage 2 (S2): Refinement -Section 3.3: We then train a neural network f ? to predict a series of refinement steps for camera and pose, staring from the initial guess so to optimize <ref type="bibr" target="#b8">(9)</ref>.</p><p>Advantages. This approach has several key advantages: 1) it primes the refinement stage with a "good enough" guess to start from the correct basin of the highly non-convex pose likelihood objective given multi-view heatmaps; 2) it provides us with a modular framework, letting us swap pre-trained modules for single-view 2D and 3D without re-training the entire pipeline whenever a better approach becomes available; 3) the neural optimizer provides orders of magnitude faster inference than classical iterative refinement, and allows the entire framework to be written within the same coherent computation framework (i.e. neural networks vs. neural networks plus classical optimization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-processing</head><p>We assume that we have access to a 2D pose estimation model (e.g. PoseNet <ref type="bibr" target="#b34">[35]</ref>) that produces 2D localization heatmaps h j,c for each joint j from RGB image I c . We approximate each heatmap h j,c with an M -component mixture of spherical Gaussians g j,c . This compressed format reduces the dimensionality of the input to the neural optimizer (Section 3.3). To fit parameters g j,c of a mixture of spherical Gaussians to a localization 2D histogram h j,c , we treat the heatmap as a regular grid of samples weighted by corresponding probabilities, and apply weighted EM algorithm <ref type="bibr" target="#b10">[11]</ref> directly to weighted samples, as described in the supplementary Section 8.2. Single-view pose estimation. To initialize camera parameters via rigid alignment (Section 3.2), we need a singleimage 3D pose estimation model trained without 3D supervision (e.g. EpipolarPose <ref type="bibr" target="#b26">[27]</ref>) that produces per-camera rough 3D pose estimates Q = {q c,j } given an image I c from that camera. These single-image estimates q c,j are assumed to be in the camera frame, meaning that first two spatial coordinates of q c,j correspond to pixel coordinates of joint j on image I c , and the third coordinate corresponds to its single-image relative zero-mean depth estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initialization -Figure 4</head><p>The goal of this stage is to acquire an initial guess for the 3D pose and cameras (J init , C init ) using single-view rough camera-frame 3D pose estimates Q made by a model trained without 3D supervision <ref type="bibr" target="#b26">[27,</ref><ref type="bibr">44]</ref>. We assume fixed initial parameters of the first camera</p><formula xml:id="formula_11">c init 0 = (R init 0 , t init 0 , s init 0 ) = (I,0, 1)<label>(11)</label></formula><p>and define initial estimates of rotations, scales and translations of remaining cameras as solutions the following orthogonal rigid alignment problem:   <ref type="figure">Figure 5</ref>. Refinement -We train a neural optimizer f ? to predict iterative refinement that minimizes the reprojection error with the ground truth re-projection, using the current guess and joint heatmaps as an input. During inference, we do not need ground truth 2D projections. that can be solved using SVD of the outer product of mean-centered 3D poses <ref type="bibr">[40]</ref>. The initial guess for the 3D pose J init then is the average of single-view 3D pose predictions Q rigidly aligned back into the first camera frame by corresponding estimated optimal rotations, scales and shifts:</p><formula xml:id="formula_12">arg min Rc,tc,sc j q c,j ? (s c ? R c ? q 0,j + I T [0:1] ? t c ) 2 (12)</formula><formula xml:id="formula_13">J init = 1 C c R T c ? (q c ? I T [0:1] ? t c ))/s c<label>(13)</label></formula><p>3.3. Refinement - <ref type="figure">Figure 5</ref> We train a neural network f ? to predict a series of updates to 3D pose and camera estimates that leads to a refined estimate starting from the initialization from Section 3.2:</p><formula xml:id="formula_14">J (i+1) = J (i) + dJ (i) , J (0) = J init<label>(14)</label></formula><formula xml:id="formula_15">C (i+1) = C (i) + dC (i) , C (0) = C init .<label>(15)</label></formula><p>To ensure that inferred camera parameters C stay valid under any update dC predicted by a network, camera scale (always positive) is represented in log-scale, and camera rotation uses a continuous 6D representation <ref type="bibr" target="#b38">[50]</ref>.</p><formula xml:id="formula_16">At each refinement step dJ (i) , dC (i) = F (i) ? (. . . ) the sub-network F (i)</formula><p>? of the overall network f ? is provided with as much information as possible to perform a meaningful update towards the optimal solution:</p><p>? (J (i) , C (i) ) -the current estimate to be refined; ? G={g j,c } -a collection of Gaussian mixtures compactly representing the heatmaps density distributions;</p><formula xml:id="formula_17">? K (i) ={k (i) j,c = ?(j (i) j , c (i)</formula><p>c )} -the set of projections of each joint j (i) into each camera frame c (i) ; ? L(J (i) , C (i) |G) -the likelihood of the current estimate of joints given the heatmap mixture parameters.</p><p>These learnt updates seek to minimize the L2 distance between predicted and ground truth 2D coordinates of keypoints in each frame, mirroring the maximum likelihood objective (9) we defined earlier:</p><formula xml:id="formula_18">arg min ? L k (?) = (i) j,c k (i+1) j,c ? k gt j,c 2 2<label>(16)</label></formula><p>where, in practice, we train refinement steps F ? (i) progressively, one after the other. Architecture design. The architecture of F ? needs to be very carefully designed to respect the symmetries of the problem at hand. The inferred updates to J (i+1) ought to be invariant to the order of cameras, while updates to C (i+1) ought to be permutation-equivariant w.r.t. the current estimates of C (i) , rows of K (i) , and Gaussian mixtures G. Formally, for any inputs and permutation of cameras ?:</p><formula xml:id="formula_19">dJ, dC = F ? (J (i) , C (i) , G, K (i) , L) (17) dJ , dC = F ? (J (i) , C (i) ? , G ? , K (i) ? , L)<label>(18)</label></formula><p>we need to guarantee that dJ = dJ and dC = dC ? . To archive this, we concatenate view-invariant inputs J (i) and L to each row of view-dependant inputs C (i) , G, K (i) , pass them though a permutation-equivariant MLP <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> with aggregation layers concatenating first and second moments of feature vectors back to these feature vectors, and apply mean aggregation and a non-permutation-equivariant MLP to get the final pose update, as illustrated in <ref type="figure">Figure 6</ref>.</p><p>Limitations. We assume a weak camera model, making our method less accurate on captures shot using wideangle (short-focus) lenses. To achieve best performance, our method requires accurate 2D keypoint ground truth for training, but we also report performance without using GT keypoints during training ( <ref type="table">Table 2</ref>). We implic-  <ref type="figure">Figure 6</ref>. Architecture -In order for predicted updates to respect symmetries of the problem at hand, we copy and concatenate viewinvariant inputs (current pose estimate, average heatmap likelihood -dashed line) to each row of view-specific inputs (current cameras and joint projections, heatmaps), pass them though a Permutation-Equivariant MLP Block shown above. To get permutation-invariant final pose update we additionally apply MLP to averaged output pose embeddings.</p><formula xml:id="formula_20">J (i) C1 (i) G1 K1 (i) ? (i) J (i) C2 (i) G2 K2 (i) ? (i)</formula><p>itly assume that the subject is of comparable size (in pixels) across all views, and expect re-weighting of different components of the reprojection loss (16) might otherwise be necessary. While tracking and pose estimation can be used for malicious purposes, such as mass surveillance, we believe that societal benefits brought by such technological advances <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">39</ref>] outweigh possible abuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Pose prior (i.e. "bone-length" experiment)</head><p>We illustrate the modularity of our solution by effortlessly injecting a subject-specific bone-legth prior into our metaoptimizer. Given two joints j n and j m connected in the human skeleton E by an edge e = (n, m), we define the and our parametric likelihood (7) becomes:</p><formula xml:id="formula_21">p(K|H, B, ?) ? p(K|H, ?) ? N (b N (J ? (H, B))|B, ? 2 b I)</formula><p>and its parameters ? can be estimated equivalently to <ref type="bibr" target="#b8">(9)</ref> via maximum over p(K, H, B| ?) using observations from p(K, H, B) available during training, effectively resulting in an additional loss term penalizing derivations of bone lengths of predicted poses from provided bone lengths:</p><formula xml:id="formula_22">L b (?) = (i) b N (J (i+1) ) ? B 2 2 .<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we specify datasets and metrics we used to validate the performance of the proposed method and a set of baselines and ablation experiments we conducted to evaluate the improvement in error provided by each stage and each supervision signal.</p><p>Data. We evaluated our method on Human3.6M <ref type="bibr" target="#b16">[17]</ref> dataset with four fixed cameras and a more challenging SkiPose-PTZ [37] dataset with six moving pan-tilt-zoom cameras. We used standard train-test evaluated protocol for H36M <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref> with subjects 1, 5, 6, 7, and 8 used for training, and 9 and 11 used for testing. We additionally pruned the H36M dataset by taking each <ref type="bibr" target="#b15">16</ref> Metrics. We report Procrustes aligned Mean Per Joint Position Error (PMPJPE) and Normalized Mean Per Joint Position Error (NMPJPE) that measure the L2-error of 3D joint estimates after applying the optimal rigid alignment (including scale) to the predicted 3D pose and the ground truth 3D pose (for NMPJPE), or only optimal shift and scale (for PMPJPE). We also report the total amount of time (?t) it takes to perform 3D pose inference from multi-view RGB.</p><p>Baselines. On H36M we lower-bound the error with the state-of-the-art fully-supervised baseline of Iskakov et al. <ref type="bibr" target="#b18">[19]</ref> that uses ground truth camera parameters to aggregate multi-view predictions during inference. We also compare the performance of our method to methods that use multiview 2D supervision during training but only perform inference on a single view at a time: self-supervised Epipo-larPose (EP) <ref type="bibr" target="#b26">[27]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input frames</head><p>Human3.6M RGB from four uncalibrated synchronized cameras <ref type="figure">Figure 7</ref>. Qualitative Results -The top row shows input frames we used for pose estimation, overlayed with the GT pose (black). Two bottom rows show predictions made by evaluated methods on H36M with four cameras (left) and SkiPose with two cameras (right). We include predictions for Initialization (Stage 1), MetaPose (Stage 1+2), MetaPose with an Iterative Refinement (S1+IR), and AniPose initialized with GT. We also provide errors in the format: PMPJPE/NMPJPE. A video demonstration of qualitative results across both datasets can be found in the supplementary material or at this link: https://bit.ly/cvpr22 6639.</p><p>been released to date and authors did not respond to a request to share code.</p><p>We also compared our method against the "classical" bundle adjustment initialized with ground truth extrinsic camera parameters of all cameras, and set fixed GT intrinsics, therefore putting it into unrealistically favorable conditions. We used the well-tested implementation of bundle adjustment in AniPose <ref type="bibr" target="#b24">[25]</ref> that uses an adapted version of the 3D registration algorithm of Zhou et al. <ref type="bibr" target="#b36">[48]</ref>. This approach takes point estimates of keypoint locations as an input (i.e. no uncertainty) and iteratively detects outliers and refines camera parameters and joint 3D positions using the second-order Trust Region Reflective algorithm <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. Architecture. For monocular 2D pose estimation, we used the stacked hourglass network <ref type="bibr" target="#b33">[34]</ref> pre-trained on COCO pose dataset <ref type="bibr" target="#b14">[15]</ref>. For monocular 3D estimation in Stage 1, we applied EpipolarPose <ref type="bibr" target="#b26">[27]</ref> on Human3.6M and Canon-Pose [44] on SkiPosePTZ. We note that differences in the joint labeling schemes used by these monocular 3D methods and our evaluation set do not affect the quality of camera initialization we acquire via rigid alignment, as long as monocular 3D estimates for all views follow a consistent labeling scheme. Each neural optimizer step is trained separately, and stop gradient is applied to all inputs. We refer our readers to Section 8.3 in supplementary for a more detailed description of all components we used to train our neural optimizer and their reference performance. <ref type="table" target="#tab_2">Table 1</ref> The proposed method (MetaPose S1+S2) outperforms the classical bundle-adjustment baseline initialized with ground truth cameras (AniPose <ref type="bibr" target="#b24">[25]</ref>  performance, namely: 1 that Stage 1 primes the neural optimizer with a good enough initialization that leads it to a good solution; 2 that our solution is modular enabling swapping existing priming and pose estimation networks, as well as additional losses, and re-training only the neural optimizer; 3 that our method achieves lower latency then both classical and (GPU-accelerated) probabilistic bundle adjustment. We expand upon these and other related finding in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results -</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablations</head><p>Iterative refiner. We measured the speed gain we get from using the neural optimizer f ? by replacing Stage 2 with a test-time GPU-accelerated gradient descent (Adam <ref type="bibr" target="#b25">[26]</ref>) over the probabilistic bundle adjustment objective (6) with GMM-parameterized heatmaps. Section 1 in <ref type="table">Table 2</ref> shows that the proposed method (S1+S2) is up to seven times faster than the iterative refinement (S1+IR), and is at least 10mm more accurate. We also measured the contribution of keypoint supervision towards prediction accuracy of S2 compared to iterative refinement. To do that, we trained Stage 2 to minimize the same GMM-parameterized probabilistic bundle adjustment objective (6) instead of the re-projection loss <ref type="bibr" target="#b15">(16)</ref>. The resulting self-supervised model (S1+S2/SS) outperforms the iterative refinement, suggesting that the proposed architecture regularizes the pose estimation problem. Note that our self-supervised results also outperform prior work that uses weak-and self-supervision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr">44]</ref>. Random initialization. We measured the effect of replacing single-view pose estimates q c,j used to initialize the pose and cameras in Stage 1 with random Gaussian noise. Section 2 in <ref type="table">Table 2</ref> shows that while the neural optimizer (RND+S2) is more resilient to poor initialization then the classical one (RND+IR), a good initialization is necessary to achieve the state-of-art performance (S1+S2). Moreover, marginally better results with GT initialization (GT+IR) show that the proposed initialization already brings the optimizer in the neighbourhood of the correct solution, and that further improvement in the quality of the initial guess will not provide significant gains in accuracy. Non-equivariant network. We measured the effect of letting the model "memorize" the camera order by replacing equivariant blocks with MLPs that receive multi-view information as a single concatenated vector. The resulting model (S2/MLP) achieved marginally better performance on H36M and marginally worse performance on SkiPose (  <ref type="table">Table 3</ref> in supplementary. Further ablations (supplementary). The teacher-student loss proposed by Ma et al. <ref type="bibr" target="#b30">[31]</ref> to draw predicted solutions into the basin of the right solution hurts the performance in all experiments <ref type="table" target="#tab_6">(Table 8</ref>), suggesting that Stage 1 already provides good-enough initialization to start in the correct basin of the objective. We also ran the iterative refiner from ground truth initialization with re-projection losses with different camera models: results suggests that the weak camera model contributed to 10-15mm of error on H36M and no error on SkiPose; see <ref type="table" target="#tab_2">Table 10</ref>. The performance of Meta-Pose on H36M starts to severely deteriorate at around 5% of the training data; see <ref type="table" target="#tab_2">Table 11</ref>. Replacing GMM with a single Gaussian decreased the performance only in two-camera H36M setup by 4mm, and did not significantly influence the performance in other cases; see <ref type="table" target="#tab_2">Table 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we propose a new modular approach to 3D pose estimation that requires only 2D supervision for training and significantly improves upon the state-of-the-art by fusing per-view outputs of singe-view modules with a simple view-equivariant neural network. Our modular approach not only enables practitioners to analyze and improve the performance of each component in isolation, and channel future improvements in respective sub-tasks into improved 3D pose estimation "for free", but also provides a common "bridge" that enables easy inter-operation of different schools of thought in 3D pose estimation -enriching both the "end-to-end neural world" with better model-based priors and improved interpretability, and the "iterative refinement world" with better-conditioned optimization problems, transfer-learning, and faster inference times. We provide a detailed ablation study dissecting different sources of the remaining error, suggesting that future progress in this task might come from the adoption of a full camera model, further improvements in 2D pose localization, better pose priors and incorporating temporal signals from video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>We would like to thank Bastian Wandt, Nori Kanazawa and Diego Ruspini for help with CanonPose [44], stacked hourglass pose estimator, and interfacing with AniPose, respectively.  <ref type="table">Table 3</ref>. Ablation -PMPJPE? of our method on Human3.6M with different number of cameras with different inputs passed to the neural optimizer. Heatmap H contributes most to the final performance, but all inputs are necessary to achieve the state-ofart performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Ablation Tables</head><p>? <ref type="table">Table 3</ref> shows the performance of the neural optimizer trained with different subsets of inputs; ? <ref type="table">Table 4</ref> shows the latency breakdown across model components and models; ? <ref type="table" target="#tab_3">Table 5</ref> shows that the performance of equivariant (S1+S2) and non-equivariant (S1+S2/MLP) models differs by at most 4mm on both datasets; ? <ref type="table">Table 6</ref> shows that MetaPose outperforms prior work with corresponding supervision signals; ? <ref type="table" target="#tab_8">Table 7</ref> shows that the personalized bone length prior improves the performance of both the iterative and neural refiners in the majority of cases; ? <ref type="table" target="#tab_6">Table 8</ref> shows that the student-teacher loss inspired by Ma et al. <ref type="bibr" target="#b30">[31]</ref> to draw the predicted solution into the correct basin of the loss hurts the performance in all cases; ? <ref type="table">Table 9</ref> summarizes reference performance of monocular pose estimation components across different splits of data (train, val, test) for reproducibility, and shows strong overfitting on SkiPose; ? <ref type="table" target="#tab_2">Table 10</ref> shows that at least 20mm of error is due to imperfect heatmaps, up to 10mm is due to the weak camera model, and only up to 3mm is due to imperfect init; ? <ref type="table" target="#tab_2">Table 11</ref> shows that on H36M with just 1/5th of the entire training dataset (i.e. 5k labeled training samples, each sample containing several cameras) we can get a model that achieves PMPJPE within 5-10mm of the performance we achieve on full data. ? <ref type="table" target="#tab_2">Table 13</ref> shows the effect of varying the number of Gaussian mixture components on the performance of different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Weighted EM-algorithm for spherical GMM</head><p>We used grid points x i weighted by corresponding probabilities p i to fit a GMM to a 2D probability heatmap. Following Frisch and Hanebeck <ref type="bibr" target="#b10">[11]</ref> on each step t = 0 . . . T of the EM algorithm we performed usual (non-weighted) E-step to compute the new assignment matrix ? </p><formula xml:id="formula_23">w (t+1) m = i ? (t+1) i,m p i m i ? (t+1) i,m p i ? (t+1) m = i ? (t+1) i,m p i x i i ? (t+1) i,m p i ? (t+1) m = i ? (t+1) i,m p i ||x i ? ? (t+1) m || 2 2 i ? (t+1) i,m p i 8.3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Implementation Details</head><p>Architecture. For monocular 2D pose estimation we used the stacked hourglass network <ref type="bibr" target="#b33">[34]</ref> pre-trained on COCO pose dataset <ref type="bibr" target="#b14">[15]</ref>. We additionally trained a linear regression adapter to convert between COCO and H36M label formats (see supplementary <ref type="figure">Figure 8</ref> for labeling format comparison). The resulting procedure yields good generalization on H36M, as shown in supplementary <ref type="table">Table 9</ref>). The COCO-pretrained network generalized very poorly to SkiPosePTZ dataset because of the visual domain shift, so we fine-tuned the stacked hourglass network using ground truth 2D labels. For monocular 3D estimates used in Stage 1, we applied EpipolarPose <ref type="bibr" target="#b26">[27]</ref> on Human3.6M and CanonPose [44] on SkiPosePTZ. We would like to note that, despite the significant shift in the labeling format between predictions of these monocular 3D methods and the format used in datasets we used for evaluation, this does not affect the quality of camera initialization we acquired via rigid alignment. Similar to prior work <ref type="bibr" target="#b30">[31]</ref>, each "neural optimizer step" is trained separately, and the fresh new neural net is used at each stage, and stop gradient is applied to all inputs. For MLP architecture, we used L fully-connected 512-dimensional layers followed by a fully-connected 128dimensional, all with selu with L=4 for H36M and L=2 for SkiPose. For equivalent network, the optimal network for H36M had following layers:  <ref type="table">Table 4</ref>. Latency breakdown in seconds for estimating the full 3D pose on H36M with four cameras on a GPU (V100, top) and a CPU (bottom) across four components: per-view 2D heatmap estimation (PoseNet), heatmap GMM fitting, per-view monocular 3D and initialization, multi-view bundle adjustment (neural network forward pass in case of MetaPose S1+S2, Adam <ref type="bibr" target="#b25">[26]</ref> in case of S1+IR, and a 2nd-order CPU-only TRR <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> solver in case of AniPose). MetaPose (S1+S2) achieves lowest error with an at least six times (on GPU; two times on CPU) faster inference as the iterative refiner. <ref type="figure">Figure 8</ref>. Both H36M ground truth poses, COCO dataset (used to train the hourglass network), and EpipolarPose predictions (used to generate the 3D initialization) have different label formats from H36M. We trained a small "adapter" to convert COCO-to-H36M, and used EpipolarPose predictions as-is. network on different splits of different datasets. It shows that both the 2D network and MetaPose to certain degree overfit to SkiPose because of its smaller size. Videos with test predictions can be found in the attached video file and following this link: https://bit.ly/cvpr22 6639.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Closed Form Expressions for Stage 1</head><p>Below we describe the solution to the rigid alignment problem <ref type="bibr" target="#b11">(12)</ref> for monocular 3D pose estimates q c and inferred weak camera parameters from them. Assume that we have monocular 3D predictions q c in frame of the camera c.</p><p>The parameters of the first camera are assumed to be known and fixed R</p><formula xml:id="formula_24">(0) init =I, t (0) =0, s (0) =1</formula><p>whereas the rotation of other cameras are inferred using optimal rigid alingment R </p><formula xml:id="formula_25">(c) init = (U (c) ) T V (c) where U (c) , ?, V (c) = SVD(centered(q c ) ? centered(q 0 )) T )</formula><p>where?(a) = ( K k a k )/K is the center of the 3D pose and centered(a) k = (a k ??(a)) and the initial pose estimate is the average of aligned, rotated and predictions from other cameras. The initial guess for the pose is the average of all monocular poses rotated into the first camera frame:</p><formula xml:id="formula_27">J init = 1 C C c=0 (s (c) init ? R (c) init centered(q c )) +?(q 0 ) (22)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">6D rotation re-parameterization</head><p>We used for following parameterization: R(x, y) = stack[n(x), n(x ? y), n(x ? (x ? y))] where n(x) is a normalization operation, and a ? b is a vector product. This is essentially Gram-Schmidt orthogonalization. Rows of the resulting matrix is guaranteed to form an orthonormal basis. This rotation representation was shown to be better suited for optimization <ref type="bibr" target="#b38">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.">Stable Gaussian Mixture Likelihood</head><p>We used the following numerically stable spherical GMM log-likelihood to compute <ref type="formula" target="#formula_7">(6)</ref>:</p><formula xml:id="formula_28">log r w r ? exp(? 1 2 (x ? ? r ) T ? (? 2 r ? I) ?1 (x ? ? r ) (2?) 2 ? 4 r = log r exp log w r 2?? 2 r ? ||x ? ? r || 2 2? 2 r = LSE r log w r 2?? 2 r + ? ? ||x ? ? r || 2 2? 2 r</formula><p>where (?, ? 2 , w) are mean, variance and weight of the corresponding mixture component, LSE(l 0 , . . . , l r ) is a numerically stable "log-sum-exp" often implemented as LSE(l 0 , . . . , l r ) = l * + log( k exp(l k ? l * )), where l * = max(l 0 , . . . , l r ), and ? is a small number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7.">Teacher loss</head><p>In addition to the reprojcetion loss, the student-teacher ablation (S2/TS) used the following additional loss inspired by Ma et al. <ref type="bibr" target="#b30">[31]</ref> to draw the predicted solution J neur into the basin of the correct solution by penalizing its deviation from the solution J ref produced by the iterative refiner (IR). Metapose (S1+S2) 42 45 50 Metapose (S1+S2/MLP) 46 44 54  <ref type="table">Table 6</ref>. MetaPose outperforms SotA on H36M (top) and SkiPose (bottom) -Same notation as in <ref type="table" target="#tab_2">Table 1</ref>. Also includes the self-supervised (S2/SS) and iterative solver (SS/IR) flavours of MetaPose. Supervision signal used during training: 2Dground truth 2D keypoints, 3D -ground truth 3D poses, S -selfsupervision (i.e. using a pose estimation network pre-trained on a different dataset), 2/3D -2D keypoint data with 3D poses on few subjects.</p><formula xml:id="formula_29">L y J neur ,R (c) neur , t (c) neur , s (c) neur ; J ref , R (c) ref , t (c) ref , s (c) ref =? p ? ||J neur ? J ref || 2 2 + ? t ? ||t (c) neur ? t (c) ref || 2 2 +? R ? ||(R (c) neur ) T R (c) ref ? I|| 2 2 +? s ? || log(s (c) neur ) ? log(s (c) ref )|| 2 2 .<label>(23)</label></formula><p>(a) H36M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method 4 3 2</head><p>Metapose S1+S2 32 36 44 Metapose S1+S2 + bone <ref type="bibr">31 34 37</ref> Metapose S1+IR 43 52 53 Metapose S1+IR + bone 38 44 47</p><p>Metapose S1+S2/SS 39 47 50 Metapose S1+S2/SS + bone 38 45 50</p><formula xml:id="formula_30">(b) SkiPose Method 6 4 2</formula><p>Metapose S1+S2 41 43 47 Metapose S1+S2 + bone 45 46 49</p><p>Metapose S1+IR 30 32 77 Metapose S1+IR + bone <ref type="bibr">26 28 46</ref> Metapose S1+S2/SS 41 46 95 Metapose S1+S2/SS + bone 44 45 53 MetaPose S1+S2 42 45 50 MetaPose S1+S2/TS 42 43 72 <ref type="table" target="#tab_6">Table 8</ref>. Teacher-student loss analogous to the one proposed by Ma et al. <ref type="bibr" target="#b30">[31]</ref> to bring the neural optimizer into the basin of the correct solution either hurts or does significantly affect the performance in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train</head><p>Validation Test GT log-prob.</p><p>-5.17 -5.58 -5.06</p><p>Stage: S1 S1+IR S1+S2 S1 S1+IR S1+S2 S1 S1+IR S1+S2 Stage: S1 S1+IR S1+S2 S1 S1+IR S1+S2 S1 S1+IR S1+S2 (b) SkiPose <ref type="table">Table 9</ref>. Details about predictions across different stages: initialization using monocular 3d (S1), iterative refinement (S1+IR), and neural refinement (S1+S2) on H36M with four cams (top) and SkiPose with six cams (bottom). 2D error is scaled so that the entire pose lies in [0, 1] 2 . The GT log probability is the log probability of ground truth points given predicted heatmaps and measures how well heatmaps generated by our 2D prediction network match the ground truth. Significantly larger discrepancy between GT log probabilities on train and test on SkiPose shows that 2D pose network overfits much more on SkiPose than on H36M due to its limited size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) H36M</head><p>Method 2D S1 4 3 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1+IR</head><p>HT EP 43 52 53 S1+IR</p><p>HT GT 40 49 48 S1+IR</p><p>full-GT EP 17 20 24 S1+IR</p><p>full-GT GT 14 <ref type="bibr">16 20 S1+IR</ref> weak-GT EP 4 6 18 S1+IR</p><p>weak-GT GT 1.4 1.7 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) SkiPose</head><p>Method 2D S1 6 4 2 S1+IR HT CP 30 33 77 S1+IR HT GT 28 30 41 S1+IR full-GT CP 8 8 29 S1+IR</p><p>weak-GT CP 8 7 28 <ref type="table" target="#tab_2">Table 10</ref>. MetaPose S1+IR trained with either ground truth pseudo-heatmaps centered around full and weak-projected 3D joints and with different S1 initialization (either predicted via EpipolarPose or "perfect"). This experiment shows that imperfect heatmaps contribute to at least 20mm of error in both cases, weak camera model contribute to 10mm of error on H36M and no error on SkiPose, and imperfect initialization contributes to at most 3mm of error.  <ref type="table" target="#tab_2">Table 13</ref>. The number of Gaussian Mixture components does not significantly affect the performance of the network in all cases on both SkiPose (top) and H36M (bottom), except for MetaPose S1+IR on SkiPose with a single Gaussian. <ref type="figure">Figure 9</ref>. Full MetaPose (S1+S2) outperforms initialization (S1), Iterative Solver (S1+IR), and AniPose w/ GT camera init. <ref type="figure">Figure 12</ref>. AniPose w/ GT camera initialization can yields low re-projection error but high 3D estimation error. <ref type="figure">Figure 13</ref>. AniPose with GT init fails due to poor choice of 2D predictions to ignore during refinement.</p><p>23 <ref type="figure" target="#fig_3">Figure 14</ref>. MetaPose fails on few extreme poses that have much poorer than average initialization quality. <ref type="figure">Figure 16</ref>. MetaPose fails on poses that have much poorer (than average) initialization quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Initialization -We form an initial guess for the 3D pose and the cameras by taking the mean of rigid aligned 3D poses estimated from each RGB image using an external single-view weakly-supervised 3D pose estimation network<ref type="bibr" target="#b26">[27,</ref> 44].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>C0 (i) G0 K0 (i) ? (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>bone length b e (J) = j n ? j m 2 . However, as our bundle adjustment is performed up to scale we ought to define scale-invariant bone lengths b N (J) = b(J)/?(b(J)) by expressing length of each bone relative to the average length of other bones?(b) = ( e b e )/|E|. If we assume that during training and inference we observe noisy normalized bonelenghs vectors B = b N (J) + ?, where ? ? N (0, ? 2 b I). Then, the joint probability (1) becomes: p(J, C, K, H, B) = p(B|J) p(H|K) p(K|J, C) p(J) p(C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>points x i and spherical clusters m = 0 . . . M with means ? (t) m , and standard variations ? (t) m , and weights w (t) m , followed by a weighted M-step:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The scale s and shift t can be acquired by comparing the original monocular q c,[:,0:2] in pixels to [R (c) init centered(q 0 )] [:,0:2] rotated back into each camera frame, for example: init ) T centered(q 0 )] [:,0:2] || || centered(q c ) [:,0:2] || (20) t (c) init =?([(R (c) init ) T centered(q 0 )] [:,0:2] ) ??([q c ] [:,0:2] ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>-th frame from it, resulting in 24443 train and 8516 test examples, each example containing information from four cameras. We evaluated our method on the subset (1035 train / 230 test) of SkiPose [37] that was used in CanonPose [44] that excludes 280 examples with visibility obstructed by snow. In each dataset, we used the first 64 examples from the train split as a validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and CanonPose (CP) [44], as well as the weakly supervised baselines of Iqbal et al. [18] and Rhodin et al. [37]. On SkiPose we compared our model with the only two baselines available in the literature: Canon-Pose [44] and Rhodin et al. [37]. We did not evaluate Epipo-larPose on SkiPose because it requires fixed cameras to perform the initial self-supervised pseudo-labeling. We did not evaluate Iqbal et al.<ref type="bibr" target="#b17">[18]</ref> on SkiPose because no code has</figDesc><table><row><cell>Input frames</cell><cell>Ski-Pose PTZ RGB from two uncalibrated synchronized cameras</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison to prior work -Performance of different methods with four and two cameras on Human3.6M (top) and six and two cameras SkiPose-PTZ (bottom), Procrustes and Normalized MPJPE in millimeters, inference time in seconds. See Tables 4 and 6 in the supplementary material for an extended comparison and breakdown of runtime performance.</figDesc><table><row><cell>w/ GT) by +40mm</cell></row><row><cell>on H36M with four cameras, and +8mm on SkiPose with</cell></row></table><note>six cameras. With fewer cameras the performance gap in- creases further. MetaPose also outperforms semi-, weakly-, and self-supervised baselines reported in prior work [18, 27, 37, 44] by more then 10mm. We would like to re-iterate core advantages of the proposed method beyond its highTable 2. Ablations on H36M. Notation consistent with Table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>), likely due to fixed cameras positions in H36M</cell></row><row><cell>and moving cameras in SkiPose.</cell></row><row><cell>Bone lengths. We trained a model with an additional bone</cell></row><row><cell>length prior (Sec. 3.3.1) that improved PMPJPE with two</cell></row><row><cell>cameras by 7mm. The two camera setup is ill-conditioned,</cell></row><row><cell>hence can better exploit priors like bone lengths.</cell></row></table><note>Inputs of neural optimizer. Unsurprisingly, among all in- puts passed to the neural optimizer, heatmap H contributed most to the final performance, but all components were nec- essary to achieve the state-of-art performance; see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>[ 36 ]</head><label>36</label><figDesc>Helge Rhodin, Mathieu Salzmann, and Pascal Fua. Unsupervised geometry-aware representation for 3d human pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. 2</figDesc><table><row><cell>J, C</cell><cell></cell></row><row><cell>H</cell><cell></cell></row><row><cell>K</cell><cell></cell></row><row><cell>L</cell><cell></cell></row><row><cell>4 32 33 32 45 32 48 33 32 45 45 32 46 45 34 nan</cell><cell>[37] Helge Rhodin, Jorg Sporri, Isinsu Katircioglu, Victor Con-</cell></row><row><cell>3 36 35 36 49 37 50 38 39 49 50 37 50 49 44 nan</cell><cell>stantin, Frederic Meyer, Erich Mueller, Mathieu Salzmann,</cell></row><row><cell>2 44 44 46 53 43 59 40 56 53 58 45 55 51 45 nan</cell><cell>and Pascal Fua. Learning monocular 3d human pose esti-</cell></row><row><cell></cell><cell>mation from multi-view images. Conference On Computer</cell></row><row><cell></cell><cell>Vision And Pattern Recognition (CVPR), 2018. 1, 2, 6, 7, 15</cell></row><row><cell></cell><cell>[38] Romer Rosales, Matheen Siddiqui, Jonathan Alon, and Stan</cell></row><row><cell></cell><cell>Sclaroff. Estimating 3d body pose using uncalibrated cam-</cell></row><row><cell></cell><cell>eras. In Proceedings of the 2001 IEEE Computer Society</cell></row><row><cell></cell><cell>Conference on Computer Vision and Pattern Recognition.</cell></row><row><cell></cell><cell>CVPR 2001, volume 1, pages I-I. IEEE, 2001. 2</cell></row><row><cell></cell><cell>[39] Elham Saraee, Yiwen Gu, Shreya Pandit, San Tran, Eugenia</cell></row><row><cell></cell><cell>Shandelman, Saurabh Singh, Timothy J Nordahl, Terry El-</cell></row><row><cell></cell><cell>lis, and Margrit Betke. Exercisecheck: Data analytics for a</cell></row><row><cell></cell><cell>remote monitoring and evaluation platform for home-based</cell></row><row><cell></cell><cell>physical therapy. In Proceedings of the 12th ACM Interna-</cell></row><row><cell></cell><cell>tional Conference on PErvasive Technologies Related to As-</cell></row><row><cell></cell><cell>sistive Environments, pages 110-118, 2019. 6</cell></row><row><cell></cell><cell>[40] Peter H Sch?nemann. A generalized solution of the orthog-</cell></row><row><cell></cell><cell>onal procrustes problem. Psychometrika, 31(1):1-10, 1966.</cell></row><row><cell></cell><cell>5</cell></row><row><cell></cell><cell>[41] Kosuke Takahashi, Dan Mikami, Mariko Isogawa, and</cell></row><row><cell></cell><cell>Hideaki Kimata. Human pose as calibration pattern; 3d hu-</cell></row><row><cell></cell><cell>man pose estimation with multiple unsynchronized and un-</cell></row><row><cell></cell><cell>calibrated cameras. In Proceedings of the IEEE Conference</cell></row><row><cell></cell><cell>on Computer Vision and Pattern Recognition Workshops,</cell></row><row><cell></cell><cell>pages 1775-1782, 2018. 2</cell></row><row><cell></cell><cell>[42] Hanyue Tu, Chunyu Wang, and Wenjun Zeng. Voxelpose:</cell></row><row><cell></cell><cell>Towards multi-camera 3d human pose estimation in wild en-</cell></row><row><cell></cell><cell>vironment, 2020. 2</cell></row><row><cell></cell><cell>[43] Bastian Wandt and Bodo Rosenhahn. Repnet: Weakly su-</cell></row><row><cell></cell><cell>pervised training of an adversarial reprojection network for</cell></row><row><cell></cell><cell>3d human pose estimation. In Computer Vision and Pattern</cell></row><row><cell></cell><cell>Recognition (CVPR), June 2019. 1, 2</cell></row><row><cell></cell><cell>[44] Bastian Wandt, Marco Rudolph, Petrissa Zell, Helge Rhodin,</cell></row><row><cell></cell><cell>and Bodo Rosenhahn. CanonPose: Self-supervised monoc-</cell></row><row><cell></cell><cell>ular 3D human pose estimation in the wild. In Computer</cell></row><row><cell></cell><cell>Vision and Pattern Recognition (CVPR), June 2021. 1, 2, 4,</cell></row><row><cell></cell><cell>5, 6, 7, 8, 9, 13, 15</cell></row><row><cell></cell><cell>[45] Rongchang Xie, Chunyu Wang, and Yizhou Wang. Meta-</cell></row><row><cell></cell><cell>fuse: A pre-trained fusion model for human pose estimation.</cell></row><row><cell></cell><cell>In Proceedings of the IEEE/CVF Conference on Computer</cell></row><row><cell></cell><cell>Vision and Pattern Recognition (CVPR), June 2020. 1, 2</cell></row><row><cell></cell><cell>[46] Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu,</cell></row><row><cell></cell><cell>William T. Freeman, Rahul Sukthankar, and Cristian Smin-</cell></row><row><cell></cell><cell>chisescu. Weakly supervised 3d human pose and shape re-</cell></row><row><cell></cell><cell>construction with normalizing flows. In Computer Vision -</cell></row><row><cell></cell><cell>ECCV 2020, pages 465-481, 2020. 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>shows that it hurts the performance of the model.8.8. Qualitative ResultsWe provide qualitative examples (failure cases, success cases) on the test set of H36M and SkiPose inFigures 9-19. Videos with more test prediction visualizations are available at: https://bit.ly/cvpr22 6639. Circles around joints on 2D views represent the absolute reprojection error for that joint for that view. Our qualitative findings:</figDesc><table><row><cell>1. MetaPose considerably improves over the initial guess</cell></row><row><cell>when a lot of self-occlusion is present</cell></row><row><cell>2. MetaPose fails on extreme poses for which monocular</cell></row><row><cell>estimation fails (e.g. somersaults)</cell></row><row><cell>3. In two-camera SkiPose setup, AniPose often yields</cell></row><row><cell>smaller reprojection error while producing very bad</cell></row><row><cell>3D pose results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="7">Equivariant (S1+S2) and non-equivariant (S1+S2/MLP)</cell></row><row><cell cols="7">performance networks have comparable performance across dif-</cell></row><row><cell cols="7">ferent numbers of cameras on H36M (top) and SkiPose (bottom).</cell></row><row><cell></cell><cell cols="2">(a) H36M</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Method and supervision type</cell><cell cols="5">PMPJPE? NMPJPE? ?t [s] 4 2 4 2</cell></row><row><cell>Isakov et al. [19]</cell><cell cols="2">3D 20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AniPose [25] w/ GT</cell><cell cols="6">S 75 167 103 230 7.1</cell></row><row><cell>Rhodin et al. [37]</cell><cell cols="2">2/3D 65</cell><cell>-</cell><cell>80</cell><cell>-</cell><cell>-</cell></row><row><cell>CanonPose [44]</cell><cell cols="2">S 53</cell><cell>-</cell><cell>82</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">EpipolarPose (EP) [27] S 71</cell><cell>-</cell><cell>78</cell><cell>-</cell><cell>-</cell></row><row><cell>Iqbal et al. [18]</cell><cell cols="2">2D 55</cell><cell>-</cell><cell>66</cell><cell>-</cell><cell>-</cell></row><row><cell>MetaPose (S1)</cell><cell cols="3">S 74 87</cell><cell cols="3">83 95 0.2</cell></row><row><cell>MetaPose (S1+S2)</cell><cell cols="3">2D 32 44</cell><cell cols="3">49 55 0.2</cell></row><row><cell>MetaPose (S1+IR)</cell><cell cols="3">S 43 66</cell><cell cols="3">53 75 1.7</cell></row><row><cell>MetaPose (S1+S2/SS)</cell><cell cols="3">S 39 50</cell><cell cols="3">56 63 0.2</cell></row><row><cell></cell><cell cols="2">(b) SkiPose</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Method and supervision type</cell><cell cols="5">PMPJPE? NMPJPE? ?t [s] 6 2 6 2</cell></row><row><cell>AniPose [25] w/ GT</cell><cell cols="6">S 50 62 221 273 7.1</cell></row><row><cell>Rhodin et al. [37]</cell><cell cols="2">2/3D -</cell><cell>-</cell><cell>85</cell><cell>-</cell><cell>-</cell></row><row><cell>CanonPose (CP) [44]</cell><cell cols="2">S 90</cell><cell>-</cell><cell cols="2">128 -</cell><cell>-</cell></row><row><cell>MetaPose (S1)</cell><cell cols="6">S 81 86 140 144 0.3</cell></row><row><cell>MetaPose (S1+S2)</cell><cell cols="3">2D 42 50</cell><cell cols="3">53 59 0.4</cell></row><row><cell>MetaPose (S1+IR)</cell><cell cols="3">S 30 77</cell><cell cols="3">54 94 2.5</cell></row><row><cell cols="4">MetaPose (S1+S2/SS) S 42 95</cell><cell cols="3">59 102 0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Personalized bone lengths prior helps in all cases for H36M (top), especially in the few-camera setup; and in the majority of cases on SkiPose (bottom).</figDesc><table><row><cell>(a) H36M</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>4</cell><cell>3</cell><cell>2</cell></row><row><cell>MetaPose S1+S2</cell><cell cols="3">32 36 44</cell></row><row><cell cols="4">MetaPose S1+S2/TS 38 45 45</cell></row><row><cell cols="2">(b) SkiPose</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>6</cell><cell>4</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc># cam % 100 89 84 79 73 68 63 58 52 47 42 37 31 29 26 24 21 18 16 13 10 8 5 3 4 32 32 32 32 33 36 33 35 33 35 34 42 37 39 36 38 36 41 48 41 41 44 4870 3 36 35 36 37 37 36 37 39 37 37 38 39 40 39 41 45 42 43 44 45 46 51 53 70 2 44 48 48 47 46 48 48 40 54 60 43 43 51 57 53 58 54 50 52 48 48 51 68 87 Table 11. Test PMPJE of MetaPose on H36M as a function of the fraction of training examples with 2D ground truth used (i.e. first X%). Reminder: we never use any ground truth 3D annotations for either cameras or poses, these are percentages of 2D labels used for training. We can see that MetaPose produces high-accuracy predictions (within 10mm of the original performance) with up to 1/5-th (?18%) of the H36M training 2D pose annotations (?5k training examples each containing multiple cameras). The few-camera setup exhibits more variations in test error due to random network initialization.</figDesc><table><row><cell cols="2">(a) H36M</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">GMM 4</cell><cell>3</cell><cell>2</cell></row><row><cell>MetaPose S1+IR</cell><cell>4</cell><cell cols="3">43 52 53</cell></row><row><cell>MetaPose S1+IR</cell><cell>3</cell><cell cols="3">42 51 52</cell></row><row><cell>MetaPose S1+IR</cell><cell>2</cell><cell cols="3">42 51 52</cell></row><row><cell>MetaPose S1+IR</cell><cell>1</cell><cell cols="3">42 52 53</cell></row><row><cell>MetaPose S1+S2</cell><cell>4</cell><cell cols="3">32 39 44</cell></row><row><cell>MetaPose S1+S2</cell><cell>3</cell><cell cols="3">31 36 47</cell></row><row><cell>MetaPose S1+S2</cell><cell>2</cell><cell cols="3">32 36 50</cell></row><row><cell>MetaPose S1+S2</cell><cell>1</cell><cell cols="3">32 36 48</cell></row><row><cell cols="2">(b) SkiPose</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">GMM 6</cell><cell>4</cell><cell>2</cell></row><row><cell>MetaPose S1+IR</cell><cell>4</cell><cell cols="3">30 33 77</cell></row><row><cell>MetaPose S1+IR</cell><cell>3</cell><cell cols="3">30 32 77</cell></row><row><cell>MetaPose S1+IR</cell><cell>2</cell><cell cols="3">31 34 75</cell></row><row><cell>MetaPose S1+IR</cell><cell>1</cell><cell cols="3">43 43 58</cell></row><row><cell>MetaPose S1+S2</cell><cell>4</cell><cell cols="3">42 45 50</cell></row><row><cell>MetaPose S1+S2</cell><cell>3</cell><cell cols="3">44 41 50</cell></row><row><cell>MetaPose S1+S2</cell><cell>2</cell><cell cols="3">42 49 51</cell></row><row><cell>MetaPose S1+S2</cell><cell>1</cell><cell cols="3">41 43 47</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 10. MetaPose improves over the initial guess under high self-occlusion.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 11. MetaPose improves over the initial guess under high self-occlusion.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 15. MetaPose improves over the initial guess under high self-occlusion.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 17. AniPose with GT init fails due to poor choice of 2D predictions to ignore during refinement.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 18. With two cameras AniPose with GT camera init often yields low reprojection error but bad 3D estimation error</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 19. With two cameras AniPose with GT camera init often yields low reprojection error but bad 3D estimation error</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A subspace, interior, and conjugate gradient method for large-scale bound-constrained minimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Branch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Li</surname></persName>
		</author>
		<idno>doi: 10 . 1137 / S1064827595289108</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximate solution of the trust region problem by minimization over two-dimensional subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">B</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald A</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5714" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-view tracking for multi-human 3d pose estimation at over 100 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Vector neurons: A general framework for so (3)-equivariant networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congyue</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Poulenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12229</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Phuoc</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hgaze typing: Head-gesture assisted gaze typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kurauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Eye Tracking Research and Applications</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gaussian mixture estimation from weighted samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Frisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><forename type="middle">D</forename><surname>Hanebeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Animation from observation: Motion capture and motion editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gleicher</surname></persName>
		</author>
		<idno>Novem- ber 1999. 1</idno>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="51" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Home-based physical therapy with an interactive computer vision system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreya</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elham</forename><surname>Saraee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Nordahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>R?za Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7779" to="7788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weaklysupervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised learning of interpretable keypoints from unlabelled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="190" to="204" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human model fitting towards in-thewild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momen (e) t: Flavor the moments in learning to classify shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Joseph-Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Zvirin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anipose: a toolkit for robust markerless 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Karashchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><forename type="middle">L</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Evyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elischa</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiman</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bingni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuthill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-supervised 3d human pose estimation via part guided novel image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mugalodi</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep feedback inverse problem solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivabalan</forename><surname>Manivasagam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiview-consistent semi-supervised learning for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural descent for visual 3d human pose and shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14484" to="14493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
