<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic segmentation</term>
					<term>real-time</term>
					<term>deep convo- lutional neural networks</term>
					<term>autonomous driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation is a key technology for autonomous vehicles to understand the surrounding scenes. The appealing performances of contemporary models usually come at the expense of heavy computations and lengthy inference time, which is intolerable for self-driving. Using light-weight architectures (encoder-decoder or two-pathway) or reasoning on low-resolution images, recent methods realize very fast scene parsing, even running at more than 100 FPS on a single 1080Ti GPU. However, there is still a significant gap in performance between these real-time methods and the models based on dilation backbones. To tackle this problem, we proposed a family of efficient backbones specially designed for real-time semantic segmentation. The proposed deep dual-resolution networks (DDRNets) are composed of two deep branches between which multiple bilateral fusions are performed. Additionally, we design a new contextual information extractor named Deep Aggregation Pyramid Pooling Module (DAPPM) to enlarge effective receptive fields and fuse multi-scale context based on low-resolution feature maps. Our method achieves a new state-of-the-art trade-off between accuracy and speed on both Cityscapes and CamVid dataset. In particular, on a single 2080Ti GPU, DDRNet-23-slim yields 77.4% mIoU at 102 FPS on Cityscapes test set and 74.7% mIoU at 230 FPS on CamVid test set. With widely used test augmentation, our method is superior to most state-of-the-art models and requires much less computation. Codes and trained models are available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Semantic segmentation is a key technology for autonomous vehicles to understand the surrounding scenes. The appealing performances of contemporary models usually come at the expense of heavy computations and lengthy inference time, which is intolerable for self-driving. Using light-weight architectures (encoder-decoder or two-pathway) or reasoning on low-resolution images, recent methods realize very fast scene parsing, even running at more than 100 FPS on a single 1080Ti GPU. However, there is still a significant gap in performance between these real-time methods and the models based on dilation backbones. To tackle this problem, we proposed a family of efficient backbones specially designed for real-time semantic segmentation. The proposed deep dual-resolution networks (DDRNets) are composed of two deep branches between which multiple bilateral fusions are performed. Additionally, we design a new contextual information extractor named Deep Aggregation Pyramid Pooling Module (DAPPM) to enlarge effective receptive fields and fuse multi-scale context based on low-resolution feature maps. Our method achieves a new state-of-the-art trade-off between accuracy and speed on both Cityscapes and CamVid dataset. In particular, on a single 2080Ti GPU, DDRNet-23-slim yields 77.4% mIoU at 102 FPS on Cityscapes test set and 74.7% mIoU at 230 FPS on CamVid test set. With widely used test augmentation, our method is superior to most state-of-the-art models and requires much less computation. Codes and trained models are available online.</p><p>Index Terms-Semantic segmentation, real-time, deep convolutional neural networks, autonomous driving</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S EMANTIC segmentation is a fundamental task in which each pixel of the input image should be assigned to the corresponding label <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. It plays a vital role in many practical applications, such as medical image segmentation, navigation of autonomous vehicles, and robots <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. With the rise of deep learning technologies, convolutional neural networks are applied to image segmentation and greatly outperform traditional methods based on handcrafted features. Since the fully convolutional network (FCN) <ref type="bibr" target="#b5">[6]</ref> was proposed to handle semantic segmentation problems, a series of novel networks have been proposed. DeepLab <ref type="bibr" target="#b6">[7]</ref> eliminates some of downsampling in ResNet to maintain high resolution and utilizes convolutions with large dilations <ref type="bibr" target="#b7">[8]</ref> to enlarge receptive fields. From then on, dilated convolutions based backbones with context extraction modules have become the standard layout widely used in a variety of methods, including DeepLabV2 <ref type="bibr" target="#b8">[9]</ref>, DeepLabV3 <ref type="bibr" target="#b9">[10]</ref>, PSPNet <ref type="bibr" target="#b10">[11]</ref>, and DenseASPP <ref type="bibr" target="#b11">[12]</ref>.</p><p>Since semantic segmentation is a kind of dense prediction task, neural networks need to output high-resolution feature maps of large receptive fields to produce satisfactory results, which is computationally expensive. This problem is especially critical for scene parsing of autonomous driving which requires enforcement on very large images to cover a wide field of view. Therefore, the above methods are very time-consuming in the inference stage and can not be directly deployed on the actual autonomous vehicles. They can not even process an image in one second because of utilizing multi-scale test to improve accuracy.</p><p>With the ever-increasing demand for mobile device deployment, real-time segmentation algorithms <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b16">[17]</ref> are getting more and more attention. DFANet <ref type="bibr" target="#b17">[18]</ref> employs deeply multiscale feature aggregation and lightweight depthwise separable convolutions, achieving 71.3% test mIoU with 100 FPS. Different from the encoder-decoder paradigm, authors in <ref type="bibr" target="#b18">[19]</ref> propose a novel bilateral network composed of a spatial path and a context path. Specially, the spatial path utilizes three relatively wide 3?3 convolutional layers to capture spatial details, and the context path is a compact pre-trained backbone for extracting contextual information. Such bilateral methods including <ref type="bibr" target="#b19">[20]</ref> achieved higher inference speed than encoderdecoder structures at that time.</p><p>Recently, some competitive real-time methods aiming at semantic segmentation of road scenes are proposed. These methods can be divided into two categories. One utilizes the GPU-efficient backbones, especially ResNet-18 <ref type="bibr">[</ref>  <ref type="bibr" target="#b22">[23]</ref> using extra training data, these recent works do not show the potential for higher quality results. Some of them suffer from a lack of scalability due to deliberately designed architectures and tuned hyper-parameters. Additionally, ResNet-18 is of little advantage given the prosperity of more powerful backbones.</p><p>In this paper, we propose the dual-resolution networks with deep high-resolution representation for real-time semantic segmentation of high-resolution images, especially road-driving images. Our DDRNets start from one trunk and then divide into two parallel deep branches with different resolutions. One deep branch generates relatively high-resolution feature maps and the other extracts rich semantic information through multiple downsampling operations. Multiple bilateral connections are bridged between two branches to achieve efficient information fusion. Besides, we propose a novel module named DAPPM which inputs low-resolution feature maps, extracts multi-scale context information, and merges them in a cascaded way. Before training on semantic segmentation dataset, the dual-resolution networks are trained on ImageNet following common paradigms.</p><p>According to extensive experimental results on three popular benchmarks (i.e., Cityscapes, CamVid, and COCOStuff), DDRNets attain an excellent balance between segmentation accuracy and inference speed. Our method achieves new stateof-the-art accuracy on both Cityscapes and CamVid compared with other real-time algorithms, without attention mechanism and extra bells or whistles. With standard test augmentation, DDRNet is comparable to state-of-the-art models and requires much fewer computing resources. We also report statistically relevant performances and conduct ablation experiments to analyze the effect of architecture improvements and standard training tricks.</p><p>The main contributions are summarized as follows:</p><p>? A family of novel bilateral networks with deep dualresolution branches and multiple bilateral fusions is proposed for real-time semantic segmentation as efficient backbones. ? A novel module is designed to harvest rich context information by combining feature aggregation with pyramid pooling. When executed on low-resolution feature maps, it leads to little increase in inference time. ? Our method achieves a new state-of-the-art trade-off between accuracy and speed with the 2080Ti, 77.4% mIoU at 102 FPS on Cityscapes test set and 74.7% mIoU at 230 FPS on CamVid test set. To our best knowledge, we are the first to achieve 80.4% mIoU in nearly real time (22 FPS) on Cityscapes only using fine annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent years, dilation convolutions based methods have boosted the performance of semantic segmentation under many challenging scenes. And pioneering works explore more possibilities for lightweight architectures such as the encoderdecoder and the two-pathway. In addition, contextual information is proved to be very crucial for scene parsing tasks. In this section, we group the related works into three categories, i.e., high-performance semantic segmentation, real-time semantic segmentation, and context extraction modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. High-performance Semantic Segmentation</head><p>The output of the last layer of a common encoder can not be used directly to predict the segmentation masks due to the lack of spatial details. And effective receptive fields will be too small to learn high-level semantic information if only getting rid of downsampling of the classification backbones. An acceptable strategy is to utilize dilated convolutions to set up the long-range connection between pixels while removing the last two downsampling layers <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, as shown in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>. However, it also poses new challenges to realtime inference due to the exponential growth of high-resolution feature-map dimensions and inadequate optimization of dilated convolution implementation. There is a fact that most state-ofthe-art models are built on dilation backbones and are therefore largely unqualified for scene parsing of self-driving.</p><p>Some works attempt to explore the substitute of the standard dilation backbones. Authors of DeepLabv3plus <ref type="bibr" target="#b24">[25]</ref> propose a simple decoder that fuses upsampled feature maps with low-level feature maps. It alleviates the requirement for high-resolution feature maps generated directly from dilated convolutions. DeepLabv3plus can achieve competitive results though the output stride of the encoder is set to 16. HRNet <ref type="bibr" target="#b25">[26]</ref> highlights the deep high-resolution representations and reflects higher efficiency than dilation backbones. We find that the higher computational efficiency and inference speed of HRNet owe to its much thinner high-resolution information flows. Taking HRNetV2-W48 as an example, the dimensions of 1/4 resolution and 1/8 resolution features are 48 and 96, respectively, which are much smaller than those of pretrained ResNets <ref type="bibr" target="#b26">[27]</ref> with dilation convolutions. Although high-resolution branches of HRNet are much thinner, they can be greatly enhanced by parallel low-resolution branches and repeated multi-scale fusions.</p><p>Our work begins with the deep, thin, high-resolution representations and puts forward more compact architectures. They maintain high-resolution representations and extract high-level contextual information simultaneously through two concise trunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real-time Semantic Segmentation</head><p>Almost all real-time semantic segmentation models employ two basic methods: encoder-decoder methods and twopathway methods. Lightweight encoders which play an significant role in both methods are also discussed.</p><p>1) Encoder-decoder Architecture: Compared to dilated convolution based models, encoder-decoder architectures intuitively cost less computation and inference time. The encoder is usually a deep network with repeated spatial reductions to extract contextual information and the decoder restores the resolution by interpolation or transposed convolution [28] to complete dense predictions, as shown in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>. Specially, the encoder can be a lightweight backbone pre-trained on ImageNet or an efficient variant trained from scratch like ERFNet <ref type="bibr" target="#b4">[5]</ref> and ESPNet <ref type="bibr" target="#b15">[16]</ref>. SwiftNet <ref type="bibr" target="#b20">[21]</ref> defends the advantage of pre-training encoders on ImageNet and leverages lightweight lateral connections to assist with upsampling. Authors in <ref type="bibr" target="#b28">[29]</ref> propose a strategy of multiply spatial fusion and class boundary supervision. FANet <ref type="bibr" target="#b21">[22]</ref> achieves a good tradeoff between speed and accuracy with fast attention modules and extra downsampling throughout the network. SFNet <ref type="bibr" target="#b22">[23]</ref> delivers a Flow Alignment Module (FAM) to align feature maps of adjacent levels for better fusion.</p><p>2) Two-pathway Architecture: The encoder-decoder architecture reduces computational effort, but due to the loss of some information during repeated downsampling, it can not be completely recovered by unsampling, which impairs the accuracy of semantic segmentation. The two-pathway architecture is proposed in order to alleviate this problem <ref type="bibr" target="#b18">[19]</ref>, as shown in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>. In addition to one pathway for extracting semantic information, the other shallow pathway of high resolution provides rich spatial details as a supplement. To further improve the accuracy, BiSeNetV2 <ref type="bibr" target="#b23">[24]</ref> uses global average pooling for context embedding and proposes attention based feature fusion. The two pathways in BiSeNetV1&amp;V2 are initially separate while the two branches in Fast-SCNN <ref type="bibr" target="#b19">[20]</ref> share the learning to downsample module. CABiNet <ref type="bibr" target="#b29">[30]</ref> adopts the overall architecture of Fast-SCNN but uses the MobileNetV3 <ref type="bibr" target="#b30">[31]</ref> as the context branch.</p><p>Other than existing two-pathway methods, the deep and thin high-resolution branch of DDRNets enables multiple feature fusions and sufficient ImageNet pre-training while guaranteeing the inference efficiency. Our method can be easily scaled to achieve higher accuracy (above 80% mIoU on Cityscapes).</p><p>3) Lightweight Encoders: There are many computationally efficient backbones can be used as the encoder, such as MobileNet <ref type="bibr" target="#b31">[32]</ref>, ShuffleNet <ref type="bibr" target="#b32">[33]</ref> and small version of Xception <ref type="bibr" target="#b33">[34]</ref>. MobileNet replaces standard convolutions with depthwise separable convolutions to reduce parameters and computation. The strong regularization effect of depthwise separable convolutions is alleviated by inverted residual blocks in MobileNetV2 <ref type="bibr" target="#b34">[35]</ref>. ShuffleNet utilizes the compactness of grouped convolutions and proposes a channel shuffle operation to facilitate information fusion between different groups. However, these networks contain numerous depthwise separable convolutions which can not be efficiently implemented with the existing GPU architecture. Therefore, although the FLOPs of ResNet-18 <ref type="bibr" target="#b26">[27]</ref> is about six times of MobileNetV2 1.0?, inference speed of the former is higher than the latter on single 1080Ti GPU <ref type="bibr" target="#b20">[21]</ref>. However, the existing lightweight backbones may be suboptimal for semantic segmentation because they are usually overly tuned for image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Context Extraction Modules</head><p>Another key to semantic segmentation is how to capture richer contextual information. Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b8">[9]</ref> consists of parallel atrous convolutional layers with different rates which can attend to multi-scale contextual information. Pyramid Pooling Module (PPM) <ref type="bibr" target="#b10">[11]</ref> in PSPNet is more computationally efficient than ASPP by implementing pyramid pooling ahead of convolutional layers. Unlike the local nature of convolutional kernels, self-attention mechanism is good at capturing global dependencies. In this way, Dual Attention Network (DANet) <ref type="bibr" target="#b35">[36]</ref> takes advantage of both position attention and channel attention to further improve feature representation. Object Context Network (OCNet) <ref type="bibr" target="#b36">[37]</ref> utilizes self-attention mechanism to explore object context which is defined as a set of pixels belonging to the same object category. Authors of CCNet <ref type="bibr" target="#b37">[38]</ref> raise criss-cross attention to improve the efficiency of memory usage and computation. However, these context extraction modules are designed and performed for high-resolution feature maps, too time consuming for lightweight models. Taking the low-resolution feature maps as input, we strengthen the PPM module with more scales and deep feature aggregation. When appended to the end of the low-resolution branch, the proposed module outperforms the PPM and Base-OC module in OCNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, the whole pipeline is described, which consists of two main components: the Deep Dual-resolution </p><formula xml:id="formula_0">3 ? 3, 128 3 ? 3, 128 ? 2 3 ? 3, 64 3 ? 3, 64 ? 2 3 ? 3, 256 3 ? 3, 256 ? 3 3 ? 3, 128 3 ? 3, 128 ? 3</formula><p>Bilateral fusion Bilateral fusion conv5 1 7 ? 7, 28 ? 28 </p><formula xml:id="formula_1">3 ? 3, 256 3 ? 3, 256 ? 2 3 ? 3, 64 3 ? 3, 64 ? 2 3 ? 3, 512 3 ? 3, 512 ? 3 3 ? 3, 128 3 ? 3, 128 ? 3 Bilateral fusion Bilateral fusion ? ? ? 1 ? 1, 256 3 ? 3, 256 1 ? 1, 512 ? ? ? ? 1 ? ? ? 1 ? 1, 64 3 ? 3, 64 1 ? 1, 128 ? ? ? ? 1 ? ? ? 1 ? 1, 512 3 ? 3, 512 1 ? 1, 1024 ? ? ? ? 1 ? ? ? 1 ? 1, 128 3 ? 3, 128 1 ? 1, 256 ? ? ? ? 1 conv5 2 7 ? 7 High-to-low</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Dual-resolution Network</head><p>For convenience, we can add an additional high-resolution branch to the widely used classification backbone such as ResNets. To achieve a trade-off between resolution and inference speed, we let the high-resolution branch create feature maps whose resolution is 1/8 of the input image resolution. Therefore, the high-resolution branch is appended to the end of the conv3 stage. Note that the high-resolution branch does not contain any downsampling operations and has a one-toone correspondence with the low-resolution branch to form deep high-resolution representations. Then multiple bilateral feature fusions can be performed at different stages to fully fuse the spatial information and semantic information.</p><p>The detailed architectures of DDRNet-23-slim and DDRNet-39 are shown in <ref type="table" target="#tab_1">Table I</ref>. We modify the input stem of the original ResNet, replacing one 7?7 convolutional layer with two sequential 3?3 convolutional layers. Residual basic blocks are utilized to construct the trunk and the subsequent two branches. To expand the output dimension, one bottleneck block is added at the end of each branch. The bilateral fusion includes fusing the high-resolution branch into the low-resolution branch (high-to-low fusion) and fusing the low-resolution into the high-resolution branch (low-to-high fusion). For high-to-low fusion, high-resolution feature maps are downsampled by a sequence of 3?3 convolutions with a stride of 2 prior to pointwise summation. For low-to-high resolution, low-resolution feature maps are first compressed by a 1?1 convolution and then upsampled with bilinear interpolation. <ref type="figure" target="#fig_2">Fig. 3</ref> shows how bilateral fusion is implemented. The i-th high-resolution feature maps X Hi and low-resolution feature maps X Li can be written as:</p><formula xml:id="formula_2">X Hi = R(F H (X H(i?1) ) + T L?H (F L (X L(i?1) ))) X Li = R(F L (X L(i?1) ) + T H?L (F H (X H(i?1) )))<label>(1)</label></formula><p>where F H and F L correspond to the sequence of residual basic blocks with high resolution and low resolution, T L?H and T H?L refer to the low-to-high and high-to-low transformer, R denotes the ReLU function. We totally construct four dual-resolution networks of different depths and widths. DDRNet-23 is twice as wide as DDRNet-23-slim and DDRNet-39 1.5? is also a wider version of DDRNet-39. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Aggregation Pyramid Pooling Module</head><p>Here, we propose a novel module to further extract contextual information from low-resolution feature maps.    <ref type="figure">Fig. 4</ref>. The overview of DDRNets on semantic segmentation. "RB" denotes sequential residual basic blocks. "RBB" denotes the single residual bottleneck block. "DAPPM" denotes the Deep Aggregation Pyramid Pooling Module. "Seg. Head" denotes the segmentation head. Black solid lines denote information paths with data processing (including upsampling and downsampling) and black dashed lines denote information paths without data processing. "sum" denotes the pointwise summation. Dashed boxes denote the components which are discarded in the inference stage. shows the interior structure of the DAPPM. Taking feature maps of 1/64 image resolution as input, large pooling kernels with exponential strides are performed to generate feature maps of 1/128, 1/256, 1/512 image resolution. Input feature maps and image-level information generated by global average pooling are also utilized. We argue that it is inadequate to blend all the multi-scale contextual information by a single 3?3 or 1?1 convolution. Inspired by Res2Net <ref type="bibr" target="#b38">[39]</ref>, we first upsample the feature maps and then uses more 3?3 convolutions to fuse contextual information of different scales in a hierarchial-residual way. Considering an input x, each scale y i can be written as:</p><formula xml:id="formula_3">y i = ? ? ? ? ? C 1?1 (x), i = 1; C 3?3 (U (C 1?1 (P 2 i +1,2 i?1 (x))) + y i?1 ), 1 &lt; i &lt; n; C 3?3 (U (C 1?1 (P global (x))) + y i?1 ), i = n.</formula><p>(2) where C 1?1 is 1?1 convolution, C 3?3 is 3?3 convolution, U denotes upsampling operation, P j,k denotes the pool layer of which kernel size is j and stride is k, P global denotes the global average pooling. In the end, all feature maps are concatenated and compressed using a 1?1 convolution. Besides, a 1?1 projection shortcut is added for easy optimization. Similar to SPP in SwiftNet <ref type="bibr" target="#b20">[21]</ref>, DAPPM is implemented with the sequence BN-ReLU-Conv.</p><p>Inside a DAPPM, contexts extracted by larger pooling kernels are integrated with deeper information flow, and multiscale nature is formed by integrating different depths with different sizes of pooling kernels. <ref type="table" target="#tab_1">Table II</ref> shows that DAPPM is able to provide much richer context than PPM. Although DAPPM contains more convolution layers and more complex fusion strategy, it hardly affects the inference speed because the input resolution is only 1/64 of the image resolution. For example, with a 1024?1024 image, the maximum resolution of feature maps is 16?16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Overall Architecture for Semantic Segmentation</head><p>An overview of our method is depicted in <ref type="figure">Fig. 4</ref>. Some changes are made to the dual-resolution network to accommodate the semantic segmentation task. First, the stride of 3?3 convolution in the RBB of the low-resolution branch is set to 2 to further downsample. Then, a DAPPM is added to the output of the low-resolution branch, which extracts rich contextual information from the high-level feature maps of 1/64 image resolution. Besides, the last high-to-low fusion is replaced by low-to-high fusion implemented by bilinear interpolation and summation fusion. At last, we devise a simple segmentation head consisting of a 3?3 convolutional layer followed by a 1?1 convolutional layer. The computational load of the segmentation head can be adjusted by changing the output dimension of the 3?3 convolutional layer. We set it to 64 for DDRNet-23-slim, 128 for DDRNet-23, and 256 for DDRNet-39. Note that except for the segmentation head and the DAPPM module, all the modules have been pre-trained on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Deep Supervision</head><p>Extra supervision during the training stage can ease the optimization of deep convolutional neural networks (DCNNs). In PSPNet, an auxiliary loss is added to supervise the output of res4 22 block of ResNet-101 and the corresponding weight is set to 0.4 according to experimental results <ref type="bibr" target="#b10">[11]</ref>. BiSeNetV2 <ref type="bibr" target="#b23">[24]</ref> proposes a booster training strategy in which extra segmentation heads are added at the end of each stage of the semantic branch. However, it needs numerous experiments to find the optimal weights to balance each loss, and leads to a non-negligible increase in training memory. To acquire better results, SFNet [23] utilizes a similar strategy named Cascaded Deeply Supervised Learning. In this paper, we only adopt simple extra supervision for a fair comparison with most of the methods. We add the auxiliary loss as shown in <ref type="figure">Fig. 4</ref> and set the weight to 0.4 following the PSPNet. The auxiliary segmentation head is discarded during the testing stage. The final loss which is the weighted sum of cross-entropy loss can be expressed as:</p><formula xml:id="formula_4">L f = L n + ?L a<label>(3)</label></formula><p>where L f , L n , L a represents the final loss, normal loss, auxiliary loss respectively and ? denotes the weight of auxiliary loss, which is 0.4 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Datasets</head><p>Cityscapes <ref type="bibr" target="#b39">[40]</ref> is one of the well-known datasets focusing on urban street scene parsing. It contains 2975 finely annotated images for training, 500 images for validation, and 1525 images for testing. We do not use extra 20000 coarsely labeled images during training. There is a total of 19 classes available for the semantic segmentation task. The resolution of images is 2048?1024, which is challenging for real-time semantic segmentation. CamVid <ref type="bibr" target="#b40">[41]</ref> consists of 701 densely annotated frames and the resolution of each frame is 960?720. It comprises 367 images for training, 101 images for validation, and 233 images for testing. We merge the train set and validation set for training and evaluate our models on the test set using 11 classes following previous works <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>COCOStuff <ref type="bibr" target="#b41">[42]</ref> provides 10K complex images with dense annotations of 182 categories, including 91 thing and 91 stuff classes. Note that 11 of the thing classes do not have any segmentation annotations. We follow the split in <ref type="bibr" target="#b41">[42]</ref> (9K for training and 1K for testing) for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Train Setting</head><p>Before finetuning on semantic segmentation tasks, the dualresolution networks are trained on ImageNet <ref type="bibr" target="#b42">[43]</ref> following the same data augmentation strategy as previous works <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b43">[44]</ref>. All the models are trained with an input resolution of 224?224, a batch size of 256, and 100 epochs on four 2080Ti GPUs. The initial learning rate is set to 0.1 and is reduced by 10 times at epoch 30, 60, and 90. We train all the networks using SGD with a weight decay of 0.0001 and a Nesterov momentum of 0.9. Top-1 errors on ImageNet validation set are shown in <ref type="table" target="#tab_1">Table III</ref>. Although the efficiency of DDRNet is not superior to many advanced lightweight backbones which are elaborately designed on ImageNet, it still achieves start-of-theart results on semantic segmentation benchmarks considering a speed trade-off. The training settings of Cityscapes, CamVid, and COCOStuff are introduced as follows.</p><p>1) Cityscapes: Following <ref type="bibr" target="#b48">[49]</ref>, we use the SGD optimizer with the initial learning rate of 0.01, the momentum of 0.9, and the weight decay of 0.0005. We adopt the ploy learning policy with the power of 0.9 to drop the learning rate and implement the data augmented method including random cropping images, random scaling in the range of 0.5 to 2.0, and random horizontal flipping. Images are randomly cropped into 1024?1024 for training following <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b22">[23]</ref>. All the models are trained with 484 epochs (about 120K iterations), a batch size of 12, and syncBN on four 2080Ti GPUs. For the models evaluated on the test server, we feed images from train and val set simultaneously during training. For a fair comparison with <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b22">[23]</ref>, online hard example mining (OHEM) <ref type="bibr" target="#b49">[50]</ref> is also used.</p><p>2) CamVid: We set the initial learning rate to 0.001 and train all the models for 968 epochs. Images are randomly cropped into 960?720 for training following <ref type="bibr" target="#b17">[18]</ref>. All the models are trained on a single GPU and other training de <ref type="table" target="#tab_1">-TABLE IV  ACCURACY AND SPEED COMPARISON ON CITYSCAPES. WE REPORT RESULTS ON BOTH VAL SET AND TEST SET. SINCE INFERENCE SPEED OF DIFFERENT  MODELS IS MEASURED UNDER DIFFERENT CONDITIONS, THE CORRESPONDING GPU MODELS AND INPUT RESOLUTIONS ARE REPORTED. OUR GFLOPS  CALCULATION ADOPTS 2048?1024 IMAGE AS INPUT. THE CORRESPONDING SPEED IS MEASURED USING TENSORRT ACCELERATION IF THE METHOD IS  MARKED</ref>  tails are identical to those for Cityscapes. When employing Cityscapes pre-train, we finetune the models for 200 epochs.</p><p>3) COCOStuff: The initial learning rate is 0.001 and the total training epochs are 110. We resize the short side of the images to 640 before data augmentation. The crop size is 640?640, as same as that of BiSeNetV2 <ref type="bibr" target="#b23">[24]</ref>. Other training details are identical to those for Cityscapes while the weight decay is 0.0001. In the inference phase, we fix the image resolution to 640?640.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Measure of Inference Speed and Accuracy</head><p>The inference speed is measured on a single GTX 2080Ti GPU by setting the batch size to 1, with CUDA 10.0, CUDNN 7.6, and PyTorch 1.3. Similar to MSFNet and SwiftNet, we exclude batch normalization layers after convolutional layers because they can be integrated into convolutions during inference. We use the protocols established by <ref type="bibr" target="#b50">[51]</ref> for a fair comparison (image size: 2048?1024 for Cityscapes, 960?720 for CamVid, and 640?640 for COCOStuff).</p><p>Following ResNet <ref type="bibr" target="#b26">[27]</ref>, we report the best results, average results, and standard deviations of four trials except for the cityscapes test set of which accuracy is provided by the official server.</p><p>D. Speed and Accuracy Comparisons 1) Cityscapes: As can be observed from <ref type="table" target="#tab_1">Table IV</ref> and <ref type="figure" target="#fig_0">Fig. 1</ref>, our method achieves a new state-of-the-art trade-off between real-time and high accuracy. Specially, DDRNet-23slim (our smallest model) achieves 77.4% mIoU on the test set at 102 FPS. It outperforms DFANet A and MSFNet* by 6.1% mIoU with similar inference speed, and reasons approximately 2.5 times as fast as MSFNet. Besides, it runs 40% faster than the smallest SFNet and achieves a 2.9% mIoU gain on the test set. It is worth noting that our method also towers over those methods based on architecture search for realtime semantic segmentation such as CAS <ref type="bibr" target="#b46">[47]</ref> and GAS <ref type="bibr" target="#b47">[48]</ref> with similar inference speed. For the wider model, DDRNet-23 achieves the overall best accuracy among the published real-time methods in <ref type="table" target="#tab_1">Table IV</ref>, attaining 79.4% mIoU at 37 FPS. DDRNet-23 has a performance gain of 0.5% over SFNet (ResNet-18) and runs twice as fast as it.</p><p>We keep going deeper with DDRNets and achieve 80.4% mIoU on the Cityscapes test server at 22 FPS, only using finely annotated data. If benefitting from pre-training on Mapillary <ref type="bibr" target="#b51">[52]</ref> dataset and TensorRT acceleration like <ref type="bibr" target="#b22">[23]</ref>, our method can establish a skyscraping baseline for real-time semantic segmentation of road scenes. On the Cityscapes val set, DDRNet-23-slim is superior to all published results in <ref type="table" target="#tab_1">Table  IV with</ref>    <ref type="bibr" target="#b20">[21]</ref> 73.9 -GTX 1080Ti SwiftNetRN-18 <ref type="bibr" target="#b20">[21]</ref> 72.6 -GTX 1080Ti BiSeNet1 <ref type="bibr" target="#b18">[19]</ref> 65.6 175 GTX 1080Ti BiSeNet2 <ref type="bibr" target="#b18">[19]</ref> 68.7 116 GTX 1080Ti BiSeNetV2 ? <ref type="bibr" target="#b23">[24]</ref> 72.4 124 GTX 1080Ti BiSeNetV2-L ? <ref type="bibr" target="#b23">[24]</ref> 73.2 33 GTX 1080Ti CAS <ref type="bibr" target="#b46">[47]</ref> 71.2 169 TitanXp GAS <ref type="bibr" target="#b47">[48]</ref> 72.8 153 TitanXp SFNet(DF2) <ref type="bibr" target="#b22">[23]</ref> 70.4 134 GTX 1080Ti SFNet(ResNet-18) <ref type="bibr" target="#b22">[23]</ref> 73. <ref type="bibr">8 36</ref> GTX 1080Ti MSFNet* <ref type="bibr" target="#b28">[29]</ref> 72.7 160 GTX 2080Ti MSFNet <ref type="bibr" target="#b28">[29]</ref> 75. 23 achieves a new overall best result of 79.5% mIoU. <ref type="figure" target="#fig_7">Fig. 6</ref> shows the visualized results of DDRNet-23-slim and DDRNet-23 under different scenes.</p><p>2) CamVid: As shown in <ref type="table" target="#tab_7">Table V</ref>, DDRNet-23-slim achieves 74.7% mIoU on the CamVid test set at 230 FPS without Cityscapes pre-training. It obtains the second-highest accuracy and runs faster than all the other methods. In particular, the performance of DDRNet-23 is superior to MSFNet, the previous state-of-the-art method. DDRNet-23 also has a big performance gain over BiSeNetV2-L and SFNet (ResNet-18) and runs about two times faster than them. Given that the training pixels of CamVid are much less than that of Cityscapes, we believe that the outstanding performances of DDRNets partly attribute to adequate ImageNet pre-training. In addition, our Cityscapes pre-trained models achieve new state-of-the-art accuracy with the real-time speed. Specially, Cityscapes pre-trained DDRNet-23 realizes 80.6% mIoU with 94 FPS, stronger and much faster than BiSeNetV2-L. The corresponding visualized results are shown in <ref type="figure" target="#fig_8">Fig. 7</ref>.</p><p>3) COCOStuff: We also validate our method on COCOStuff which is a more challenging dataset for real-time semantic segmentation due to the plentiful categories. The stride of RBB in the low-resolution branch is set to 1, for the image resolution is smaller than the other two datasets. Time to reshape images and predicted masks is not counted. <ref type="table" target="#tab_1">Table VI</ref> demonstrates that our method outperforms BiSeNetV2 by a substantial degree under very challenging scenarios. Our DDRNet-23 achieves a similar accuracy with PSPNet50 while running 20 times as fast as it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparisons with State-of-the-art Results</head><p>In this part, we further demonstrate the capacity of DDRNet for semantic segmentation by comparing it to state-of-the-art models on the Cityscapes test set. Such methods frequently employ multi-scale and horizontal flip inference to achieve better results regardless of time cost. For a fair comparison with them, we also apply multiple scales including 0.50?, 0.75?, 1?, 1.25?, 1.5?, 1.75?, 2? with left-right flipping during test. As is shown in <ref type="table" target="#tab_1">Table VII</ref>, standard test augmentation improves the accuracy of DDRNet-39 from 80.4% to 81.9%. Our DDRNet-39 outperforms numerous powerful models which are integrated with self-attention modules such as CCNet, DANet, and OCNet. It is noteworthy that our method only requires 11% computation of DANet. DDRNet-39 also gets ahead of SFNet (based on ResNet-101 backbone) which is a state-of-the-art method for real-time semantic segmentation, only requiring 34% computation of it. DDRNet-39 1.5? of which size is closer to other models in <ref type="table" target="#tab_1">Table VII</ref> achieves a very competitive performance (82.4% mIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparisons with HRNet</head><p>The major difference between DDRNet and HRNet is the number of parallel branches. Besides, we append the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ablative Experiments on Cityscapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Standard Bells and Whistles:</head><p>We analyze the effect of some basic training tricks which are also adopted by recent advanced method SFNet <ref type="bibr" target="#b22">[23]</ref>. As shown in <ref type="table" target="#tab_1">Table IX</ref>, the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) DAPPM:</head><p>We compare the DAPPM with the pyramid pooling based methods (PPM), self-attention based modules (Base-OC), and the res2net module. The results in <ref type="table" target="#tab_12">Table X</ref> shows that the proposed module improves the performance of scene parsing from 74.1% mIoU to 77.8% mIoU while the inference speed is hardly affected. Compared to the PPM and RES2, DAPPM also achieves 1% mIoU gain while the Base-OC, another state-of-the-art method, gets a relatively poor performance with low-resolution feature maps.  3) Dual-resolution Networks: For faster experiments, we train all the bilateral networks from scratch with a initial learning rate of 0.05, a crop size of 1024?512, 600 epochs in total, and without using OHEM. As shown in <ref type="table" target="#tab_1">Table XI</ref>, using thinner detail branch results in 1.3% accuracy decrease and running much faster than the baseline. Appending the detail branch to the middle layer of the network contributes to the deep high-resolution representation and also improves the inference speed because it avoids computing with higher resolution. The bottleneck expands the feature dimension, which generates richer features for the DAPPM and the final segmentation head. The bilateral fusions further improve the segmentation accuracy at a small time cost. Finally, our dualresolution network takes less computation resources and time than the baseline while achieving better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we are devoted to the real-time and accurate semantic segmentation of road scenes and present a simple solution for it without using extra bells or whistles. In particular, novel deep dual-resolution networks are proposed as efficient backbones for real-time semantic segmentation. And a new module is designed for extracting multi-scale contextual information from low-resolution feature maps. To our best knowledge, we are the first to introduce deep highresolution representation into real-time semantic segmentation and our simple strategy outperforms all previous real-time models on three popular benchmarks. DDRNets mainly consist of residual basic blocks and bottleneck blocks, providing a wide range of speed and accuracy trade-off by scaling model width and depth. Due to the simplicity and efficiency of our method, it can be seen as a strong baseline for unifying realtime and high-accuracy semantic segmentation. Further studies will focus on improving the baseline and transferring the backbones to other downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The authors are with Research Institute of Intelligent Control and Systems, Harbin Institute of Technology, Harbin 150001, China (e-mail: hui-huipan@hit.edu.cn) A comparison of speed-accuracy trade-off on Cityscapes test set. The red triangles indicate our methods while blue triangles represent other methods. Green circles represent architecture search methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A comparison about dilation methods, encoder-decoder methods, two-pathway methods and our deep dual-resolution network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The details of bilateral fusion in DDRNet. Point-wise summation is implemented before ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>RB</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The detailed architecture of Deep Aggregation Pyramid Pooling Module. The number of multi-scale branches can be adjusted according to input resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>36 . 3</head><label>363</label><figDesc>GFLOPs and 5.7M parameters. And DDRNet-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Visualized segmentation results on Cityscapes val set. The four columns left-to-right refer to the input image, the ground truth, the output of DDRNet-23-slim, and the output of DDRNet-23. The first four rows show the performance of two models while the last two rows represent some segmentation failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Visualized segmentation results on CamVid test set. The color of ignored labels during testing is set to black. The three columns left-to-right refer to the input image, the ground truth, and the output of DDRNet-23. The first four rows show the successful samples while the last two rows represent some segmentation failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>21]-[23]. The arXiv:2101.06085v2 [cs.CV] 1 Sep 2021 other develops complex lightweight encoders trained from scratch, one of which, BiSeNetV2 [24] hits a new peak in terms of real-time performance, achieving 72.6% test mIoU at 156 FPS on Cityscapes. However, except for</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>ARCHITECTURES OF DDRNET-23-SLIM AND DDRNET-39 FOR IMAGENET. 'CONV4?r' DENOTES THAT CONV4 IS REPEATED r TIMES. FOR DDRNET-23-SLIM, r = 1 AND FOR DDRNET-39, r = 2.</figDesc><table><row><cell>stage</cell><cell>output</cell><cell cols="2">DDRNet-23-slim</cell><cell cols="2">DDRNet-39</cell></row><row><cell>conv1</cell><cell>112 ? 112</cell><cell cols="2">3 ? 3, 32, stride 2</cell><cell cols="2">3 ? 3, 64, stride 2</cell></row><row><cell></cell><cell></cell><cell cols="2">3 ? 3, 32, stride 2</cell><cell cols="2">3 ? 3, 64, stride 2</cell></row><row><cell>conv2</cell><cell>56 ? 56</cell><cell>3 ? 3, 32 3 ? 3, 32</cell><cell>? 2</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>? 3</cell></row><row><cell>conv3</cell><cell>28 ? 28</cell><cell>3 ? 3, 64 3 ? 3, 64</cell><cell>? 2</cell><cell>3 ? 3, 128 3 ? 3, 128</cell><cell>? 4</cell></row><row><cell>conv4?r</cell><cell>14 ? 14, 28 ? 28</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II CONSIDERING</head><label>II</label><figDesc>AN INPUT IMAGE OF 1024?1024, THE GENERATED CONTEXT SIZES OF PPM AND DAPPM ARE LISTED</figDesc><table><row><cell></cell><cell>PPM</cell><cell>DAPPM</cell></row><row><cell></cell><cell></cell><cell>[16]</cell></row><row><cell></cell><cell></cell><cell>[16, 8]</cell></row><row><cell>Output scale</cell><cell>[16, 6, 3, 2, 1]</cell><cell>[16, 8, 4]</cell></row><row><cell></cell><cell></cell><cell>[16, 8, 4, 2]</cell></row><row><cell></cell><cell></cell><cell>[16, 8, 4, 2, 1]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III TOP</head><label>III</label><figDesc>-1 ERROR RATES, PARAMETER SIZE AND GFLOPS OF FOUR SCALED-UP DDRNETS</figDesc><table><row><cell>Model</cell><cell>top-1 err.</cell><cell>Params.</cell><cell>GFLOPs</cell></row><row><cell>DDRNet-23-slim</cell><cell>29.8</cell><cell>7.57M</cell><cell>0.98G</cell></row><row><cell>DDRNet-23</cell><cell>24.1</cell><cell>28.22M</cell><cell>3.88G</cell></row><row><cell>DDRNet-39</cell><cell>22.7</cell><cell>40.13M</cell><cell>6.95G</cell></row><row><cell>DDRNet-39 1.5?</cell><cell>21.6</cell><cell>76.86M</cell><cell>14.85G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V ACCURACY</head><label>V</label><figDesc>AND SPEED COMPARISON ON CAMVID. MSFNET RUNS AT 1024?768 AND MSFNET* RUNS AT 768?512 WHILE OTHER METHODS RUN AT 960?720. THE CORRESPONDING SPEED IS MEASURED USING TENSORRT ACCELERATION IF THE METHOD IS MARKED WITH ?</figDesc><table><row><cell>Model</cell><cell>MIoU</cell><cell>Speed (FPS)</cell><cell>GPU</cell></row><row><cell></cell><cell cols="2">w/o Cityscapes pre-training</cell><cell></cell></row><row><cell>DFANet A [18]</cell><cell>64.7</cell><cell>120</cell><cell>TitanX</cell></row><row><cell>DFANet B [18]</cell><cell>59.3</cell><cell>160</cell><cell>TitanX</cell></row><row><cell>SwiftNetRN-18 pyr</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI ACCURACY</head><label>VI</label><figDesc>AND SPEED COMPARISON ON COCOSTUFF. THE INPUT RESOLUTION IS 640?640 AND THE RESULT OF PSPNET50 COMES FROM [24]. THE CORRESPONDING SPEED IS MEASURED USING TENSORRT ACCELERATION IF THE METHOD IS MARKED WITH ? THE-ART MODELS ON CITYSCAPES TEST SET. OS DENOTES THE FINAL OUTPUT STRIDE. ALL THE METHODS TRAIN MODELS ON BOTH TRAIN AND VAL SET EXCEPT PSPNET MARKED WITH ?ONLY USING TRAIN SET. GFLOPS CALCULATION ADOPTS 1024?1024 IMAGE AS INPUT AND MOST OF RESULTS ABOUT GFLOPS AND PARAMS CAN BE FOUND IN<ref type="bibr" target="#b22">[23]</ref> </figDesc><table><row><cell>Model</cell><cell>MIoU</cell><cell></cell><cell>PixAcc</cell><cell>Speed (FPS)</cell></row><row><cell>PSPNet50 [11]</cell><cell>32.6</cell><cell></cell><cell>-</cell><cell>6.6</cell></row><row><cell>ICNet [15]</cell><cell>29.1</cell><cell></cell><cell>-</cell><cell>35.7</cell></row><row><cell>BiSeNetV2 ? [24]</cell><cell>25.2</cell><cell></cell><cell>60.5</cell><cell>87.9</cell></row><row><cell>BiSeNetV2-L ? [24]</cell><cell>28.7</cell><cell></cell><cell>63.5</cell><cell>42.5</cell></row><row><cell>DDRNet-23</cell><cell cols="2">32.1(31.8?0.2)</cell><cell cols="2">64.7(64.7?0.1) 129.2</cell></row><row><cell>DDRNet-39</cell><cell cols="2">34.8(34.6?0.1)</cell><cell cols="2">66.6(66.7?0.2) 83.8</cell></row><row><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell></row><row><cell>STATE-OF-Model</cell><cell>OS</cell><cell>mIoU</cell><cell>GFLOPs</cell><cell>Params.</cell></row><row><cell>SAC [55]</cell><cell>8</cell><cell>78.1</cell><cell>-</cell><cell>-</cell></row><row><cell>DepthSeg [56]</cell><cell>8</cell><cell>78.2</cell><cell>-</cell><cell>-</cell></row><row><cell>PSPNet ? [11]</cell><cell>8</cell><cell>78.4</cell><cell>1065.4</cell><cell>65.7M</cell></row><row><cell>ResNet38 [57]</cell><cell>8</cell><cell>78.4</cell><cell>-</cell><cell>-</cell></row><row><cell>BiSeNet [19]</cell><cell>8</cell><cell>78.9</cell><cell>219.1</cell><cell>51.0M</cell></row><row><cell>DFN [58]</cell><cell>8</cell><cell>79.3</cell><cell>1121.0</cell><cell>90.7M</cell></row><row><cell>PSANet [59]</cell><cell>8</cell><cell>80.1</cell><cell>1182.6</cell><cell>85.6M</cell></row><row><cell>DenseASPP [12]</cell><cell>8</cell><cell>80.6</cell><cell>632.9</cell><cell>35.7M</cell></row><row><cell>CCNet [38]</cell><cell>8</cell><cell>81.4</cell><cell>1153.9</cell><cell>66.5M</cell></row><row><cell>DANet [36]</cell><cell>8</cell><cell>81.5</cell><cell>1298.8</cell><cell>66.6M</cell></row><row><cell>OCNet [37]</cell><cell>8</cell><cell>81.7</cell><cell>-</cell><cell>-</cell></row><row><cell>OCRNet [60]</cell><cell>8</cell><cell>81.8</cell><cell>-</cell><cell>-</cell></row><row><cell>HRNetV2-W48 [49]</cell><cell>4</cell><cell>81.6</cell><cell>348.1</cell><cell>65.9M</cell></row><row><cell>SFNet [23]</cell><cell>4</cell><cell>81.8</cell><cell>417.5</cell><cell>50.3M</cell></row><row><cell>DDRNet-39</cell><cell>8</cell><cell>81.9</cell><cell>140.6</cell><cell>32.3M</cell></row><row><cell>DDRNet-39 1.5?</cell><cell>8</cell><cell>82.4</cell><cell>303.0</cell><cell>70.2M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII COMPARATIVE</head><label>VIII</label><figDesc>EXPERIMENTS BETWEEN DDRNET AND HRNET IN TERMS OF MIOU, FPS AND TRAIN MEMORY</figDesc><table><row><cell>Model</cell><cell>mIoU</cell><cell>FPS</cell><cell>Train mem.</cell></row><row><cell>HRNetV2-W18-Small-v1 [49]</cell><cell>70.3</cell><cell>67.2</cell><cell>1989MiB</cell></row><row><cell>HRNetV2-W18-Small-v2 [49]</cell><cell>76.2</cell><cell>31.1</cell><cell>2745MiB</cell></row><row><cell>DDRNet-23-slim</cell><cell>76.9</cell><cell>101.6</cell><cell>1629MiB</cell></row><row><cell cols="4">multi-scale context extraction module to the end of the low-</cell></row><row><cell cols="4">resolution branch. Experimental results in Table VIII demon-</cell></row><row><cell cols="4">strate the improvement of DDRNet over HRNet in both</cell></row><row><cell cols="4">inference time and training memory usage. We get the val</cell></row><row><cell cols="4">results of two small HRNets from the official implementation.</cell></row><row><cell cols="4">Training memory is measured on a single 2080Ti with a batch</cell></row><row><cell cols="4">size of 2 and a crop size of 1024?512, excluding the auxiliary</cell></row><row><cell>segmentation head.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX INFLUENCES</head><label>IX</label><figDesc></figDesc><table><row><cell cols="5">OF STANDARD BELLS AND WHISTLES, INCLUDING DEEP</cell></row><row><cell cols="5">SUPERVISION (DS), OHEM AND TRAINING AT A CROP SIZE OF</cell></row><row><cell cols="4">1024?1024 (THE DEFAULT IS 1024?512)</cell><cell></cell></row><row><cell>Model</cell><cell>DS</cell><cell>OHEM</cell><cell>1024?1024</cell><cell>mIoU</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell></cell><cell>76.1</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell></cell><cell>76.1</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell></cell><cell>76.9</cell></row><row><cell>DDRNet-23-slim</cell><cell></cell><cell></cell><cell></cell><cell>77.8</cell></row><row><cell cols="5">accuracy is raised from 76.1 to 77.8 with deep supervision,</cell></row><row><cell cols="4">OHEM, and training at a larger crop size.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE X COMPARISON</head><label>X</label><figDesc>OF DAPPM AND OTHER CONTEXT EXTRACTION MODULES. RES2 DENOTES THE RES2NET MODULE AND BASE-OC IS THE OBJECT CONTEXT MODULE PROPOSED IN<ref type="bibr" target="#b36">[37]</ref> </figDesc><table><row><cell>PPM</cell><cell>RES2</cell><cell>Base-OC</cell><cell>DAPPM</cell><cell>mIoU</cell><cell>Speed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>74.1</cell><cell>107.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.8</cell><cell>104.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.8</cell><cell>103.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>75.6</cell><cell>104.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.8</cell><cell>101.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XI ABLATION</head><label>XI</label><figDesc>STUDY OF DUAL-RESOLUTION NETWORKS. THE BASELINE IS ADAPTED FROM BISENETV2 BY REPLACING THE COMPLICATED SEMANTIC BRANCH WITH OUR LOW-SOLUTION BRANCH. '+THINER DETAIL BRANCH' REPRESENTS CUTTING THE DIMENSION OF THE DETAIL BRANCH IN HALF. '+CONV3' REPRESENTS APPENDING THE DETAIL BRANCH TO THE END OF CONV3 STAGE. '+RESIDUAL' DENOTES REPLACING THE 3?3 CONVOLUTIONS WITH THE RESIDUAL BASIC BLOCKS. '+BOTTLENECK' DENOTES ADDING A BOTTLENECK BLOCK TO THE END OF EACH BRANCH. '+LOW-TO-HIGH FUSION' OR '+BILATERAL FUSION' DENOTES PERFORMING THE MULTIPLE LOW-TO-HIGH FUSION OR BILATERAL FUSION</figDesc><table><row><cell>Model</cell><cell>mIoU</cell><cell>Params.</cell><cell>GFLOPs</cell><cell>Speed</cell></row><row><cell>Baseline</cell><cell>72.2</cell><cell>4.3M</cell><cell>70.2</cell><cell>60.2</cell></row><row><cell>+Thiner detail branch</cell><cell>70.9</cell><cell>3.8M</cell><cell>34.0</cell><cell>103.7</cell></row><row><cell>+Conv3</cell><cell>71.4</cell><cell>4.0M</cell><cell>31.7</cell><cell>128.7</cell></row><row><cell>+Residual</cell><cell>71.2</cell><cell>4.0M</cell><cell>31.7</cell><cell>125.2</cell></row><row><cell>+Bottleneck</cell><cell>73.3</cell><cell>5.2M</cell><cell>34.4</cell><cell>110.2</cell></row><row><cell>+Low-to-high fusion</cell><cell>74.0</cell><cell>5.3M</cell><cell>34.5</cell><cell>107.6</cell></row><row><cell>+Bilateral fusion</cell><cell>74.6</cell><cell>5.7M</cell><cell>36.3</cell><cell>101.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning markov random field for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1814" to="1828" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coarse-to-fine semantic segmentation from image-level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="225" to="236" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Task decomposition and synchronization for semantic biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7497" to="7510" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Her2net: A deep framework for semantic segmentation and classification of cell membranes and nuclei in breast cancer evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2189" to="2200" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A wavelet tour of signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Small object augmentation of urban scenes for realtime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5175" to="5190" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Context-integrated and featurerefined network for lightweight object parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5079" to="5093" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9522" to="9531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fast-scnn: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04502</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of roaddriving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12607" to="12616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Real-time semantic segmentation with fast attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03815</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10120</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02147</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Real-time semantic segmentation via multiply spatial fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07217</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cabinet: Efficient context aggregation network for low-latency semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00993</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1209" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ML-ITS, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Customizable architecture search for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11641" to="11650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph-guided architecture search for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Fasterseg: Searching for faster real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10917</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal random fields for efficient video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8915" to="8924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2031" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="956" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
