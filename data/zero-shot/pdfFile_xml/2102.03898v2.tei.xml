<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AttributeNet: Attribute Enhanced Vehicle Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolfo</forename><surname>Quispe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corp</orgName>
								<address>
									<addrLine>One Microsoft Way</addrLine>
									<postCode>98052-6399</postCode>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<postCode>13083-852</postCode>
									<settlement>Brazil</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helio</forename><surname>Pedrini</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<postCode>13083-852</postCode>
									<settlement>Brazil</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AttributeNet: Attribute Enhanced Vehicle Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vehicle re-identification</term>
					<term>attribute recognition</term>
					<term>interaction</term>
					<term>convolutional neural networks</term>
					<term>information distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vehicle Re-Identification (V-ReID) is a critical task that associates the same vehicle across images from different camera viewpoints. Many works explore attribute clues to enhance V-ReID; however, there is usually a lack of effective interaction between the attribute-related modules and final V-ReID objective. In this work, we propose a new method to efficiently explore discriminative information from vehicle attributes (for instance, color and type). We introduce AttributeNet (ANet) that jointly extracts identity-relevant features and attribute features. We enable the interaction by distilling the ReID-helpful attribute feature and adding it into the general ReID feature to increase the discrimination power. Moreover, we propose a constraint, named Amelioration Constraint (AC), which encourages the feature after adding attribute features onto the general ReID feature to be more discriminative than the original general ReID feature. We validate the effectiveness of our framework on three challenging datasets. Experimental results show that our method achieves the state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vehicle Re-Identification (V-ReID) aims to match/associate the same vehicle across images. It has many applications for vehicle tracking and retrieval. Like the Person Re-Identification (P-ReID) problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, V-ReID has gained increasing attention in the computer vision community <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. This task is challenging due to drastic changes in view points and illumination, resulting in a small inter-class and large intra-class difference.</p><p>Recently, there is a trend to explore additional clues for better V-ReID, such as using semantic maps <ref type="bibr" target="#b4">[5]</ref>, attributes (such as type, color) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, viewpoints <ref type="bibr" target="#b10">[11]</ref>, and vehicle parts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. In this work, we focus on the exploration of attributes to enhance the discrimination power of feature representations. Attributes are in general invariant to viewpoint changes and robust to environment alterations.</p><p>Most of the previous attribute-based works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> share a common characteristic in their de-Email addresses: edquispe@microsoft.com (Rodolfo Quispe), culan@microsoft.com (Cuiling Lan), wezeng@microsoft.com (Wenjun Zeng), helio@ic.unicamp.br (Helio Pedrini) sign: a global feature representation is extracted from an input image using a backbone network (for instance, ResNet <ref type="bibr" target="#b15">[16]</ref>), where this feature is followed by two types of heads, one for re-identification (ReID), and the other for attribute recognition. We refer to this design as the Vanilla-Attribute Design (VAD) and illustrate a representative VAD based Network (VAN) in <ref type="figure">Figure 1</ref>. One direct way to use the VAD for V-ReID is to concatenate the embedding features generated from the backbone (that is, global feature) and the attribute-based modules <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>VAD aims to drive the network to learn features that are discriminative for both V-ReID and attribute recognition, where the attributes are in general invariant to viewpoint and illumination changes. However, there is a lack of effective interaction between the attribute-based branches and V-ReID branch, where the attribute modules learn features for attribute recognition but are not explicitly designed to serve for V-ReID. Wang et al. <ref type="bibr" target="#b7">[8]</ref> explore attributes to generate attention masks, but these masks are used only to filter the information from the global feature instead of introducing the rich attribute representation into the final feature representation.</p><p>We propose Attribute Net (ANet) to enrich the in- <ref type="figure">Figure 1</ref>: Illustration of VAD based Network (VAN) for V-ReID. It is composed of a backbone network that learns to extract information from an input image and n branches to predict attributes based on attention modules. We use this VAN in our ANet as the first part of our framework. teraction between the attribute features and the V-ReID feature. ANet is designed to distill attribute information and add it into the global representation (from the backbone) to generate more discriminative features.  <ref type="figure">Figure 1</ref>) present the proposed ANet. Particularly, we combine the feature maps of different attribute branches to have a unique and generic representation G of all the attributes.</p><p>We distill the helpful attribute feature from G and compensate it onto the global V-ReID feature F to have the final feature map J, where the spatial average pooled feature of J is the final ReID feature for matching. Moreover, we introduce a new supervision objective, named Amelioration Constraint (AC), to push the compensated V-ReID feature J to be more discriminative than the V-ReID feature F before the compensation from attribute feature.</p><p>The main contributions of this work are:</p><p>? We propose a new architecture, named ANet, for effective V-ReID, which enhances the interaction between the attribute-supervised modules and V-ReID branch. This drives the distilled attribute features to serve for V-ReID.</p><p>? We introduce an Amelioration Constraint (AC), which encourages the attribute compensated fea- <ref type="figure">Figure 2</ref>: Illustration of the Joint Module. Note that the network to extract feature maps F, A 1 , ? ? ? , A n is shown in <ref type="figure">Figure 1</ref> and is not shown here. We distill the helpful attribute feature from G and compensate it onto the global V-ReID feature F to have the final feature map J, where the spatial average pooled feature of J is the final ReID feature for matching. Moreover, we introduce a new supervision objective, named Amelioration Constraint (AC) to push J to be more discriminative than the V-ReID feature F before the compensation from attribute feature. ture to be more discriminative than the V-ReID feature before compensation.</p><p>Experiments on three challenging datasets demonstrate the effectiveness of our ANet, which outperforms baselines significantly and achieves the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>For vehicle ReID, many approaches explore generative adversarial networks (GANs) <ref type="bibr" target="#b17">[18]</ref>, graph networks (GNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>, semantic parsing (SP) <ref type="bibr" target="#b4">[5]</ref> and vehicle part detection (VPD) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12]</ref> to improve the performance. Some of them tend to describe the vehicle details <ref type="bibr" target="#b17">[18]</ref> and local regions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12]</ref>. PRND <ref type="bibr" target="#b19">[20]</ref> and PGAN <ref type="bibr" target="#b11">[12]</ref> detect predefined regions (such as back mirrors, light and wheels) and describe them with deep features. SAVER <ref type="bibr" target="#b17">[18]</ref> modify the input image with the vehicle details erased using a GAN. Then, this synthetic image is combined with the input image to create a new version with the details visually enhanced for ReID. Some works aim to handle the drastic viewpoint changes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref>. Liu et al. <ref type="bibr" target="#b12">[13]</ref> describe each vehicle view based on semantic parsing and also encode the spacial relationship between them using GNs.</p><p>Vehicle ReID and person ReID share similar purpose in terms of image matching but with respect to the match of different objects (i.e., vehicle and person, respectively). Some approaches have explored attribute information for person ReID. Lin et al. <ref type="bibr" target="#b16">[17]</ref> labeled attributes for large datasets and proposed the first deep learning approach using attributes for ReID. Their architecture follows VAD directly by considering a confidence score to re-weigh and create a unique attribute representation and concatenate it with the backbone features. We show that this design is not the most effective way to leverage attributes. Later, other works for person ReID using attributes were proposed to address the imbalance of attribute classes <ref type="bibr" target="#b20">[21]</ref>, feature and frame disentangle based attributes in video ReID <ref type="bibr" target="#b21">[22]</ref>, one-shot learning <ref type="bibr" target="#b22">[23]</ref> and unsupervised scenarios <ref type="bibr" target="#b23">[24]</ref>. However, all of them follow the initial VAD developed by Lin et al. and assume that all the attribute information is equally important for ReID.</p><p>For vehicle ReID, similarly, some approaches have explored attribute information <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> or combined attributes with other clues <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref>. Most of the previous attribute-based works used attribute information to regularize the feature learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. PAMTRI <ref type="bibr" target="#b6">[7]</ref> focuses on solving viewpoint changes using a pose estimation module that learns from keypoints, which is combined with attribute clues to create a final vehicle embedding. StRDAN <ref type="bibr" target="#b9">[10]</ref> and DF-CVTC <ref type="bibr" target="#b5">[6]</ref> expand the idea of introducing viewpointinvariant features and used synthetic images during training. They regress the attribute classes from the backbone features, along with the ReID supervision based on the backbone features following VAD. However, using separate heads for different tasks ignores the interaction between the two tasks, where the attribute branches should serve for better ReID. There is a lack of efficient interaction between attribute and ReID information.</p><p>Our work addresses this issue by distilling useful attribute information and compensating into the ReID feature representation to have a more discriminative representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed ANet</head><p>Our proposed ANet is designed to exploit attribute information for effective V-ReID. In previous works that use attributes, there is a lack of interaction between the global V-ReID head and the attribute regression heads, which makes that the feature information is not effectively exploited for V-ReID.</p><p>To address this issue, we propose ANet (as shown in <ref type="figure">Figures 1 and 2</ref>). It consists of two parts: VAD based Network (VAN) and Joint Module (JM). VAN is based on a Backbone with two heads, where one of them is to learn global V-ReID features and the other to regress attributes. VAN outputs an initial feature representation of V-ReID and multiple Attribute features from the input image. Then, the JM distills V-ReID-helpful attribute information and compensates it into the global features. JM promotes the interaction between the two heads of VAN. Furthermore, we propose an Amelioration Constraint (AC), which drives the attribute compensated feature to be more discriminative than the original V-ReID feature before the compensation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">VAD based Network</head><p>VAD based Network (VAN), shown in <ref type="figure">Figure 1</ref>, aims to learn V-ReID features and regress attributes. This design is similar to previous literature work, where the attribute branches are expected to drive the learning of robust features since the attributes are in general invariant to illumination and viewpoint changes. Backbone. A backbone network is used to extract feature map F(I) ? R h?w?c from an input image I, where h, w and c are height , width and channels of F(I), respectively. We follow the previous works and use ResNet <ref type="bibr" target="#b15">[16]</ref> as the backbone. V-ReID Head/Branch. On top of the backbone feature F(I), we append a spatial global average pooling (GAP) layer followed by a fully-connected (FC) layer to generate the V-ReID feature f (I) as</p><formula xml:id="formula_0">f (I) = W f GAP (F(I)) + b f ,<label>(1)</label></formula><p>where W f and b f denote the weights and bias of the FC layer used to reduce the dimension of the pooled feature,</p><formula xml:id="formula_1">W f ? R s f ?c and b f ? R s f , where s f is the predefined di- mension of the output. f (I)</formula><p>is followed by Triplet Loss L f tri and Cross Entropy Loss L f ID . Attribute Heads/Branches. On top of the backbone feature F(I), we add n attribute branches for attribute classification, where n is the number of available attributes in the training dataset, one branch for each attribute. For the i-th attribute branch, we use a spatial and channel attention module to obtain attribute-related feature A i (I) ? R h?w?c as</p><formula xml:id="formula_2">A i (I) = F(I) ? Att i (F(I)),<label>(2)</label></formula><p>where Att i (I) ? R c denotes the response of the attention module.</p><p>To make classification for the i-th attribute, we apply GAP and a FC layer to get a feature vector a i as</p><formula xml:id="formula_3">a i (I) = W a i GAP (A i (I)) + b a i ,<label>(3)</label></formula><p>where W a i and b a i denote the weights and bias of the FC layer, W a i ? R s a ?c and b a i ? R s a , where s a is the predefined size of the output. a i (I) is followed by a classifier with a cross entropy loss L i att to recognize which class it belongs to for the i-th attribute.</p><p>In summary, VAN is trained by minimizing the loss L V AN as</p><formula xml:id="formula_4">L V AN = L f tri + L f ID + ? A n i=1 L i att ,<label>(4)</label></formula><p>where ? A is a hyper-parameter for balancing the importance of V-ReID loss and attribute-related losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint Module</head><p>The Joint Module (JM) is illustrated in <ref type="figure">Figure 2</ref>. JM aims to distill V-ReID helpful information from the attribute features and compensate it to the V-ReID feature for the final feature matching. First, we merge the attribute feature maps from multiple branches to have a unified attribute feature map G(I). Then, we distill discriminative V-ReID helpful information from G(I) and compensate it onto F(I) to create a Joint Feature J(I).</p><p>To encourage a higher discriminative capability of the Joint Feature, we propose an Amelioration Constraint (AC). Attribute Feature G(I). To facilitate the distillation of helpful attribute features, we combine all the attribute feature maps A i (I), where i = 1, ? ? ? , n, to have a unified attribute feature map G(I). We achieve this by summarizing the attribute feature maps followed by a convolution layer and a residual connection as</p><formula xml:id="formula_5">G(I) = n i=1 A i (I) + ? A ( n i=1 A i (I)),<label>(5)</label></formula><p>where ? A is implemented by a 1 ? 1 convolutional layer followed by batch normalization (BN) and ReLU activation, that is,</p><formula xml:id="formula_6">? A (x) = ReLU(W A x), W A ? R c?c .</formula><p>We omit BN to simplify the notation. For the combined attribute feature map G(I), we add supervision from attributes to preserve the attribute information. Given n attributes, m i is the number of classes for the i-th attribute. There are in total n i=1 m i attribute patterns. We apply a GAP layer on G(I) to get the feature vector g(I). Then, the Triplet Loss L g tri is used as supervision to pull the features for the same attribute pattern and push the features for the different attribute patterns. We name this supervision as Attributebased Triplet Loss. Joint Feature J(I). To distill V-ReID-helpful attribute information from G(I) to enhance F(I), we use two convolution layers to have distilled feature G reid (I)</p><formula xml:id="formula_7">G reid (I) = ? g1 (? g2 (G(I))),<label>(6)</label></formula><p>where ? g1 and ? g2 are implemented similarly to ? A but we use a 3 ? 3 convolutional layer instead of 1 ? 1,</p><formula xml:id="formula_8">? g1 (x) = ReLU(W g1 x), ? g2 (x) = ReLU(W g2 x), W g1 ? R c?c and W g2 ? R c?c .</formula><p>By adding G reid (I) onto the V-ReID feature F(I), we have the Joint Feature J(I) as</p><formula xml:id="formula_9">J(I) = F(I) + G reid (I).<label>(7)</label></formula><p>J(I) combines V-ReID information from F(I) and the relevant V-ReID-helpful information from the attributes G(I). Similar to the supervision on F(I), we add Triplet Loss L j tri and Cross Entropy Loss L j ID on the spatially average pooled feature j(I), where j(I) is obtained as</p><formula xml:id="formula_10">j(I) = W j ? GAP (J(I)) + b j ,<label>(8)</label></formula><p>where W j and b j represent the weights and bias of a FC layer, W j ? R s j ?c and b j ? R s j , s j is the predefined dimension of the output. JM is trained by minimizing</p><formula xml:id="formula_11">L JM L JM = L j tri + L j ID + ? G L g tri ,<label>(9)</label></formula><p>where ? G is a hyperparameter balancing the importance of the compensated V-ReID loss and the attribute related loss. Finally, we can train the entire network ANet end-toend by minimizing L</p><formula xml:id="formula_12">L = L JM + ?L V AN ,<label>(10)</label></formula><p>where ? is a hyperparameter to balance the importance of L JM and L V AN . Amelioration Constraint. To further boost the capabilities of the network, we define the Amelioration Constraint (AC). AC aims to explicitly push j(I) to be more discriminative than f (I). We separately apply AC for cross entropy loss and triplet loss. AC for Cross Entropy Loss: For image I, we define it as</p><formula xml:id="formula_13">AC ID (I) = softplus(L j ID (I) ? L f ID (I)),<label>(11)</label></formula><p>where softplus(?) = ln(1 + exp(?)) is a monotonically increasing function that helps to reduce the optimization difficulty by avoiding negative values <ref type="bibr" target="#b25">[26]</ref>. L f ID (I) and L j ID (I) represent the identity cross entropy loss with respect to feature f (I) and j(I), respectively. Minimizing AC ID (I) encourages the network to have a lower classification error for j(I) than that for f (I).</p><p>AC for Triplet Loss: We seek j(I) to represent an enhanced feature of f (I), where j(I) has a higher discriminative capability than f (I). Thus, we encourage the feature distance D(?, ?) between an anchor sample/image I and a positive sample I + to be smaller w.r.t. feature j(?) than feature f (?). Similarly, we encourage the feature distance D(?, ?) between an anchor sample/image I and a negative sample I ? to be larger w.r.t. feature j(?) than feature f (?). Then, AC for triplet loss AC tri is defined as</p><formula xml:id="formula_14">AC tri (I) = softplus(D( j(I), j(I + )) ? D( f (I), f (I + )))+ softplus(D( f (I), f (I ? )) ? D( j(I), j(I ? ))).<label>(12)</label></formula><p>We notice that training with AC ID , AC tri in an end-toend leads to unstable learning. Thus, we follow two steps in training. In the first step, we minimize L. In the second step, we freeze the backbone (that is, all operations before f ) and minimize L . Compared with L in Equation <ref type="formula" target="#formula_0">(10)</ref>, the AC losses are enabled and the losses on feature f are disabled in L as</p><formula xml:id="formula_15">L = L + AC tri + AC ID ? ?(L f tri + L f ID ).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present the datasets used in our experiments, the implementation details, an ablation study and a comparison against the state of the art to validate our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our vehicle re-identification method on three challenging benchmark datasets. For the first two datasets, the validation protocols is based on mean Average Precision (mAP) and Cumulative Matching Curve (CMC) @1 (at rank-1/R1) and @5 (at rank-5/R5) as they have fixed gallery and query sets. For Vehicle-ID, we follow the protocol proposed by the authors of the dataset, which randomly chooses one image of each vehicle ID as gallery and the rest as query. The final R1 and R5 results are reported after repeating this process 10 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We follow other works in the literature to implement the backbone for a fair comparison. We use a modified version of ResNet-50 <ref type="bibr" target="#b15">[16]</ref> with Instance-Batch Normalization (IBN) <ref type="bibr" target="#b28">[29]</ref> and remove the last pooling layer to obtain the feature map F(I) for an image I. Each attention module Att i (I) is based on SE <ref type="bibr" target="#b29">[30]</ref> with the reduction ratio of 16. For the FC layers, we set s a = 128 and s f = s j = 512.</p><p>We use cross entropy loss with label smoothing (LS) regularization <ref type="bibr" target="#b30">[31]</ref> and triplet loss with hard positivenegative mining <ref type="bibr" target="#b31">[32]</ref>, following the Bag-of-Tricks <ref type="bibr" target="#b32">[33]</ref>.</p><p>In one of the datasets, not all input images have attribute labels. For these samples, we simply do not backpropagate the losses from L i att and L g tri . We found this works well since we use batch size of 512 (4 images per ID) and the missing labels are alleviated by the other IDs in the batch. Note that these missing labels do not affect our AC ID and AC tri , so ANet can still learn from those cases.</p><p>The input images are resized to 256?256 pixels and augmented by random horizontal flipping, random zooming and random input erasing <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. All models are trained on 8 V100 GPUs with NVLink for 210 epochs with Amsgrad. An initial learning rate is set to 0.0006 and the learning rate is decayed by 0.1 at epochs 60, 120 and 150. The first learning step minimizes L for the first 150 epochs, then the second step <ref type="table">Table 1</ref>: Ablation study on the effectiveness of our designs. We indicate the feature vector used for testing using the symbol in parenthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VeRi776</head><p>Vehicle optimizes L for 60 epochs. n = 2 for all datasets, where we consider vehicle color (for instance, red, yellow, green) and type (such as sedan and truck). During testing, the feature vectors are L2-normalized for matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Our ablation study contains four subsections. In the first two subsections, we analyze the effectiveness of our ANet and its components (i.e., Joint Module and AC). In the third subsection, we aim to analyze the design of previous methods using attributes represented by VAN. Specifically, we analyze the effects of using attributes to define attention masks, instead of the FC layers, which are common in existing works. The final subsection studies the influence of hyperparameters ? A , ? G and ?, as well as IBN and LS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Effectiveness of using Attributes on V-ReID</head><p>We first evaluate the effects of using attributes in V-ReID and show the comparisons in <ref type="table">Table 1</ref>. Baseline denotes the scheme which generates feature f using only the backbone, without using attribute-related designs. VAN denotes the vanilla scheme that explores attributes as shown in <ref type="figure">Figure 1</ref>, using the same backbone as Baseline.</p><p>For our VAN, we can use the V-ReID feature f (i) (i.e., VAN ( f )), or use the concatenation of f (I) and attribute features a i (I), i = 1, ? ? ? , n (i.e., VAN ( f a)) in inference. We can see that: 1) VAN ( f ), where the attributes regularize the feature learning, outperforms Baseline significantly on Vehicle-ID and VeRi-Wild. Specially, using attributes improves the rank-1 by 0.5% for VeRi776, 2.8% at rank-1 and 3.3% at rank-5 for Vehicle-ID, 6.6% in mAP and 1.3% at rank-1 for VeRi-Wild; 2) using VAN ( f a) has lower performance than VAN ( f ). This is because not all the attribute information a i (I) is equally important for V-ReID. Allocating the relative contributions of each attribute is needed to have satisfactory results. Hence how to distill taskoriented attribute information to efficiently benefit V-ReID is important, which is what our ANet aims to address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">ANet: A Superior Way to Distill Attributes Information</head><p>We propose ANet to distill attribute information for more effective V-ReID. Here we study the effectiveness of our Joint Module design, and the AC losses. <ref type="table">Table 1</ref> shows the comparisons. We can see that: (i) Our final scheme ANet ( j) significantly outperforms the basic network VAN ( f ), by 2.0% in mAP on VeRi776, 1.9%/1.5%/1.5% in Rank-1 on Small/Medium/Large scales of Vehicle-ID, 2.7%/2.7%/3.3% in mAP on Small/Medium/Large scales of VeRi-Wild; (ii) our proposed AC losses generates higher discrimination after the compensation of distilled attribute feature than that before and is very helpful to promote the distill of discriminative information from attribute feature for V-ReID purpose.</p><p>These results show that the interaction between the V-ReID and attribute features of VAN improves the network performance, thanks to the distill of V-ReID oriented attribute features.</p><p>To better understand the effects of ANet, we visualize the attention maps of G(I) and G reid (I) and show some in <ref type="figure">Figure 3</ref>. G(I) encodes generic features of the attributes, where the activations are flatter and do not have a special focus on the vehicle parts. In contrast, G reid (I) represents a portion of the information of G(I) that is helpful for V-ReID. We can observe that the activation maps focus more on the vehicle.</p><p>To further analyze the effectiveness of our proposed ANet, we compare our interaction design with that using attributes as attention, which we refer to as ANet (att). In ANet (att), attention is learned based on attributes using CBAM <ref type="bibr" target="#b37">[38]</ref> and is used to generate attribute-guided features similar to AGNet <ref type="bibr" target="#b7">[8]</ref>. In this Input G(I) G reid (I) <ref type="figure">Figure 3</ref>: Comparison of activation maps. The first row represents the input images, second and third row their corresponding activation maps for G(I) (attribute features) and G reid (I) (attribute features oriented to V-ReID), respectively. The first column is the query image, the second to sixth columns represent the vehicle retrieved at rank-1, rank-2, rank-3, rank-4 and rank-5. case, J(I) is defined through Equation <ref type="formula" target="#formula_0">(14)</ref> as We compare the performance of ANet (att) with our Anet in <ref type="table" target="#tab_2">Table 2</ref>. We can see that using attention directly to increase the interaction between attribute and ReID heads is not as effective as ours. Distilling the ReID-relevant information from the attribute head defined by G reid provides a superior performance. Furthermore, ANet (att) has a performance similar to the simple baseline VAN.</p><formula xml:id="formula_16">J(I) = F(I) ? CBAM(G(I)) + F(I).<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">VAN: Attention vs Fully Connected</head><p>We use VAN as our attribute-based baseline, which is similar to previous works that explore vehicle attributes. However, previous works commonly used simple FC layers, instead of attention blocks for the attribute branches. Using attention facilitates the distillation of attribute features. As shown in <ref type="table" target="#tab_3">Table 3</ref>, attention outperforms the use of FC layers by 1.2% in rank-1 on Vehicle-ID, as well as 1.4% and 1% in mAP on VeRi776 and VeRi-Wild datasets, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Hyperparameter Analysis</head><p>Both hyperparameters ? A (in Equation (4) and ? G (in Equation (9) balance the importance of the attribute information in the total loss. We study their influence and show the results in <ref type="table" target="#tab_4">Table 4</ref>. We can see that assigning the same weight for both attribute and V-ReID signals (e.g., ? A = ? G = 1) provides the best results.</p><p>Interestingly, assigning a low weight to the attribute signals (e.g., ? A = 0.01 and ? G = 0.01) decreases their impact and results in inferior performance. Furthermore, giving high weights to attribute signal (e.g., ? A = 100 and ? G = 100) is better than assigning them with rather low weights. This shows the importance of the attribute information in our pipeline for V-ReID. Finally, ? (in Equation (10) balances the importance of VAN and JM.</p><p>We observe that a high weight to VAN (e.g., ? = 100) significantly decreases the performance, where the contribution of our JM is small. The best weight to combine VAN and JM is ? = 1. Based on this analysis, we set ? A = ? G = ? = 1 and use these values in the remaining experiments.</p><p>In both our baseline scheme and final scheme, we fol-  <ref type="table" target="#tab_5">Tables 5 and 6</ref>, respectively.</p><p>For IBN, we can observe a considerable decrease of 1.7% in mAP on VeRi776 when not using IBN, whereas a decrease of 0.7% in R1. For Vehicle-ID, the difference is also significant, a decrease of 2.4% for R1 and 0.5% for R5. For VeRi-Wild, not using IBN has a smaller effect than on other datasets, that is, 0.2% in both R1 and mAP. Without using LS, there is a decrease of 1.1% in mAP on VeRi776, 1.2% in R1 on Vehicle-ID, 0.1% in mAP on VeRi-Wild, respectively. In general, we can observe that IBN has a more significant importance than LS in the final performance.</p><p>To analyze each of the weights ? A , ? G and ? for our loss functions, we conduct an independent analysis per parameter (for instance, in order to analyze lambda A , we evaluate 5 different values for it and fix ? G = ? = 1). Results are shown in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-Art Methods</head><p>We compare our method with approaches that also use attributes information <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. We also compare our method with the most recent works that leverage clue/approach such as vehicle parsing maps <ref type="bibr" target="#b4">[5]</ref>, vehicle parts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>, GANs <ref type="bibr" target="#b17">[18]</ref>, Teacher-Student (TS) distillation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, camera viewpoints <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>, and Graph Networks (GN) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>. HPGN creates a pyramid of spacial graph network to explore the spatial significance of the backbone tensor. PCRNet studies the correlation between parsed vehicle parts through a graph network. VAnet <ref type="bibr" target="#b10">[11]</ref> learns two metrics for similar viewpoints and different viewpoints in two feature spaces, respectively.</p><p>We also compare against FastReid <ref type="bibr" target="#b42">[43]</ref>, a strong baseline network for re-identification that performs an extensive search of hyperparameters, augmentation methods, and use some architecture design tricks to achieve excellent performance. We also implemented our design on top of it by taking it as our backbone, which we named ANet + FastReid. Note that the reported results of FastReid were obtained by our running of their released code. <ref type="table" target="#tab_7">Tables 7, 8 and 9</ref> show the comparisons on VeRi776, Vehicle-ID, and VeRi-Wild, respectively.</p><p>VeRi776. Compared with attribute-based methods (first group in <ref type="table" target="#tab_7">Table 7</ref>), our scheme ANet + FastReid outperforms the best results in this group by 5.1% in mAP; and 1.5% for rank-1 and rank-5. By comparing with methods that do not use attributes, we can see that it performs the second best in mAP, and achieves the best for rank-1 and rank-5. VKD <ref type="bibr" target="#b41">[42]</ref> is better than ours in mAP and is inferior to ours at rank-1 and rank-5, where VKD uses camera labels in training to be viewpoint-invariant and trains a model based on the Teacher-Student framework.  Vehicle-ID. Our method outperforms attribute-based methods (first group in <ref type="table" target="#tab_8">Table 8</ref>) consistently. For rank-1, our scheme ANet + FastReid outperforms the best attribute-based method by 8.2%, 4.4% and 4.9% for small, medium and large scales, respectively. When compared with methods using other clues, ours achieves in the best results on the large set and competitive performance on the other sets.</p><p>VeRi-Wild. Previous attribute based methods have not yet reported results for this latest dataset. From Table 9, we can see that our schemes ANet and ANet + FastReid achieve the best performance in mAP. is a method based on semantic parsing to describe each vehicle view and region. It has better results on rank-1/rank-5 but it is not as competitive as in the two previous datasets.</p><p>We observed that none of the existing methods consistently achieve the best results on all the datasets. This may be because different datasets have different main challenges. Our proposed ANet shows a more consistent state-of-the-art performance on all the datasets, thanks to the generic capabilities of attributes on V-ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we proposed ANet, a novel framework to leverage attribute information for vehicle reidentification. ANet addresses the problem of lack of interaction between the V-ReID features and attribute features of previous methods. Particularly, we encourage the network to distill task-oriented information from the attribute branches and compensate it into the global V-ReID feature to enhance the discrimination capability of the feature. Evaluation on three datasets shows the effectiveness of our methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figures 1 and 2 (with input feature maps obtained from the VAN as illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>VeRi776 [27]: It contains over 50,000 images of 776 vehicles with 20 camera views. It includes attribute labels for color and type. It considers 576 vehicles for training and 200 vehicles for test. because the images were captured for a period of one month and include severe changes in background, illumination, viewpoint and occlusions.</figDesc><table><row><cell>? Vehicle-ID [15]: It includes 221,763 images of</cell></row><row><cell>26,267 vehicles, captured from either front or back</cell></row><row><cell>views. The training set contains 110,178 images of</cell></row><row><cell>13,134 vehicles and the test set contains 111,585</cell></row><row><cell>images of 13,133 vehicles. The testing data is fur-</cell></row><row><cell>ther divided into three sets with 200 (small), 1,600</cell></row><row><cell>(medium) and 2,400 (large) vehicles. Some images</cell></row><row><cell>in this dataset have attribute labels for vehicle color</cell></row><row><cell>and type but not for all the images.</cell></row></table><note>? VeRi-Wild [28]: This is the largest vehicle re- identification dataset. It considers 174 camera views, 416,314 images and 40,671 IDs. It includes attribute labels for vehicle model, color and type. The testing set is divided into three sets with 3,000 (small), 5,000 (medium) and 10,000 (large) IDs. This is the most challenging dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our interaction design with that using attributes as attention. Note that the results for Vehicle-ID and VeRi-Wild are reported using their small scale test set.</figDesc><table><row><cell></cell><cell>VeRi776</cell><cell>Vehicle-ID VeRi-Wild</cell></row><row><cell>Method</cell><cell cols="2">mAP R1 R1 R5 mAP R1</cell></row><row><cell>Baseline</cell><cell cols="2">78.1 96.1 81.3 94.4 78,1 94.6</cell></row><row><cell cols="3">ANet (att) 78.2 96.1 83.9 96.2 84.9 95.5</cell></row><row><cell>ANet</cell><cell cols="2">80.1 96.9 86.0 97.4 85.8 95.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of choice for implementation of attribute branches for the attribute-based baseline VAN. fc represents an implementation using fully connected layers and att represents an implementation based on SE attention blocks. Results for Vehicle-ID and VeRi-Wild are reported using their small scale test set.</figDesc><table><row><cell></cell><cell>VeRi776</cell><cell>Vehicle-ID VeRi-Wild</cell></row><row><cell cols="3">Method mAP R1 R1 R5 mAP R1</cell></row><row><cell>fc</cell><cell cols="2">76.7 95.8 83.3 96.0 82.1 94.3</cell></row><row><cell>att</cell><cell cols="2">78.1 96.6 84.1 96.5 83.1 94.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the influence of ? A , ? G and ?. We evaluate using VeRi776 by keeping the non-tested hyperparameters fixed. For example, in order to analyze ? A , we set ? G = 1, ? = 1.</figDesc><table><row><cell>Results</cell><cell>? A 0.01 0.1 1.0 10.0 100.0</cell></row><row><cell>mAP</cell><cell>59.1 79.4 80.1 77.5 65.9</cell></row><row><cell>R1</cell><cell>88.6 96.5 97.1 96.2 91.4</cell></row><row><cell>R5</cell><cell>94.7 98.6 98.6 98.4 96.1</cell></row><row><cell>Results</cell><cell>? G 0.01 0.1 1.0 10.0 100.0</cell></row><row><cell>mAP</cell><cell>60.3 61.7 80.1 76.7 68.1</cell></row><row><cell>R1</cell><cell>90.0 90.9 97.1 95.8 91.6</cell></row><row><cell>R5</cell><cell>95.2 95.5 98.6 98.6 96.7</cell></row><row><cell>Results</cell><cell>? 0.01 0.1 1.0 10.0 100.0</cell></row><row><cell>mAP</cell><cell>78.3 79.7 80.1 64.4 56.4</cell></row><row><cell>R1</cell><cell>96.8 96.4 97.1 92.1 87.9</cell></row><row><cell>R5</cell><cell>98.3 98.2 98.6 96.3 94.5</cell></row><row><cell cols="2">low the common practice and use Instance Batch Nor-</cell></row><row><cell cols="2">malization (IBN) and Label Smoothing (LS). Here, we</cell></row><row><cell cols="2">study the influence of IBN and LS on the performance</cell></row><row><cell cols="2">of our ANet and show the results in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Influence of Instance Batch Normalization (IBN). Results for Vehicle-ID and VeRi-Wild are reported using their small scale test set.</figDesc><table><row><cell></cell><cell>VeRi776</cell><cell>Vehicle-ID VeRi-Wild</cell></row><row><cell>Method</cell><cell cols="2">mAP R1 R1 R5 mAP R1</cell></row><row><cell cols="3">Baseline w/o IBN 69.5 91.0 71.5 83.1 69.2 89.2</cell></row><row><cell>Baseline</cell><cell cols="2">78.1 96.1 81.3 94.4 78,1 94.6</cell></row><row><cell>ANet w/o IBN</cell><cell cols="2">78.4 96.2 84.4 96.9 85.6 95.7</cell></row><row><cell>ANet</cell><cell cols="2">80.1 96.9 86.0 97.4 85.8 95.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Influence of Label Smoothing (LS). Results for Vehicle-ID and VeRi-Wild are reported using their small scale test set.</figDesc><table><row><cell></cell><cell>VeRi776</cell><cell>Vehicle-ID VeRi-Wild</cell></row><row><cell>Method</cell><cell cols="2">mAP R1 R1 R5 mAP R1</cell></row><row><cell cols="3">Baseline w/o LS 73.2 93.2 73.8 88.4 74.3 90.1</cell></row><row><cell>Baseline</cell><cell cols="2">78.1 96.1 81.3 94.4 78,1 94.6</cell></row><row><cell>ANet w/o LS</cell><cell cols="2">79.0 96.8 85.8 97.2 85.7 95.5</cell></row><row><cell>ANet</cell><cell cols="2">80.1 96.9 86.0 97.4 85.8 95.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of our proposed method against the state of the art on VeRi776. The first and second best results are marked by bold and underline, respectively.</figDesc><table><row><cell>Method</cell><cell cols="4">Clue/Approach mAP R1 R5</cell></row><row><cell>PAMAL [39]</cell><cell>attributes</cell><cell cols="3">45.0 72.0 88.8</cell></row><row><cell>MADVR [40]</cell><cell>attributes</cell><cell cols="3">61.1 89.2 94.7</cell></row><row><cell>DF-CVTC [6]</cell><cell>attributes</cell><cell cols="3">61.0 91.3 95.7</cell></row><row><cell>PAMTRI [7]</cell><cell>attributes</cell><cell cols="3">71.8 92.8 96.9</cell></row><row><cell>AGNet [8]</cell><cell>attributes</cell><cell cols="3">71.5 95.6 96.5</cell></row><row><cell>SAN [9]</cell><cell>attributes</cell><cell cols="3">72.5 93.3 97.1</cell></row><row><cell>StRDAN [10]</cell><cell>attributes</cell><cell>76.1</cell><cell>-</cell><cell>-</cell></row><row><cell>VAnet [11]</cell><cell>viewpoint</cell><cell cols="3">66.3 89.7 95.9</cell></row><row><cell>PRND [20]</cell><cell>veh. parts</cell><cell cols="3">74.3 94.3 98.6</cell></row><row><cell>UMTS [41]</cell><cell>TS</cell><cell cols="3">75.9 95.8 -</cell></row><row><cell>PCRNet [13]</cell><cell cols="4">GN + parsing 78.6 95.4 98.4</cell></row><row><cell>SAVER [18]</cell><cell>GAN</cell><cell cols="3">79.6 96.4 98.6</cell></row><row><cell>PVEN [5]</cell><cell>parsing</cell><cell cols="3">79.5 95.6 98.4</cell></row><row><cell>HPGN [19]</cell><cell>GN</cell><cell cols="3">80.1 96.7 -</cell></row><row><cell>VKD [42]</cell><cell cols="4">viewpoint + TS 82.2 95.2 98.0</cell></row><row><cell>Baseline</cell><cell>attributes</cell><cell cols="3">78.1 96.1 98.3</cell></row><row><cell>ANet (Ours)</cell><cell>attributes</cell><cell cols="3">80.1 97.1 98.6</cell></row><row><cell>FastReid [43]</cell><cell>backbone</cell><cell cols="3">81.0 97.1 98.3</cell></row><row><cell>ANet + FastReid (Ours)</cell><cell>attributes</cell><cell cols="3">81.2 96.8 98.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison of our proposed method against the state of the art on Vehicle-ID. The first and second best results are marked by bold and underline, respectively.</figDesc><table><row><cell>Small</cell><cell>Medium</cell><cell>Large</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison of our proposed method against the state of the art on VeRi-Wild. The first and second best results are marked by bold and underline, respectively. 96.7 99.2 77.0 95.4 98.8 69.7 93.4 97.8</figDesc><table><row><cell>Small</cell><cell>Medium</cell><cell>Large</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was done while the first author is affiliated with Microsoft Corp. We are thankful to Microsoft Research, S?o Paulo Research Foundation (FAPESP grant #2017/12646-3), National Council for Scientific and Technological Development (CNPq grant #309330/2018-1) and Coordination for the Improvement of Higher Education Personnel (CAPES) for their financial support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved Person Re-Identification Based on Saliency and Semantic Parsing with Deep Neural Network Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Quispe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pedrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quispe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Db-Net</forename><surname>Top</surname></persName>
		</author>
		<title level="m">Top DropBlock for Activation Enhancement in Person Re-Identification, in: 25th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2980" to="2987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Anastasiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<title level="m">IEEE Computer Vision and Pattern Recognition Conference Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="452" to="460" />
		</imprint>
	</monogr>
	<note>The 2019 AI City Challenge</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A Survey of Advances in Vision-based Vehicle Re-Identification, Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ullah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="50" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parsing-based View-aware Embedding Network for Vehicle Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08997</idno>
		<title level="m">Attributes Guided Feature Learning for Vehicle Re-identification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pamtri: Pose-Aware Multi-Task Learning for Vehicle Re-Identification using Highly Randomized Synthetic Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03872</idno>
		<title level="m">Attributeguided Feature Learning Network for Vehicle Re-identification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stripe-based and Attribute-Aware Network: A Two-Branch Deep Model for Vehicle Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement Science and Technology</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="608" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vehicle Re-Identification with Viewpoint-aware Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8282" to="8291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06023</idno>
		<title level="m">Part-Guided Attention Learning for Vehicle Re-identification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Beyond the Parts: Learning Multi-view Cross-part Correlation for Vehicle Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="907" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RAM: A Region-aware Deep Model for Vehicle Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Deep Relative Distance Learning: Tell the Difference between Similar Vehicles</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2167" to="2175" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving Person Re-identification by Attribute and Identity Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Devil is in the Details: Self-Supervised Attention for Vehicle Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorramshahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="369" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring Spatial Significance via Hybrid Pyramidal Graph Network for Vehicle Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14684</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Part-Regularized Near-Duplicate Vehicle Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3997" to="4005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person Re-identification by Deep Learning Attribute-Complementary Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attribute-driven Feature Disentangling and Temporal Aggregation for Video Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4913" to="4922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive Learning for Person Re-identification with One Example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2872" to="2881" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transferable Joint Attribute-identity Deep Learning for Unsupervised Person Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Coarse-to-Fine Structured Feature Embedding for Vehicle Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6853" to="6860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Style Normalization and Restitution for Generalizable Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Provid: Progressive and Multimodal Vehicle Reidentification for Large-Scale Urban Surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="645" to="658" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Veri-wild: A Large Dataset and a new Method for Vehicle Re-identification in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3235" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bag of Tricks and a Strong Baseline for Deep Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropblock: A Regularization Method for Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10093</idno>
		<title level="m">Torchreid: A Library for Deep Learning Person Re-Identification in Pytorch</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Omni-Scale Feature Learning for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06827</idno>
		<title level="m">Learning Generalisable Omni-Scale Representations for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<title level="m">CBAM: Convolutional Block Attention Module</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note>European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Partial Attention and Multi-Attribute Learning for Vehicle Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tumrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-Attribute Driven Vehicle Re-Identification with Spatial-Temporal Re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="858" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Uncertainty-Aware Multi-Shot Knowledge Distillation for Image-Based Object Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Robust Re-Identification by Multiple Views Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02631</idno>
		<title level="m">FastReID: A Pytorch Toolbox for General Instance Re-identification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
