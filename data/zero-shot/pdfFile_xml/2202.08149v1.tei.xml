<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELF-SUPERVISED CLASS-COGNIZANT FEW-SHOT CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-17">February 17, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ojas</forename><surname>Kishore Shirekar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shell Global Solutions International B.V</orgName>
								<orgName type="institution">Delft University of Technology (TU Delft)</orgName>
								<address>
									<settlement>Delft, Amsterdam</settlement>
									<country>The Netherlands, The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Jamali-Rad</surname></persName>
							<email>hadi.jamali-rad@shell.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shell Global Solutions International B.V</orgName>
								<orgName type="institution">Delft University of Technology (TU Delft)</orgName>
								<address>
									<settlement>Delft, Amsterdam</settlement>
									<country>The Netherlands, The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SELF-SUPERVISED CLASS-COGNIZANT FEW-SHOT CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-17">February 17, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised learning is argued to be the dark matter of human intelligence 1 . To build in this direction, this paper focuses on unsupervised learning from an abundance of unlabeled data followed by fewshot fine-tuning on a downstream classification task. To this aim, we extend a recent study on adopting contrastive learning for self-supervised pre-training by incorporating class-level cognizance through iterative clustering and re-ranking and by expanding the contrastive optimization loss to account for it. To our knowledge, our experimentation both in standard and cross-domain scenarios demonstrate that we set a new state-of-the-art (SoTA) in (5-way, 1 and 5-shot) settings of standard mini-ImageNet benchmark as well as the (5-way, 5 and 20-shot) settings of cross-domain CDFSL benchmark. Our code and experimentation can be found in our GitHub repository: https://github.com/ojss/c3lr 2 .</p><p>The authors thank Delft University of Technology and Shell Global Solutions B.V. for permission to publish this work. 1 Yann LeCun's note; Meta AI blog post on self-supervised learning. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Few-shot learning has received an upsurge of attention recently because it highlights a fundamental gap between human intelligence and data-hungry supervised deep learning methods. We humans can learn in a self-supervised fashion and/or with very little supervision. To tackle this challenge, few-shot classification is cast as the task of predicting class labels for a set of unlabeled data points (query set) given only a small set of labeled ones (support set). The query and support samples are typically drawn from the same distribution. Few-shot classification approaches are typically comprised of two sequential phases <ref type="bibr" target="#b0">[Medina et al., 2020</ref><ref type="bibr" target="#b1">, Chen et al., 2020</ref><ref type="bibr" target="#b2">, Ji et al., 2019</ref><ref type="bibr" target="#b3">, Ye et al., 2020</ref>: (i) pre-training on an abundant dataset (sometimes called "base"), followed by (ii) fine-tuning on an unseen dataset containing "novel" classes. Typically, the target classes in pre-training and fine-tuning phases are mutually exclusive. In this paper, we focus on self-supervised (also sometimes interchangeably called "unsupervised" in the literature) setting where we have no access to the class labels of the base dataset in the pre-training phase or their distribution.</p><p>The art here is to devise a synthetic class label assignment technique and corresponding loss function in the pretraining phase to efficiently transfer the learning to the fine-tuning phase. To this aim, studies have proposed two different approaches. The first approach follows a meta-learning strategy to create (synthetic) "tasks" similar to the the downstream episodic training in the fine-tuning phase <ref type="bibr" target="#b4">[Finn et al., 2017</ref><ref type="bibr" target="#b5">, Hsu et al., 2018</ref><ref type="bibr" target="#b6">, Khodadadeh et al., 2019</ref>. The second one follows some sort of transfer learning approach, where a representation learning step in the pre-training phase is followed by episodic fine-tuning <ref type="bibr" target="#b0">[Medina et al., 2020</ref><ref type="bibr" target="#b7">, Tian et al., 2020</ref><ref type="bibr" target="#b8">, Dhillon et al., 2019</ref>. In the latter case, typically a feature extractor (encoder) is trained using metric learning to capture the global structure of the unlabeled data. Next, a simple predictor (typically a linear layer) is adopted in conjunction with the extractor for quick</p><formula xml:id="formula_0">L x1 x2 x3 xi's augmented Q times f ? Re-Ranking R HDBSCAN C1 C2 C3 C Pre-training m1 f ? (x i ) f ? (x i,1 ) f ? (x i,2 ) f ? (x i,3 ) m2 m3 S Q (N -way, K-shot) tasks Supervised Fine-Tuning f ? c 1 c 2 c 3 f ? (x q i )</formula><p>Figure 1: C 3 LR schematic view and training procedure. In the figure, x q i is an image sampled from the query set Q.</p><p>adaptation to the novel classes in the fine-tuning phase. The better the feature extractor captures the global structure of the unlabeled data, the less the predictor requires training samples and the faster it adapts itself to the unseen classes in the fine-tuning phase.</p><p>Recent studies <ref type="bibr" target="#b0">[Medina et al., 2020</ref><ref type="bibr" target="#b9">, Chen et al., 2021</ref><ref type="bibr" target="#b8">, Dhillon et al., 2019</ref> demonstrate that the second approach based on transfer learning outperforms meta-learning based methods in cross-domain settings, where the training and novel classes come from totally different distributions. Their results also show that a properly-devised transfer learning based unsupervised approach comes pretty close to the performance of a fully supervised counterpart <ref type="bibr" target="#b0">[Medina et al., 2020</ref><ref type="bibr" target="#b2">, Ji et al., 2019</ref>, something that we will also confirm through experimentation. Most recently, a new state-of-the-art (SoTA) in self-supervised few shot classification has been set by extending the prototypical networks (ProtoNets) <ref type="bibr" target="#b10">[Snell et al., 2017]</ref> using a contrastive loss <ref type="bibr" target="#b1">[Chen et al., 2020]</ref>. This approach (called ProtoTransfer <ref type="bibr" target="#b0">[Medina et al., 2020]</ref>) constructs a contrastive metric embedding that clusters unlabeled prototypical samples and their augmentations. Inspired by this idea, we propose class-cognizant contrastive learning (C 3 LR, Algorithm 1) to further extend it to incorporate class-level insights from the global structure of data. This is done via an unsupervised iterative re-ranking and clustering step resulting in clusters of unlabeled embeddings followed by a modified contrastive loss now containing a term that specifically promotes this class-level global structure. Our experimentation demonstrates that C 3 LR outperforms its predecessor ProtoTransfer in (5-way, 1 and 5-shot) settings of Ominglot <ref type="bibr">[Lake et al., 2015]</ref> and mini-Imagenet <ref type="bibr" target="#b12">[Vinyals et al., 2016]</ref> benchmarks by about 1% and 2%+, respectively. The performance improvement goes up to 4.5% in the cross-domain setting of the CDFSL benchmark <ref type="bibr" target="#b13">[Guo et al., 2019]</ref>. As a result, to our best knowledge, C 3 LR sets a new SoTA for most challenging settings of mini-ImageNet and CDFSL benchmarks.</p><p>2 Class-Cognizant Contrastive Learning (C 3 LR)</p><p>In this section, we first describe our problem formulation. We then discuss the two phases of the proposed approach: self-supervised pre-training and few-shot supervised fine-tuning. The mechanics of the proposed approach and a sketch of the training procedure is shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Let us denote the training data of size M as</p><formula xml:id="formula_1">D tr = {(x i , y i )} M i=1 with (x i , y i )</formula><p>representing an image x i and its class label y i . In the pre-training phase, we take L random samples from D tr and augment each sample Q times by drawing augmentation functions ? q (.), ?q ? [Q] from the set A. This results in a batch of size B = (Q + 1)L total samples. Note that the data labels are unknown in the pre-training phase. In the fine-tuning phase, we deal with the so-called episodic training on a set of tasks T containing N classes each with K samples per task drawn from the test dataset</p><formula xml:id="formula_2">D tst = {(x i , y i )} M i=1 of size M .</formula><p>From now on, we refer to this task construct as (N -way, K-shot) denoted by (N, K). An episode consists of a labeled support set, S, from which the model learns and an unlabeled query set, Q, on which the model predicts. Note that both S and Q contain a set of tasks of the form (N, K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Supervised Pre-Training</head><p>The fact that we do not have access to class labels calls for a self-supervised pre-training stage. As discussed earlier, we build upon the idea of employing contrastive learning for prototypical transfer learning following the footsteps of <ref type="bibr" target="#b0">[Medina et al., 2020]</ref>. The high-level idea here is to not only enforce the latent embeddings of augmented images come</p><formula xml:id="formula_3">Algorithm 1: Class-Cognizant Contrastive Learning (C 3 LR) Require: L, Q, f ? , A, ?, d[?, ?] 1 while not done do 2 Sample minibatch {x i } L i=1 3 forall i ? {1, . . . , L} do 4 forall q ? {1, . . . , Q} do 5x i,q = ? q (x i ); ? q ? A. 6 end 7 end 8 R = ReRank f ? {x i } L i=1 , f ? {x i, q } L,Q i=1,q=1 9 C = {C 1 , C 2 , . . . , C P } ? HDBSCAN(R) 10 M = {m p } P p=1 ; m p = x j ?Cp xj |Cp| 11 let r(i, q, p) = ? log exp(?d[f ? (xi,q),mp]) P p=1 exp(?d[f ? (xi,q),mp]) 12 let (i, q) = ? log exp(?d[f ? (xi,q),f ? (xi)]) L k=1 exp(?d[f ? (xi,q),f ? (x k )]) 13 L 1 = 1 LQ P p=1 L i=1 Q q=1 r(i, q, p) 14 L 2 = 1 LQ L i=1 Q q=1 (i, q) 15 L = L 1 + L 2 16 ? ? ? ? ?? ? L 17 end</formula><p>close to that of the source image in the embedding space (the classical contrastive setting), but also enforce embeddings of the images belonging to each cluster (and their augmentations) come closer to each other, for which a preceding unsupervised cluster formation step is required. This can help enforce similar classes into separate clusters, which will in turn be used as additional information in a modified two-term contrastive loss in Algorithm 1. Let us walk you through the process in further details.</p><p>Algorithm 1 starts with batch generation (lines 2 to 7): each mini-batch consists of L random samples {x i } L i=1 from D tr , where x i is treated as a 1-shot support sample for which we create Q randomly augmented versionsx i,q as query samples (line 5). This leads to a batch size of B = (Q + 1)L. Then embeddings are generated by passing the samples through an encoder f ? network. This is where the first major modification to ProtoTransfer <ref type="bibr" target="#b0">[Medina et al., 2020]</ref> comes into play. Before the contrastive loss comes into action, we apply re-ranking and clustering (lines 8 to 10) to discover class-level global structure of data and enforce similar classes into separate clusters in the embedding space. Note that this step remains to be unsupervised in that the class labels are not required. The re-ranking step (line 8) makes use of the k-reciprocal nearest neighbors as the distance metric between latent embeddings <ref type="bibr" target="#b14">[Zhong et al., 2017]</ref>, which has been shown to outperform the Euclidean distance <ref type="bibr" target="#b2">[Ji et al., 2019]</ref> when used for subsequent clustering. HDBSCAN clustering <ref type="bibr" target="#b15">[McInnes et al., 2017]</ref> is then applied on the re-ranked embeddings R and returns a set of clusters populated in C. HDBSCAN is versatile enough to discover and create required number of clusters P . With clusters at hand, we are now in a position to extend the standard loss proposed in <ref type="bibr" target="#b0">[Medina et al., 2020]</ref> to contain a class-cognizant term (in lines 11 and 13), with lines 12 and 14 reflecting on the classical contrastive loss of ProtoTransfer <ref type="bibr" target="#b0">[Medina et al., 2020]</ref>. This new loss term L 1 enables a progressive improvement in class-level cluster formation and in turn learning similar representations for cluster members, while L 2 encourages clustering of the embeddings of the augmented query samples {f ? (x i,q )} around their prototypes {f ? (x i )}. Here, both terms use an Euclidean distance metric in the embedding space denoted by d[?, ?]. Finally, the new loss L = L 1 + L 2 is optimized with mini-batch stochastic gradient decent with respect to the parameters ? of the encoder networks f ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Supervised Fine-Tuning</head><p>The pre-trained encoder f ? will be used for the downstream few-shot classification task. To this aim, following <ref type="bibr" target="#b0">[Medina et al., 2020</ref><ref type="bibr" target="#b10">, Snell et al., 2017</ref>, we concatenate f ? with a single-layer nearest-neighbor classifier f ? (resulting in a similar architecture as in ProtoNet <ref type="bibr" target="#b10">[Snell et al., 2017]</ref>) and fine-tune this last layer. In this phase, we first calculate the class prototypes c n (embeddings) for class n using the encoder f ? on the support set S n :</p><formula xml:id="formula_4">c n = 1 |S n | (xi,yi)?Sn f ? (x i ).</formula><p>These prototypes are then used to initialize the classifier f ? following <ref type="bibr" target="#b0">[Medina et al., 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimentation</head><p>In this section, we first discuss our experimental setup; we then present our numerical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets. We conduct several in-domain experiments to benchmark C 3 LR. For this purpose, we make use of commonly adopted datasets Omniglot <ref type="bibr">[Lake et al., 2015]</ref> and mini-Imagenet <ref type="bibr" target="#b12">[Vinyals et al., 2016]</ref> to compare against unsupervised few-shot learning approaches. Omniglot contains 1623 different handwritten characters borrowed from 50 unique alphabets out of which we use 1028 characters for training, 172 for validation and 423 for testing. We resize the grayscale images to 28 ? 28 pixels. Mini-ImageNet contains 100 classes with 600 samples in each class amounting to a total of 60, 000 images that we resize to 84 ? 84 pixels. Out of the 100 classes, we use 64 classes for training, 16 for validation and 20 for testing. For both datasets, the settings are the most commonly adopted ones in literature <ref type="bibr" target="#b12">[Vinyals et al., 2016</ref><ref type="bibr" target="#b0">, Medina et al., 2020</ref><ref type="bibr" target="#b2">, Ji et al., 2019</ref><ref type="bibr" target="#b6">, Khodadadeh et al., 2019</ref>. The augmentations (in A) used for the experimentations follow <ref type="bibr" target="#b0">[Medina et al., 2020]</ref>. We also compare our method on a more challenging cross-domain few-shot learning (CDFSL) benchmark <ref type="bibr" target="#b13">[Guo et al., 2019]</ref>. This benchmark consists of four datasets with increasing similarities to mini-ImageNet. In that order, we have grayscale chest X-ray images from ChestX <ref type="bibr" target="#b16">[Wang et al., 2017]</ref>, dermatological skin lesion images from ISIC2018 , satellite aerial images from EuroSAT <ref type="bibr" target="#b18">[Helber et al., 2017]</ref>, and crop disease images from CropDiseases <ref type="bibr" target="#b19">[Mohanty et al., 2016]</ref>. We also use Caltech-UCSD Birds (CUB) dataset <ref type="bibr" target="#b20">[Wah et al., 2011]</ref> for further analysis of cross-domain performance. CUB is composed of 11, 788 images from 200 unique bird species. We use 100 images for training, 50 for validation and 50 for test.</p><p>Training. The Conv4 model <ref type="bibr" target="#b12">[Vinyals et al., 2016]</ref> is pre-trained on the respective training splits of the datasets, with an initial learning rate of 0.001, multiplied by 0.5 every 25, 000 steps via the Adam optimizer <ref type="bibr" target="#b21">[Kingma and Ba, 2014]</ref>. Based on the derivations in <ref type="bibr" target="#b10">[Snell et al., 2017]</ref> and similar usage in <ref type="bibr" target="#b0">[Medina et al., 2020]</ref>, we initialize the classification layer f ? with weights set to W n = 2c n and biases set to b n = ? c n 2 . For validation, we create 15 (N -way, K-shot) tasks using the validation split from which the corresponding validation accuracy and loss are calculated. Experiments involving CDFSL benchmark follow <ref type="bibr" target="#b13">[Guo et al., 2019</ref><ref type="bibr" target="#b0">, Medina et al., 2020</ref>, where we pre-train a ResNet10 encoder using C 3 LR on mini-ImageNet images of size 224 ? 224 for 400 epochs with the Adam optimizer and a constant learning rate of 0.001.</p><p>Evaluation scenarios and baseline. Our testing scheme uses 600 test episodes on which the pre-trained encoder (using C 3 LR) is fine-tuned and tested. All our results indicate 95% confidence intervals over 3 runs each with 600 test episodes. The standard deviation values are thus calculated according to the 3 runs to provide more solid measures for comparison. For our in-domain benchmarks, we test on (5-way, 1-shot) and (5-way, 5-shot) classification tasks. While our cross-domain testing is done using (5-way, 5-shot) and (5-way, 20-shot) classification tasks. We compare our performance with a suit of recent self-supervised few-shot baselines such as ProtoTransfer <ref type="bibr" target="#b0">[Medina et al., 2020]</ref>, UFLST <ref type="bibr" target="#b2">[Ji et al., 2019]</ref>, LASIUM <ref type="bibr" target="#b22">[Khodadadeh et al., 2020]</ref> and CACTUS <ref type="bibr" target="#b5">[Hsu et al., 2018]</ref>, to name a few. Furthermore, we also compare with a set of supervised approaches (such as MAML <ref type="bibr" target="#b4">[Finn et al., 2017]</ref>, ProtoNet <ref type="bibr" target="#b10">[Snell et al., 2017]</ref> , etc.) the best performing of which are obviously expected to outperform ours as well as other self-supervised methodologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Evaluation</head><p>In-domain evaluation. <ref type="table">Table 1</ref> summarizes our performance evaluation results on Omniglot and mini-ImageNet datasets for (N -way, K-shot) scenarios with N = 5 and K = 1, 5. The top section compares the performance of the proposed approach (C 3 LR) with the most recent relevant self-supervised competitors. As can be seen, for Omniglot, we outperform ProtoTransfer <ref type="bibr" target="#b0">[Medina et al., 2020]</ref> (which we build on) by about 1% in both K = 1, 5 shot scenarios. We score the second overall best in (5-way, 1-shot) falling behind UFLST <ref type="bibr" target="#b2">[Ji et al., 2019]</ref>. For the mini-ImageNet benchmark, to our knowledge, we set a new SoTA outperforming ProtoTransfer by 2%+. Interestingly, our performance beats some of the supervised baselines (bottom section of the table) adopting similar encoder architecture Conv4 for mini-ImageNet and comes close to K = 5-shot performances on Omniglot. Obviously, the SoTA supervised few-shot <ref type="table">Table 1</ref>: Accuracy (%? std.) for (N -way, K-shot) classification tasks. Style: best and second best.</p><p>Omniglot mini-ImageNet Method(N, K) (5,1) (5,5) (5,1) (5,5)</p><p>CACTUs-MAML <ref type="bibr" target="#b5">[Hsu et al., 2018]</ref> 68.84 ? 0.80 87.78 ? 0.50 39.90 ? 0.74 53.97 ? 0.70 CACTUs-ProtoNet <ref type="bibr" target="#b5">[Hsu et al., 2018]</ref> 68.12 ? 0.84 83.58 ? 0.61 39.18 ? 0.71 53.36 ? 0.70 UMTRA <ref type="bibr" target="#b6">[Khodadadeh et al., 2019]</ref> 83.80 95.43 39.93 50.73 AAL-ProtoNet <ref type="bibr" target="#b23">[Antoniou and Storkey, 2019]</ref> 84.66 ? 0.70 89.14 ? 0.27 37.67 ? 0.39 40.29 ? 0.68 AAL-MAML++ <ref type="bibr" target="#b23">[Antoniou and Storkey, 2019]</ref> 88.40 ? 0.75 97.96 ? 0.32 34.57 ? 0.74 49.18? 0.47 UFLST <ref type="bibr" target="#b2">[Ji et al., 2019]</ref> 97.03 99.19 33.77 ? 0.70 45.03 ? 0.73 ULDA-ProtoNet <ref type="bibr" target="#b24">[Qin et al., 2020]</ref> --40.63 ? 0.61 55.41 ? 0.57 ULDA-MetaOptNet <ref type="bibr" target="#b24">[Qin et al., 2020]</ref> --40.71 ? 0.62 54.49 ? 0.58 U-SoSN+ ArL <ref type="bibr" target="#b25">[Zhang et al., 2021]</ref> --41.13 ? 0.84 55.39 ? 0.79 LASIUM <ref type="bibr" target="#b22">[Khodadadeh et al., 2020]</ref> 83.26 ? 0.55 95.29 ? 0.22 40.19 ? 0.58 54.56 ? 0.55 ProtoTransfer <ref type="bibr">(L = 50)</ref>  <ref type="bibr" target="#b0">[Medina et al., 2020]</ref> 88.00 ? 0.64 96.48 ? 0.26 45.67 ? 0.79 62.99 ? 0.75 ProtoTransfer <ref type="bibr">(L = 200)</ref> 88.   learning approaches have the advantage of having access to the all the labels, as such due to the supervision signal, are expected to outperform the unsupervised approaches like ours.</p><p>Cross-domain evaluation. So far we have demonstrated that the proposed approach excels for in-domain scenarios. The next step is to assess the performance under more challenging cross-domain scenarios ( <ref type="table" target="#tab_1">Table 2 and Table 3)</ref> where we pre-train on a certain dataset in an unsupervised fashion, then fine-tune and test on a different dataset. <ref type="table" target="#tab_1">Table 2</ref> illustrates the results of a Conv4 encoder trained on CUB and tested on tasks derived from mini-ImageNet. Here again C 3 LR shows a clear improvement of 3%+ compared to ProtoTransfer (with pre-training sample sizes L = 50, 200).</p><p>The important message here is that the proposed approach enhances ProtoTransfer in generalizing to truly unseen data. To further investigate the performance on cross-domain scenarios, we next focus on CDFSL benchmark <ref type="bibr" target="#b13">[Guo et al., 2019]</ref> containing several datasets. Here, we pre-train on mini-ImageNet and fine-tune and test on ChestX <ref type="bibr" target="#b16">[Wang et al., 2017]</ref>, ISIC2018 , EuroSAT <ref type="bibr" target="#b18">[Helber et al., 2017]</ref>, and CropDiseases <ref type="bibr" target="#b19">[Mohanty et al., 2016]</ref>. We compare the performance against ProtoTransfer and two of its variants with UMTRA <ref type="bibr" target="#b6">[Khodadadeh et al., 2019]</ref> as pre-training strategy (all proposed in <ref type="bibr" target="#b0">[Medina et al., 2020]</ref>). We also compare with a couple of closely related supervised approaches from <ref type="bibr" target="#b13">[Guo et al., 2019]</ref>, for the sake of reference. As can be seen, except for ChestX where we marginally come short of ProtoTransfer, for the other three datasets we outperform the second best competitor (ProtoTransfer) by about 0.5%+ to 4.5%+ with the most significant improvement in the case of EuroSAT. Interestingly, once again the performance of C 3 LR is not far off that of the related supervised approaches (bottom of the table) even sometimes outperforming the supervised approaches especially in (5-way, 20-shot) scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Concluding Remarks</head><p>Inspired by the idea of using contrastive learning for unsupervised few-shot classification, we build upon the recently proposed idea of ProtoTransfer <ref type="bibr" target="#b0">[Medina et al., 2020]</ref> by incorporating class cognizance through: (i) an unsupervised iterative re-ranking and clustering step, followed by (ii) an adjusted optimization loss formulation. We demonstrate that our proposed approach (C 3 LR) offers considerable performance improvement above its predecessor ProtoTransfer in both in/cross-domain few-shot classification scenarios setting a new SoTA in mini-ImageNet and CDFSL benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%? std.) for (N -way, K-shot) classification on mini-ImageNet with pre-training on CUB.</figDesc><table><row><cell>Training</cell><cell>Testing</cell><cell>(5,1)</cell><cell>(5,5)</cell></row><row><cell>ProtoTransfer (L = 50) [Medina et al., 2020]</cell><cell>ProtoTune [Medina et al., 2020]</cell><cell>35.37 ? 0.63</cell><cell>52.38 ? 0.66</cell></row><row><cell>ProtoTransfer (L = 200)</cell><cell>ProtoTune</cell><cell>34.67 ? 0.84</cell><cell>51.45 ? 0.72</cell></row><row><cell>C 3 LR (ours)</cell><cell>ProtoTune</cell><cell>39.61 ? 1.11</cell><cell>55.53 ? 1.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (%? std.) of (N -way, K-shot) classification on the CDFSL benchmark. Style: best and second best.<ref type="bibr" target="#b0">Medina et al., 2020]</ref> 24.94 ? 0.43 28.04 ? 0.44 39.21 ? 0.53 44.62 ? 0.49 74.91 ? 0.72 80.42 ? 0.66 79.81 ? 0.65 86.84 ? 0.50 UMTRA-ProtoTune<ref type="bibr" target="#b0">[Medina et al., 2020]</ref> 25.00 ? 0.43 30.41 ? 0.44 38.47 ? 0.55 51.60 ? 0.54 68.11 ? 0.70 81.56 ? 0.54 82.67 ? 0.60 92.04 ? 0.43 ProtoTransfer<ref type="bibr" target="#b0">[Medina et al., 2020]</ref> 26.71 ? 0.46 33.82 ? 0.48 45.19 ? 0.56 59.07 ? 0.55 75.62 ? 0.67 86.80 ? 0.42 86.53 ? 0.56 95.06 ? 0.32 C 3 LR (ours) 26.00 ? 0.41 33.39 ? 0.47 45.93 ? 0.54 59.95 ? 0.53 80.32 ? 0.65 88.09 ? 0.45 87.90 ? 0.55 95.38 ? 0.31 ProtoNet [Guo et al., 2019] (sup.) 24.05 ? 1.01 28.21 ? 1.15 39.57 ? 0.57 49.50 ? 0.55 73.29 ? 0.71 82.27 ? 0.57 79.72 ? 0.67 88.15 ? 0.51 Pre+Mean-Cent. [Guo et al., 2019] (sup.) 26.31 ? 0.42 30.41 ? 0.46 47.16 ? 0.54 56.40 ? 0.53 82.21 ? 0.49 87.62 ? 0.34 87.61 ? 0.47 93.87 ? 0.68 Pre+Linear [Guo et al., 2019] (sup.) 25.97 ? 0.41 31.32 ? 0.45 48.11 ? 0.64 59.31 ? 0.48 79.08 ? 0.61 87.64 ? 0.47 89.25 ? 0.51 95.51 ? 0.31</figDesc><table><row><cell>Method(N, K)</cell><cell>(5,5)</cell><cell>(5,20)</cell><cell>(5,5)</cell><cell>(5,20)</cell><cell>(5,5)</cell><cell>(5,20)</cell><cell>(5,5)</cell><cell>(5,20)</cell></row><row><cell></cell><cell>ChestX</cell><cell></cell><cell>ISIC</cell><cell></cell><cell></cell><cell>EuroSAT</cell><cell cols="2">CropDiseases</cell></row><row><cell>UMTRA-ProtoNet [</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised prototypical transfer learning for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnout</forename><surname>Devos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grossglauser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11325</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised few-shot learning via self-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12178</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Few-shot learning via embedding adaptation with set-toset functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8808" to="8817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised meta-learning for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislau</forename><surname>Boloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="266" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02729</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised learning for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1745" to="1749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1126/SCIENCE.AAB3050/SUPPL{_}FILE/LAKE-SM.PDF</idno>
		<ptr target="https://www.science.org" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07200</idno>
		<title level="m">Tajana Rosing, and Rogerio Feris. A New Benchmark for Evaluation of Cross-Domain Few-Shot Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">hdbscan: Hierarchical density based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Astels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Source Softw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadi</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Marchetti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using Deep Learning for Image-Based Plant Disease Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salath?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Plant Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1419</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised meta-learning through latent-space interpolation in generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharare</forename><surname>Zehtabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Vahidian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislau</forename><surname>B?l?ni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10236</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Assume, augment and learn: Unsupervised few-shot meta-learning via random labels and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09884</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Diversity helps: Unsupervised few-shot learning via distribution shift-based data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiexin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05805</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking class relations: Absoluterelative supervised and unsupervised few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlei</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9432" to="9441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
