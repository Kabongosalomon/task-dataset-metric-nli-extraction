<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangfu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud registration is a key task in many computational fields. Previous correspondence matching based methods require the inputs to have distinctive geometric structures to fit a 3D rigid transformation according to point-wise sparse feature matches. However, the accuracy of transformation heavily relies on the quality of extracted features, which are prone to errors with respect to partiality and noise. In addition, they can not utilize the geometric knowledge of all the overlapping regions. On the other hand, previous global feature based approaches can utilize the entire point cloud for the registration, however they ignore the negative effect of non-overlapping points when aggregating global features. In this paper, we present OM-Net, a global feature based iterative network for partialto-partial point cloud registration. We learn overlapping masks to reject non-overlapping regions, which converts the partial-to-partial registration to the registration of the same shape. Moreover, the previously used data is sampled only once from the CAD models for each object, resulting in the same point clouds for the source and reference. We propose a more practical manner of data generation where a CAD model is sampled twice for the source and reference, avoiding the previously prevalent over-fitting issue. Experimental results show that our method achieves state-of-the-art performance compared to traditional and deep learning based methods. Code is available at https://github.com/megviiresearch/OMNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud registration is a fundamental task that has been widely used in various computational fields, e.g., augmented reality <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref>, 3D reconstruction <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">19]</ref> and autonomous driving <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b9">10]</ref>. It aims to predict a 3D rigid transformation aligning two point clouds, which may be potentially obscured by partiality and contaminated by noise. <ref type="bibr">*</ref>   Iterative Closest Point (ICP) <ref type="bibr" target="#b2">[3]</ref> is a well-known algorithm for the registration problem, where 3D transformations are estimated iteratively by singular value decomposition (SVD) given the correspondences obtained by the closest point search. However, ICP easily converges to local minima because of the non-convexity problem. For this reason, many methods <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b35">35]</ref> are proposed to improve the matching or search larger transformation space, and one prominent work is the Go-ICP <ref type="bibr" target="#b35">[35]</ref>, which uses a branch-and-bound algorithm to cross the local minima. Unfortunately, it is much slower than the original ICP. All these methods are sensitive to the initial positions.</p><p>Recently, several deep learning (DL) based approaches are proposed <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">37]</ref> to handle the large rotation angles. Roughly, they could be divided into two categories: correspondence matching based methods and global feature based methods. Deep Closest Point (DCP) <ref type="bibr" target="#b32">[32]</ref> determines the correspondences from learned features. DeepGMR <ref type="bibr" target="#b37">[37]</ref> integrates Gaussian Mixture Model (GMM) to learn pose-invariant point-to-GMM correspondences. However, they do not take the partiality of inputs into consideration. PRNet <ref type="bibr" target="#b33">[33]</ref>, RPMNet <ref type="bibr" target="#b36">[36]</ref> and IDAM <ref type="bibr" target="#b16">[17]</ref> are presented to mitigate this problem by using Gumbel-Softmax [15] with Sinkhorn normalization <ref type="bibr" target="#b31">[31]</ref> or a convolutional neural network (CNN) to calculate matching matrix. However, these methods require the inputs to have distinctive local geometric structures to extract reliable sparse 3D feature points. As a result, they can not utilize the geometric knowledge of the entire overlapping point clouds. In contrast, global feature based methods overcome this issue by aggregating global features before estimating transformations, e.g., PointNetLK <ref type="bibr" target="#b0">[1]</ref>, PCRNet <ref type="bibr" target="#b27">[27]</ref> and Feature-metric Registration (FMR) <ref type="bibr" target="#b12">[13]</ref>. However, all of them ignore the negative effect of non-overlapping regions.</p><p>In this paper, we propose OMNet: an end-to-end iterative network that estimates 3D rigid transformations in a coarseto-fine manner while preserving robustness to noise and partiality. To avoid the negative effect of non-overlapping points, we predict overlapping masks for the two inputs separately at each iteration. Given the accurate overlapping masks, the non-overlapping points are rejected during the aggregation of global features, which converts the partial-to-partial registration to the registration of the same shape. As such, regressing rigid transformation becomes easier given global features without interferes. This desensitizes the initial position of the inputs and enhances the robustness to noise and partiality. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the robustness of our method to the inputs with different overlapping ratios. Experiments show that our approach achieves state-ofthe-art performance compared with previous algorithms. Furthermore, ModelNet40 <ref type="bibr" target="#b34">[34]</ref> dataset is adopted for the registration <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">37]</ref>, which has been originally applied to the task of classification and segmentation. Previous works follow the data processing of Point-Net <ref type="bibr" target="#b21">[21]</ref>, which has two problems: (1) a CAD model is sampled only once during the point cloud generation, yielding the same source and the reference points, which often causes over-fitting issues; <ref type="bibr" target="#b1">(2)</ref> ModelNet40 dataset involves some axisymmetrical categories where it is reasonable to obtain an arbitrary angle on the symmetrical axis. We propose a more suitable method to generate a pair of point clouds. Specifically, the source and reference point clouds are randomly sampled from the CAD model separately. Meanwhile, the data of axisymmetrical categories are removed. In summary, our main contributions are:</p><p>? We propose a global feature based registration network OMNet, which is robust to noise and different partial manners by learning masks to reject non-overlapping regions. Mask prediction and transformation estimation can be mutually reinforced during iteration. ? We expose the over-fitting issue and the axisymmetrical categories that existed in the ModelNet40 dataset when adopted to the registration task. In addition, we propose a more suitable method to generate data pairs for the registration task. ? We provide qualitative and quantitative comparisons with other works under clean, noisy and different partial datasets, showing state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Correspondence Matching based Methods. Most correspondence matching based methods solve the registration problem by alternating two steps: (1) set up correspondences between the source and reference point clouds; <ref type="bibr" target="#b1">(2)</ref> compute the least-squares rigid transformation between the correspondences. ICP <ref type="bibr" target="#b2">[3]</ref> estimates correspondences using spatial distances. Subsequent variants of ICP improve performance by detecting keypoints <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">23]</ref> or weighting correspondences <ref type="bibr" target="#b11">[12]</ref>. However, due to the non-convexity of the first step, they are often strapped into local minima. To address this, Go-ICP <ref type="bibr" target="#b35">[35]</ref> uses a branch-and-bound strategy to search the transformation space at the cost of a much slower speed. Recently proposed Symmetric ICP <ref type="bibr" target="#b22">[22]</ref> improves the original ICP by designing the objective function. Instead of using spatial distances, PFH <ref type="bibr" target="#b25">[25]</ref> and FPFH <ref type="bibr" target="#b24">[24]</ref> design rotation-invariant descriptors and establish correspondences from handcrafted features. To avoid computation of RANSAC <ref type="bibr" target="#b7">[8]</ref> and nearest-neighbors, FGR <ref type="bibr" target="#b40">[40]</ref> uses alternating optimization techniques to accelerate iterations. More recent DL based method DCP <ref type="bibr" target="#b32">[32]</ref> replaces the handcrafted feature descriptor with a CNN. Deep-GMR <ref type="bibr" target="#b37">[37]</ref> further estimates the points-to-components correspondences in the latent GMM. In summary, the main problem is that they require the inputs to have distinctive geometric structures, so as to promote sparse matched points. However, not all regions are distinctive, resulting in a limited number of matches or poor distributions. In addition, the transformation is calculated only from matched sparse points and their local neighbors, leaving the rest of the points untouched. In contrast, our work can use the predicted overlapping regions to aggregate global features.</p><p>Global Feature based Methods. Different from correspondence matching based methods, the previous global feature based methods compute rigid transformation from the entire point clouds (including overlapping and nonoverlapping regions) of the two inputs without correspondences. PointNetLK <ref type="bibr" target="#b0">[1]</ref> pioneers these methods, which adapts PointNet <ref type="bibr" target="#b21">[21]</ref> with the Lucas &amp;Kanade (LK) algorithm <ref type="bibr" target="#b18">[18]</ref> into a recurrent neural network. PCRNet <ref type="bibr" target="#b27">[27]</ref> improves the robustness against the noise by alternating the LK algorithm with a regression network. Furthermore, FMR <ref type="bibr" target="#b12">[13]</ref> adds a decoder branch and optimizes the global feature distance of the inputs. However, they all ignore the negative effect of the non-overlapping points and fail to register partial-to-partial inputs. Our network can deal with partiality and shows robustness to different partial manners.  Partial-to-partial Registration Methods. Registration of partial-to-partial point clouds is presented as a more realistic problem by recent works <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b16">17]</ref>. In particular, PRNet <ref type="bibr" target="#b33">[33]</ref> extends DCP as an iterative pipeline and tackles the partiality by detecting keypoints. Moreover, the learnable Gumble-Softmax [15] is used to control the smoothness of the matching matrix. RPMNet <ref type="bibr" target="#b36">[36]</ref> further utilizes Sinkhorn normalization <ref type="bibr" target="#b31">[31]</ref> to encourage the bijectivity of the matching matrix. However, they suffer from the same problem as the correspondence matching based methods, i.e., they can only use sparse points. In contrast, our method can utilize information from the entire overlapping points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our pipeline is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. We represent the 3D transformation in the form of quaternion q and translation t. At each iteration i, the source point cloud X is transformed by the rigid transformation q i?1 , t i?1 estimated from the previous step into the transformed point cloudX i . Then, the global features of two point clouds are extracted by the feature extraction module (Sec. C.1). Concurrently, the hybrid features from two point clouds are fused and fed to an overlapping mask prediction module (Sec. C.2) to segment the overlapping region. Meanwhile, a transformation regression module (Sec. C.3) takes the fused hybrid features as input and outputs the transformation q i , t i for the next iteration. Finally, the loss functions are detailed in Sec. C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global Feature Extraction</head><p>The feature extraction module aims to learn a function f (?), which can generate distinctive global features F X and F Y from the source point cloud X and the reference point cloud Y respectively. An important requirement is that the orientation and the spatial coordinates of the original input should be maintained, so that the rigid transformation can be estimated from the difference between the two global features. Inspired by PointNet <ref type="bibr" target="#b21">[21]</ref>, at each iteration, the global features of inputX i and Y are given by:</p><formula xml:id="formula_0">F i ? = max{M i?1 ? ? f (?)}, ? ? {X i , Y},<label>(1)</label></formula><p>where f (?) denotes a multi-layer perceptron network (MLP), which is fed withX i and Y to generate point-wise features f ? X and f i Y . M i?1 X and M i?1 Y are the overlapping masks ofX i and Y, which are generated by the previous step and detailed in Sec. C.2. The point-wise features fX and f Y are aggregated by a max-pool operation max{?}, which can deal with an arbitrary number of orderless points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overlapping Mask Prediction</head><p>In partial-to-partial scenes, especially those including the noise, there exists non-overlapping regions between the input point clouds X and Y. However, not only does it have no contributions to the registration procedure, but it also interferes to the global feature extraction, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. RANSAC <ref type="bibr" target="#b7">[8]</ref> is widely adopted in traditional methods to find the inliers when solving the most approximate matrix for the scene alignment. Following a similar idea, we propose a mask prediction module to segment the overlapping region automatically. Refer to PointNet <ref type="bibr" target="#b21">[21]</ref>, point segmentation only takes one point cloud as input and requires a combination of local and global knowledge. However, overlapping region prediction requires additional geometric information from both two input point clouds X and Y. We can achieve this in a simple yet highly effective manner.</p><p>Specifically, at each iteration, the global features F ? X and F i Y are fed back to point-wise features by concatenating with each of the point features f ? X and f i Y accordingly. Then, a MLP g(?) is applied to fuse the above hybrid features, which are further used to segment overlapping regions and regress the rigid transformation. So we can obtain two overlapping masks M iX and M i Y as,</p><formula xml:id="formula_1">M iX = h g f ? X ? F ? X ? F i Y ? M i?1 X ,<label>(2)</label></formula><formula xml:id="formula_2">M i Y = h g f i Y ? F i Y ? F ? X ? M i?1 Y ,<label>(3)</label></formula><p>where h(?) denotes the overlapping prediction network, which consists of several convolutional layers followed by a softmax layer. We define the fused point-wise features of the inputs X and Y produced by g(?) as g X and g Y . ? denotes the concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Rigid Transformation Regression</head><p>Given the point-wise features g iX and g i Y at each iteration i, we concatenate them with the features outputting from intermediate layers of the overlapping mask prediction module. Therefore, the features used to regress transformation can be enhanced by the classification information in the mask prediction branch. Meanwhile, the features used to predict the masks benefit from the geometric knowledge in the transformation branch. Then, the concatenated features are fed to the rigid transformation regression network, which produces a 7D vector, with the first 3 values of the 7D vector we use to represent the translation vector t ? R 3 and the last 4 values represent the 3D rotation in the form of quaternion <ref type="bibr" target="#b29">[29]</ref> q ? R 4 , q T q = 1. r(?) represents the whole process in every iteration i, i.e.</p><formula xml:id="formula_3">q i , t i = r max{g iX ? h iX ?M i?1 X ? g i Y ? h i Y ?M i?1 Y } ,<label>(4)</label></formula><p>where h iX and h i Y are the concatenated features from the mask prediction branch. M i?1 X and M i?1 Y are used to eliminate the interference of the non-overlapping points.</p><p>After N iterations, we obtain the overall transformation between the two inputs by accumulating all the estimated transformations at each iteration.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Functions</head><p>We simultaneously predict overlapping masks and estimate rigid transformations, so that two loss functions are proposed to supervise the above two procedures separately.</p><p>Mask Prediction Loss. The goal of mask prediction loss is to segment the overlapping region in the input point clouds X and Y. To balance the contributions of positive and negative samples, the frequency weighted softmax cross-entropy loss is exploited at each iteration i, i.e. <ref type="bibr" target="#b4">(5)</ref> where M p denotes the probability of points belonging to the overlapping region, and ? is the overlapping ratio of the inputs. We define the assumed mask label M g to represent the overlapping region of the two inputs, which is computed by setting fixed threshold (set to 0.1) for the closest point distances between the source that transformed by the ground-truth transformation and reference. Each element is</p><formula xml:id="formula_4">L mask = ??M i g log(M i p )?(1??)(1?M i g ) log(1?M i p ),</formula><formula xml:id="formula_5">Mg = 1 if point xj corresponds to y k 0 otherwise .<label>(6)</label></formula><p>The current mask is estimated based on the previous mask, so the label needs to be recalculated for each iteration.</p><p>Transformation Regression Loss. Benefiting from the continuity of the quaternions, it is able to employ a fairly straightforward strategy for training, measuring the deviation of {q, t} from ground truth for the generated point cloud pairs. So the transformation regression loss at iteration i is</p><formula xml:id="formula_6">Lreg = |q i ? qg| + ??t i ? tg?2,<label>(7)</label></formula><p>where subscript g denotes ground-truth. We notice that using the combination of ? 1 and ? 2 distance can marginally improve performance during the training and the inference. ? is empirically set to 4.0 in most of our experiments. The overall loss is the sum of the two losses:</p><formula xml:id="formula_7">L total = L mask + Lreg.<label>(8)</label></formula><p>We compute the loss for every iteration, and they have equal contribution to the final loss during training. <ref type="table">TS  OS  TS  OS  TS  OS  TS  OS  TS  OS  TS   ICP [3</ref> </p><formula xml:id="formula_8">RMSE(R) MAE(R) RMSE(t) MAE(t) Error(R) Error(t) Method OS</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first describe the pre-processing for the datasets and the implementation details of our method in Sec. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Implementation Details</head><p>ModelNet40. We use the ModelNet40 dataset to test the effectiveness following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b16">17]</ref>. The Mod-elNet40 contains CAD models from 40 categories. Previous works use processed data from PointNet <ref type="bibr" target="#b21">[21]</ref>, which has two issues when adopted to registration task: (1) for each object, it only contains 2,048 points sampled from the CAD model. However, in realistic scenes, the points in X have no exact correspondences in Y. Training on this data cause over-fitting issue even adding noise or resampling, which is demonstrated by the experiment shown in our supplementary; (2) it involves some axisymmetrical categories, including bottle, bowl, cone, cup, flower pot, lamp, tent and vase, <ref type="figure" target="#fig_4">Fig. 4</ref> shows some examples. However, giving fixed ground-truths to axisymmetrical data is illogical, since it is possible to obtain arbitrary angles on the symmetrical axis for accurate registration. Fixing the label on symmetrical axis makes no sense. In this paper, we propose a proper manner to generate data. Specifically, we uniformly sample 2,048 points from each CAD model 40 times with different random seeds, then randomly choose 2 of them as X and Y. It guarantees that we can obtain C 2 40 = 780 combinations for each object. We denote the data that points are sampled only once from CAD models as once-sampled (OS) data, and refer our data as twice-sampled (TS) data. Moreover, we simply remove the axisymmetrical categories.</p><p>To evaluate the effectiveness and robustness of our network, we use the official train and test splits of the first 14 categories (bottle, bowl, cone, cup, flower pot and lamp are removed) for training and validation respectively, and the test split of the remaining 18 categories (tent and vase are removed) for test. This results in 4,196 training, 1,002 validation, and 1,146 test models. Following previous works <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b16">17]</ref>, we randomly generate three Euler angle rotations within [0 ? , 45 ? ] and translations within [?0.5, 0.5] on each axis as the rigid transformation.</p><p>Stanford 3D Scan. We use the Stanford 3D Scan dataset <ref type="bibr" target="#b6">[7]</ref> to test the generalizability of our method. The dataset has 10 real scans. The partial manner in PRNet <ref type="bibr" target="#b33">[33]</ref> is applied to generate partially overlapping point clouds.</p><p>7Scenes. 7Scenes <ref type="bibr" target="#b30">[30]</ref> is a widely used registration benchmark where data is captured by a Kinect camera in indoor environments. Following <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b12">13]</ref>, multiple depth images are projected into point clouds, then fused through truncated signed distance function (TSDF). The dataset is divided into 293 and 60 scans for training and test. The partial manner in PRNet <ref type="bibr" target="#b33">[33]</ref> is applied. Implementation Details. Our network architecture is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. We run N = 4 iterations during training and test. Nevertheless, the {q, t} gradients are stopped at the beginning of each iteration to stabilize the training. Since the masks predicted by the first iteration may be inaccurate at the beginning of training, some overlapping points may be misclassified and affect the sequent iterations, we apply the masks after the second iteration. We train our network with Adam <ref type="bibr" target="#b15">[16]</ref> optimizer for 260k iterations. The initial learning rate is 0.0001 and is multiplied by 0.1 after 220k iterations. The batch size is set to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baseline Algorithms</head><p>We compare our method to traditional methods: ICP <ref type="bibr" target="#b2">[3]</ref>, Go-ICP <ref type="bibr" target="#b35">[35]</ref>, Symmetric ICP <ref type="bibr" target="#b22">[22]</ref>, FGR <ref type="bibr" target="#b40">[40]</ref>, as well as recent DL based works: PointNetLK <ref type="bibr" target="#b0">[1]</ref>, DCP <ref type="bibr" target="#b32">[32]</ref>, RPM-Net <ref type="bibr" target="#b36">[36]</ref>, FMR <ref type="bibr" target="#b12">[13]</ref>, PRNet <ref type="bibr" target="#b33">[33]</ref>, IDAM <ref type="bibr" target="#b16">[17]</ref> and Deep-GMR <ref type="bibr" target="#b37">[37]</ref>. We use implementations of ICP and FGR in Intel Open3D <ref type="bibr" target="#b41">[41]</ref>, Symmetric ICP in PCL <ref type="bibr" target="#b26">[26]</ref> and the others released by their authors. Moreover, the test set is fixed by setting random seeds. Note that the normals used in FGR and RPMNet are calculated after data pre-processing, which is slightly different from the implementation in RPMNet. We use the supervised version of FMR.</p><p>Following <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b36">36]</ref>, we measure anisotropic errors: root mean squared error (RMSE) and mean absolute error (MAE) of rotation and translation, and isotropic errors:</p><formula xml:id="formula_9">Error(R) = ? R ?1 g R p , Error(t) = ?t g ? t p ? 2 ,<label>(9)</label></formula><p>where R g and R p denote the ground-truth and predicted rotation matrices converted from the quaternions q g and q p respectively. All metrics should be zero if the rigid alignment is perfect. The angular metrics are in units of degrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on ModelNet40</head><p>To evaluate the effectiveness of different methods, we conduct several experiments in this section. The data preprocessing settings of the first 3 experiments are the same as PRNet <ref type="bibr" target="#b33">[33]</ref> and IDAM <ref type="bibr" target="#b16">[17]</ref>. In addition, the last experiment shows the robustness of our method to different partial manners, which is used in RPMNet <ref type="bibr" target="#b36">[36]</ref>. Unseen Shapes. In this experiment, we train models on training set of the first 14 categories and evaluate on validation set of the same categories without noise. Note that all points in X have exact correspondences in Y for the OS data. We partial X and Y by randomly placing a point in space and computing its 768 nearest neighbors respectively, which is the same as used in <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b16">17]</ref>. All DL based methods are trained independently on both OS and TS data. <ref type="table" target="#tab_2">Table 1</ref>(a) shows the results.</p><p>We can find that ICP <ref type="bibr" target="#b2">[3]</ref> performs poorly because of the large difference in initial positions. Go-ICP <ref type="bibr" target="#b35">[35]</ref> and FGR <ref type="bibr" target="#b40">[40]</ref> achieve better performances, which are comparable to some DL based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref>. Note that the large performance gap of FGR on two different data is caused by the calculation manner of normals. We use normals that are computed after data pre-processing, so that normals of X and Y are different in our TS data. In addition, the results of IDAM <ref type="bibr" target="#b16">[17]</ref> are marginally worse than PRNet <ref type="bibr" target="#b33">[33]</ref> because of the fixing manner of the test data, which is used in other DL based methods. Our method achieves very accurate registration and ranks first in all metrics. Example results on TS data are shown in <ref type="figure">Fig. 6(a)</ref>.</p><p>Unseen Categories. We evaluate the performance on unseen categories without noise in this experiment. Models are trained on the first 14 categories and tested on the other 18 categories. The data pre-processing is the same as the first experiment. The results are summarized in <ref type="table" target="#tab_2">Table 1</ref>(b). We can find that the performances of all DL based methods become marginally worse without training on the same categories. Nevertheless, traditional algorithms are not affected so much because of the handcrafted features. Our approach outperforms all the other methods. A qualitative comparison of the registration results can be found in <ref type="figure">Fig. 6(b)</ref>.</p><p>Gaussian Noise. In this experiment, we add noises that sampled from N (0, 0.01 2 ) and clipped to [?0.05, 0.05], then repeat the first experiment (unseen shapes). Table 1(c) shows the results. FGR is sensitive to noise, so that it performs much worse than the noise-free case. All DL based methods get worse with noises injected on the OS data. The performances of correspondences matching based methods (DCP, PRNet and IDAM) show an opposite tendency on the TS data compared to the global feature based methods (PointNetLK, FMR and ours), since the robustness of local feature descriptor is improved by the noise augmentation during training. Our method achieves the best performance. Example results are shown in <ref type="figure">Fig. 6(c)</ref>.   <ref type="table">Table 2</ref>. Results on the twice-sampled (TS) unseen categories with Gaussian noise using the partial manner of RPMNet.</p><p>Different Partial Manners. We notice that the previous works <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36]</ref> use different partial manners. To evaluate the effectiveness on different partial data, we also test the performance of different algorithms on the test set used in <ref type="bibr" target="#b36">[36]</ref>. We retrain all DL based methods and show the results of the most difficult situation (unseen categories with Gaussian noise) in <ref type="table">Table 2</ref>. For details about the partial manners, please refer to our supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Real Data</head><p>To further evaluate the generalizability, we conduct experiments on the Stanford 3D Scan and 7Scenes datasets. Since the Stanford 3D Scan dataset only has 10 real scans, we directly use the model trained on the ModelNet40 without fine-tuning. Some qualitative examples are shown in <ref type="figure" target="#fig_8">Fig. 9</ref>. Furthermore, we evaluate our method on the 7Scenes indoor dataset. The point clouds are normalized into the unit sphere. Our model is trained on 6 categories (Chess, Fires, Heads, Office, Pumpkin and Stairs) and tested on the other category (Redkitchen). <ref type="figure" target="#fig_0">Fig. 10</ref> shows some examples. For more results, please refer to our supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>We perform ablation studies on the unseen shapes with noise TS data to show the effectiveness of our components and settings. As shown in <ref type="table">Table 3</ref>, we denote our model that removed the following components as Baseline (B): Mask prediction module (M), Mask prediction Loss (ML), Fusion layers (F) before the regression module, and Connection (C) between mask prediction and regression modules. Besides, we only use top-k points based on the mask prediction probabilities to estimate rigid transformations. Moreover, we set different ? in the loss function.  <ref type="table">Table 3</ref>. Ablation studies of each component and different settings.</p><p>We can see that without being supervised by the mask prediction loss, it has no improvement based on the baseline, which shows that the mask prediction can not be trained unsupervised. Comparing the third to the fifth lines with the baseline, we can find that all the components improve the performance. Since we do not estimate the matching candidates among the overlapping points, the top-k points from the source and reference may not be correspondent and distributed in the point clouds centrally, so that the results of top-k models are worse than using the entire masks. Furthermore, we adjust the ? in the loss function. Since the data generation manner of <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36]</ref> constrain the translation within [?0.5, 0.5] as we use the ? 2 loss for the translation, the translation loss is smaller than the quaternion, so that a large ? aims to form comparable terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this section, we conduct several experiments to better understand how various settings affect our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effects of Mask</head><p>To have a better intuition about the overlapping masks, we visualize the intermediate results in <ref type="figure" target="#fig_5">Fig. 5</ref>. We reshape the global feature vector of length 1,024 into a 32?32 square matrix and compute the error between the transformed sourceX and reference Y. At the first few iterations, the global feature differences are large, and the inputs are poorly aligned given inaccurate overlapping masks. With continuous iterating, the global feature difference becomes extremely small, while the alignment and predicted overlapping masks are almost perfect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Robustness Against Noise</head><p>To further demonstrate the robustness of our method, we train and test our models on ModelNet40 under different noise levels, as shown in <ref type="figure">Fig. 7</ref>. We add noise sampled from N (0, ? 2 ) and clipped to [?0.05, 0.05]. The data is the same as the third experiment in Sec 4.3. Our method achieves comparable performance under various noise levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Different Overlapping Ratio</head><p>We test the best models of all methods from the first experiment in Sec. 4.3 on the ModelNet40 TS validation set with the overlapping ratio decreasing from 1.0 to 0.1. We first partial X, then randomly select two adjacent parts from overlapping and non-overlapping regions of Y. <ref type="figure">Fig. 8</ref> shows the results. Unfortunately, DeepGMR fails to obtain sensible results. Our method shows the best performance.    <ref type="figure" target="#fig_0">Figure 10</ref>. Example results on 7Scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented the OMNet, a novel method for partial-to-partial point cloud registration. Previous global feature based methods pay less attention to partiality. They treat the input points equally, which are easily disturbed by the non-overlapping regions. Our method learns the overlapping masks to reject non-overlapping points for robust registration. Besides, we propose a practical data generation manner to solve the over-fitting issue and remove the axisymmetrical categories in the ModelNet40 dataset. Experiments show the state-of-the-art performance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>This supplementary material provides more details on experiments in the main paper and includes more experiments to validate and analyze our proposed method.</p><p>In Sec. B, we describe details in two data generation manners for point cloud registration, which are proposed by PRNet <ref type="bibr" target="#b33">[33]</ref> and RPMNet <ref type="bibr" target="#b36">[36]</ref> separately. In Sec. C, we show more experimental results including the performance on the validation and test sets, which are generated by the above two pre-processing manners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details in Experiments</head><p>In this section, we describe two data preparation manners for the partial-to-partial point cloud registration. One of the manners proposed by PRNet <ref type="bibr" target="#b33">[33]</ref> is detailed in Sec. B.1, while the other used in RPMNet <ref type="bibr" target="#b36">[36]</ref> is described in Sec. B.2. <ref type="figure" target="#fig_0">Fig. 11</ref> illustrates some examples of the partialto-partial data, which shows the difference between these two pre-processing manners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Data Generation Manner of PRNet</head><p>We use this manner to generate data for the first three experiments on the ModelNet40 dataset in our main paper. Two point clouds are randomly chosen from 40 sampled point clouds as the source point cloud X and reference point cloud Y respectively, each of which contains 2,048 points. Along each axis, we randomly draw a rigid transformation: the rotation along each axis is sampled in [0 ? , 45 ? ] and the translation is in [?0.5, 0.5]. The rigid transformation is applied to Y, leading to X. After that, we simultaneously partial X and Y by randomly placing a point in space and computing its 768 nearest neighbors in X and Y respectively. The left column in <ref type="figure" target="#fig_0">Fig. 11</ref> shows some examples. However, the point clouds X and Y are similar in most cases, which means that the overlapping ratio is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Data Generation Manner of RPMNet</head><p>We use this manner to generate data for the last experiment on the ModelNet40 dataset in our main paper. We use the same approach as which in RPMNet to obtain the source point cloud X and the reference point cloud Y. For each point cloud, we sample a half-space with a random direction and shift it such that approximately 70% of the points are retained. Then, the point clouds are downsampled to 717 points to maintain a similar point density as the previous experiments. We sample rotations by sampling three Euler angle rotations in the range [0 ? , 45 ? ] and translations in the range [?0.5, 0.5] on each axis. The rigid transformation is applied to X, leading to Y, which is opposite to PRNet. The right column in <ref type="figure" target="#fig_0">Fig. 11</ref> shows some examples. The points in X and Y are more decentralized than those generated in the manner of PRNet, which means that the  overlapping ratio is small in some cases. As a result, it is more difficult to register with this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Experiments on ModelNet40</head><p>In this section, we provide more experimental results on ModelNet40 <ref type="bibr" target="#b34">[34]</ref> dataset to validate the robustness and effectiveness of our method. First, we show the decrement of performances between evaluating on the once-sampled (OS) and the twice-sampled (TS) data in Sec. C.1, which demonstrates the over-fitting issue. Then, we show the results on the test set that generated in the manner of PRNet in Sec. C.2, and the results on the dataset that generated in the manner of RPMNet are shown in Sec. C.3. Besides, the comparison of speed shows the computational efficiency of our method in Sec. C.4. Finally, Sec. C.5 explores that how many iterations are needed.  <ref type="table" target="#tab_2">512  33 37  73  15  79  138  58  27  9  24  1024  56 92  77  17  84  158  115  28  9  25  2048  107 237  83  26  114 295  271  33  9  27  4096  271 673  89  88  -764  726  62  10  32   Table 5</ref>. Speed comparison for registering a point cloud pair of various sizes (in milliseconds). The missing result in the table is due to the limitation in GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Over-fitting Issue</head><p>In this experiment, we demonstrate that deep learning (DL) based methods can easily over-fit the original data distribution even with added noises, as shown in <ref type="table" target="#tab_6">Table 4</ref>. Note that we train and evaluate on 8 axisymmetrical categories, where point clouds are only rotated at the z-axis. We can find that all partial-to-partial methods can achieve good performances on the OS data, however, performances are obviously decreasing when only changing the data sampling manner from OS to TS. As a result, all the methods can over-fit the distribution of OS data, while failing to register the axisymmetrical TS data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Results on PRNet dataset</head><p>In our main paper, we only show the results on the validation set with Gaussian noise generated in the partial manner of PRNet <ref type="bibr" target="#b33">[33]</ref>. To further demonstrate the robustness of our method, we show results on the unseen categories with Gaussian noise. We add noises sampled from N (0, 0.01 2 ) and clipped to [?0.05, 0.05] on each axis, then repeat the second experiment (unseen categories) in our main paper. <ref type="table" target="#tab_9">Table 6</ref> shows the performances of various methods. Our method achieves accurate registration and ranks first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Results on RPMNet dataset</head><p>In this subsection, we show 4 experimental results on the ModelNet40 dataset with or without Gaussian noise that generated in the data partial manner of RPMNet <ref type="bibr" target="#b36">[36]</ref>.</p><p>Unseen Shapes. In this experiment, we train the models on the training set of the first 14 categories and evaluate the registration performances on the validation set of the same categories without noise. <ref type="table">Table 7</ref>(a) shows the results.</p><p>We can find that all traditional methods and most DL based methods perform poorly because of the large difference in initial positions and partiality. Note that the normals are calculated after the data pre-processing, so that the normals of points in X can be different from their correspondences in Y. Although Go-ICP <ref type="bibr" target="#b35">[35]</ref> attempts to improve the performance of the original ICP <ref type="bibr" target="#b2">[3]</ref> by adopting a bruteforce branch-and-bound strategy, it may not suitable for this scene and brings negative implications. Our method outperforms all the traditional and DL based methods except 3 metrics on the OS data compared with RPMNet.  <ref type="bibr" target="#b40">[40]</ref> is sensitive to noise, so that it performs much worse than the noise-free case. Almost all the DL based methods become worse with noise injected. Our method achieves the best performance compared to all competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Efficiency</head><p>We profile the inference times in <ref type="table">Table 5</ref>. We test DL based models on a NVIDIA RTX 2080Ti GPU and two 2.10 GHz Intel Xeon Gold 6130 CPUs for the other methods. For our approach, we provide the time of N = 4 iterations. The computational time is averaged over the entire test set. The speeds of traditional methods are variant under different settings. Note that ICP is accelerated using the k-D tree. We do not compare with Go-ICP because its obviously slow speed. Our method is faster especially with large inputs but is slower than the non-iterative DCP and DeepGMR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Number of Iterations</head><p>The model is trained with different iterations to show that how many iterations are needed. The anisotropic and isotropic errors are calculated after each iteration, as illustrated in <ref type="figure" target="#fig_0">Fig. 12</ref>. We can find that most performance gains are in the first two iterations, and we choose N = 4 for the trade-off between the speed and the accuracy in all experiments.  <ref type="table">Table 7</ref>. Results on point clouds of unseen shapes in ModelNet40, which are generated in the manner of RPMNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Our OMNet shows robustness to the various overlapping ratios of inputs. All inputs are transformed by the same 3D rigid transformation. Error(R) and Error(t) are isotropic errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of our OMNet. During process of feature extraction, the global features FX and F Y are duplicated N times to concatenate with the point-wise features fX and f Y , where N is the number of points in the inputs. The same background color denotes sharing weights. Superscripts denote the iteration count.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>We show 4 orderless points of each point cloud. The same subscript denotes the corresponding points. Yellow indicates the maximum of each channel in the features of overlapping points and green indicates the interference of non-overlapping points. The global features of X and Y are the same only when they are weighted by the masks M X and M Y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Example CAD models of 8 axisymmetrical categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>We show the registration result (top left), the difference between the global features of the inputs X and Y (top right), and the predicted masks (bottom) at each iteration. Red and blue indicate the predicted overlapping and non-overlapping regions respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .Figure 7 .Figure 8 .</head><label>678</label><figDesc>Example results on ModelNet40. (a) Unseen shapes, (b) Unseen categories, and (c) Unseen shapes with Gaussian noise. Errors of our method under different noise levels. Isotropic errors of different overlapping ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Error(R)=2.831 , Error(t)=0.0222 Error(R)=2.323 , Error(t)=0.0151 Error(R)=2.831 , Error(t)=0.0222 Error(R)=2.323 , Error(t)=0.0151</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Example results on Stanford 3D Scan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Some examples of the partial-to-partial data. The left and right columns denote the point clouds that are processed in the manner of PRNet and RPMNet respectively. All of them are registered by the ground truth transformations. Blue denotes the source point cloud X and red denotes the reference point cloud Y. The pairs of RPMNet are more decentralized than those of PRNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Results on ModelNet40. For each metric, the left column OS denotes the results on the original once-sampled data, and the right column TS denotes the results on our twice-sampled data. Red indicates the best performance and blue indicates the second-best result.</figDesc><table><row><cell></cell><cell>]</cell><cell cols="2">21.043 21.246</cell><cell>8.464</cell><cell>9.431</cell><cell cols="4">0.0913 0.0975 0.0467 0.0519 16.460 17.625 0.0921 0.1030</cell></row><row><cell></cell><cell>Go-ICP [35]</cell><cell cols="2">13.458 11.296</cell><cell>3.176</cell><cell>3.480</cell><cell>0.0462 0.0571 0.0149 0.0206</cell><cell>6.163</cell><cell>7.138</cell><cell>0.0299 0.0407</cell></row><row><cell>(a) Unseen Shapes</cell><cell>Symmetric ICP [22] FGR [40] PointNetLK [1] DCP [32] PRNet [33] FMR [13] IDAM [17]</cell><cell cols="2">5.333 4.741 16.429 14.888 6.875 28.865 4.291 5.786 1.588 3.677 2.740 3.456 4.744 7.456</cell><cell>4.787 1.110 7.467 3.006 0.976 1.448 1.346</cell><cell cols="5">6.069 16.168 0.0269 0.1380 0.0070 0.0774 0.0572 0.0745 0.0517 0.0668 7.603 0.0832 0.0842 0.0443 0.0464 14.324 14.742 0.0880 0.0920 9.424 12.103 0.0992 0.1290 2.152 30.192 0.0136 0.1530 3.872 0.0426 0.0602 0.0291 0.0388 5.871 7.903 0.0589 0.0794 2.201 0.0146 0.0307 0.0101 0.0204 1.871 4.223 0.0201 0.0406 1.736 0.0250 0.0292 0.0112 0.0138 2.793 3.281 0.0218 0.0272 4.387 0.0395 0.0604 0.0108 0.0352 2.610 8.577 0.0216 0.0698</cell></row><row><cell></cell><cell>DeepGMR [37]</cell><cell cols="2">13.266 21.985</cell><cell>6.883</cell><cell cols="5">11.113 0.0748 0.0936 0.0476 0.0587 13.536 20.806 0.0937 0.1171</cell></row><row><cell></cell><cell>Ours</cell><cell>0.898</cell><cell>1.045</cell><cell>0.325</cell><cell>0.507</cell><cell>0.0078 0.0084 0.0049 0.0056</cell><cell>0.639</cell><cell>0.991</cell><cell>0.0099 0.0112</cell></row><row><cell></cell><cell>ICP [3]</cell><cell cols="2">17.236 18.458</cell><cell>8.610</cell><cell>9.335</cell><cell cols="4">0.0817 0.0915 0.0434 0.0505 16.824 18.194 0.0855 0.0993</cell></row><row><cell></cell><cell>Go-ICP [35]</cell><cell cols="2">13.572 14.162</cell><cell>3.416</cell><cell>4.190</cell><cell>0.0448 0.0533 0.0152 0.0206</cell><cell>6.688</cell><cell>8.286</cell><cell>0.0299 0.0409</cell></row><row><cell>(b) Unseen Categories</cell><cell>Symmetric ICP [22] FGR [40] PointNetLK [1] DCP [32] PRNet [33] FMR [13] IDAM [17] DeepGMR [37]</cell><cell cols="2">6.599 6.390 18.294 21.041 7.415 29.838 6.754 7.683 2.712 6.506 5.041 5.119 6.852 8.346 18.890 23.472</cell><cell>5.962 1.240 9.730 4.366 1.372 2.304 1.761 9.322</cell><cell cols="5">6.552 16.361 0.0375 0.1470 0.0081 0.0818 0.0654 0.0759 0.0592 0.0684 11.713 13.113 0.1134 0.1315 2.204 31.153 0.0156 0.1630 10.740 0.0917 0.1130 0.0526 0.0629 18.845 20.438 0.1042 0.1250 4.747 0.0612 0.0675 0.0403 0.0427 8.566 9.764 0.0807 0.0862 3.472 0.0171 0.0388 0.0118 0.0257 2.607 6.789 0.0237 0.0510 2.349 0.0383 0.0296 0.0158 0.0147 4.525 4.553 0.0314 0.0292 4.540 0.0540 0.0590 0.0138 0.0329 3.433 8.679 0.0275 0.0656 12.863 0.0870 0.0987 0.0559 0.0658 17.513 24.425 0.1108 0.1298</cell></row><row><cell></cell><cell>Ours</cell><cell>2.079</cell><cell>2.514</cell><cell>0.619</cell><cell>1.004</cell><cell>0.0177 0.0147 0.0077 0.0078</cell><cell>1.241</cell><cell>1.949</cell><cell>0.0154 0.0154</cell></row><row><cell></cell><cell>ICP [3]</cell><cell cols="2">19.945 21.265</cell><cell>8.546</cell><cell>9.918</cell><cell cols="4">0.0898 0.0966 0.0482 0.0541 16.599 18.540 0.0949 0.1070</cell></row><row><cell></cell><cell>Go-ICP [35]</cell><cell cols="2">13.612 12.337</cell><cell>3.655</cell><cell>3.880</cell><cell>0.0489 0.0560 0.0174 0.0218</cell><cell>7.257</cell><cell>7.779</cell><cell>0.0348 0.0433</cell></row><row><cell>(c) Gaussian Noise</cell><cell>Symmetric ICP [22] FGR [40] PointNetLK [1] DCP [32] PRNet [33] FMR [13] IDAM [17]</cell><cell cols="8">5.208 22.347 34.035 10.309 19.188 0.1070 0.1601 0.0537 0.0942 19.934 35.775 0.1068 0.1850 6.769 4.703 5.991 0.0518 0.0680 0.0462 0.0609 9.174 11.895 0.0897 0.1178 20.131 22.399 11.864 13.716 0.0972 0.1092 0.0516 0.0601 18.552 20.250 0.1032 0.1291 4.862 4.775 3.433 2.964 0.0486 0.0474 0.0340 0.0300 6.653 6.024 0.0690 0.0616 1.911 3.197 1.213 2.047 0.0180 0.0294 0.0123 0.0195 2.284 3.932 0.0245 0.0392 2.898 3.551 1.747 2.178 0.0246 0.0273 0.0133 0.0155 3.398 4.200 0.0260 0.0307 5.551 6.846 2.990 3.997 0.0486 0.0563 0.0241 0.0318 5.741 7.810 0.0480 0.0629</cell></row><row><cell></cell><cell>DeepGMR [37]</cell><cell cols="2">17.693 20.433</cell><cell>8.578</cell><cell cols="5">10.964 0.0849 0.0944 0.0531 0.0593 16.504 20.830 0.1048 0.1183</cell></row><row><cell></cell><cell>Ours</cell><cell>1.009</cell><cell>1.305</cell><cell>0.548</cell><cell>0.757</cell><cell>0.0089 0.0103 0.0061 0.0075</cell><cell>1.076</cell><cell>1.490</cell><cell>0.0123 0.0149</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.1. Concurrently, the experimental settings of competitors are presented in Sec. 4.2. Moreover, we show the results for different experiments to demonstrate the effectiveness and robustness of our method in Sec. 4.3 and Sec. 4.4. Finally, we perform ablation studies in Sec. 4.5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Results on 8 axisymmetrical categories in ModelNet40.OS and TS denotes the results on the once-sampled and our twicesampled data separately. The performances of all methods are decreasing when changing the data sampling manner from OS to TS.</figDesc><table><row><cell></cell><cell cols="2">RMSE(R)</cell><cell cols="2">MAE(R)</cell><cell cols="2">Error(R)</cell></row><row><cell>Method</cell><cell>OS</cell><cell>TS</cell><cell>OS</cell><cell>TS</cell><cell>OS</cell><cell>TS</cell></row><row><cell cols="2">RPMNet [36] 0.312</cell><cell>6.531</cell><cell cols="3">0.200 2.972 0.432</cell><cell>8.454</cell></row><row><cell>PRNet [33]</cell><cell cols="6">5.979 13.773 3.779 9.670 7.714 20.692</cell></row><row><cell>IDAM [17]</cell><cell>1.605</cell><cell>7.725</cell><cell cols="4">0.905 4.364 1.850 10.940</cell></row><row><cell>Ours</cell><cell>0.766</cell><cell>6.258</cell><cell cols="3">0.347 2.877 0.873</cell><cell>8.606</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Unseen Categories. We evaluate the performance on unseen categories without noise in this experiment. The models are trained on the first 14 categories and tested on the other 18 categories. The results are summarized in Table 7(b). We can find that the performances of all DL based methods become worse without training on the same categories. Nevertheless, the traditional methods are not affected so much due to the handcrafted features. Our method outperforms all the traditional and DL based methods. Gaussian Noise. To evaluate the capability of robustness to noise, we add noises sampled from N (0, 0.01 2 ) on each axis and clipped to [?0.05, 0.05], then repeat the first two experiments (unseen shapes and unseen categories). Table 7(c)(d) show the performances of different algorithms. FGR</figDesc><table><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RMSE(R) MAE(R)</cell><cell>Error(R) RMSE(t)</cell><cell>MAE(t) Error(t)</cell><cell>0.08</cell></row><row><cell>2 4 6</cell><cell>Rotation Error</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Translation Error</cell><cell>0.02 0.04 0.06</cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4 Number of Iterations 5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>0.00</cell></row><row><cell cols="9">Figure 12. Anisotropic and isotropic errors of our method over</cell></row><row><cell cols="4">registration iterations.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Results on point clouds of unseen categories with Gaussian noise in ModelNet40, which are generated in the manner of PRNet. Red indicates the best performance and blue indicates the second-best result.<ref type="bibr" target="#b22">22</ref>.906 12.651 13.599 0.1887 0.1994 0.1241 0.1286 25.085 26.819 0.2626 0.2700 Go-ICP [35] 69.747 64.455 39.646 34.017 0.3035 0.3196 0.1788 0.1888 68.329 68.920 0.3893 0.4091 Symmetric ICP [22] 12.291 12.333 10.841 10.746 0.1488 0.1456 0.1212 0.1186 21.399 21.437 0.2577 0.2521 FGR [40] 46.161 41.644 27.475 26.193 0.2763 0.2872 0.1818 0.1951 49.749 51.463 0.3745 0.4003 PointNetLK [1] 27.903 42.777 18.661 28.969 0.2525 0.3210 0.1752 0.2258 36.741 53.307 0.3671 0.4613 23.174 11.134 12.405 0.1902 0.1932 0.1214 0.1231 22.580 25.147 0.2634 0.2639 Go-ICP [35] 72.221 72.030 40.516 39.308 0.3162 0.3468 0.1860 0.1977 74.420 77.519 0.4089 0.4405 Symmetric ICP [22] 11.087 11.731 9.671 10.042 0.1453 0.1436 0.1157 0.1163 19.174 20.292 0.2517 0.2486 FGR [40] 53.186 47.816 33.189 30.572 0.3059 0.3149 0.2117 0.2185 63.019 59.759 0.4368 0.4459 PointNetLK [1] 24.162 26.235 16.222 17.874 0.2369 0.2582 0.1684 0.1805 32.108 36.109 0.3555 0.3771 21.893 12.786 13.402 0.1917 0.1963 0.1265 0.1278 25.417 26.632 0.2667 0.2679 Go-ICP [35] 70.417 65.402 40.303 34.988 0.3072 0.3233 0.1822 0.1929 69.175 71.054 0.3962 0.4170 Symmetric ICP [22] 12.183 12.576 10.723 10.987 0.1487 0.1478 0.1210 0.1203 21.169 21.807 0.2576 0.2560 FGR [40] 49.133 46.213 31.347 30.116 0.3002 0.3034 0.2068 0.2141 56.652 58.968 0.4230 0.4364 PointNetLK [1] 26.476 29.733 19.258 21.154 0.2542 0.2670 0.1853 0.1937 37.688 42.027 0.3831 0.3964</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell cols="2">RMSE(R) OS TS</cell><cell cols="2">MAE(R) OS TS</cell><cell></cell><cell></cell><cell>RMSE(t) OS TS</cell><cell></cell><cell>MAE(t) OS TS</cell><cell></cell><cell>Error(R) OS TS</cell><cell>Error(t) OS TS</cell></row><row><cell cols="2">ICP [3]</cell><cell></cell><cell cols="2">17.439 18.588</cell><cell>8.954</cell><cell cols="2">9.628</cell><cell cols="6">0.0848 0.0920 0.0460 0.0521 17.435 18.720 0.0905 0.1026</cell></row><row><cell cols="3">Go-ICP [35]</cell><cell cols="2">13.081 15.214</cell><cell>3.617</cell><cell cols="2">4.650</cell><cell cols="5">0.0455 0.0566 0.0169 0.0223</cell><cell>7.184</cell><cell>9.002</cell><cell>0.0334 0.0445</cell></row><row><cell cols="3">Symmetric ICP [22]</cell><cell>6.447</cell><cell>7.096</cell><cell>5.790</cell><cell cols="2">6.280</cell><cell cols="6">0.0615 0.0688 0.0552 0.0617 11.340 12.531 0.1065 0.1191</cell></row><row><cell cols="3">FGR [40]</cell><cell cols="2">19.027 33.723</cell><cell>8.383</cell><cell cols="8">19.268 0.1041 0.1593 0.0498 0.0914 15.902 35.971 0.0981 0.1828</cell></row><row><cell cols="3">PointNetLK [1]</cell><cell cols="11">27.589 29.747 16.047 18.550 0.1516 0.1841 0.0955 0.1081 30.406 32.760 0.1907 0.1959</cell></row><row><cell cols="3">DCP [32]</cell><cell>7.353</cell><cell>7.300</cell><cell>4.923</cell><cell cols="2">4.378</cell><cell cols="5">0.0657 0.0389 0.0451 0.0272</cell><cell>9.624</cell><cell>8.853</cell><cell>0.0902 0.0539</cell></row><row><cell cols="3">PRNet [33]</cell><cell>3.241</cell><cell>5.883</cell><cell>1.632</cell><cell cols="2">3.037</cell><cell cols="5">0.0181 0.0380 0.0127 0.0237</cell><cell>3.095</cell><cell>5.974</cell><cell>0.0254 0.0472</cell></row><row><cell cols="3">FMR [13]</cell><cell>4.819</cell><cell>5.304</cell><cell>2.488</cell><cell cols="2">2.779</cell><cell cols="5">0.0345 0.0323 0.0158 0.0172</cell><cell>4.824</cell><cell>5.392</cell><cell>0.0313 0.0342</cell></row><row><cell cols="3">IDAM [17]</cell><cell>5.188</cell><cell>8.008</cell><cell>3.114</cell><cell cols="2">4.559</cell><cell cols="5">0.0377 0.0484 0.0208 0.0291</cell><cell>5.836</cell><cell>8.774</cell><cell>0.0418 0.0578</cell></row><row><cell cols="2">Ours</cell><cell></cell><cell>2.203</cell><cell>2.563</cell><cell>0.910</cell><cell cols="2">1.215</cell><cell cols="5">0.0155 0.0183 0.0078 0.0098</cell><cell>1.809</cell><cell>2.360</cell><cell>0.0156 0.0196</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RMSE(R)</cell><cell cols="2">MAE(R)</cell><cell></cell><cell cols="2">RMSE(t)</cell><cell cols="2">MAE(t)</cell><cell>Error(R)</cell><cell>Error(t)</cell></row><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell>OS</cell><cell>TS</cell><cell>OS</cell><cell>TS</cell><cell></cell><cell>OS</cell><cell>TS</cell><cell>OS</cell><cell>TS</cell><cell>OS</cell><cell>TS</cell><cell>OS</cell><cell>TS</cell></row><row><cell></cell><cell></cell><cell>ICP [3]</cell><cell></cell><cell cols="10">20.036 22.840 10.912 12.147 0.1893 0.1931 0.1191 0.1217 22.232 24.654 0.2597 0.2612</cell></row><row><cell cols="2">(a) Unseen Shapes</cell><cell cols="12">Go-ICP [35] Symmetric ICP [22] 10.419 11.295 70.776 71.077 39.000 38.266 0.3111 0.3446 0.1807 0.1936 71.597 76.492 0.3996 0.4324 8.992 9.592 0.1367 0.1394 0.1082 0.1124 17.954 19.571 0.2367 0.2414 FGR [40] 48.533 46.766 29.661 29.635 0.2920 0.3041 0.1965 0.2078 55.855 57.685 0.4068 0.4263 PointNetLK [1] 23.866 27.482 15.070 18.627 0.2368 0.2532 0.1623 0.1778 29.374 36.947 0.3454 0.3691 DCP [32] 12.217 11.109 9.054 8.454 0.0695 0.0851 0.0524 0.0599 7.835 9.216 0.1049 0.1259 RPMNet [36] 1.347 2.162 0.759 1.135 0.0228 0.0267 0.0089 0.0141 1.446 2.280 0.0193 0.0302 FMR [13] 7.642 8.033 4.823 4.999 0.1208 0.1187 0.0723 0.0726 9.210 9.741 0.1634 0.1617</cell></row><row><cell></cell><cell></cell><cell>DeepGMR [37]</cell><cell></cell><cell cols="10">72.310 70.886 49.769 47.853 0.3443 0.3703 0.2462 0.2582 82.652 86.444 0.5044 0.5354</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>0.771</cell><cell>1.384</cell><cell>0.277</cell><cell cols="2">0.542</cell><cell cols="4">0.0154 0.0226 0.0056 0.0093</cell><cell>0.561</cell><cell>1.118</cell><cell>0.0122 0.0198</cell></row><row><cell cols="2">(b) Unseen Categories</cell><cell cols="12">ICP [3] 20.387 DCP [32] 13.387 12.507 RPMNet [36] 3.934 7.491 FMR [13] 10.365 11.548 DeepGMR [37] 75.773 68.425 53.689 46.269 0.3485 0.3667 0.2481 0.2595 85.210 87.192 0.5074 0.5323 9.971 9.414 0.0762 0.1020 0.0570 0.0730 11.128 12.102 0.1143 0.1493 1.385 2.403 0.0441 0.0575 0.0150 0.0258 2.606 4.635 0.0318 0.0556 6.465 7.109 0.1301 0.1330 0.0816 0.0837 12.159 13.827 0.1773 0.1817</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>3.719</cell><cell>4.014</cell><cell>1.314</cell><cell cols="2">1.619</cell><cell cols="4">0.0392 0.0406 0.0151 0.0179</cell><cell>2.657</cell><cell>3.206</cell><cell>0.0321 0.0383</cell></row><row><cell>Unseen Shapes</cell><cell>with Gaussian Noise</cell><cell cols="4">ICP [3] 20.245 DCP [32] 12.387 12.393 RPMNet [36] 1.670 2.955 FMR [13] 8.026 8.591</cell><cell>9.147 0.889 5.051</cell><cell cols="2">9.534 1.374 5.303</cell><cell cols="4">0.0656 0.1008 0.0495 0.0717 0.0310 0.0360 0.0111 0.0163 0.1244 0.1249 0.0755 0.0776</cell><cell>8.341 1.692 9.657</cell><cell>8.955 2.746 10.383 0.1702 0.1719 0.0989 0.1516 0.0242 0.0353</cell></row><row><cell cols="2">(c)</cell><cell>DeepGMR [37]</cell><cell></cell><cell cols="10">74.958 70.810 52.119 47.954 0.3520 0.3689 0.2538 0.2597 86.935 87.444 0.5189 0.5360</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>0.998</cell><cell>1.522</cell><cell>0.555</cell><cell cols="2">0.817</cell><cell cols="4">0.0172 0.0189 0.0078 0.0098</cell><cell>1.078</cell><cell>1.622</cell><cell>0.0167 0.0208</cell></row><row><cell cols="2">(d) Unseen Categories with Gaussian Noise</cell><cell cols="12">ICP [3] 20.566 DCP [32] 13.117 12.730 RPMNet [36] 4.118 6.160 FMR [13] 10.604 11.674 DeepGMR [37] 75.257 68.560 53.470 46.579 0.3509 0.3735 0.2519 0.2654 84.121 87.104 0.5180 0.5455 9.741 9.556 0.0779 0.1072 0.0591 0.0774 11.350 12.173 0.1187 0.1586 1.589 2.467 0.0467 0.0618 0.0175 0.0274 2.983 4.913 0.0378 0.0589 6.725 7.400 0.1300 0.1364 0.0827 0.0867 12.627 14.121 0.1788 0.1870</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>3.572</cell><cell>4.356</cell><cell>1.570</cell><cell cols="2">1.924</cell><cell cols="4">0.0391 0.0486 0.0172 0.0223</cell><cell>3.073</cell><cell>3.834</cell><cell>0.0359 0.0476</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PointNetLK: Robust &amp; efficient point cloud registration using pointnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rangaprasad Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronald T Azuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoperators &amp; Virtual Environments</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="355" to="385" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A method for registration of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interaction</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="73" to="272" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse iterative closest point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics forum</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Carmigniani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borko</forename><surname>Furht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Anisetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ceravolo</surname></persName>
		</author>
		<title level="m">Ernesto Damiani, and Misa Ivkovic. Augmented reality technologies, systems and applications. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="341" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 23rd annual conference on Computer graphics and interactive techniques</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Robust registration of 2d and 3d point sets. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1145" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometrically stable sampling for the icp algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Ikemoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Digital Imaging and Modeling</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Threedimensional registration using range and intensity information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Rioux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Baribeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Videometrics III</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">2350</biblScope>
			<biblScope unit="page" from="279" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature-metric registration: A fast semi-supervised approach for robust point cloud registration without correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time 3d reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual ACM Symposium on User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangning</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">3d reconstruction: the registration problem. Computer vision, Graphics, and Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Merickel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="206" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A review of point cloud registration algorithms for mobile robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="104" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A symmetric objective function for icp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient variants of the icp algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3-D Digital Imaging and Modeling (3DIM)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (FPFH) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blodow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
	<note>Zoltan Csaba Marton, and Michael Beetz</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3D is here: Point Cloud Library (PCL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICRA</title>
		<meeting>ICRA</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pcrnet: Point cloud registration network using pointnet encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinit</forename><surname>Sarode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rangaprasad Arun Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howie</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07906</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized-icp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Haehnel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Animating rotation with quaternion curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Shoemake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Computer Graphics and Interactive Techniques</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A relationship between arbitrary positive matrices and doubly stochastic matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sinkhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prnet: Selfsupervised learning for partial-to-partial registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Go-icp: Solving 3d registration efficiently and globally optimally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rpm-net: Robust point matching using learned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Zi Jian Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepgmr: Learning latent gaussian mixture models for registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of autonomous driving: Common practices and emerging technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekim</forename><surname>Yurtsever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Carballo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58443" to="58469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3dmatch: Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast global registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Open3d: A modern library for 3d data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Error(R)=1</title>
		<idno>=0.0140</idno>
		<imprint>
			<biblScope unit="volume">758</biblScope>
		</imprint>
	</monogr>
	<note>Error(t)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Error(R)=2</title>
		<idno>=0.0169</idno>
		<imprint>
			<biblScope unit="volume">839</biblScope>
		</imprint>
	</monogr>
	<note>Error(t)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Error(R)=2</title>
		<idno>=0.0222</idno>
		<imprint>
			<biblScope unit="volume">831</biblScope>
		</imprint>
	</monogr>
	<note>Error(t)</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Error(R)=1</title>
		<idno>=0.0166</idno>
		<imprint>
			<biblScope unit="volume">819</biblScope>
		</imprint>
	</monogr>
	<note>Error(t)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Error(R)=4</title>
		<idno>=0.0148</idno>
		<imprint>
			<biblScope unit="volume">941</biblScope>
		</imprint>
	</monogr>
	<note>Error(t)</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Error(R)=2</title>
		<idno>=0.0151</idno>
		<imprint>
			<biblScope unit="volume">323</biblScope>
		</imprint>
	</monogr>
	<note>Error(t)</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Error(R)=1</title>
		<idno>=0.0143</idno>
		<imprint>
			<biblScope unit="volume">425</biblScope>
		</imprint>
	</monogr>
	<note>Error(t)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Error(R)=4.090, Error(t)</title>
		<idno>=0.0226</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The model is trained on the training set on ModelNet40, and no fine-tuning is done on Stanford 3D Scan. In each cell separated by the horizontal line, the top row shows the initial positions of the two point clouds, and the bottom row shows the results of registration. Anisotropic and isotropic errors of each</title>
	</analytic>
	<monogr>
		<title level="m">Figure 13. Results on the Stanford 3D Scan dataset</title>
		<imprint/>
	</monogr>
	<note>result is shown bellow the point clouds</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
