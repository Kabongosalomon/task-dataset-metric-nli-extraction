<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximum Likelihood Training of Implicit Nonlinear Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
							<email>dongjoun57@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeonghu</forename><surname>Na</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Se</roleName><forename type="first">Jung</forename><surname>Kwon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naver</forename><surname>Clova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsoo</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naver</forename><surname>Clova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Maximum Likelihood Training of Implicit Nonlinear Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>KAIST Il-Chul Moon KAIST / Summary.AI</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Whereas diverse variations of diffusion models exist, extending the linear diffusion into a nonlinear diffusion process is investigated by very few works. The nonlinearity effect has been hardly understood, but intuitively, there would be promising diffusion patterns to efficiently train the generative distribution towards the data distribution. This paper introduces a data-adaptive nonlinear diffusion process for score-based diffusion models. The proposed Implicit Nonlinear Diffusion Model (INDM) learns by combining a normalizing flow and a diffusion process. Specifically, INDM implicitly constructs a nonlinear diffusion on the data space by leveraging a linear diffusion on the latent space through a flow network. This flow network is key to forming a nonlinear diffusion, as the nonlinearity depends on the flow network. This flexible nonlinearity improves the learning curve of INDM to nearly Maximum Likelihood Estimation (MLE) against the non-MLE curve of DDPM++, which turns out to be an inflexible version of INDM with the flow fixed as an identity mapping. Also, the discretization of INDM shows the sampling robustness. In experiments, INDM achieves the state-of-the-art FID of 1.75 on CelebA. We release our code at https://github.com/byeonghu-na/INDM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Diffusion models have recently achieved success on the task of sample generation, and some works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> claim state-of-the-art performance over Generative Adversarial Networks (GAN) <ref type="bibr" target="#b2">[3]</ref>. This success is highlighted particularly in likelihood-based models, including normalizing flows <ref type="bibr" target="#b3">[4]</ref>, autoregressive models <ref type="bibr" target="#b4">[5]</ref>, and Variational Auto-Encoders (VAE) <ref type="bibr" target="#b5">[6]</ref>. Moreover, this success is noteworthy because it is achieved merely using linear diffusion processes, such as Variance Exploding (VE) Stochastic Differential Equation (SDE) <ref type="bibr" target="#b6">[7]</ref>, and Variance Preserving (VP) SDE <ref type="bibr" target="#b7">[8]</ref>.</p><p>This paper extends linear diffusions of VE/VP SDEs to a data-adaptive trainable nonlinear diffusion. To motivate the extension, though there are structural similarities between diffusion models and VAEs, the inference part of a linear diffusion process has not been trained while its counterpart of VAE (the encoder) has been trained. We introduce Implicit Nonlinear Diffusion Models (INDM) to train its forward SDE, the inference part in diffusion models. INDM constructs the nonlinearity of the data diffusion by transforming a linear latent diffusion back to the data space.</p><p>We implement the transformation between the data and latent spaces with a normalizing flow. The invertibility of the flow mapping is key to learning a nonlinear inference part. Invertibility is necessary for constructing the nonlinearity, and we clarify this by comparing INDM with LSGM <ref type="bibr" target="#b8">[9]</ref>, a latent diffusion model with VAE. Altogether, INDM provides the following advantages over the existing models.</p><p>? INDM achieves fast and tractable optimization with implicit modeling.</p><p>? INDM learns not only drift but volatility coefficients of the forward SDE.</p><p>? INDM trains its network with Maximum Likelihood Estimation (MLE).</p><p>? INDM is robust on the sampling discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>A diffusion model is constructed with bidirectional forward and reverse stochastic processes.</p><p>Forward and Reverse Diffusions A forward diffusion process diffuses an input data variable, x 0 ? p r , to a noise variable, and the corresponding reverse diffusion process <ref type="bibr" target="#b9">[10]</ref> of this forward diffusion denoises a noise variable to regenerate the input variable. The forward diffusion is fully described by an SDE of dx t = f (x t , t) dt + G(x t , t) dw t , and the corresponding reverse SDE becomes dx t = f (x t , t) ? div(GG T )(x t , t) ? (GG T )(x t , t)? xt log p t (x t ) dt + G(x t , t) dw t . Here, w t ? R d is an abstraction of a random walk process with independent increments, where d is the data dimension, and dw t is the standard Wiener processes with backwards in time.</p><p>Generative Diffusion Having that the drift (f ? R d ) and the volatility (G ? R d?d ) terms are given apriori, diffusion models <ref type="bibr" target="#b0">[1]</ref> estimate the data score, ? xt log p t (x t ), with the score network, s ? (x t , t). By plugging the score network in the data score, we obtain another diffusion process, called the generative SDE, described by dx ? t = f (x ? t , t) ? div(GG T )(x ? t , t) ? (GG T )(x ? t , t)s ? (x ? t , t) dt + G(x ? t , t) dw t . Starting from a prior distribution of x ? T ? ? and solving the SDE time backwards, Song et al. <ref type="bibr" target="#b0">[1]</ref> construct the generative stochastic process of {x ? t } T t=0 that perfectly reconstructs the reverse process of {x t } T t=0 under two conditions: 1) s ? (x t , t) = ? xt log p t (x t ) and 2) x T ? ?. We define a generative distribution, p ? , as the distribution of x ? 0 . Score Estimation The diffusion model estimates the data score with the score network by minimizing the denoising score loss <ref type="bibr" target="#b0">[1]</ref>, given by L({x t } T t=0 , ?; ?) = T 0 ?(t)E x0,xt [ s ? (x t , t) ? ? xt log p 0t (x t |x 0 ) <ref type="bibr" target="#b1">2</ref> 2 ] dt, where p 0t (x t |x 0 ) is a transition probability of x t given x 0 ; and ? is the weighting function that determines the level of contribution for each diffusion time. When G(x t , t) = g(t), Song et al. <ref type="bibr" target="#b10">[11]</ref>, Huang et al. <ref type="bibr" target="#b11">[12]</ref> proved that this loss with the likelihood weighting (? = g 2 ) turns out to be a variational bound of the negative log-likelihood: E x0 [? log p ? (x 0 )] ? L({x t } T t=0 , g 2 ; ?) ? E x T [log ?(x T )], up to a constant, see Appendix A.1 for a detailed discussion. Choice of Drift (f ) and Volatility (G) Terms The original diffusion model strictly limits the scope of diffusion process to be a family of linear diffusions that f is a linear function of x t and G is an identity matrix multiplied by a t-function. For instance, VESDE <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> satisfies f ? 0 with G = d? 2 (t)/ dtI and VPSDE <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> satisfies f = ? 1 2 ?(t)x t ? x t with G = ?(t)I. Few concurrent works have extended linear diffusions to nonlinear diffusions by 1) applyng a latent diffusion using VAE in LSGM <ref type="bibr" target="#b8">[9]</ref>, 2) applying a flow network to nonlinearize the drift term in DiffFlow <ref type="bibr" target="#b12">[13]</ref>, and 3) reformulating the diffusion model into a Schrodinger Bridge Problem (SBP) <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. We further analyze these approaches in Section 5. 3 Motivation of Nonlinear Diffusion Process <ref type="figure" target="#fig_0">Figure 1</ref> illustrates various diffusion processes on a spiral toy dataset. In the top row, the diffusion path of VPSDE keeps its overall structure of the initial data manifold during the data deformation procedure to N (0, I). The drift vector field illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>-(a) as black arrows presents that VPSDE linearly deforms its data distribution. Unlike the linear diffusion, the middle row of <ref type="figure" target="#fig_0">Figure 1</ref> with a nonlinear drift shows that the data is not linearly deformed to N (0, I). The nonlinearity of the drift term represented as rotating black arrows is the source of such nonlinear deformation at the intermediate steps,</p><p>x 0.2 ? x 0.6 . When it comes to the volatility term, the last row of <ref type="figure" target="#fig_0">Figure 1</ref> presents the process with nonlinear G. <ref type="figure" target="#fig_3">Figure 2</ref>-(c) illustrates the covariance matrices of the perturbation distribution at t = 0 with linear and nonlinear volatility terms, where the perturbation distribution induced by the volatility term is N (0, G(x t , t)G T (x t , t)) 1 . It shows the non-diagonal and data-dependent covariances of GG T in red ellipses of a nonlinear volatility term, and the isotropic blue circles of linear diffusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implicit Nonlinear Diffusion Model</head><p>There are two ways to nonlinearize the drift and volatility coefficients in SDE: explicit and implicit parametrizations. While explicit is a straightforward way to model the nonlinearity, it becomes impractical particularly in the training procedure. Concretely, in each of the training iteration, the denoising loss L({x t } T t=0 , ?; ?) requires 1) the perturbed samples x t from p 0t (x t |x 0 ) and 2) the calculation of ? log p 0t (x t |x 0 ), and these two steps require long execution time because the transition probability p 0t (x t |x 0 ) is intractable for nonlinear diffusions in general. Therefore, we parametrize f ? and G ? implicitly for fast and tractable optimization. As visualized in <ref type="figure" target="#fig_1">Figure 3</ref>, we impose a linear diffusion model on the latent space, and connect this latent variable with the data variable through a normalizing flow. The nonlinear diffusion on the data space, then, is induced from the latent diffusion leveraged to the data space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Latent Diffusion Processes</head><p>Latent Diffusion Processes Let us define z ? 0 to be a transformed latent variable z ? 0 = h ? (x 0 ), where h ? is a transformation of the normalizing flow. Then, a forward linear diffusion</p><formula xml:id="formula_0">dz ? t = ? 1 2 ?(t)z ? t dt + g(t) dw t , (Latent Forward SDE)</formula><p>starting at z ? 0 = h ? (x 0 ) with x 0 ? p r , describes the forward diffusion process on the latent space (blue diffusion path in <ref type="figure" target="#fig_1">Figure 3</ref>). The corresponding reverse latent diffusion is given by</p><formula xml:id="formula_1">dz ? t = [? 1 2 ?(t)z ? t ? g 2 (t)? z ? t log p ? t (z ? t )] dt + g(t) dw t ,</formula><p>where p ? t is the probability of z ? t . Forward Data Diffusion We have not defined the data diffusion process yet. We build the data diffusion from the latent diffusion and the normalizing flow. From the invertibility, we define random variables on the data space by transforming the latent linear diffusion back to the data space:</p><formula xml:id="formula_2">x ? t := h ?1 ? (z ? t )</formula><p>for any t ? [0, T ]. Then, from the Ito's formula <ref type="bibr" target="#b16">[17]</ref>, the process</p><formula xml:id="formula_3">{x ? t } T t=0 follows dx ? t = f ? (x ? t , t) dt + G ? (x ? t , t) dw t , (Data Forward SDE) starting with x ? 0 = h ?1 ? (z ? 0 )</formula><p>. From x ? 0 = h ?1 ? (h ? (x 0 )) = x 0 ? p r , we call this process by induced diffusion that permeates the data variable on the data space. We emphasize that this induced diffusion collapses to a linear diffusion if h ? id = id. See Appendix A.2 for details on drift and volatility terms.</p><p>Generative Data Diffusion A diffusion model estimates the forward latent score s ? (z, t) = ? log p ? t (z) with the score network, s ? (z, t), to mimic the forward linear diffusion on the latent space. Then, the generative SDE on the latent space becomes</p><formula xml:id="formula_4">dz ? t = ? 1 2 ?(t)z ? t ? g 2 (t)s ? (z ? t , t) dt + g(t) dw t (Latent Gen. SDE)</formula><p>with a starting variable z ? T ? ?. Thus, the process {x ?,? t } T t=0 of x ?,? t := h ?1 ? (z ? t ) becomes a generative data diffusion (purple path in <ref type="figure" target="#fig_1">Figure 3</ref></p><formula xml:id="formula_5">) with SDE of dx ?,? t = f ? ? div(G ? G T ? ) ? (G ? G T ? ?h ? )s ? h ? (x ?,? t ), t dt + G ? dw t . (Data Gen. SDE)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Training and Sampling</head><p>Likelihood Training Theorem 1 estimates Negative Evidence Lower Bound (NELBO) of Negaitve Log-Likelihood (NLL). For the notational simplicity, we define the targetted score function by</p><formula xml:id="formula_6">s ? (z ? t , t) := ? log p ? t (z ? t ). (Target of Score Estimation) Also, suppose L {z ? t } T t=0 , g 2 ; ? = 1 2 T 0 g 2 (t)E z ? 0 ,z ? t s ? (z ? t , t) ? ? log p 0t (z ? t |z ? 0 ) 2 2 dt, where p 0t (z ? t |z ? 0 )</formula><p>is the transition probability of the latent forward diffusion. In Theorem 1, we drop the constant terms that do not hurt the essence of the theorem to keep the simplicity. See full details and the proof in Appendix G. Theorem 1. Suppose that p ?,? is the likelihood of a generative random variable x ?,? 0 . Then, the negative log-likelihood is upper bounded by</p><formula xml:id="formula_7">E x0 ? log p ?,? (x 0 ) ? L {x t } T t=0 , g 2 ; {?, ?} , where L {x t } T t=0 , g 2 ; {?, ?} = 1 2 T 0 g 2 (t)E z ? t s ? (z ? t , t) ? s ? (z ? t , t) 2 2 dt + D KL (p ? T ?) (1) = ?E x0 log det ?h ? (x 0 ) + L {z ? t } T t=0 , g 2 ; ? ? E z ? T log ?(z ? T ) .<label>(2)</label></formula><p>Eq. (1) is the KL divergence D KL (? ? ? ?,? ), where ? ? and ? ?,? are the path measures of the forward and generative diffusions on the data space. Eq. (1) explains the reasoning of why s ? is the target of the score estimation. However, the KL divergence is intractable, and Theorem 1 provides an equivalent tractable loss by Eq. (2), the summation of the flow loss with the denoising loss.</p><p>Algorithm 1 describes the line-by-line algorithm of INDM training. We obtain the flow loss by taking a flow evaluation. Afterward, we compute the denoising loss. We train the flow with Eq. (2). However, we train the score with L {x t } T t=0 , ?; {?, ?} with various ? settings for a better Fr?chet Inception Distance (FID) <ref type="bibr" target="#b17">[18]</ref>.</p><p>Latent Sampling While either of red or purple path in <ref type="figure" target="#fig_1">Figure 3</ref> could synthesize the samples, we choose the red path for the fast sampling (because the red path feed-forwards the flow only once). Starting from a pure noise z ? T ? ?, we denoise z ? T to z ? 0 by solving the generative process backward on the latent space. Then, we transform the fully denoised latent z ? 0 to the data space Get latent with flow by Moreover, LSGM has a pair of key differences in its training. First, the latent dimension of LSGM is 40,080, which is 15? higher dimension than the data dimension (3,072) on CIFAR-10 <ref type="bibr" target="#b18">[19]</ref>. In contrast, INDM always keeps its latent dimension by the data dimension. See <ref type="table" target="#tab_12">Table 9</ref> to compare the latent dimensions of INDM with LSGM on benchmark datasets. Furthermore, LSGM is repeatedly reported <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref> for its training instability on the best FID setting of ? = ? 2 (i.e., L simple <ref type="bibr" target="#b7">[8]</ref>). Meanwhile, INDM is consistently stable for any of training configurations, see <ref type="table" target="#tab_11">Table 8</ref>.</p><formula xml:id="formula_8">x ?,? 0 = h ?1 ? (z ? 0 ).</formula><formula xml:id="formula_9">z ? 0 = h ? (x 0 ) for x 0 ? p r 3: Compute ?E x0 log det ?h ? (x 0 ) 4: Get diffused latents {z ? t } T t=0 with a linear SDE 5: Compute L {z ? t } T t=0 , g 2 ; ? ? E z ? T log ?(z ? T ) 6: Compute flow loss L f = L {x t } T t=0 , g 2 ; {?, ?} 7: Update ? ? ? ? ? ?L f ?? 8: Compute L {z ? t } T t=0 , ?; ? ? E z ? T log ?(z ? T ) 9: Compute score loss L s = L {x t } T t=0 , ?; {?,</formula><p>DiffFlow Zhang and Chen <ref type="bibr" target="#b12">[13]</ref> explicitly model the drift term f ? as a flow network, so the forward diffusion becomes dx t = f ? dt + g dw t . However, there are differences between DiffFlow and INDM: 1) DiffFlow does not nonlinearize the volatility; 2) DiffFlow is too slow for its explicit parametrization <ref type="table" target="#tab_0">(Table 18)</ref>; 3) the flexibility of f ? is too restricted; 4) DiffFlow has a larger loss variance <ref type="table" target="#tab_0">(Table 10</ref>). See Appendix D.2 for the full details of our arguments. Focusing on the slow training, observe that the denoising loss E x0,</p><formula xml:id="formula_10">x ? t [ s ? (x ? t , t) ? ? log p 0t (x ? t |x 0 ) 2 2</formula><p>] requires a pair of heavy computations: (A) sampling from x ? t , and (B) computation of ? log p 0t (x ? t |x 0 ). Intractable transition probability p 0t (x ? t |x 0 ) is the major bottleneck of the slow training. To overcome the bottleneck, DiffFlow discretizes the continuous diffusion with N variables of a discrete diffusion and uses the DDPM-style loss <ref type="bibr" target="#b7">[8]</ref>, which does not need to calculate the transition probability. Under the discretization, however, the forward sampling of x ? t takes O(N ) flow evaluations for every network update. This sampling issue is an inevitable fundamental problem when we parametrize the coefficients explicitly. Having that the flow evaluation is generally more expensive than score evaluation given the same number of parameters, a fast sampling is achievable only if we reduce N . However, it hurts the flexibility of a diffusion process, so DiffFlow suffers from the trade-off between training speed and model flexibility. On the other hand, the training of INDM is invariant of N , and INDM is free from such a trade-off. Analogously, DiffFlow generates a sample with the purple path in <ref type="figure" target="#fig_1">Figure 3</ref>, so it takes O(N ) flow evaluations, contrastive to INDM with a single flow evaluation in its sampling with the red path.</p><p>SBP De Bortoli et al. <ref type="bibr" target="#b14">[15]</ref> learn the diffusion process with a problem of min ? ? ?P(pr,?) D KL (? ? ?), where P(p r , ?) is the collection of path measure with p r and ? as its marginal distributions at t = 0 and t = T , respectively. It is a bi-constrained optimization problem as any path measure on the search space that should satisfy boundary conditions at both t = 0 and t = T . ? is the reference measure of a linear diffusion dx t = f (x t , t) dt + g(t)w t ; and the forward and reverse SDEs of ? ? are</p><formula xml:id="formula_11">dx t = [f (x t , t)+g 2 (t)? log ?(x t , t)] dt+g(t) dw t and dx t = [f (x t , t)?g 2 (t)? log?(x t , t)] dt+ g(t) dw t , respectively, where (?,?)</formula><p>is the solution of a coupled PDE, called Hopf-Cole transform <ref type="bibr" target="#b20">[21]</ref>. Solving this coupled PDE is intractable, so the estimation target of SBP is ? log ? and ? log?. As f and g are assumed to be linear functions, the nonlinearity of SBP is fully determined by (?,?). Analogous to DiffFlow, sampling from x t in SBP needs a long time. Few works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> detour this training issue using the experience replay memory. Aside from the training time, the KL minimization puts the global optimal nonlinear diffusion ? ? * near a neighborhood of the linear diffusion ?. In other words, the optimal ? ? * is the closest path measure on P(p r , ?) to ?, so the inferred nonlinear diffusion would be the most linear diffusion on the space of P(p r , ?). For the demonstration, we illustrate g 2 (t)? log ?(x t , t)?t 2 / g(t)?w t 2 in <ref type="figure" target="#fig_4">Figure 4</ref>. We used the released checkpoint of SB-FBSDE <ref type="bibr" target="#b15">[16]</ref>, an algorithm for solving SBP, trained with VESDE on CIFAR-10. As f ? 0 in VESDE, this norm ratio measures how much nonlinearity is counted on a diffusion trajectory compared to the linear effect. <ref type="figure" target="#fig_4">Figure 4</ref> shows that the ratio approaches zero except at the small range around t ? 0, meaning that the nonlinear effect is virtually ignorable than the linear effect. Therefore, <ref type="figure" target="#fig_4">Figure 4</ref> implies that the diffusion process is nearly linear in most of the diffusion time. We give a detailed discussion of SBP in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This section investigates characteristics of INDM. We show that INDM training is faster and nearly MLE in Section 6.1, and INDM sampling is robust on discretization step sizes in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benefit of INDM in Training</head><p>Having that DDPM++ is a collapsed INDM with a fixed identity transformation h ? id = id, the difference lies in whether to train ? or not. This trainable nonlinearity provides the better optimization of INDM, as evidenced in <ref type="figure" target="#fig_6">Figure 5</ref>-(a), experimented on CIFAR-10 using VPSDE. It shows a pair of critical characteristics of INDM training: 1) it is faster than DDPM++ training, and 2) it is asymptotically an MLE training. For the training speed, recall that the regression target of the score estimation is s ? , and this target is fixed in DDPM++ while keep moving in INDM. The target is constantly updated through the direction of s ? in Eq. (1) by optimizing s ? ?s ? For the MLE training, as the flow training is intricately entangled with the score training, we analyze INDM training for a specific class of score networks. First, we define S sol (Definition 1 in Appendix B) to be the class of forward score functions of a linear diffusion with some initial distribution. Then, it turns out that it is the whole class of zero variational gap (=NLL-NELBO).</p><formula xml:id="formula_12">Theorem 2. Gap(? ? , ? ?,? ) := D KL (? ? ? ?,? ) ? D KL (p r p ?,? ) = 0 if and only if s ? ? S sol .</formula><p>Song et al. <ref type="bibr" target="#b10">[11]</ref> partially reveal the connection between the gap with S sol , by proving the if part of Theorem 2, in Theorem 2 of Song et al. <ref type="bibr" target="#b10">[11]</ref> (see Lemma 2 in Appendix B). We completely characterize this connection by proving the only-if part in Theorem 2. Surprisingly, the variational gap is irrelevant to the flow parameters, and the MLE training of INDM implies that the score network is nearby S sol throughout the training. Combining Theorem 2 with the global optimality analysis raises a qualitative discrepancy in the optimization of DDPM++ and INDM by Theorem 3.  Theorem 3 implies that there exists an optimal flow that the forward and generative SDEs on the latent space coincide, for any score network in S sol , if the flow is flexible enough. Therefore, INDM attains infinitely many (= |S sol |) global optimums in its optimization space. On the other hand, DDPM++ has only a unique optimal score network, i.e., s ? * = s ? id . Thus, Theorem 3 potentially explains the faster convergence of INDM. We give a detailed analysis in Appendix B.</p><formula xml:id="formula_13">Theorem 3. For any fixed s ? ? S sol , if ? * ? arg min ? D KL (? ? ? ?,? ), then s ? * (z, t) = ? log p ? * t (z) = s ? (z, t), and D KL (? ? * ? ? * ,? ) = D KL (p r p ? * ,? ) = Gap(? ? * , ? ? * ,? ) = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Benefit of INDM in Sampling</head><p>Figure 5-(b,c) equally illustrate that INDM is more robust on the discretization step sizes in FID than DDPM++/NCSN++. To analyze the sample quality with respect to discretizations, recall that the Euler-Maruyama discretization of the generative SDE (or called the reverse diffusion sampler, or simply the predictor <ref type="bibr" target="#b0">[1]</ref>) iteratively updates the samplez k with</p><formula xml:id="formula_14">z t k?1 =z t k + ? k 1 2 ?(t k )z t k + g 2 (t k )s ? (z t k , t k ) + g(t k ) ? ? k ,</formula><p>until time reaches to zero, where ? k = t k ? t k?1 is the step size of the discretized sampler and ? N (0, I). The sampling error is the distributional discrepancy between the sample distribution of h ?1 ? (z 0 ) and the data distribution. Theorem 4 decomposes the sampling error with three factors: 1) the prior error E pri , 2) the discretization error E dis , and 3) the score error E est . Note that Theorem 4 is a straightforward application of the analysis done by De Bortoli et al. <ref type="bibr" target="#b14">[15]</ref> and Guth et al. <ref type="bibr" target="#b21">[22]</ref>. We omit regularity conditions to avoid unnecessary complications; see Appendix C.1. Theorem 4 (De Bortoli et al. <ref type="bibr" target="#b14">[15]</ref> and Guth et al. <ref type="bibr" target="#b21">[22]</ref>). Assume that 1) There are a pair of implications from Theorem 4. E pri (?) and E est (?, ?) are independent of the discretization steps.</p><formula xml:id="formula_15">sup z,t s ? * (z, t) ? ? log p ? t (z) ? M , 2) sup z,t ? 2 log p ? t (z) ? K, and 3) sup z,t ? t ? log p ? t (z) / z ? Le ??t , for some K, L, M, ? &gt; 0. Then p r ? (h ?1 ? ) # ? p ? 0,N T V ? E pri (?) + E dis (?) + E est (?, ?) + o( ? ? + e ?T ), where E pri (?) = ? 2e ?T D KL (p ? T ?) 1/2 , E dis (?) = 6 ? ?(1+E[ z 4 ] 1/4 )(1+K+L(1+1/ ? 2?)), and E est (?, ?) = 2T M 2 with ? = max ? k 2 / min ? k . 7ROHUDQFHRI5.2'(6ROYHU ),' ''30? ''30? ''30? ''30? ''30? ,1'0</formula><formula xml:id="formula_16">E dis (?)/ ?</formula><p>? is the discretization sensitivity, entirely determined by the latent distribution's smoothness. To the deep understanding of the second implication, let us assume h ?a (x) = ax for some scalar a &gt; 1, then the sensitivity is antiproportional to a with the identical discretizations, i.e., E dis (? a ) ? 1 a E dis (? id ). With a smaller sensitivity of ? a , there is more room to reduce the number of discretization steps for ? a . <ref type="figure" target="#fig_7">Figure 6</ref> empirically supports the theory, showing that the sampler (at the large tolerance with 10 ?2 ) becomes more robust as a increases, on CIFAR-10.</p><p>Before we derive a concrete result from the implication, observe that the flow h ? is maximizing det(?h ? ) in Eq. (2). To understand the effect of flow training on the discretization sensitivity, let us restrict the hypothesis class of the transformation to be linear mappings of h ?a (x) = ax. Then, as the determinant increases by a, the trained diffusion model would be insensitive to the discretizations. Now, for the general case, <ref type="figure" target="#fig_8">Figure 7</ref> CIFAR-10, respectively. Also, E[ z 4 ] 1/4 of INDM is slightly larger (1.3x) than DDPM++. Therefore, with these observations combined, we conclude that INDM is less sensitive to the discretization steps than DDPM++, from its loss design. Second, the robustness could originate from the geometry of the diffusion trajectory. The forward solution of VPSDE is</p><formula xml:id="formula_17">-(a,b) illustrate ? 2 log p ? t (z) and ? t ? log p ? t (z) / z on (a) ? 2 log p ? t (z) (b) ?t? log p ? t (z) / z 'LIIXVLRQ7LPH &amp;RVLQH6LPLODULW\ ,1'0 ''30 (c) Cosine Similarity</formula><formula xml:id="formula_18">x t = ?(t)x 0 + 1 ? ? 2 (t) with ?(t) = e ? 1 2</formula><p>t 0 ?(s) ds , where the first term is a contraction mapping and the second term is the random perturbation. The contraction mapping points toward the origin, but the overall vector field of the diffusion path points outward because the prior manifold lies outside the data manifold, as shown in <ref type="table" target="#tab_3">Table 2</ref>. This contrastive force leads the drift and volatility coefficients works in repulsive and raises a highly nonlinear diffusion trajectory in DDPM++, see <ref type="figure" target="#fig_0">Figure 11</ref> for a toy illustration. On the other hand, the flow mapping of INDM pushes the latent manifold outside the prior manifold, and the drift and volatility coefficients act coherently. Hence, INDM has the relatively linear diffusion path; see <ref type="bibr">Figures 12,</ref><ref type="bibr" target="#b12">13</ref>, and 14 for a quick intuition. <ref type="figure" target="#fig_8">Figure 7</ref>-(c) measures the cosine similarity of the ODE's diffusion trajectory with the straight line connecting the initial-final points of each trajectory. <ref type="figure" target="#fig_8">Figure 7</ref>-(c) implies that DDPM++ is under an inefficient nonlinear trajectory that reverts backward near the end of the trajectory, as in <ref type="figure" target="#fig_0">Figure 11</ref>. In contrast, INDM trajectory is relatively efficient and linear ( <ref type="figure" target="#fig_0">Figure  15</ref>), which yields robust sampling by discretization steps; see Appendix C.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>This section quantitatively analyzes suggested INDM on CIFAR-10 and CelebA 64 ? 64. Throughout the experiments, we use NCSN++ with VESDE and DDPM++ with VPSDE <ref type="bibr" target="#b0">[1]</ref> as the backbones of diffusion models, and a ResNet-based flow model <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> as the backbone of the flow model. See Appendix F for experimental details. We experiment with a pair of weighting functions for the score training. One is the likelihood weighting <ref type="bibr" target="#b10">[11]</ref> with ?(t) = g 2 (t), and we denote INDM (NLL) for this weighing choice. The other is the variance weighting <ref type="bibr" target="#b7">[8]</ref> ?(t) = ? 2 (t) with an emphasis on FID, and we denote INDM (FID) for this weighting choice.</p><p>We use either the Predictor-Corrector (PC) sampler <ref type="bibr" target="#b0">[1]</ref> or a numerical ODE solver (RK45 <ref type="bibr" target="#b26">[27]</ref>) of the probability flow ODE <ref type="bibr" target="#b0">[1]</ref>. For a better FID, we find the optimal signal-to-noise value <ref type="table" target="#tab_0">(Table  14)</ref>, sampling temperature <ref type="table" target="#tab_0">(Table 15</ref>), and stopping time <ref type="table" target="#tab_0">(Table 16</ref>). Moreover, sampling from z ? T rather than ? improves FID because E pri (?) collapses to zero in Theorem 4, see Appendix F.2.2. We compute NLL/NELBO for performances of density estimation with Bits Per Dimension (BPD). We compute NLL with the uniform dequantization, instead of the variational dequantization <ref type="bibr" target="#b24">[25]</ref> because it requires training an auxiliary network <ref type="bibr" target="#b10">[11]</ref> only for the evaluation after the model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Correction on Likelihood Evaluation</head><p>A continuous diffusion model truncates the time horizon from [0, T ] to [ , T ] to avoid training instability <ref type="bibr" target="#b25">[26]</ref>. In the model evaluation, this positive truncation could potentially be the primary source of poor evaluation <ref type="figure" target="#fig_0">(Figure 1</ref>-(c) of Kim et al. <ref type="bibr" target="#b25">[26]</ref>), so we fix = 10 ?5 as default in our training and evaluation. In the model evaluation, as the score network is untrained on [0, ), we calculate NLL by the Right-Hand-Side (RHS) of Eq. (3),   Here, p m 0 and p m are the model probability distributions at t = 0 and t = , respectively; and p m 0 (?|x ) is the model's reconstruction probability of x 0 given x . RHS of Eq. (3) is a generic formula to compute NLL in continuous diffusion models, including DDPM++, LSGM, and INDM. Previous continuous models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9]</ref> have approximated E x0 [? log p m 0 (x 0 )] by E x0 [? log p m (x 0 )]. There are two significant differences between our and the previous calculation: 1) the input of p m is replaced with x from x 0 <ref type="table" target="#tab_0">(Table 11)</ref>; 2) the residual term of log</p><formula xml:id="formula_19">NLL = E x0 [? log p m 0 (x 0 )] ? E x0,x ? log p m (x ) + log p m 0 (x 0 |x ) p 0 (x |x 0 ) .<label>(3)</label></formula><formula xml:id="formula_20">p m 0 (x0|x )</formula><p>p0 (x |x0) is added. With this modification, our NLL differs from the previous NLL of E x0 [? log p m (x 0 )] by about 0.03-0.06 in VPSDE, see <ref type="table" target="#tab_4">Table 4</ref>. We report both previous/corrected ways in <ref type="table" target="#tab_4">Table 4</ref> and report corrected NLL/NELBO as default; see Appendix E for theoretical justification of our NLL/NELBO corrections.  <ref type="table" target="#tab_7">Table 3</ref>. Therefore, we pre-train the score network by DDPM++ as default. This pretraining is intended to search the data nonlinearity near well-trained linear diffusions. <ref type="table" target="#tab_7">Table 3</ref> shows that training INDM after 500k of pre-training steps performs better than DDPM++ on both NLL and FID. Appendix F.3 conducts the ablation study of pre-training steps.   <ref type="table" target="#tab_6">Table 6</ref>. See Appendix F.8 for an extended comparison and Appendix F.9 for samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Quantitative Results on Image Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Flow Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Application Task: Dataset Interpolation</head><p>The linear SDEs fixedly perturb data variables, so such SDEs should have the end distribution p T (x T ) as an easy-to-compute distribution. With the nonlinear extension, a complex diffusion process exists to transport p <ref type="bibr" target="#b0">(1)</ref> r to another arbitrary p <ref type="bibr" target="#b1">(2)</ref> r . However, a common practice of diffusion models constrains only the starting variable by</p><formula xml:id="formula_21">x ? 0 ? p (1) r , so the ending variable of x ? T is free to deviate from p (2)</formula><p>r . Previous works have tackled this data interpolation task by using a conditional diffusion model <ref type="bibr" target="#b31">[32]</ref> or a couple of jointly trained source-and-target diffusion models <ref type="bibr" target="#b32">[33]</ref> on paired datasets. Among unconditional diffusion models using unpaired datasets, SBP <ref type="bibr" target="#b14">[15]</ref> is a natural approach for the task by imposing bi-constraints with P(p r , ?) replaced by P(p </p><p>r with unpaired datasets. We train the score and flow networks with a loss</p><formula xml:id="formula_23">D KL (? ? ? ?,? ) INDM NELBO + D KL (p (2) r p ? ) Interpolation Loss ,</formula><p>where p ? is the probability distribution of x ? T , which is calculated by a single feed-forward computation of the flow network, see Algorithm 2 in Appendix F.2.3. The additional interpolation loss forces the diffusion bridge {x ? t } T t=0 to ahead towards p <ref type="bibr" target="#b1">(2)</ref> r by minimizing the KL divergence between x ? T ? p ? and p <ref type="bibr" target="#b1">(2)</ref> r . As the destined variable of the bridge becomes</p><formula xml:id="formula_24">x ? T = h ?1 ? (z T ) ? h ?1 ? (?)</formula><p>, the flow network is what constructs the interpolated bridge between a couple of data variables, see <ref type="table" target="#tab_11">Figures 8 and 9</ref>. Particularly, <ref type="figure" target="#fig_10">Figure 9</ref> shows that LSGM fails to interpolate a letter to a digit, and we attribute this failure to the non-existence of a diffusion bridge in LSGM. Also, INDM models p (1) r with p ?,? and p <ref type="bibr" target="#b1">(2)</ref> r with p ? , so we could compute NLL of each dataset separately: 1.10 BPD for MNIST and 1.56 BPD for EMNIST. In contrast, SBP cannot separately estimate densities on each dataset. We emphasize that no additional neural network is needed for the interpolation task with INDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This paper expands the linear diffusion to trainable nonlinear diffusion by combining an invertible transformation and a diffusion model, where the nonlinear diffusion learns the forward diffusion out of variational family of inference measures. A limitation of INDM lies in the training/evaluation time.</head><p>Potential risk from this work is the negative use of deep generative models, such as deepfake images.</p><p>[56] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. PMLR, 2015.</p><p>[57] Cheng Lu, Jianfei Chen, Chongxuan Li, Qiuhao Wang, and Jun Zhu. Implicit normalizing flows.</p><p>In International Conference on Learning Representations, 2021.</p><p>[58] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017.</p><p>[59] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162-8171. PMLR, 2021.</p><p>[60] Lawrence F Shampine. Some practical runge-kutta formulas. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Derivations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Derivation of Variational Bound for Nonlinear Diffusion</head><p>The variational bound derived in Song et al. <ref type="bibr" target="#b10">[11]</ref> is only applicable when G(x t , t) is reduced to g(t)I. This section, therefore, derives the variational bound of a general diffusion SDE of</p><formula xml:id="formula_25">dx t = f (x t , t) dt + G(x t , t)</formula><p>dw t , and we analyze why learning f and G is infeasible if 1) the transition probability of p 0t (x t |x 0 ) is intractable and 2) G is anisotropic by x t .</p><p>From Anderson <ref type="bibr" target="#b9">[10]</ref>, the corresponding reverse SDE of</p><formula xml:id="formula_26">dx t = f (x t , t) dt + G(x t , t) dw t is dx t = f (x t , t) ? div(GG T ) ? GG T ? log p t (x t ) dt + G(x t , t) dw t ,<label>(4)</label></formula><p>and the generative SDE becomes</p><formula xml:id="formula_27">dx t = f (x t , t) ? div(GG T ) ? GG T s(x t , t) dt + G(x t , t) dw t .<label>(5)</label></formula><p>Then, from the Girsanov theorem <ref type="bibr" target="#b33">[34]</ref> and the martingale property <ref type="bibr" target="#b16">[17]</ref>, using the disintegration property of the KL divergence, we have</p><formula xml:id="formula_28">D KL (? ?) = D KL (p T (x T ) ?(x T )) + 1 2 T 0 E xt s(x t , t) ? ? log p t (x t ) T GG T s(x t , t) ? ? log p t (x t ) dt,<label>(6)</label></formula><p>where ? is the path measure of Eq. (4) and ? is the path measure of Eq. (5). Therefore, from the data processing inequality <ref type="bibr" target="#b34">[35]</ref>, we get</p><formula xml:id="formula_29">D KL (p r p) ? D KL (? ?) = D KL (p T (x T ) ?(x T )) + 1 2 T 0 E xt s(x t , t) ? ? log p t (x t ) T GG T s(x t , t) ? ? log p t (x t ) dt,</formula><p>where p is the generative distribution at t = 0.</p><p>Now, from the Fokker-Planck equation, the density function satisfies</p><formula xml:id="formula_30">?p t ?t = ? j ? ?x t,j f j (x t , t)p t (x t ) ? i ? ?x t,j (H ij (x t , t)p t (x t )) , where H(x t , t) = 1 2 G(x t , t)G(x t , t) T .</formula><p>Then, analogous to Theorem 4 of Song et al. <ref type="bibr" target="#b10">[11]</ref>, the entropy becomes</p><formula xml:id="formula_31">H(p r ) ? H(p T ) = ? T 0 ? ?t H(p t ) dt = T 0 ?p t ?t log p t (x t ) dx t dt = ? T 0 j ? ?x t,j f j (x t , t)p t (x t ) ? i ? ?x t,i (H ij (x t , t)p t (x t )) log p t (x t ) dx t dt = T 0 j f j (x t , t)p t (x t ) ? i ? ?x t,i (H ij (x t , t)p t (x t )) ? log p t (x t ) ?x t,j dx t dt = T 0 p t (x t ) j f j (x t , t) ? log p t (x t ) ?x t,j dx t dt ? T 0 j i ?H ij ?x t,i p t + H ij ?p t ?x t,i ? log p t ?x t,j dx t dt = ? T 0 E xt [div(f (x t , t))] dt ? T 0 E xt [div(H(x t , t)) T ? log p t (x t )] + E xt [? log p t (x t ) T H(x t , t)? log p t (x t )] dt = ? 1 2 T 0 E xt 2div(f (x t , t)) + div(G(x t , t)G(x t , t) T ) T ? log p t (x t ) +? log p t (x t ) T G(x t , t)G(x t , t) T ? log p t (x t ) dt.</formula><p>Therefore, the variational bound of the model log-likelihood is derived by</p><formula xml:id="formula_32">E pr(x0) ? log p(x 0 ) = D KL (p r p) + H(p r ) ? D KL (? ?) + H(p r ) = 1 2 T 0 E xt (? log p t (x t ) ? s(x t , t)) T GG T (? log p t (x t ) ? s(x t , t)) dt +E x T ? log ?(x T ) + H(p r ) ? H(p T ) = 1 2 T 0 E xt s(x t , t) ? ? log p t (x t ) T GG T s(x t , t) ? ? log p t (x t ) ?? log p t (x t ) T GG T ? log p t (x t ) ? div(GG T ) T ? log p t (x t ) ? 2div(f (x t , t)) dt +E x T ? log ?(x T ) .</formula><p>Using the integration by parts, this variational bound transforms to</p><formula xml:id="formula_33">E pr(x0) ? log p ? (x 0 ) ? 1 2 T 0 E xt s T GG T s + 2div(GG T s) ? div(GG T )? log p t ? 2div(f ) dt + E x T ? log ?(x T )</formula><p>Also, this variational bound is equivalently formulated as</p><formula xml:id="formula_34">E pr(x0) ? log p(x 0 ) ? 1 2 T 0 E x0,xt s(x t , t) ? ? log p 0t (x t |x 0 ) T GG T s(x t , t) ? ? log p 0t (x t |x 0 ) ? ? log p 0t (x t |x 0 ) T GG T ? log p 0t (x t |x 0 ) ? div(GG T ) T ? log p 0t (x t |x 0 ) ? 2div(f ) dt + E x T ? log ?(x T ) .</formula><p>Therefore, optimizing the nonlinear drift (f ) and diffusion (G) terms are intractable in general for two reasons. First, the transition probability of p 0t (x t |x 0 ) is intractable for nonlinear SDEs. To compute p 0t (x t |x 0 ), one needs the Feynman-Kac formula <ref type="bibr" target="#b11">[12]</ref> which requires expectation on every sample paths, see Appendix E.4.</p><p>Second, even if p 0t (x t |x 0 ) is tractable, computing the above variational bound would not be scalable due to the matrix multiplication of GG T that is of O(d 2 ) complexity and the divergence computation <ref type="bibr" target="#b35">[36]</ref>. These would become the main source of training bottleneck if dimension increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Derivation of Nonlinear Drift and Volatility Terms for INDM</head><p>Throughout this section, we omit ? for notational simplicity. With the linear SDE on latent space</p><formula xml:id="formula_35">dz t = ? 1 2 ?(t)z t dt + g(t) dw t , z 0 = h(x 0 ) with x 0 ? p r ,<label>(7)</label></formula><formula xml:id="formula_36">from x t = h ?1 (z t ), the k-th component of the induced variable satisfies dx t,k = ?h ?1 k ?t dt + ? zt h ?1 k (z t ) T dz t + 1 2 tr ? 2 zt h ?1 k (z t ) dz t dz T t<label>(8)</label></formula><p>by the multivariate Ito's Lemma <ref type="bibr" target="#b16">[17]</ref>. Plugging the linear SDE of Eq. (7), Eq. (8) is transformed to</p><formula xml:id="formula_37">dx t,k = ? zt h ?1 k (z t ) T ? 1 2 ?(t)z t dt + g(t) dw t + 1 2 g 2 (t)tr ? 2 zt h ?1 k (z t ) dt = ? 1 2 ?(t) ? zt h ?1 k (z t ) T z t + 1 2 g 2 (t)tr ? 2 zt h ?1 k (z t ) dt + g(t) ? zt h ?1 k (z t ) T dw t ,<label>(9)</label></formula><p>because ?h ?1 k ?t = 0. Then, Eq. (9) in vector form becomes</p><formula xml:id="formula_38">dx t = f (x t , t) dt + G(x t , t) dw t ,</formula><p>where the vector-valued drift and the matrix-valued volatility terms are given by</p><formula xml:id="formula_39">f (x t , t) = ? 1 2 ?(t)? zt h ?1 (z t )z t + 1 2 g 2 (t)tr ? 2 zt h ?1 (z t ) G(x t , t) = g(t)? zt h ?1 (z t ).<label>(10)</label></formula><p>Here,</p><formula xml:id="formula_40">? 2 zt h ?1 (z t ) is a 3-dimensional tensor with (i, j, k)-th element to be ? 2 zt h ?1 k (z t ) i,j</formula><p>, and the trace operator applied on this tensor is defined as a vector of tr</p><formula xml:id="formula_41">? 2 zt h ?1 1 (z t ) , ..., tr ? 2 zt h ?1 d (z t ) T .</formula><p>From the inverse function theorem <ref type="bibr" target="#b36">[37]</ref>, the Jacobian of the inverse function</p><formula xml:id="formula_42">? zt h ?1 (z t ) equals to the inverse Jacobian ? xt h(x t ) ?1 . Therefore, Eq. (10) is transformed to f (x t , t) = ? 1 2 ?(t) ? xt h(x t ) ?1 h(x t ) + 1 2 g 2 (t)tr ? 2 zt h ?1 (z t ) G(x t , t) = g(t) ? xt h(x t ) ?1 .<label>(11)</label></formula><p>Now, we derive the second term of f in terms of x t as follows: observe that k</p><formula xml:id="formula_43">?h ?1 i ?z t,k ?h k ?xt,j = ? i,j , where ? i,j = 1 if i = j and 0 otherwise. Differentiating both sides with respect to x t,l , we have k ? ?x t,l ?h ?1 i ?z t,k ?h k ?x t,j + ?h ?1 i ?z t,k ? ?x t,l ?h k ?x t,j = 0,</formula><p>where the first term is</p><formula xml:id="formula_44">k,m ?h m ?x t,l ? ?z t,m ?h ?1 i ?z t,k ?h k ?x t,j = k,m ? xt h(x t ) T l,m ? 2 zt h ?1 i (z t ) m,k ? xt h(x t ) k,j ,</formula><p>using the chain rule, and the second term becomes</p><formula xml:id="formula_45">k ?h ?1 i ?z t,k ? ?x t,l ?h k ?x t,j = k ? zt h ?1 (z t ) i,k ? 2 xt h k (x t ) l,j .</formula><p>From the above, we derive the trace term of f in Eq. <ref type="formula" target="#formula_42">(11)</ref> as</p><formula xml:id="formula_46">tr ? 2 zt h ?1 (z t ) = ?tr ? xt h(x t ) ?T ? xt h(x t ) ?1 * ? 2 xt h(x t ) ? xt h(x t ) ?1 , where ? 2 xt h(x t ) is a 3-dimensional tensor with (i, j, k)-th element to be ? 2 xt h k (x t ) i,j .</formula><p>Also, we define * operation as the element-wise matrix multiplication given by</p><formula xml:id="formula_47">? xt h(x t ) ?1 * ? 2 xt h(x t ) i,j := ? xt h(x t ) ?1 ? 2 xt h(x t ) i,j .</formula><p>Combining all together, thus, we derive the nonlinear drift term in Eq. (11) as a function of x t given by</p><formula xml:id="formula_48">f (x t , t) = ? 1 2 ?(t) ? xt h(x t ) ?1 h(x t ) ? 1 2 g 2 (t)tr ? xt h(x t ) ?T ? xt h(x t ) ?1 * ? 2 xt h(x t ) ? xt h(x t ) ?1 .</formula><p>B Details on Section 6.1</p><p>It is one of central topics in the community of VAE to obtain a tighter ELBO <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. This section analyzes the variational gap and further theoretical analysis in diffusion models. Before we start, we remind the generalized Helmholtz decomposition in Lemma 1. Lemma 1 (Helmholtz Decomposition <ref type="bibr" target="#b39">[40]</ref>). Any twice continuously differentiable vector field s that decays faster than z ?c 2 for z 2 ? ? and c &gt; 0 can be uniquely decomposed into two vector fields, one rotation-free and one divergence-free: s = ? log p + u.</p><p>A rotation-free vector field ? log p, or the divergence part, is a score function of a probability density p, and a divergence-free vector field u, or the rotation part, satisfies div(u) ? 0. From this decomposition, any score network is decomposed by s ? (z t , t) = ? log p ? t (z t ) + u ? t (z t ) for some probability p ? t and vector field u ? t , for any t ? (0, T ]. Then, the generative SDE of the full score network</p><formula xml:id="formula_49">dz t = f (z t , t) ? g 2 (t)s ? (z t , t) dt + g(t) dw t<label>(12)</label></formula><p>and the generative SDE of the divergence part</p><formula xml:id="formula_50">dz t = f (z t , t) ? g 2 (t)? log p ? t (z t ) dt + g(t) dw t<label>(13)</label></formula><p>has distinctive path measures. Throughout this section, f (z t , t) does not have to be a linear vector field, such as ? 1 2 ?(t)z t . If ? ? and ? ? are the path measures of SDEs of Eqs. <ref type="formula" target="#formula_7">(12)</ref> and <ref type="formula" target="#formula_19">(13)</ref>, respectively, then using the Girsanov theorem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref> and the martingale property <ref type="bibr" target="#b16">[17]</ref>, we have</p><formula xml:id="formula_51">D KL (? ? ? ? ) = 1 2 T 0 g 2 (t)E zt?? ? |t ? log p ? t (z t ) ? s ? (z t , t) 2 2 dt = 1 2 T 0 g 2 (t)E zt?? ? |t u ? (z t , t) 2 2 dt.<label>(14)</label></formula><p>This KL divergence of two path measures quantifies how much the score network contains the rotation part u ? t . Recall that the forward SDE satisfies</p><formula xml:id="formula_52">dz t = f (z t , t) dt + g(t) dw t ,</formula><p>which starts at p ? 0 , and the marginal distribution of its path measure ? ? at t is p ? t . As NELBO is equivalent to</p><formula xml:id="formula_53">D KL (? ? ? ?,? ) = 1 2 T 0 g 2 (t)E p ? t (zt) s ? (z t , t) ? ? log p ? t (z t ) 2 2 dt + D KL p ? T (z T ) ?(z T ) ,<label>(15)</label></formula><p>for ?-optimization, the optimal ? * satisfies s ? * (z t , t) = ? log p ? t (z t ). At this optimality, ? should satisfy a pair of constraints: 1) the zero-rotation part u ? * t ? 0, which is equivalent to</p><formula xml:id="formula_54">D KL (? ? * ? ? * ) = 0; 2) the coincidence of ? log p ? t (z t ) ? ? log p ? t (z t ).</formula><p>The starting point to analyze the variational gap with respect to the Helmholtz decomposition is the next theorem. We defer the proofs in Section G.</p><formula xml:id="formula_55">Proposition 1. Suppose q ? t is the marginal distribution of ? ? at t. The variational gap is Gap ? ? ({x t }), ? ?,? ({x t }) :=D KL ? ? ({x t }) ? ?,? ({x t }) ? D KL p ? 0 (x 0 ) q ? 0 (x 0 ) = 1 2 T 0 g 2 (t)E p ? t (zt) ? log q ? t (z t ) ? s ? (z t , t) 2 2</formula><p>Score-only error dt.</p><formula xml:id="formula_56">Remark 1. The generative SDE of dz ? t = [? 1 2 ?(t)z ? t ? g 2 (t)s ? (z ? t , t)] dt + g(t)</formula><p>dw t does not necessarily start from the prior ?. Proposition 1 holds for an arbitrary distribution p ? T . At the same spirit, Proposition 1 holds for any distribution p ? 0 . Remark 2. Throughout the section, we follow the assumptions made in Appendix A of Song et al. <ref type="bibr" target="#b10">[11]</ref>. On top of that, we assume that both s ? and q ? t are continuously differentiable.</p><p>The variational gap derived in Proposition 1 does not include the forward score, ? log p ? t (z t ), except for taking the expectation, E p ? t (zt) . Therefore, the gap is intuitively connected to the score training, rather than the flow training. To elucidate the logic, we decompose the variational gap in Proposition 1 into</p><formula xml:id="formula_57">Gap ? ? , ? ?,? = 1 2 T 0 g 2 (t)E p ? t (zt) ? log q ? t (z t ) ? s ? (z t , t) 2 2 dt (16) ? 1 2 T 0 g 2 (t)E p ? t (zt) ? log q ? t (z t ) ? ? log p ? t (z t ) 2 2 + ? log p ? t (z t ) ? s ? (z t , t) 2 2 dt. = 1 2 T 0 g 2 (t)E p ? t (zt) ? log q ? t (z t ) ? ? log p ? t (z t ) 2 2</formula><p>How close is ? ? to forward measure</p><formula xml:id="formula_58">+ u ? t (z t ) 2 2</formula><p>How close is u ? t to zero dt.</p><p>The second term, ? log p ? t (z t ) ? s ? (z t , t) 2 2 (see Eq. <ref type="formula" target="#formula_26">(14)</ref>), equals to the L 2 -norm of the rotation term, u ? t (z t , t) 2 2 , so it measures how close is the score network to the space of</p><formula xml:id="formula_59">S div := {s : R d ? R d | the rotation term of s is zero}.</formula><p>On the other hand, having assumed the rotation part to be zero, the first term,</p><formula xml:id="formula_60">? log q ? t (z t ) ? ? log p ? t (z t ) 2 2</formula><p>, becomes zero only if the generative score ? log q ? t equals to a forward score ? log q t with certain initial distribution q 0 , meaning that if there exists a q 0 and q t is a marginal density of the forward SDE starting from q 0 , then</p><formula xml:id="formula_61">? log q t (z t ) = ? log q ? t (z t ) is equivalent to ? log q t (z t ) = ? log p ? t (z t ),</formula><p>and only in that case, ? log q ? t (z t ) = ? log p ? t (z t ). Therefore, the gap becomes tight if 1) u ? t ? 0 and 2) ? log q ? t = ? log q t for some q t following the forward SDE, which is concretely proved in Lemma 2. It turns out that this is the only case of the gap being zero proved in Theorem 2. For that, we provide a rigorous definition of the class of score functions of interest as below. Definition 1. Let S sol ? S div be a sub-family of rotation-free score functions s :</p><formula xml:id="formula_62">R d ? R d such that s(z t , t) = ? log p t (z t ) almost everywhere for p t that is the marginal distribution of the path measure of dz t = f (z t , t) dt + g(t) dw t at t.</formula><p>Remark 3. Analogous to Theorem 1, no condition for the starting and ending variables is imposed in</p><formula xml:id="formula_63">Definition 1. Remark 4. S sol is the space of score functions of the forward SDE dz t = f (z t , t) dt + g(t) dw t with arbitrary initial variable.</formula><p>Although Song et al. <ref type="bibr" target="#b10">[11]</ref> focused on the data diffusion, their theory is applicable for a diffusion process that starts with an arbitrary initial distribution. Lemma 2 describes the theoretic analysis done by Song et al. <ref type="bibr" target="#b10">[11]</ref>. Lemma 2 (Theorem 2 of Song et al. <ref type="bibr" target="#b10">[11]</ref>).</p><formula xml:id="formula_64">Gap(? ? , ? ?,? ) = 0 if s ? ? S sol .</formula><p>With Lemma 2, however, we cannot certainly be sure that the score network s ? of INDM falls to S sol when the variational gap is zero. Thus, we take a step further to identify the connection of zero variational gap and the class of rotation-free score functions S sol in Theorem 2. This Theorem 2 completely characterizes all admissible score networks that achieve the zero variational gaps, and we are certain that the zero variational gap implies s ? ? S sol , which turns out to be a solution space in Theorem 3. From Theorem 2, the variational gap is strictly positive as long as the rotation part of the score network remains to be nonzero. NELBO of Eq. (15) optimizes its score network to-</p><formula xml:id="formula_65">wards s ? (z t , t) ? ? log p ? t (z t ) := s ? (z t , t), which is equivalent to log p ? t (z t ) ? ? log p ? t (z t ) (or equiv- alently, log q ? t (z t ) ? ? log p ? t (z t )) and u ? t (z t , t) ? 0.</formula><p>In contrast to DDPM++ with fixed ?, optimizing D KL (? ? ? ?,? ) w.r.t ? finds the closest s ? among S sol to s ? . Thus, if s ? ? S sol , then s ? * = s ? , which is proved in Theorem 3. If s ? / ? S sol , then s ? * is not equal to s ? , anymore, but s ? * will be the closest among S sol to s ? because</p><formula xml:id="formula_66">D KL (? ? ? ?,? ) is the weighted L 2 -norm of s ? ? s ? . Theorem 3. For any fixed s? ? S sol , if ? * ? arg min ? D KL (? ? ? ?,? ), then s ? * (z t , t) = ? log p ? * t (z t ) = s?(z t , t), and D KL (? ? * ? ? * ,? ) = D KL (p r p ? * ,? ) = Gap(? ? * , ? ? * ,? ) = 0.</formula><p>Indeed, Theorem 3 implies that the whole class of S sol is the solution space, which means that any s ? in S sol is a candidate for an optimal score function as there always exists ? * corresponding to a given ? that achieves the perfect match of the model distribution to the data distribution. This is contrastive to DDPM++ that only has a unique optimal point of s ? * (z t , t) = ? log p ? id t (z t ) ? S sol . <ref type="figure" target="#fig_0">Figure 10</ref> illustrates that the optimal point of DDPM is a single point in S sol , whereas any s ? ? S sol is a candidate for the optimal point of INDM by Theorem 3. In other words, the number of DDPM optimality is one, while INDM has infinite number of optimalities.</p><p>B.1 Restricting Search Space of s ? into S div Due to the space limit, the argument in this section has not been included in the main paper. Below, we provide the rationale that it is the number of optimal points that affect the NLL performance. For that, we optimize DDPM++ with a regularization, suggested in Proposition 5. This regularization restricts the score network from not deviating S div too far by keeping the rotation term, u ? t , being consistently small. Consequently, a fastly converging rotation term is advantageous in reducing the variational gap (see Inequality <ref type="formula" target="#formula_28">(16)</ref>), and this regularization helps the MLE training of DDPM++.</p><p>Proposition 2 proves that S div is identical to a class of score functions that have symmetric derivatives. From this, Proposition 3 provides a motivation of the regularization by proving that a symmetric matrix satisfies a certain equality. Then, Proposition 4 implies that the formula suggested in Proposition 3 indeed measures how close is the matrix symmetric. Lastly, Proposition 5 provides the minimum variance estimator of the formula. With these propositions, we conclude that the constraint of</p><formula xml:id="formula_67">E 1, 2 T 2 ?s ? (z t , t) ? (?s ? ) T (z t , t) 1 2 = 0<label>(17)</label></formula><p>with 1 and 2 sampled from the random variable suggested in Proposition 5 would optimize s ? in the space of S div . Using the Lagrangian form, we could add the left-hand-side of Eq. (17) as a regularization term in NELBO to force the score network not deviate from S div too much.</p><p>With the clear mathematical properties, however, obtaining the full matrix of ?s ? is a bottleneck in the computation of the regularization term. Specifically, each row of ?s ? needs to be computed separately <ref type="bibr" target="#b40">[41]</ref>, so it takes O(d) complexity to compute ?s ? , which is prohibitively expensive. Therefore, we use a trick to reduce O(d) to O(1) motivated from the Hutchinson's estimator <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b22">23]</ref>: first, we compute the gradient of T 2 s ? and T 1 s ? , separately. Afterwards, we apply vector multiplication between 1 and ?( T 2 s ? ), which gives us T 2 ?s ? 1 ; and analogously, the multiplication of 2 with ?( T 1 s ? ) yields T 2 (?s ? ) T 1 . This trick requires only second time of gradient computations to estimate the regularization. Hence, the computational complexity of T 2 (?s ? ? ?s T ? ) 1 is O(1). Proposition 2. s ? ? S div if and only if ? zt s ? (z t , t) is symmetric.</p><formula xml:id="formula_68">Proposition 3. A matrix A ? R d?d is symmetric if and only if E 1 , 2?N (0,I) ( T 2 (A ? A T ) 1 ) 2 = 0.</formula><p>In fact, we can prove a bit stronger results in the next propositions. Proposition 4. Let 1 and 2 be vectors of d independent samples from a random variable U with mean zero. Then</p><formula xml:id="formula_69">E 1, 2 [( T 2 (A ? A T ) 1 ) 2 ] = E U [U 2 ] 2 A ? A T 2 F and Var T 2 (A ? A T ) 1 2 = Var(U 2 ) Var(U 2 ) + 2 Var(U ) + E U [U ] 2 2 a,b (?A) 4 ab + 2 Var(U ) + E U [U ] 2 2 3Var(U 2 ) + 2 Var(U ) + E U [U ] 2 2 a b =d (?A) 2 ab (?A) 2 ad + 2 Var(U ) + E U [U ] 2 4 a =c b =d (?A) 2 ab (?A) 2 cd + 3 a =c b =d (?A) ab (?A) ad (?A) cb (?A) cd ,</formula><p>where (?A) ab := A ab ? A ba .</p><p>Proposition 5. Let U be the discrete random variable which takes the values 1, ?1 each with probability 1/2. Then ( T 2 (A ? A T ) 1 ) 2 is the unbiased estimator of A ? A T 2 F . Moreover, U is the unique random variable amongst zero-mean random variables for which the estimator is an unbiased estimator, and attains a minimum variance.</p><p>Summing altogether, if it is the main focus to eliminate the rotation term in the score estimation, we could optimize D KL (? ? ? ?,? ) + ?E 1, 2 ( T 2 (?s ? ? ?s T ? ) T 1 ) 2 , where 1 and 2 are the random variables of minimum variance, as proposed in Proposition 5. In practice, we find that the above regularized training loss is unnecessary for INDM because we already achieves the nearly MLE training, but it helps DDPM++ to reduce the variational gap at the expense of 4? slower training speed than the training with unregularized loss in DDPM++. Even with reduced variational gap, we find that NLL of DDPM++ is improved only marginally only on certain training scenarios, and has no effect in most trials, so we leave the detailed effect of MLE training in diffusion models as a future work. Notably, therefore, we conclude that the NLL gain in INDM, compared to DDPM++, essentially originates from ?-training and its consequential expanded solution space to S sol . C Details on Section 6.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Full Statement of Theorem 4</head><p>We provide a full statement of Theorem 4. Theorem 4 is heavily influenced by the theoretic analysis of De Bortoli et al. <ref type="bibr" target="#b14">[15]</ref>, Guth et al. <ref type="bibr" target="#b21">[22]</ref>, and it could be considered as merely an application of their results. It is possible that the inequality in Theorem 4 could not be tight, but empirically the robustness is significantly connected to the initial distribution's smoothness.</p><p>Theorem 4. Assume that there exists M ? 0 such that for any t ? [0, T ] and z ? R d , the score estimation is close enough to the forward score by M ,</p><formula xml:id="formula_70">s ? (x, t) ? ? log p ? t (x) ? M , with s ? ? C([0, T ]?R d , R d ). Assume that ? log p ? t (z) is C 2 in both t and z, and that sup z,t ? 2 log p ? t (z) ? K and ? ?t ? log p ? t (z) ? M e ??t z for some K, M, ? &gt; 0. Suppose (h ?1 ? ) # s a push- forward map. Then p r ? (h ?1 ? ) # p ? 0,N T V ? E pri (?) + E dis (?) + E est (?, ?), where E pri (?) = ? 2e ?T D KL (p ? T ?) 1/2 is the error originating from the prior mismatch; E dis (?) = 6 ? ?(1 + E p ? 0 (z) [ z 4 ] 1/4 )(1 + K + M (1 + 1 ? 2? ))</formula><p>is the discretization error with ? = max ? k 2 min ? k ; E est (?, ?) = 2T M 2 is the score estimation error.   <ref type="figure" target="#fig_0">Figure 11</ref> illustrates the diffusion trajectories of the probability flow ODE of VPSDE. It shows that the trajectories are highly nonlinear, and this section is devoted to analyze why such nonlinear trajectory occurs. <ref type="figure" target="#fig_0">Figure 12</ref> shows two diffusion paths differing only on their scales on (a) the two moons dataset and (b) the ring dataset. The standard Gaussian distribution at T has a larger variance than the initial data at the top row and has a smaller one at the bottom row on each dataset. For the visualization purpose, we zoom in the top row, and we zoom out the bottom row for each dataset, but we fix the xlim and ylim arguments in the matplotlib package <ref type="bibr" target="#b42">[43]</ref> rowwisely. With this discrepancy of the initial data scale, the particle trajectory at the bottom row is much more straightforward than in the top row, and it implies that the scale of initial data matters to the straightness of the bridge even if the diffusion SDE is identically linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Geometric Interpretation of Latent Diffusion</head><p>A behind rationale for this observation comes from the closed-form solution of VPSDE. Suppose the forward diffusion follows VPSDE of dx t = ? 1 2 ?(t)x t dt + ?(t) dw t . Then, the solution of this   where ? N (0, I). As the drift term ? 1 2 ?(t)x t ahead towards the origin of R d , the solution in Eq. <ref type="formula" target="#formula_36">(18)</ref> is a summation of the contraction mapping to the origin, 0 ? R d , with a random noise function, where the magnitude of the random perturbation depends solely on the diffusion coefficient,</p><formula xml:id="formula_71">DDPM++ (x 0 = z ? id 0 ) -0.05 0.25 -1 1 INDM (z</formula><formula xml:id="formula_72">g(t) = ?(t). If x 0 is inflated by cx 0 , then it be- comes x t = c ? e ? 1 2 t 0 ?(s) ds x 0 + 1 ? e ? t</formula><p>0 ?(s) ds with contraction mapping multiplied by c. Therefore, as c increases, the contraction force outweighs the random perturbing effect, and the particle trajectory is becoming straight.</p><p>On a high-dimensional dataset, most of the mass of the standard Gaussian ? = N (0, I), which is the prior, is concentrated on a thin spherical shell with squared radius of d, according to the Gaussian annulus theorem <ref type="bibr" target="#b43">[44]</ref>, as described in the black circle of <ref type="figure" target="#fig_0">Figure 13</ref>. On CIFAR-10, the data distribution has the smaller average square radius of E pr(x0) x 0 2 2 = 776 &lt; 3072 = d, whereas the latent distribution has a larger average square radius of</p><formula xml:id="formula_73">E pr(x0) h ? (x 0 ) 2 2 = E p ? 0 (z ? 0 ) z ? 0 2</formula><p>2 &gt; d than a standard Gaussian distribution. The latent radius varies from 5, 385 to 31, 399 by experimental settings. Thus, the latent manifold is located outside of the prior on CIFAR-10 as depicted in <ref type="figure" target="#fig_0">Figure 13</ref>.</p><p>When the latent manifold envelops the prior manifold, i.e., z ? 0 2 &gt; z ? T 2 , the drift term, ? 1 2 ?(t)z ? t , and the vector of z ? T ? z ? 0 aligns towards the origin. On the other hand, if the initial manifold is located inside the prior manifold, i.e., x 0 2 &lt; x T 2 , then the drift term points towards the opposite direction of x T ? x 0 . This leads that the contraction mapping disturbs the particle to move towards x T , and it is the random perturbation that leads the particle to converge to x T . In latent trajectory, the contraction mapping driven by the drift term helps the particle moving towards z ? T . Therefore, the particle trajectory is more straightforward in the latent trajectory, which moves outside of the prior manifold, compared to the data trajectory that lives inside of the prior manifold. This clarifies why the sampling-friendly bridge is constructed in INDM.  <ref type="figure" target="#fig_0">Figure 14</ref>: (a,b) Latent manifold by training iterations (c,d) Diffusion trajectories by training iterations. We use Python Optimal Transport (POT) library <ref type="bibr" target="#b44">[45]</ref> to obtain the optimally transported Monge map between 1,000 samples from the latent starting variable and the latent ending variable. We only visualize 10 samples out of 1,000 transport maps for a clear implication. In (c), we train the score network further until converged (with the fixed flow) to visualize accurate diffusion paths. <ref type="figure" target="#fig_0">Figure 14</ref> presents the 2d toy case of the two moons dataset. It illustrates a simple visualization of the flow training. <ref type="figure" target="#fig_0">Figure 14</ref> shows that even though the latent manifold is located near the data manifold at the initial phase of training in <ref type="figure" target="#fig_0">Figure 14</ref> In addition, <ref type="figure" target="#fig_0">Figure 14</ref> illustrates the Monge trajectories between the latent initial distribution and the prior distribution. As theoretically demonstrated in Gaussian and empirically shown in general distribution in Khrulkov and Oseledets <ref type="bibr" target="#b45">[46]</ref>, the encoder map of VPSDE is nearly optimal transport under the squared Euclidean cost function, where the encoder map is the mapping from the initial point to the final point passed through the probability flow ODE. <ref type="figure" target="#fig_0">Figure 14</ref> supports this, and the diffusion trajectory becomes more straight alike to the optimal Monge map after the training.  The diffusion process on latent space is firstly introduced in LSGM. LSGM transforms the data variable to a latent variable, and estimates the prior distribution with a diffusion model. Suppose ?, ?, and ? represent for the parameters for the score network, the encoder network, and the decoder network, respectively. Then, LSGM optimizes the loss of</p><formula xml:id="formula_74">D KL (p r p ?,? ) ? D KL p r (x 0 )q ? (z 0 |x 0 ) p ? (z 0 )p ? (x 0 |z 0 ) = D KL p r (x 0 )q ? (z 0 |x 0 ) q ? (z 0 )p ? (x 0 |z 0 ) + D KL q ? (z 0 ) p ? (z 0 ) ? D KL p r (x 0 )q ? (z 0 |x 0 ) q ? (z 0 )p ? (x 0 |z 0 ) + D KL ? ? ({z t } T t=0 ) ? ? ({z t } T t=0 ) = L LSGM (?, ?, ?) where q ? (z 0 ) is the marginal distribution of the encoder posterior, q ? (z 0 ) = p r (x 0 )q ? (z 0 |x 0 ) dx 0 .</formula><p>As well as INDM, LSGM also optimizes the log-likelihood of the model distribution by using a diffusion model in the latent space. Though both INDM and LSGM losses include a denoising score loss on the latent space (which is the KL divergence between path measures on the latent space), L LSGM (?, ?, ?) is not equivalent to the KL divergence between the forward and generative path measures on the data space, in contrast to</p><formula xml:id="formula_75">INDM with D KL (? ? ({x t } T t=0 ? ?,? ({x t } T t=0</formula><p>) as its loss function. In fact, there is no forward SDE (green path in <ref type="figure" target="#fig_1">Figure 3</ref>) on the data space in LSGM according to Lemma 3, which is a direct application of the Borsuk-Ulam theorem <ref type="bibr" target="#b46">[47]</ref>. Lemma 3 (R n is not homeomorphic to R m <ref type="bibr" target="#b46">[47]</ref>). If n = m, there is no continuous map E : R n ? R m that has the continuous inverse map E ?1 : R m ? R n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3 implies that there is no inverse function of the encoder as long as the latent dimension is different from the data dimension (and the activation function is continuous, such as ReLU). From this, LSGM cannot define a random variable on the data space by</head><formula xml:id="formula_76">x ? t = E ?1 ? (z t ), in contrast to INDM that defines x ? t = h ?1 ? (z t ).</formula><p>This non-existence of random variables on the data space implies that the forward diffusion process does not exists as long as the latent dimension differs to the data dimension.</p><p>With the above theoretic dilemma of LSGM, one could build a generative diffusion process on the data space. If x ?,? t := D ? (z ? t ), where z ? t is a generative random variable on the latent space, and D ? is a decoder map, then we could build a generative diffusion process on the data space through the Ito's formula in the same way as we did in INDM. Inspired by this, one could argue that the forward diffusion could be constructed by x ? t := D ? (z t ), where z t is a forward random variable on the latent space. This construction enables to construct a forward diffusion process on the latent space, but there are a couple of caveats to this construction. Theoretically, this forward diffusion process starts from the reconstructed variable,</p><formula xml:id="formula_77">x ? 0 = D ? (z 0 ) = D ? (E ? (x 0 )) = x rec ,</formula><p>where x 0 and x rec differs throughout the training procedure. In addition, even if we admit {x ? t } as a forward diffusion, L LSGM (?, ?, ?) cannot be derived as the KL divergence of path measures for the forward diffusion (admittably {x ? t }, but not true to be precise) and the generative diffusion (x ?,? t ) on the data space. Instead, the loss contains the encoder parameters to optimize, and the loss diverges from the KL divergence on the data space. Also, hypothetically, even if the loss is the KL divergence of the forward and generative path measures on the data space, the optimization could be drifted away from the optimal point because the forward diffusion starts from untrained reconstructed variable, x rec , which is not close to the data variable, x 0 . This analysis provides a clue to explain the training instability of LSGM as reported in Vahdat et al. <ref type="bibr" target="#b8">[9]</ref> and Dockhorn et al. <ref type="bibr" target="#b19">[20]</ref>, in contrast to INDM that is stable to train in any training configuration. <ref type="table" target="#tab_11">Table 8</ref> shows a fast comparison of LSGM and INDM with variance weighting function, sampled from t ? U[0, 1]. NaN indicates experiments that fail due to training instability, see Section 5.2 and <ref type="table" target="#tab_6">Table 6</ref> of Vahdat et al. <ref type="bibr" target="#b8">[9]</ref> and Section E.2.7 of Dockhorn et al. <ref type="bibr" target="#b19">[20]</ref>. Moreover, <ref type="table" target="#tab_12">Table 9</ref> compares INDM with LSGM in terms of the latent dimension. We compute the latent dimension of LSGM, according to their paper and released checkpoint. Contrary to the dimensional reduction property which is the crux of the auto-encoding structure, LSGM maps data into a latent space of a much higher dimension than the data dimension. LSGM is known to perform well, but having observed 15x higher latent dimension than the data dimension on CIFAR-10, the good performance was not gained for free. On the other hand, INDM always retains the same dimension to the data, while keeping the invertibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Diffusion Normalizing Flow (DiffFlow)</head><p>The Girsanov theorem <ref type="bibr" target="#b33">[34]</ref> proves that the variational bound is derived by</p><formula xml:id="formula_78">D KL (p r p ? ) ? 1 2 T 0 g 2 (t)E pr(x0) E p0t(xt|x0) s ? (x t , t) ? ? xt log p 0t (x t |x 0 ) 2 2 dt + D KL (p T ?).<label>(19)</label></formula><p>When the forward diffusion is given as</p><formula xml:id="formula_79">dx t = f ? (x t , t) dt + g(t) dw t ,</formula><p>where f ? is an explicit parametrization of the drift term by a normalizing flow with parameters ?, then the transition probability, p 0t (x t |x 0 ), becomes intractable. Therefore, optimizing the continuous variational bound is not feasible. One might detour this issue by alternatively optimizing the continuous DDPM++ loss of</p><formula xml:id="formula_80">T 0? (t)E pr(x0) E ?N (0,I) ?? ? (x t , t) 2 2 dt,<label>(20)</label></formula><p>but the denoising score loss of Eq. <ref type="formula" target="#formula_37">(19)</ref> is not equivalent to the continuous DDPM++ loss of Eq. <ref type="bibr" target="#b19">(20)</ref> when the transition probability is no longer a Gaussian distribution.</p><p>DiffFlow detours the intractability issue of the continuous loss of Eq. (19) by discretizing the nonlinear SDE in the Euler-Maruyama (EM) fashion <ref type="bibr" target="#b47">[48]</ref>. We construct the discrete random variables that approximate the nonlinear SDE by the induction. If x ?,EM t0 := x ? 0 and ?t i := t i ? t i?1 , where {t i } N t=0 are discretization timesteps with t 0 = 0 and t N = T , then the solution of the nonlinear SDE that starts from x ?,EM ti?1 is</p><formula xml:id="formula_81">x ? ti ? x ?,EM ti?1 = ti ti?1 f ? (x ? t , t) dt + ti ti?1 g(t) dw t .<label>(21)</label></formula><p>Here, the integral of the drift term is</p><formula xml:id="formula_82">ti ti?1 f ? (x ? t , t) dt = ti ti?1 f ? x ?,EM ti?1 + (x ? t ? x ?,EM ti?1 ), t i?1 + (t ? t i?1 ) dt = ti ti?1 f ? (x ?,EM ti?1 , t i?1 ) dt + O(?t 3/2 i ) =f ? (x ?,EM ti?1 , t i?1 )?t i + O(?t 3/2 i )</formula><p>, and the integral of the volatility term is</p><formula xml:id="formula_83">ti ti?1 g(t) dw t = g(t i?1 )(w ti ? w ti?1 ) + O(?t 3/2 i ) = g(t i?1 ) ?t i + O(?t 3/2 i ),</formula><p>where ? N (0, I). Therefore, DiffFlow defines the next discretized random variable, x ?,EM ti , to be</p><formula xml:id="formula_84">x ? ti =x ?,EM ti?1 + f ? (x ?,EM ti?1 , t i?1 )?t i + g(t i?1 ) ?t i + O(?t 2/3 i ) ?x ?,EM ti?1 + f ? (x ?,EM ti?1 , t i?1 )?t i + g(t i?1 ) ?t i =x ?,EM ti ,</formula><p>and this Euler-Maruyama random variable x ?,EM ti follows a Gaussian distribution of mean x ?,EM ti?1 + f ? (x ?,EM ti?1 , t i?1 )?t i and variance g 2 (t i?1 )?t i . Note that this discretization approximates the nonlinear SDE with a finite Markov chain of {x EM ti } N i=0 . DiffFlow constructs the generative process as</p><formula xml:id="formula_85">x ? ti?1 = x ? ti ? f ? (x ? ti , t i ) ? g 2 (t i )s ? (x ? ti , t i ) ?t i + g(t i ) ?t i .</formula><p>Then, from the Jensen's inequality, the discrete DDPM loss satisfies</p><formula xml:id="formula_86">D KL (p r p ?,? ) ? N ?1 i=1 E pr(x EM t 0 ) E p ? (x EM t i ,x EM t i?1 |x EM t 0 ) D KL (p ? (x EM ti?1 |x EM ti , x EM t0 ) p ? (x EM ti?1 |x EM ti )) .<label>(22)</label></formula><p>While the true inference distribution on the continuous variables, p ? (x ti?1 |x ti , x t0 ), is not a Gaussian distribution due to terms related to O(?t 3/2 i ), the inference distribution on the discretized variables,</p><formula xml:id="formula_87">p ? (x EM ti?1 |x EM ti , x EM t0 )</formula><p>, becomes a Gaussian distribution by the Euler-Maruyama-style discretization. Therefore, Eq. (22) reduces to a tractable loss that does not need to compute the transition probability:</p><formula xml:id="formula_88">D KL (p r p ?,? ) ? N ?1 i=1 E pr(x EM t 0 ) E p ? (x EM t i ,x EM t i?1 |x EM t 0 ) D KL (p ? (x EM ti?1 |x EM ti , x EM t0 ) p ? (x EM ti?1 |x EM ti )) = 1 2 N ?1 i=1 E pr(x EM t 0 ) E p ? (x EM t i ,x EM t i?1 |x EM t 0 ) 1 g 2 (t i )?t i x EM ti?1 ? x EM ti + f ? (x EM ti , t i ) ? g 2 (t i )s ? (x EM ti , t i ) ?t i 2 2 = L DiffFlow (?, ?)<label>(23)</label></formula><p>While Eq. (23) does not need to compute the transition probability, another issue of optimizing the variational bound originates from the expectation of</p><formula xml:id="formula_89">E p ? (x EM t i ,x EM t i?1 |x EM t 0</formula><p>) . The empirical Monte-Carlo estimation is too expensive because a realization of x EM ti needs i number of flow evaluations. In total, summing i over i = 1 to N requires O(N 2 ) flow evaluations to estimate the discrete variational bound of Eq. (23). Therefore, DiffFlow exchanges the summation and the expectation to reduce the number of flow evaluations by </p><formula xml:id="formula_90">L DiffFlow (?, ?) = 1 2 E {xt i } N ?1 i=0 ?p ? (xt 0 ,...,xt N ?1 ) N ?1 i=1 1 g 2 (t i )?t i x ti?1 ? x ti + f ? (x ti , t i ) ? g 2 (t i )s ? (x ti , t i ) ?t i 2 2 .<label>(24)</label></formula><formula xml:id="formula_91">= x EM ti?1 + f ? (x EM ti?1 , t i?1 )(t ? t i?1 ) + g(t i?1 ) ? t ? t i?1 on time range of t ? [t i?1 , t i ), then we have E x t ? x EM t 2 ? C ?t i ,<label>(25)</label></formula><p>where</p><formula xml:id="formula_92">C = C(T, K, E[ x 0 2 2 ]) ? O(K 2 ) is a constant with K being a Lipschits constant of f ? (x, t) ? f ? (y, t) 2 ? K x ? y 2 and f ? (x, t) 2 + |g(t)| ? K(1 + x 2 )</formula><p>for all x, y ? R d and t ? [t i?1 , t i ). Having that ?t i is fixed a-priori, the upper bound in Inequality (25) could be arbitrarily large becuase it depends on K that represents the magnitude of nonlinearity of f ? . For instance, if f ? (x t , t) = x 2 t , then there does not exist any K &gt; 0 that satisfies above Lipschitz bounds. In such case, it is unable to guarantee the tightness of the discretized Markov chain to the continuous nonlinear SDE in the classical sense. Therefore, the Euler-Maruyama approximation of the nonlinear SDE should take N as many as possible if we want to regard the finite Markov chain as a discretized nonlinear SDE, which would eventually increase the training, evaluation, and sampling time.</p><p>Second, the computational complexity of INDM is O(1) because the flow is evaluated only once at every optimization step. This is because the INDM loss is simply an addition of the flow loss and the linear diffusion loss. The training time of DiffFlow will be prohibitive as N increases.</p><p>Third, our INDM jointly models both drift and volatility terms nonlinearly, whereas DiffFlow nonlinearly models only the drift term. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> and 2-(c) in the main paper, nonlinearizing the volatility term brings a different diffusion to the overall process, compared to a diffusion that arises from a nonlinear drift. In particular, <ref type="figure" target="#fig_3">Figure 2</ref>-(c) depicts that the data-dependent volatility term yields an ellipsoidal covariance in the noise distribution, which was assumed to have a fixed diagonal covariance in previous research, as illustrated in <ref type="figure" target="#fig_7">Figure 6</ref>. In INDM, this covariance becomes the subject of matter to optimize.</p><p>DiffFlow, as its current form, cannot impose nonlinearity to the volatility term because the discretized Markov chain is not a Gaussian distribution, anymore. To clarify, suppose a SDE of <ref type="figure" target="#fig_1">Figure 3</ref> in the main paper) starts from a random variable x EM ti?1 . The next discrete random variable of the Euler-Maruyama discretization is the approximate solution of this SDE at t = t i , so let us approximate the right-hand-side of Eq. (26):</p><formula xml:id="formula_93">dx t = f ? (x t , t) dt + G ? (x t , t) dw t (think of the green path of</formula><formula xml:id="formula_94">x ti ? x EM ti?1 = ti ti?1 f ? (x t , t) dt + ti ti?1 G ? (x t , t) dw t .<label>(26)</label></formula><p>The integral of the volatility term is</p><formula xml:id="formula_95">ti ti?1 G ? (x t , t) dw t = ti ti?1 G ? x EM ti?1 + (x t ? x EM ti?1 ), t i?1 + (t ? t i?1 ) dw t and since x t ? x EM ti?1 = G ? (x EM ti?1 , t i?1 )(w t ? w ti?1 ) + O(?t i ), we get ti ti?1 G ? (x t , t) dw t = G ? (x EM ti?1 , t i?1 )(w ti ? w ti?1 ) +G ? (x EM ti?1 , t i?1 ) ?G ? (x t , t) ?x t | x EM t i?1 ti ti?1 w t ? w ti?1 dw t + O(?t 2 i ) = G ? (x EM ti?1 , t i?1 )(w ti ? w ti?1 ) +G ? (x EM ti?1 , t i?1 )? x EM t i?1 G ? (x EM ti?1 , t i?1 ) 1 2 (w ti ? w ti?1 ) 2 ? ?t i + O(?t 2 i ) = G ? (x EM ti?1 , t i?1 ) ?t i + 1 2 G ? (x EM ti?1 , t i?1 )? x EM t i?1 G ? (x EM ti?1 , t i?1 ) 2 ? 1 ?t i + O(?t 2 i ),</formula><p>where ? N (0, I) and</p><formula xml:id="formula_96">ti ti?1 w t ? w ti?1 dw t = ti ti?1 w t dw t ? w ti?1 (w ti ? w ti?1 ) = ti ti?1 1 2 d(w 2 t ) ? ti ti?1 1 2 dt ? w ti?1 (w ti ? w ti?1 ) = 1 2 (w 2 ti ? w 2 ti?1 ? ?t i ) ? w ti?1 (w ti ? w ti?1 ) = 1 2</formula><p>(w ti ? w ti?1 ) 2 ? ?t i is according to the Ito's formula <ref type="bibr" target="#b16">[17]</ref>. As G ? (x t , t) now depends on x t , the term including ( 2 ? 1) does not vanish. Therefore, x EM ti is approximated by</p><formula xml:id="formula_97">x ti =x EM ti?1 + f ? (x EM ti?1 , t i?1 )?t i + G ? (x EM ti?1 , t i?1 ) ?t i + 1 2 G ? (x EM ti?1 , t i?1 )? x EM t i?1 G ? (x EM ti?1 , t i?1 ) 2 ? 1 ?t i + O(?t 3/2 i ) ?x EM ti?1 + f ? (x EM ti?1 , t i?1 )?t i + G(x EM ti?1 , t i?1 ) ?t i + 1 2 G ? (x EM ti?1 , t i?1 )? x EM t i?1 G ? (x EM ti?1 , t i?1 ) 2 ? 1 ?t i :=x EM ti .<label>(27)</label></formula><p>The order of the term</p><formula xml:id="formula_98">1 2 G ? (x EM ti?1 , t i?1 )? x EM t i?1 G ? (x EM ti?1 , t i?1 ) 2 ? 1 ?t i is O(?t i )</formula><p>, which is the same order of the term f ? (x EM ti?1 , t i?1 )?t i . Thus, this last term including 2 cannot be ignored in the approximation.</p><p>With this approximation, the discretized random variable, x EM ti , includes a term of 2 , which is the square of the Brownian motion that does not follow a Gaussian distribution. Therefore, the variational bound of Eq. <ref type="formula" target="#formula_7">(22)</ref> is no longer reduced to a tractable loss, such as Eq. <ref type="formula" target="#formula_7">(23)</ref>, and as a consequence, Eq. <ref type="formula" target="#formula_7">(22)</ref> is not optimizable even though the nonlinear SDE is discretized. Therefore, we have to ignore the last term, 1 2 G ? ?G ? ( 2 ? 1)?t i , to tractably optimize the variational bound, but such ingorance equals to the approximation of DiffFlow, which would incur a large approximation error if G ? nonlinearly depends on x t . This leads DiffFlow limited on G ? (x t , t) = g ? (t), at its maximal capacity. This is contrastive to the result of INDM illustrated in <ref type="figure" target="#fig_7">Figure 6</ref>.</p><p>Fourth, as the generative process of DiffFlow starts from an easy-to-sample prior distribution, the flexibility of f ? is severely restricted to constrain p ?</p><formula xml:id="formula_99">T (x ? T ) ? ?(x ? T ).</formula><p>The feasible space of nonlinear f ? that satisfies this constraint does not seem to be derived explicitly. Contrastive to DiffFlow, the data diffusion does not have to end at ? in INDM. Instead, INDM assumes the linear diffusion on the latent variable, so the ending variable on the latent space, z ? T , is already close to the prior distribution. Therefore, the space of admissible nonlinear drift in INDM, which is explicitly desribed in Eq. (10), should be larger than the space of DiffFlow. A lesson from this is that the explicit parametrization seems to be intuitive, but underneath the surface, not many properties could be uncovered explicitly, whereas the implicit parametrization using the invertible transformation enjoys its explicit derivations that enable to analyze concrete properties.</p><p>Fifth, DiffFlow estimates its loss of Eq. (24) using a single (or multiple) path to update the parameters with the reparametrization trick <ref type="bibr" target="#b48">[49]</ref>. On the other hand, the discretized diffusion model estimates its loss with Eq. (23), where the sampling from p 0t (x t |x 0 ) is inexpensive because the transition probability is a Gaussian distribution. Therefore, the losses of Eqs. <ref type="bibr" target="#b23">(24)</ref> and <ref type="formula" target="#formula_7">(23)</ref> coincide in the expectation sense, but they are estimated differently between DiffFlow and diffusion models with analytic transition probabilities. Taking Var</p><formula xml:id="formula_100">1 g 2 (ti)?ti x EM ti?1 ? x EM ti + f ? (x EM ti , t i ) ? g 2 (t i )s ? (x EM ti , t i ) ?t i</formula><formula xml:id="formula_101">X i = 1 2 Var(X i ) + 2 Cov(X i , X j ) ,</formula><p>where Cov(X i , X j ) represents the covariance of two random variables X i and X j . <ref type="table" target="#tab_0">Table 10</ref> represents the ratio of these two variances,</p><formula xml:id="formula_102">Ratio := Var( X i ) Var(X i ) = Var(X i ) + 2 Cov(X i , X j ) Var(X i ) = 1 + 2 Cov(X i , X j ) Var(X i ) ,</formula><p>and it shows that the DiffFlow loss has prohibitively large variance as N increases, compared to the INDM loss, which computes its Monte-Carlo estimation in spirit of Eq. (23) with N = ?.</p><p>Note that throughout our argument, we have omitted the prior and reconstruction terms on the variational bounds in this section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Schr?dinger Bridge Problem (SBP)</head><p>Schr?dinger Bridge Problem (SBP) <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> has recently been highlighted in machine learning for its connection to the score-based diffusion model. Schr?dinger Bridge Problem is a bi-constrained problem of min ??P(pr,?)</p><formula xml:id="formula_103">D KL (? ?),</formula><p>where P(p r , ?) is a family of path measure with bi-constraints of p r and ? as its marginal distributions at t = 0 and t = T , respectively, and ? is a reference path measure that is governed by</p><formula xml:id="formula_104">dx t = f (x t , t) dt + g(t) dw t , x 0 ? p r .<label>(28)</label></formula><p>As the KL divergence becomes infinite if the diffusion coefficient of ? is not equal to g(t) (because quadratic variations of ? and ? becomes different), SBP is equivalently formulated as min ??P(pr,?)</p><formula xml:id="formula_105">D KL (? ?) = min ?v?P(pr,?) D KL (? v ?),</formula><p>where the path measure ? v ? P(p r , ?) follows the SDE of</p><formula xml:id="formula_106">dx t = f (x t , t) + g 2 (t)v(x t , t) dt + g(t)w t .<label>(29)</label></formula><p>From the Girsanov theorem and the Martingale property <ref type="bibr" target="#b49">[50]</ref>, we have </p><formula xml:id="formula_107">D KL (? v ?) = 1 2 T 0 g 2 (t)E ?v [ v(x t , t) 2 2 ] dt + D KL (? p T ),</formula><formula xml:id="formula_108">1 2 T 0 g 2 (t)E ?v [ v(x t , t) 2 2 ] dt,<label>(30)</label></formula><p>where ? v is the associated path measure of Eq. <ref type="bibr" target="#b28">(29)</ref>. Eq. (30) interprets the solution of SBP as the least energy (weighted by g 2 ) of the auxiliary vector field (v) among admissible space of vector fields (V(p r , ?)). Hence, if ? ? P(p r , ?), then the trivial vector field, v ? 0, is the solution of SBP. When the reference SDE of Eq. (28) is one of the family of linear SDEs, such as VESDE or VPSDE, then ? / ? P(p r , ?), so the trivial vector field is not the solution of SBP, anymore. Instead, ?'s ending variable is close enough to ? (e.g., D KL (p T ?) ? 10 ?5 in bpd scale <ref type="bibr" target="#b7">[8]</ref>), so the closest path measure in V(p r , ?) to ? is nearly identical to a trivial vector field, v * ? 0, and the nonlinearity of SBP is limited.</p><p>Chen et al. <ref type="bibr" target="#b15">[16]</ref> connects the optimal solution of SBP with PDEs. At the optimal point, if we denote by ? * = arg min ??P(pr,?) D KL (? ?), then this optimal diffusion process follows a forward diffusion SDE <ref type="bibr" target="#b15">[16]</ref> of</p><formula xml:id="formula_109">dx t = f (x t , t) + g 2 (t)? xt log ?(x t , t) dt + g(t) dw t , x 0 ? p r ,</formula><p>with the corresponding reverse diffusion as</p><formula xml:id="formula_110">dx t = f (x t , t) ? g 2 (t)? xt log?(x t , t) dt + g(t) dw t , x T ? ?,</formula><p>where ?(x t , t) and?(x t , t) are the solutions of a system of PDEs <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_111">?? ?t = ? ? xt ? T f ? 1 2 tr(g 2 ? 2 xt ?) ?? ?t = ? div(?f ) + 1 2 tr(g 2 ? 2 xt? ),<label>(31)</label></formula><p>such that ?(x 0 , 0)?(x 0 , 0) = p r (x 0 ) and ?(x T , T )?(x T , T ) = ?(x T ). With ? and?, the forward diffusion SDE ends exactly at ?, and the corresponding reverse SDE ends at p r . Therefore, SBP is equivalent to solve the system of PDEs given by Eq. (31).</p><p>Chen et al. <ref type="bibr" target="#b15">[16]</ref> solves the system of coupled PDEs with Eq. (31) using a theory of forward-backward SDEs, which requires a deep understanding of PDE theory. SB-FBSDE <ref type="bibr" target="#b15">[16]</ref> uses the fact that the solution (?,?) of Hopf-Cole transform in Eq. (31) is derived from the solution of the forwardbackward SDEs of</p><formula xml:id="formula_112">? ? ? ? ? ? ? dx t = f (x t , t) + g(t)z t (x t , t) dt + g(t) dw t dy t = 1 2 (z T t z t )(x t , t) dt + z T t (x t , t) dw t d? t = 1 2 (? T t?t )(x t , t) + div g(t)? t (x t , t) ? f (x t , t) + (? T t z t )(x t , t) dt +? T t (x t , t) dw t ,<label>(32)</label></formula><p>where the boundary conditions are given by x(0) = x 0 and y T +? T = log ?(x T ). The solution of the above system of forward-backward SDEs satisfies z t (x t , t) = g(t)? log ?(x t , t) and? t (x t , t) = g(t)? log?(x t , t), where (?,?) is the solution of Eq. (31). SB-FBSDE parametrizes (z t ,? t ) as ? and ?, and it estimates the solution (z t ,? t ) of Eq. (32) from MLE training of the log-likelihood log p ?,? (x 0 ).</p><p>Other than the PDE-driven approach <ref type="bibr" target="#b15">[16]</ref>, SBP has been traditionally solved via Iterative Proportional Fitting (IPF) <ref type="bibr" target="#b50">[51]</ref>. Concretely, suppose</p><formula xml:id="formula_113">dx ? t = f (x ? t , t) + g 2 (t)s ? (x ? t , t) dt + g(t) dw t , x ? 0 ? p r ,<label>(33)</label></formula><p>is a forward diffusion with a parametrized vector field of s ? , and</p><formula xml:id="formula_114">dx ? t = f (x ? t , t) ? g 2 (t)s ? (x ? t , t) dt + g 2 (t)w t , x ? T ? ?</formula><p>, is a generative diffusion with a parametrized vector field of s ? . Then, IPF get its optimal vector fields by alternatively solving below half-bridge problems ? ?n = arg min ? ? ?P(pr,?)</p><formula xml:id="formula_115">D KL (? ? ? ?n?1 ),<label>(34)</label></formula><p>? ?n = arg min ? ? ?P(?,?)</p><formula xml:id="formula_116">D KL (? ? ? ?n ),<label>(35)</label></formula><p>where the convergence of ? ? n ? ? ? * and ? ? n ? ? ? * is guaranteed in De Bortoli et al. <ref type="bibr" target="#b14">[15]</ref>. Here, analogously, P(?, ?) is a family of path measure with ? as its marginal distribution at t = T . Notably, each of the half-bridge problem is a diffusion problem with the KL divergence replaced with the reverse KL divergence. Since SBP learns the forward SDE, sampling particle paths is expensive as it requires to solve an SDE numerically, so the training of IPF is slow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Correction of Density Estimation Metrics of Diffusion Models with Time Truncation E.1 Equivalent Reverse SDEs</head><p>Throughout Section E, the diffusion process is assumed to follow a SDE of dx t = f (x t , t) dt + g(t) dw t because the below argument is generally applicable for any continuous diffusion models. For INDM, we apply the below argument on the latent space, which has a linear drift term. Let dx t = f (x t , t) ? 1+? 2 2 g 2 (t)? xt log p ? t (x t ) dt + ?g(t)w t be the reverse SDEs starting from p T , where p ? t is the probability law of the solution at t. Then, the reverse Kolmogorov equation (or Fokker-Planck equation) becomes</p><formula xml:id="formula_117">?p ? t (x t , t) ?t = ? d i=1 ? ?x i f i (x t , t) ? 1 + ? 2 2 g 2 (t) ? xt log p ? t (x t , t) i p ? t (x t , t) ? ? 2 g 2 (t) 2 d i=1 ? 2 ?x 2 i p ? t (x t , t) = ? d i=1 ? ?x i f i (x t , t)p ? t (x t , t) ? 1 + ? 2 2 g 2 (t) ?p ? t (x t , t) ?x i ? ? 2 g 2 (t) 2 d i=1 ? 2 ?x 2 i p ? t (x t , t) = ? d i=1 ? ?x i f i (x t , t)p ? t (x t , t) + 1 2 g 2 (t) d i=1 ? 2 ?x 2 i p ? t (x t , t) ,</formula><p>which is independent of ?. Therefore, it satisfies p ? t = p ? t for any ? = ? . For any ? ? [0, 1], the generative SDE is constructed by plugging </p><formula xml:id="formula_118">s ? (x t , t) in place of ? xt log p ? t (x t ) in the reverse SDE as dx t = f (x t , t) ? 1+? 2 2 g 2 (t)s ? (x t , t) dt + ?g(t)w t . Suppose we denote p ?,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Log-Likelihood for Diffusion Models with Time Truncation</head><p>Due to the unbounded score loss illustrated in <ref type="bibr" target="#b25">[26]</ref>, a diffusion model truncates the diffusion time to be [ , 1] for small enough &gt; 0. However, since the small range of diffusion time contributes significant portion of the log-likelihood <ref type="bibr" target="#b25">[26]</ref>, the effect of truncation should be counted both on training and evaluation. To describe, as we have no knowledge on the score estimation at t ? [0, ), we have estimate the data log-likelihood by using the variational inferecence:</p><formula xml:id="formula_119">log p ?,? 0 (x 0 ) = log p ?,? 0 (x 0 , x ) dx ? p 0 (x |x 0 ) log p ?,? (x )p ? 0 (x 0 |x ) p 0 (x |x 0 ) dx = E p0 (x |x0) log p ?,? (x ) + E p0 (x |x0) log p ? 0 (x 0 |x ) p 0 (x |x 0 ) ,</formula><p>where p ?,? is the generative distribution perturbed by , and p ? 0 (x 0 |x ) is the reconstruction transition probability given x . Then, we have</p><formula xml:id="formula_120">E pr(x0) [? log p ?,? 0 (x 0 )] ? E x ? log p ?,? (x ) ? E x0,x log p ? 0 (x 0 |x ) p 0 (x |x 0 ) = D KL (p p ?,? ) ? E x0,x log p ? 0 (x 0 |x ) p 0 (x |x 0 ) + H(p ),</formula><p>which is equivalent to</p><formula xml:id="formula_121">D KL (p r p ?,? 0 ) ? D KL (p p ?,? ) ? E x0,x log p ? 0 (x 0 |x ) p 0 (x |x 0 ) + H(p ) ? H(p r ) = D KL (p p ?,? ) + D KL p r (x 0 )p 0 (x |x 0 ) p (x )p ? 0 (x 0 |x ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 NELBO Correction</head><p>Suppose ? is the path measure of dx t = f (x t , t) dt+g(t) dw t on [ , T ], and ? ? ?, is the path measure</p><formula xml:id="formula_122">of dx t = f (x t , t) ? 1+? 2 2 g 2 (t)? xt log p ? t (x t ) dt + ?g(t)w t on [ , T ].</formula><p>Then, the continuous variational bound on the truncated diffusion model becomes</p><formula xml:id="formula_123">E pr(x0) ? log p ?,? 0 (x 0 ) ? D KL (p p ?,? ) ? E x0,x log p ? 0 (x 0 |x ) p 0 (x |x 0 ) + H(p ) ? D KL (? ? ? ?, ) ? E x0,x log p ? 0 (x 0 |x ) p 0 (x |x 0 ) + H(p ) = 1 2 (1 + ? 2 ) 2 4? 2 T g 2 (t)E xt s ? (x t , t) ? log p t (x t ) 2 2 dt ? E x T log ?(x T ) ?E x0,x log p ? 0 (x 0 |x ) p 0 (x |x 0 ) + H(p ) ? H(p T ) = 1 2 T E x0,xt (1 + ? 2 ) 2 4? 2 g 2 (t) log p 0t (x t |x 0 ) ? s ? (x t , t) 2 2 ? g 2 (t) ? xt log p 0t (x t |x 0 ) 2 2 ?2? xt ? f (x t , t) dt ? E x T log ?(x T ) ? E x0,x log p ? 0 (x 0 |x ) p 0 (x |x 0 ) , where H(p ) ? H(p T ) is derived to be ? 1 2 T E g 2 ? log p 0t 2</formula><p>2 + 2? ? f dt by Theorem 4 of Song et al. <ref type="bibr" target="#b10">[11]</ref>. The residual term, E pr(x0)p0 (x |x0) log</p><formula xml:id="formula_124">p ? 0 (x0|x )</formula><p>p0 (x |x0) , has been ignored both on training and evaluation in previous research. Therefore, we report the correct NELBO (denoted by w/ residual in the main paper) by counting the residual term E pr(x0)p0 (x |x0) log</p><formula xml:id="formula_125">p ? 0 (x0|x )</formula><p>p0 (x |x0) into account. Note that (1+? 2 ) 2 4? 2 is minimized when ? = 1, so our reported NELBO is based on the generative SDE at ? = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 NLL Correction</head><p>NLL of the generative SDE can be computed through the Feynman-Kac formula <ref type="bibr" target="#b11">[12]</ref> by</p><formula xml:id="formula_126">p ? ?, (x ) = E {xt}t&gt; |x ?(x T ) exp ? T tr ? xt f (x t , t) ? 1 + ? 2 2 g 2 (t)s ? (x t , t) dt .<label>(36)</label></formula><p>However, the expectation is intractable because there are infinitely-many sample paths. Fortunately, the sample variance diminishes as ? ? 0, and the generative SDE collapses to a generative ODE when ? = 0 [1], i.e., the generative SDE of ? = 0 becomes</p><formula xml:id="formula_127">dx t = f (x t , t) ? 1 2 g 2 (t)s ? (x t , t) dt,</formula><p>which corresponds to the generative ODE of forward time as</p><formula xml:id="formula_128">dx t = f (x t , t) ? 1 2 g 2 (t)s ? (x t , t) dt.<label>(37)</label></formula><p>Then, the sample path becomes deterministic, and the expectation in Eq. (36) is degenerated as the single sample path of ODE with Eq. (37) starting x . The instantaneous change-of-variable formula <ref type="bibr" target="#b0">[1]</ref>, which is a collapsed Feynman-Kac formula in Eq. <ref type="bibr" target="#b35">(36)</ref>, guarantees that there is a corresponding ODE of Eq. (37) as</p><formula xml:id="formula_129">d log p 0,? t (y t ) dt = ?tr ? yt f (y t , t) ? 1 2 g 2 (t)s ? (y t , t) .<label>(38)</label></formula><p>From the fact that the reverse SDEs have the identical marginal distributions described in Section E.1, we approximate the model log-likelihood at ? = 1 by the log-likelihood at ? = 0 at the expense of slight difference between the model distributions of different ?s. When computing the model log-likelihood at ? = 0, we integrate the ODE of Eq. (38) over [ , 1] using an ODE solver, such as the Runge-Kutta 45 method <ref type="bibr" target="#b26">[27]</ref>.</p><p>There are minor subtleties in computing the log-likelihood at ? = 0 that significantly affects to bpd evaluation. To the best of our knowledge, all the current practice on continuous diffusion models computes bpd by integrating</p><formula xml:id="formula_130">d log p 0,? t (y t ) dt = ?tr ? yt f (y t , t) ? 1 2 g 2 (t)s ? (y t , t) ,</formula><p>on t ? [ , T ], where {y t } T t= is a sample path starting from y := x 0 . This is equivalent of computing log p 0,? (x 0 ). However, strarting from x 0 incurs large discrepancy on the NLL output, compared to starting from instead of x . Since the integration is on [ , 1], the starting variable should follow x , which is a slightly perturbed variable. <ref type="table" target="#tab_0">Table 11</ref>: The difference of the integration with different initial points of x and x 0 on DDPM++ (VP, NLL). The difference increases by . To fix this subtlety, we solve the below alternative differential equation of</p><formula xml:id="formula_131">d log p 0,? t dt = ?tr ? yt f (y t , t) ? 1 2 g 2 (t)s ? (y t , t) ,<label>(39)</label></formula><p>on t ? [ , T ], where {y 0 t } T t= is a sample path starting from y := x . By replacing the initial value to x from x 0 , we could correctly compute log p ? (x ). <ref type="table" target="#tab_0">Table 11</ref> presents the difference of E x ? log p 0,? (x ) ? E x0 ? log p 0,? (x 0 ) with various . We report the correct NLL as</p><formula xml:id="formula_132">E x ? log p 0,? (x ) ? E x0,x log p ? 0 (x 0 |x ) p 0 (x |x 0 ) ,</formula><p>where log p 0,? (x ) is computed based on the initial point of x when ? = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Calculating the Residual Term</head><p>This section calculates the residual term, E pr(x0)p0 (x |x0) log</p><formula xml:id="formula_133">p ? 0 (x0|x ) p0 (x |x0) . The transition probability of p 0 (x |x 0 ) is the Gaussian distribution of N (x ; ?( )x 0 , ? 2 ( )I) if f (x t , t) = ? 1 2 ?(t)x t with ?( ) = e ? 1 2 0 ?(s) ds and ? 2 ( ) = g 2 (t) ?(t) ? g 2 (0) ?(0) + 1 ? e ? t 0 ?(s) ds ,</formula><p>see Appendix A.1 of Kim et al. <ref type="bibr" target="#b25">[26]</ref> for detailed computation. On the other hand, the generative distribution of p ?, 0 (x 0 |x ) is assumed to be a Gaussian distribution of N x 0 ; ? ?, 0 (x ), ? 2 ?, 0 I , where ? ?, 0 (x ) = 1 ?( ) x + ? 2 ( )s ? (x , ) <ref type="bibr" target="#b27">[28]</ref>. Then, we have</p><formula xml:id="formula_134">E p0 (x |x0) log p ? 0 (x 0 |x ) p 0 (x |x 0 ) = log ?( ) ? 1 2? 2 ?, 0 ( ) E p0 (x |x0) x 0 ? 1 ?( ) x t + ? 2 ( )s ? (x , ) 2 2 + 1 2 .</formula><p>We could approximate the variance of p ? 0 (x 0 |x ) to be the variance of p 0 (x 0 |x ), if p 0 (x 0 |x ) is derived as a closed-form. For that, let us assume x 0 ? N (0, ? 2 ). Then,</p><formula xml:id="formula_135">p(x 0 , x ) = p(x 0 )p 0 (x |x 0 ) ? exp ? x 0 2 2 2? 2 ? x ? ?( )x 0 2 2 2? 2 ( ) = exp ? 1 2 1 ? 2 + ? 2 ( ) ? 2 ( ) x 0 ? ?( )? 2 ? 2 ( ) + ? 2 ( )? 2 x 2 2 + O( x 2 2 ) . Therefore, p 0 (x 0 |x ) = N x 0 ?( )? 2 ? 2 ( )+? 2 ( )? 2 x , 1/( 1 ? 2 + ? 2 ( ) ? 2 ( ) )</formula><p>. When ? is sufficiently large compared to ? 2 ( ) ? 2 ( ) , the variance of p 0 (x 0 |x ) is approximately ? 2 ( ) ? 2 ( ) . Now, if x 0 ? p r , then the variance of x 0 is large enough compared to ? 2 ( ) ? 2 ( ) , so we could approximate ? 2 ?, 0 ( ) to be ? 2 ( ) ? 2 ( ) . Note that DDPM <ref type="bibr" target="#b7">[8]</ref> assumes the variance to be ? 2 ( ). We compute the residual term with ? 2 ( ) ? 2 ( ) variance for both VESDE and VPSDE. Note that this residual term is inspired from the released code of Song et al. <ref type="bibr" target="#b10">[11]</ref>. Diffusion Model We implement two diffusion models as backbone: NCSN++ (VE) <ref type="bibr" target="#b0">[1]</ref> and DDPM++ (VP) <ref type="bibr" target="#b0">[1]</ref>, where two backbones are one of the best performers in CIFAR-10 dataset. In our setting, NCSN++ assumes the score network with parametrization of s ? (z t , log ? 2 (t)), where ? 2 (t) = ? 2 min ( ?max ?min ) 2t is the variance of the transition probability p 0t (z t |z 0 ) with VESDE. As introduced in Song et al. <ref type="bibr" target="#b0">[1]</ref>, we use the Gaussian Fourier embeddings <ref type="bibr" target="#b51">[52]</ref> to model the high frequency details across the temporal embedding. DDPM++ models the score network with parametrization of ? (z t , t), which targets to estimate ??(t)? zt log p t (z t ). We use the Transformer sinusoidal temporal embedding <ref type="bibr" target="#b52">[53]</ref>.</p><p>We use the U-Net <ref type="bibr" target="#b53">[54]</ref> architecture for the score networks on both NCSN++ and DDPM++ based on <ref type="bibr" target="#b7">[8]</ref>. We stack U-Net resblocks of up-and-down convolutions with skip connections that give input information to the output layer. Also, we follow Ho et al. <ref type="bibr" target="#b7">[8]</ref> by applying the global attention at the resolution of 16 ? 16. We use four U-Net resblocks with four feature map resolutions (32 ? 32 to 4 ? 4). On CIFAR-10, we use four and eight resblocks for shallow and deep settings, respectively. The performances of shallow and deep models turn out to be insignificant, so we use four resblocks on CelebA. We provide the identical diffusion model structures to compare the baseline linear diffusion model and the INDM model in a fair setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flow Model</head><p>We build a normalizing flow model as follows. Ma et al. <ref type="bibr" target="#b23">[24]</ref> uses the autoencoding structure of decouple the global information and the local representation. The compression encoder extracts the global information, and the invertible decoder is a conditional flow conditioned by the encoded latent representation. Ma et al. <ref type="bibr" target="#b23">[24]</ref> utilizes a shallow network for the compressive encoder, and it applies Glow <ref type="bibr" target="#b54">[55]</ref> for the invertible decoder. We empirically find that resnet-based flow network outperforms the Glow-based flow, so we replace Glow to ResFlow <ref type="bibr" target="#b22">[23]</ref>.</p><p>For the ResFlow, we drop three components from the original paper: 1) the activation normalization <ref type="bibr" target="#b54">[55]</ref>, 2) the batch normalization [56], and 3) the fully connected layers. For the activation function, we use the sine function [57] on quantitative comparisons in Section 7, and we use swish function [58] on qualitative analysis in otherwise sections including Section 6. With the sine activation, the training becomes more stable, and the FID performance is significantly improved while maintaining the NLL performance. For the multi-GPU training, we use the Neumann log-determinant gradient estimator, instead of the memory-efficient estimator <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Experimental details</head><p>With the batch size of 128, we train the diffusion model with Exponential Moving Average (EMA) <ref type="bibr" target="#b27">[28]</ref> rate of 0.9999, and we do not use EMA on our flow model. Using EMA on the flow model is advantageous on NLL at the expense of FID, and we build our model with emphasis on FID. We train the model by two step. The pre-training stage trains the diffusion model about five days with a flow model fixed as an identity function on four P40 GPUs with 96Gb GPU memory for all experiments. After the pre-training, we train both flow and diffusion networks about five days. In this stage, we apply the learning rate scheduling to boost the FID score. We initiate the learning rate after the sample generation performance is saturated. For the diffusion model, we drop the learning rate from 2 ? 10 ?4 to 10 ?5 . For the flow model, we drop the learning rate from 10 ?3 to 10 ?5 for VPSDE and 5 ? 10 ?5 to 10 ?5 for VESDE. VESDE and VPSDE have different training details. We apply INDM on VESDE with ? min = 10 ?2 . Throughout the experiments, VESDE has ? max = 50 on CIFAR-10 and ? max = 90 on CelebA. On the other hand, VPSDE assumes ?(t) = ? min + (? max ? ? min )t with ? min = 0.1 and ? max = 20. Both VESDE and VPSDE truncate the diffusion time on [ , T ] in order to stabilize the diffusion model <ref type="bibr" target="#b25">[26]</ref>, where = 10 ?5 and T = 1.</p><p>With all hyperparameters identical to Song et al. <ref type="bibr" target="#b10">[11]</ref>, however, we could not achieve the reported performance. <ref type="table" target="#tab_0">Table 12</ref> compares the reported performance and the model performance trained on out implementation, of which structure is heavily based on the released code of <ref type="bibr" target="#b0">[1]</ref>. Due to the discrepancy between the reported and the regenerated performances, we compare our INDM to the regenerated performance as default to investigate the effect of nonlinear diffusion in a fair setting. Throughout the training, we used 4? NVIDIA RTX 3090.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.1 Variance Reduction</head><p>Flow Training When we train the flow network with L {x t } T t=0 , g 2 ; {?, ?}), this NELBO contains the integration of</p><formula xml:id="formula_136">L {z t } T t=0 , g 2 ; ? = 1 2 T 0 g 2 (t)E z0,zt s ? (z t , t) dt, up to a constant. Suppose L t {z t } T t=0 ; ? to be 1 2 E z0,zt [ s ? (z t , t) ? ? zt log p 0t (z t |x 0 ) 2 2 ]</formula><p>. Previous works on diffusion models <ref type="bibr">[59,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref> show that the estimation variance is largely reduced with the importance sampling, which could improve the model performance <ref type="bibr" target="#b10">[11]</ref>, and we apply this importance sampling throughout the experiments for NLL setting. Concretely, the importance sampling chooses an importance weight that is proportional to g 2 (t) ? 2 (t) , and estimates the integration by</p><formula xml:id="formula_137">L {z t } T t=0 , g 2 ; ? = T 0 g 2 (t)L t {z t } T t=0 ; ? dt ? N n=1 ? 2 (t n )L tn {z t } T t=0 ; ? , where t n is sampled from the importance distribution.</formula><p>For VESDE, it satisfies ?(t) = 0 and g(t) = ? min ( ?max ?min ) t 2 log ( ?max ?min ). The transition probability becomes p ??t (z t |z ?? ) = N (z t ; z ?? , ? 2 (t)), where ? 2 (t) = t ?? g 2 (s) ds = ? 2 min ?max ?min 2t .</p><p>Since ? 2 (t) is proportional to g 2 (t) in VESDE, the importance weight follows the uniform distribution, and the importance sampling is equivalent with choosing the uniform t. This is why there is no experimetal setting of VE with NLL.</p><p>On the other hand, VPSDE satisfies ?(t) = ? min + (? max ? ? min )t with g(t) = ?(t). Then, the transition probability becomes p 0t (z t |z 0 ) = N (z t ; ?(t)z t , ? 2 (t)I), where ?(t) = e ? 1 2 t 0 ?(s) ds and ? 2 (t) = 1 ? e ? t 0 ?(s) ds . Thus, VPSDE has the importance weight of g 2 (t) ? 2 (t) = ?(t)</p><p>1?e ? t 0 ?(s) ds . The Monte-Carlo sample from this importance weight is the solution of the inverse Cumulative Distribution Function (CDF) of the importance distribution as</p><formula xml:id="formula_138">t = F ?1 (u) ?? u = F (t) = 1 Z t g 2 (s) ? 2 (s) ds = 1 Z F(t) ? F( ) ,<label>(40)</label></formula><p>where u is a uniform sample from [0, 1], F(t) is the antiderivative of the importance weight given by F(t) = log (1 ? e ?0.5t 2 (?max??min)?t?min ) + 0.5t 2 (? max ? ? min ) + t? min , and Z is the normalizing constant given by   </p><formula xml:id="formula_139">Z = T g 2 (t) ? 2 (t) dt = log (1 ? e ?0.</formula><formula xml:id="formula_140">?? t 0 ?(s) ds = 1 2 (? max ? ? min )t 2 + ? min t = log (1 + exp (Zu + F( ))) ?? t = ?? min + ? 2 min + 2(? max ? ? min ) log 1 + exp (Zu + F( )) ? max ? ? min .</formula><p>The variation of the Monte-Carlo diffusion time depends on the uniform sample of u. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.2 Sampling Tricks to Improve FID</head><p>For ODE sampler, we use Runge Kutta 45 method [60] for the ODE solver. Since the score network was not trained beneath the truncation time, i.e., s ? (z t , t) has not been trained on t ? [0, ), keep denoising up to the zero diffusion time would harm the sample fidelity. If t min is the stopping diffusion time of the ODE, one predictor step from t min to 0 is applied to the noised sample, z tmin , in order to eliminate the residual noise in z tmin to z 0 [61]. <ref type="table" target="#tab_0">Table 13</ref> searches the optimal stopping diffusion time, and it shows that the truncation time (10 ?5 ) turns out to be the optimal stopping time. Throughout the paper, we report the FID (ODE) performance of our INDM with the training truncation time (10 ?5 ). For VESDE, the ODE sampler fails to generate realistic images, so we do not report sample generation performance. Temperature ( ) <ref type="figure" target="#fig_0">Figure 16</ref>: Ablation study for the flow temperature.</p><p>In PC sampler, for the predictor, we use the Reverse Diffusion Predictor for VESDE and the Euler-Maruyama Predictor for VPSDE.</p><p>For the corrector, we use the Langevin dynamics [62] for VESDE, and we do not use any corrector for VPSDE. We use 1) Signalto-Noise Ratio (SNR) scheduling, 2) temperature scheduling, 3) stopping time scheduling, and 4) data-adaptive prior than a fixed prior to improve FID. First, <ref type="table" target="#tab_0">Table 14</ref> presents that the optimal SNR is 0.14, which is slightly different from the optimal SNR of 0.16 in the linear diffusion <ref type="bibr" target="#b0">[1]</ref>. We use SNR of 0.14 as default in our PC sampling.  <ref type="table" target="#tab_0">Table 16</ref>: Ablation study on the final denoising step trained on VESDE in CIFAR-10. The performances are FID scores. Second, as introduced in Kingma and Dhariwal <ref type="bibr" target="#b54">[55]</ref>, we scale the generated latent, z ? 0 , by multiplying the temperature. <ref type="table" target="#tab_0">Table 15</ref> presents that the optimal temperature for VPSDE is 1.04 ? 1.05 in terms of FID on INDM (VP, FID) setting. We use the temperature of 1 for the remaining settings except INDM (VP, FID). With temperature ? , the normalizing flow puts its latent input scaled by ? to the flow network. In <ref type="figure" target="#fig_0">Figure 16</ref>, the image color with a higher temperature tends to be brighter, and we find that the optimal temperature depends on the experimental settings.</p><formula xml:id="formula_141">Model FID (x ) FID (x ?0.25 ) FID (x ?0.5 ) FID (x ?0.75 ) FID (x ?1 ) FID (x ?1.5 ) FID (x ?? ) NCSN++ (VE,</formula><p>Third, the stopping time scheduling is a method that manipulate the final denoising step. To attain the variance of VESDE as ? 2 (t) = ? 2 min (</p><formula xml:id="formula_142">? 2 max ? 2 min ) 2t , we should start the diffusion process of dz t = ? 2 (t) dw t at t = ?? because ? 2 (t) = t t0 g 2 (s) ds = ? 2 min ? 2 max ? 2 min 2t<label>(41)</label></formula><p>implies t 0 = ??. If the generative SDE is dz t = g 2 (t)s ? (z t , t) dt + ? 2 (t) dw t , then the Euler-Maruyama discretization is</p><formula xml:id="formula_143">z ti ? z ti+1 + g 2 (t i+1 )(t i ? t i+1 )s ? (z ti+1 , t i+1 ) + ?(t i+1 ) 2 ? ?(t i ) 2 ,<label>(42)</label></formula><p>where ? N (0, I). However, since the initial time of VESDE is t = ??, denoising the noised sample with the Euler-Maruyama discretization would incur arbitrary large error at the final step that denoises from t = to t = ??. Therefore, Song et al. <ref type="bibr" target="#b0">[1]</ref> suggested the reverse diffusion predictor that denoises by</p><formula xml:id="formula_144">z ? ?1 (?i) ? z ? ?1 (?i+1) + (? 2 i+1 ? ? 2 i )s ? (z ? ?1 (?i+1) , ? i+1 ) + ? 2 i+1 ? ? 2 i ,<label>(43)</label></formula><p>which is equivalent to the Euler-Maruyama discretization if t i ? t i+1 is small enough (because ?? 2 (t) ? g 2 (t)?t by Eq. (41)). The difference of Eqs. <ref type="bibr" target="#b41">(42)</ref> and <ref type="formula" target="#formula_19">(43)</ref> is minor as long as we denoise on the range of [ , T ], but only Eq. (43) enables to denoise from t = to t = ??.</p><p>However, it turns out that the direction of the score network is not aligned to the direction of the data score near t ? 0, so s ? (z , ? min ) would not be accurate enough to the perturbed data score. Therefore, the final denoising step of z ?? ? z + ? 2 min s ? (z , ? min ) + ? min , might not be mostly effective. This leads us to try the final step as</p><formula xml:id="formula_145">z ?? = z + 1 2 ? 2 ( ) ? ? 2 ( ? ?) s ? (z , ? min ),</formula><p>for various ? ? 0. After the denoising up to z ?? , we apply the inverse of the flow network to obtain x ?? = h ?1 ? (z ?? ), and <ref type="table" target="#tab_0">Table 16</ref> presents that there is a sweet spot (x ?0.5 ? x ?0.75 ) that works the best in terms of FID. We report the line searched FID performance for each of VESDE setting.</p><p>Lastly, we use p ? T instead of ? to sample from INDM (VE). This data-adaptive prior is particularly beneficial on the experiment of VESDE. In INDM (VE), the data-adaptive prior reduces FID-5k from 8.14 to 7.52, so we use this technique by default throughout out performance report. In INDM (VP), this technique is not effective, and we use the vanilla prior distribution. The reason why the data-adaptive prior is effective in VESDE is because the discrepancy of VESDE between p ? T and ? is significantly larger than VPSDE, see <ref type="figure" target="#fig_6">Figure 5</ref> of Chen et al. <ref type="bibr" target="#b15">[16]</ref>.</p><p>We compute FID <ref type="bibr" target="#b17">[18]</ref> for CIFAR-10 based on the statistics released by Song et al. <ref type="bibr" target="#b0">[1]</ref> 2 , which used the modified Inception V1 network 3 in order to compare INDM to the baselines <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref> in a fair setting. On the other hand, for the CelebA dataset, we compute the clean-FID [63] that provides consistently antialiased performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.3 Interpolation Task</head><p>For the interpolation task, we provide the line-by-line algorithm in Algorithm 2. We train with the likelihood weighting as default for our experiment on the dataset interpolation. The interpolation loss of L int consumes 0.2Gb of GPU memory, and the INDM loss of L IN DM takes 2.5Gb of GPU memory in the EMNIST-MNIST experiment.  We find that training INDM with a pre-trained score network of linear diffusion models improves FID. <ref type="table" target="#tab_0">Table 17</ref> conducts the ablation study on the number of pre-training steps. We  pre-train the score network with DDPM++ (VP, NLL) for five variations of pre-training steps (100k/200k/300k/400k/500k), and we train flow+score networks for 350k steps further with NLL setting (? = g 2 ). <ref type="table" target="#tab_0">Table 17</ref> empirically demonstrates that it is better to search the nonlinearity of the data process near the linear process. For this clear empirical advantage of pre-training, we report the quantitative performances in Section 7 with pre-training.       INDM trains the volatility term, G ? , which was fixed across previous research, except LSGM. The exact form of G ? in LSGM, however, is not derivable, so we exclude comparing LSGM in this section. As stated in Section 3, the noise distribution of a diffusion process is <ref type="figure">t)</ref>), which is anisotropic by the input data, x t , and time, t. The influence of diffusion time on this covariance matrix is illustrated in <ref type="figure" target="#fig_0">Figure 19</ref>-(a). It presents the box plot of the eigenvalue distribution of the (normalized) covariance,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Data Interpolation of INDM</head><formula xml:id="formula_146">Compute L IN DM = L({x t } T t=0 , g 2 ; {?, ?}) for x 0 ? p (1) r 3: Compute L int = E p (2) r [? log p ? (y)] for y ? p<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Training Time</head><formula xml:id="formula_147">N (0, G(x t , t)G T (x t ,</formula><formula xml:id="formula_148">G ? (x t , t)G T ? (x t , t)/g 2 (t)</formula><p>, on CIFAR-10, from t = 0 to t = T . All the eigenvalues of previous research collapse to one as they share the isotropic covariance matrix, g 2 (t)I. On the other hand, the eigenvalues of INDM is dispersive throughout the diffusion time. As the distribution becomes more dispersive, the covariance matrix becomes more unisotropic, and <ref type="figure" target="#fig_0">Figure 19</ref>-(a) implies that the learnt diffusion process is under a highly nonlinear noise perturbation in a range of large diffusion time. The covariance matrix also depends on the input data, and <ref type="figure" target="#fig_0">Figure 19</ref>-(b) illustrates the eigenvalue distribution of the covariance at distinctive data instances of x t at t = 0. The eigenvalue distribution varies by instance, implying that data is diffused inhomogeneously by its location. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.7 Relative Energy</head><formula xml:id="formula_149">K({p t , v t } t ) := T 0 p t (x) v t (x) 2 2</formula><p>dx dt/T is the kinetic energy of the transportation. The continuity equation (that guarantees the conservation of mass along time transition [67]) and the boundary conditions determine the set of admissible trajectories, and the forward diffusion constructs an admissible trajectory. We quantify how much a trajectory is close to the optimal transport as the relative energy, given by R(?) =</p><formula xml:id="formula_150">K({p ? t ,v ? t }t) W 2 2 (p ? 0 ,p ? T )</formula><p>. <ref type="table" target="#tab_3">Table 20</ref> shows that INDM's latent diffusion is more close to the optimal transport than DDPM++ on CIFAR-10. <ref type="table" target="#tab_0">Tables   Tables 21, 22, and 23</ref> gives the full details of the quantitative comparisons to baseline models. From Theorem 4 of Song et al. <ref type="bibr" target="#b10">[11]</ref>, the entropy at t = 0 equals to H(p 0 ) = H(p T ) ? 1 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.8 Full Quantitative</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.9 Random samples</head><p>T 0 E pt(zt) 2? zt ? f (z t , t) + g 2 (t) ? zt log p t (z t ) 2 2 dt, where f (z t , t) is a drift term of the diffusion for z t and p t is the probability distribution of z t . Therefore, the negative log-likelihood becomes ?E pr(x0) log p ?,? (x 0 ) = D KL (p r p ?,? ) + H(p r )</p><formula xml:id="formula_151">? D KL (? ? ({x t }) ? ?,? ({x t })) + H(p r ) = D KL (? ? ({x t }) ? ?,? ({x t })) ? E pr(x0) log det ?h ? ?x 0 + H(p 0 ) = D KL (? ? ({z t }) ? ? ({z t })) ? E pr(x0) log det ?h ? ?x 0 + H(p T ) ? 1 2 T 0 E z ? t ? d?(t) + g 2 (t) ? zt log p t (z t ) 2 2 dt.</formula><p>Now, from Theorem 1 of <ref type="bibr" target="#b10">[11]</ref>, the KL-divergence between the path measures becomes</p><formula xml:id="formula_152">D KL (? ? ({z t }) ? ? ({z t })) = D KL (p T (z T ) ?(z T ))<label>(44)</label></formula><formula xml:id="formula_153">+ 1 2 T 0 g 2 (t)E pt(zt) s ? (z t , t) ? ? zt log p t (z t ) 2 2 dt,<label>(45)</label></formula><p>so if we plug in this into the negative log-likelihood, we yield the following:</p><p>?E pr(x0) log p ?,? (x 0 )</p><p>? ?E pr(x0) log det ?h ? ?x 0 + 1 2 T 0 g 2 (t)E zt s ? (z t , t) ? ? zt log p t (z t ) 2 2 dt</p><p>On the other hand, if p ? on s &lt; t 0 for some t 0 . Therefore, for any t ? [0, T ], we conclude that s ? (z t , t) = ? log q ? t (z t ) almost everywhere and Eq. (46) becomes dz t = f (z t , t) ? g 2 (t)? log q ? t (z t ) dt + g(t) dw t . (51) As the Fokker-Planck equation of the SDE of Eq. (51) becomes</p><formula xml:id="formula_154">?q ? t ?t (z t ) = div ? f (z t , t) + g 2 (t) 2 ? log q ? t (z t ) q ? t (z t ) ,</formula><p>which coincide with the Fokker-Planck equation of the forward SDE of dz t = f (z t , t) dt + g(t) dw t , we conclude s ? ? S sol by definition.</p><p>(?) holds from Lemma 2.</p><p>Theorem 3. For any fixed s? ? S sol , if ? * ? arg min ? D KL (? ? ? ?,? ), then s ? * (z t , t) = ? log p ? * t (z t ) = s?(z t , t), and D KL (? ? * ? ? * ,? ) = D KL (p r p ? * ,? ) = Gap(? ? * , ? ? * ,? ) = 0.</p><p>Proof of Theorem 3. If s? ? S sol , there exists q 0 such that s?(z t , t) = ? log q t (z t ), where z t ? q t is governed by dz t = f (z t , t) dt + g(t) dw t that starts from z 0 ? q 0 . This implies that the generative path measure of ? ?,? coincides with some forward path measure. On the other hand, the forward latent diffusion is also governed by dz t = f (z t , t) dt + g(t) dw t that starts from z 0 ? p ? 0 . Therefore, if p ? 0 = q 0 almost everywhere, then the generative path measure of ? ?,? coincides with the forward path measure of ? ? , and it holds that D KL (? ? ? ?,? ) = Lemma 4 (Lemma S11 of De Bortoli et al. <ref type="bibr" target="#b14">[15]</ref>). Let (E, E) and (F, F) be two measurable spaces and K : E ? F ? [0, 1] be a Markov kernel. Then for any ? 0 , ? 1 ? P(E) we have</p><formula xml:id="formula_155">? 0 K ? ? 1 K T V ? ? 0 ? ? 1 T V .</formula><p>In addition, for any ? : E ? F measurable we get that</p><formula xml:id="formula_156">? # ? 0 ? ? # ? 1 T V ? ? 0 ? ? 1 T V ,</formula><p>with equality if ? is injective.</p><p>Proof of Theorem 4. For any k ? {1, ..., N }, denote R k the Markov kernel such that for any z ? R d , A ? B(R d ) and k ? {0, ..., N ? 1} we have</p><formula xml:id="formula_157">R ? k+1 (z, A) = (4?? k+1 ) ?d/2 A exp ? z ? T ? k+1 (z) 2 4? k+1 dz, where for any z ? R d , T ? k+1 (z) = z + ? k+1 {z + 2s ? (z, t k )}, where t k = k?1 l ? l . Define Q ? N = N l=1 R ? l .</formula><p>Analogously, let us define </p><formula xml:id="formula_158">R ? k+1 (z, A) = (4?? k+1 ) ?d/2 A exp ? z ? T ? k+1 (z) 2 4? k+1 dz, for T ? k+1 (z) = z + ? k+1 {z + 2? log p ? t (z, t k )} and Q ? N = N l=1 R ? l . Suppose P T</formula><p>Combining Eq. (52) with Lemma 4, we have</p><formula xml:id="formula_160">p ? 0 ? p ? 0,N T V = p ? 0 P T |0 P R T |0 ? ?Q ? N T V ? p ? 0 P T |0 P R T |0 ? ?P R T |0 T V + ?P R T |0 ? ?Q ? N T V + ?Q ? N ? ?Q ? N T V ? p ? 0 P T |0 ? ? T V Epri + ?P R T |0 ? ?Q ? N T V E dis + ?Q ? N ? ?Q ? N T V Eest .</formula><p>The first two terms, E pri (?) + E dis (?), are those terms derived in Theorem 2 of Guth et al. <ref type="bibr" target="#b21">[22]</ref>. By Lemma S13 of De Bortoli et al. <ref type="bibr" target="#b14">[15]</ref>, the last term, E est (?, ?), is bounded by</p><formula xml:id="formula_161">?Q ? N ? ?Q ? N 2 T V ? 1 2 T 0 E b ? ({z t } T t=0 , t) ? b ? ({z t } T t=0 , t) 2 dt, where b ? ({z t } T t=0 , t) = N ?1 k=0 1 [t k ,t k+1 ) (t){z t k + 2 log p ? t (z t k )} and b ? ({z t } T t=0 , t) = N ?1 k=0 1 [t k ,t k+1 ) (t){z t k + 2s ? (z t k , t k )</formula><p>} are the drift terms of piecewise generative processes, given by dz t = ? z t ? 2? log p ? t k (z t k ) dt + g(t) dw t and dz t = ? z t ? 2s ? (z t k , t k ) dt + g(t) dw t defined each of the interval [t k , t k+1 ] for k = 0, ..., N ? 1, respectively. Therefore, E est (?, ?) is bounded by <ref type="table" target="#tab_0">Table 21</ref>: Performance comparison to linear/nonlinear diffusion models on CIFAR-10. We report both before/after correction of density estimation performances. We report the baseline performances of linear diffusions by training our PyTorch implementation based on Song et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref> with identical hyperparameters and networks on both linear/nonlinear diffusions in order to quantify the effect of nonlinearity in a fair setting. Boldface numbers represent the best performance in a column, and underlined numbers represent the second best.      </p><formula xml:id="formula_162">E est (?, ?) ? 1 2 T 0 E b ? ({z t } T t=0 , t) ? b ? ({z t } T t=0 , t) 2 dt = 2 N ?1 k=0 t k+1 t k E ? log p ? t k (z t k ) ? s ? (z t k , t k ) 2 dt</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of linear (top row) and nonlinear (middle/bottom rows) diffusion processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Nonlinear Forward Path (data ? noise) ? Nonlinear Generative Path (noise ? data) Linear Forward Path (latent ? noise)Linear Generative Path (noise ? latent) INDM attains a ladder structure between the data space and the latent space. The latent vector is visualized by normalizing the latent value, see Appendix F.5.2 for further visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )Figure 2 :</head><label>a2</label><figDesc>Linear f ? xt (b) Nonlinear f (c) Nonlinear G Vector fields on various SDEs at t = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig- ure 2 -</head><label>2</label><figDesc>(b) illustrates the corresponding vector field, in which two distinctive components (orange/blue) are forced to separate each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Norm Ratio of SBP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 2 .</head><label>2</label><figDesc>This bidirectional attraction between s ? and s ? is what flow learning does in the optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of INDM with baseline models, experimented on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Sensitivity analysis on scaled-up scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>(a,b) Comparison of INDM with DDPM++ for K, L, ?. (c) Cosine similarity of forward diffusion trajectories on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>SOTA on CelebA Tables 5 Figure 8 :</head><label>58</label><figDesc>and 6 compare INDM to baseline models. With the emphasis on FID, LSGM is the State-Of-The-Art (SOTA) model on CIFAR-10, but INDM-118M (FID) is on par with LSGM-476M (FID). Moreover, we use Soft Truncation [26] to compare with LSGM (balanced). Soft Truncation softens the smallest diffusion time as a random variable in the training stage to boost sample performance by improving the score accuracy, particularly on large diffusion time. In the inference stage, Soft Truncation uses the fixed smallest diffusion time ( ). INDM (NLL) + ST INDM enables to learn a diffusion bridge between two distinctive data distributions. outperforms LSGM-109M (balanced) in terms of FID with comparable NLL. Also, INDM-121M (NLL) outperforms LSGM-269M (NLL) in FID with identical NLLs. We achieve SOTA FID of 1.75 on CelebA in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Image-to-image translation from EM-NIST letters dataset to MNIST digits dataset. INDM can alternatively train the nonlinear diffusion from p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Theorem 2 .Figure 10 :</head><label>210</label><figDesc>Gap(? ? , ? ?,? ) = 0 if and only if s ? ? S sol . ? Descriptive Illustration On Nearly MLE Training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Particle trajectories of the probability flow ODE for VPSDE on the synthetic two moons 2d dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Comparison of linear diffusion bridges on data and latent spaces in diverse datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Descriptive Illustration On Diffusion Bridge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Data Latent (a) Data and Latent Manifolds At Initial Stage of Training Data Latent (b) Data and Latent Manifolds Afer Training of 10k Steps Diffusion Trajectory Optimal Transport Trajectory Latent (Initial Point) Optimal Monge Map Final Point of ODE for VPSDE (c) Diffusion and (optimal) Monge Trajectories At Initial Stage of Training Diffusion Trajectory Latent (Initial Point) Optimal Monge Map Final Point of ODE for VPSDE (d) Diffusion and (optimal) Monge Trajectories After Training of 10k Steps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>-(a), after the training, the latent manifold is inflated to the outside of the real data in Figure 14-(b). Therefore, the probability flow ODE (deterministic trajectory), after the training, transports the initial mass to the final mass with a nearly linear line in Figure 14-(d), in contrast to the curvy VPSDE trajectory at the initial phase of training in Figure 14-(c). In this example, the flow training puts the latent manifold out of the data manifold, and this helps the robust sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 :</head><label>15</label><figDesc>Illustrative Particle Trajectory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 15 illustrates</head><label>15</label><figDesc>the concept of linearized diffusion path. As the flow inflates the latent manifold, the diffusion trajectory becomes more linear, andFigure 7supports the conceptual illustration ofFigure 15on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>2 2 2 E 1 2</head><label>221</label><figDesc>as a random variable X i , Eq. (23) is reduced to 1 [X i ], and Eq. (24) is reduced to 1 2 E sample-path [ X i ]. Therefore, the variance of the Monte-Carlo estimation of Eq. (23) becomes Var(X i ), whereas the variance of the Monte-Carlo estimation of Eq. (24) becomes 1 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>where p T is the marginal distribution of ? at t = T . If V(p r , ?) is the space of all vector fields v of which forward SDE with Eq. (29) satisfies the boundary conditions, then SBP is equivalent to min ??P(pr,?) D KL (? ?) = min v?V(pr,?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>? t as the marginal distribution of the model at t. Then, the generative SDEs with different ? have distinctive marginal distributions: p ?,? t = p ? ,? t for ? = ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>5 Ex</head><label>5</label><figDesc>? log p ? (x ) ? Ex 0 ? log p ? (x 0 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>5t 2 (</head><label>2</label><figDesc>?max??min)?t?min ) + 0.5t 2 (? max ? ? min ) + t? min T = log (1 ? e ?0.5T 2 (?max??min)?T ?min ) ? log (1 ? e ?0.5 2 (?max??min)? ?min ) + 0.5(T 2 ? 2 )(? max ? ? min ) + (T ? )? min ?23.86</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>+ ? 2 min 1 + e Zu+F ( ) ?? t log ? 2 max ? 2 min=</head><label>212</label><figDesc>log e Zu+F ( ) + ? 2 min ? log 1 + e Zu+F ( ) ? log ? 2 min .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 17 :</head><label>17</label><figDesc>Comparison of data and latent diffusions by training steps of Checkerboard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>F. 5 F. 5 . 1 Figure 18 :</head><label>55118</label><figDesc>Visualization of Latent Visualization of 2d Latent Manifold (a) Samples from x ?,? 0 (b) Samples from z ? 0 Samples from the data space and latent space on CIFAR-10 and CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 17 illustrates</head><label>17</label><figDesc>the data and latent manifolds of the 2d checkerboard dataset by training steps. The data manifold has the singularity at the origin, but this singularity disappears in the latent manifold after the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>F. 5 . 2</head><label>52</label><figDesc>Visualization of High-dimensional Latent Vector on Benchmark DatasetsFigure 18 illustrates the samples from (a) the data space and (b) the latent space. To visualize the latent vectors, we normalize the latent value into the [0, 1] d space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>F. 6</head><label>6</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 19 :</head><label>19</label><figDesc>Eigenvalue of GG T /g 2 on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figures 20 and 21 2 T 0 ?(t) ? g 2 2 T0 g 2 ? ?x0 ? 1 ,</head><label>21202221</label><figDesc>show the non cherry-picked random samples from INDM (VE, FID) on CIFAR-10 and INDM (VP, FID) on CelebA, respectively.G Proofs of Theorems and PropositionsTheorem 1. Suppose that p ?,? (x 0 ) is the likelihood of a generative random variable x ?,? 0 . Then, the negative log-likelihood is bounded byE pr(x0) ? log p ?,? (x 0 ) ? L {x t } T t=0 , g 2 ; {?, ?} , where L {x t } T t=0 , g 2 ; {?, ?} = ?E pr(x0) log det ? x0 h ? + L {z t } T t=0 , g 2 ; ? ? E z T log ?(z T ) + d (t) ? 2 (t) dt, with L {z t } T t=0 , g 2 ; ? := 1 (t)E z0,zt s ? (z t , t) ? ? zt log p 0t (z t |z 0 ) 2 2 dt. Here, p 0t (z t |z 0 ) isthe transition probability of the forward linear diffusion process on latent space. Proof of Theorem 1. From the change of variable, the transformation of z 0 = h ? (x 0 ) induces p r (x 0 ) = p 0 (z 0 ) det ?h and thus the entropy of the data distribution becomesH(p r ) = ? p r (x 0 ) log p r (x 0 ) dx 0 = ? p 0 (z 0 ) log p 0 (z 0 ) det ?h ? ?x0 ?1 dz 0 = ? p r (x 0 ) log det ?h ? ?x 0 dx 0 ? p 0 (z 0 ) log p 0 (z 0 ) dz 0 = ?E pr(x0) log det ?h ? ?x 0 ? p 0 (z 0 ) log p 0 (z 0 ) dz 0 = ?E pr(x0) log det ?h ? ?x 0 + H(p 0 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>|0 is the transition kernel from time zero to T and P R is the reverse-time measure, i.e., for anyA ? B(C) we have P R (A) = P(A R ) with A R = {t ? ?(T ? t) : ? ? A}. Then, p ? 0 P T |0 P R T |0 (A) = P T P R T |0 (A) = P R 0 P R T |0 (A) = P R T (A) = p ? 0 (A).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Now, from Lemma 4</head><label>4</label><figDesc>and the invertibility of the flow transformation, we havep r ? (h ?1 ? ) # ? p ? 0,N T V = (h ? ) # ? p r ? p ? 0,N T V = p ? 0 ? p ? 0,N T V ,whichcompletes the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 20 :</head><label>20</label><figDesc>Non cherry-picked random samples from CIFAR-10 trained on INDM (VE, deep, FID).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 21 :</head><label>21</label><figDesc>Non cherry-picked random samples fr om CelebA trained on INDM (VP, FID).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of INDM with previous works. N is the number of random variables.</figDesc><table><row><cell>Model</cell><cell>Nonlinear Diffusion</cell><cell>Implemented Data Diffusion</cell><cell>Latent Diffusion</cell><cell>Nonlinear f -Modeling</cell><cell>Nonlinear G-Modeling</cell><cell>Explicit f &amp;G Derived</cell><cell>Training Complexity</cell><cell>Sampling Cost</cell></row><row><cell>DDPM++</cell><cell></cell><cell>Continuous</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>O(1)</cell><cell>?</cell></row><row><cell>LSGM</cell><cell></cell><cell></cell><cell>Continuous</cell><cell></cell><cell></cell><cell></cell><cell>O(1)</cell><cell>?</cell></row><row><cell>SBP</cell><cell></cell><cell>Discrete</cell><cell></cell><cell>Explicit</cell><cell></cell><cell></cell><cell>O(N )</cell><cell>?</cell></row><row><cell>DiffFlow</cell><cell></cell><cell>Discrete</cell><cell></cell><cell>Explicit</cell><cell></cell><cell></cell><cell>O(N )</cell><cell>?</cell></row><row><cell>INDM</cell><cell></cell><cell>Continuous</cell><cell>Continuous</cell><cell>Implicit</cell><cell>Implicit</cell><cell></cell><cell>O(1)</cell><cell>?</cell></row><row><cell cols="3">5 Related Work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Algorithm 1 Implicit Nonlinear Diffusion Model</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1: repeat</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>: Aver-</cell></row><row><cell cols="2">age L 2 2 Norm.</cell></row><row><cell>Manifold</cell><cell>Norm</cell></row><row><cell>Data</cell><cell>776</cell></row><row><cell>Latent</cell><cell>5,385</cell></row><row><cell>Prior</cell><cell>3,072</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison to linear/nonlinear diffusion models on CIFAR-10. We report the performance of linear diffusions by training our PyTorch implementation based on Song et al.<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref> with identical hyperparameters and score networks on both linear/nonlinear diffusions to quantify the effect of nonlinearity in a fair setting. Boldface numbers represent the best performance in a column.</figDesc><table><row><cell>SDE</cell><cell>Model</cell><cell>Nonlinear Data Diffusion</cell><cell># Params</cell><cell cols="2">NLL (?) correction after correction before</cell><cell cols="2">NELBO (?) w/ residual w/o residual (after) (before)</cell><cell cols="2">Gap (?) (=NELBO-NLL) after before</cell><cell cols="2">FID (?) ODE PC</cell></row><row><cell>VE</cell><cell>NCSN++ (FID) INDM (FID)</cell><cell></cell><cell>63M 76M</cell><cell>4.86 3.22</cell><cell>3.66 3.13</cell><cell>4.89 3.28</cell><cell>4.45 3.24</cell><cell>0.03 0.06</cell><cell>0.79 0.11</cell><cell>--</cell><cell>2.38 2.29</cell></row><row><cell></cell><cell>DDPM++ (FID)</cell><cell></cell><cell>62M</cell><cell>3.21</cell><cell>3.16</cell><cell>3.34</cell><cell>3.32</cell><cell>0.13</cell><cell>0.16</cell><cell>3.90</cell><cell>2.89</cell></row><row><cell></cell><cell>INDM (FID)</cell><cell></cell><cell>75M</cell><cell>3.17</cell><cell>3.11</cell><cell>3.23</cell><cell>3.18</cell><cell>0.06</cell><cell>0.07</cell><cell>3.61</cell><cell>2.90</cell></row><row><cell>VP</cell><cell>DDPM++ (NLL)</cell><cell></cell><cell>62M</cell><cell>3.03</cell><cell>2.97</cell><cell>3.13</cell><cell>3.11</cell><cell>0.10</cell><cell>0.14</cell><cell>6.70</cell><cell>5.17</cell></row><row><cell></cell><cell>INDM (NLL)</cell><cell></cell><cell>75M</cell><cell>2.98</cell><cell>2.95</cell><cell>2.98</cell><cell>2.97</cell><cell>0.00</cell><cell>0.02</cell><cell>6.01</cell><cell>5.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison on CIFAR-10.</figDesc><table><row><cell>SDE</cell><cell>Type</cell><cell>Model</cell><cell cols="2"># Params</cell><cell>NLL FID</cell></row><row><cell></cell><cell></cell><cell>NCSN++ (FID) [1]</cell><cell></cell><cell>108M</cell><cell>4.85 2.20</cell></row><row><cell></cell><cell></cell><cell>DDPM++ (FID) [1]</cell><cell></cell><cell>108M</cell><cell>3.19 2.64</cell></row><row><cell>Linear</cell><cell></cell><cell>DDPM++ (NLL) [1]</cell><cell></cell><cell>108M</cell><cell>3.01 4.88</cell></row><row><cell></cell><cell></cell><cell>VDM [28]</cell><cell></cell><cell>-</cell><cell>2.65 7.41</cell></row><row><cell></cell><cell></cell><cell>CLD-SGM [20]</cell><cell></cell><cell>108M</cell><cell>3.31 2.25</cell></row><row><cell></cell><cell>SBP</cell><cell>SB-FBSDE [16]</cell><cell></cell><cell>102M</cell><cell>2.98 3.18</cell></row><row><cell></cell><cell></cell><cell>LSGM (FID) [9]</cell><cell></cell><cell>476M</cell><cell>3.45 2.10</cell></row><row><cell></cell><cell>VAE -based</cell><cell cols="2">LSGM (NLL) [9] LSGM (NLL) [9] LSGM (balanced) [9]</cell><cell>269M 506M 109M</cell><cell>2.97 6.15 2.87 6.89 2.96 4.60</cell></row><row><cell>Nonlinear</cell><cell></cell><cell cols="2">LSGM (balanced) [9]</cell><cell>476M</cell><cell>2.98 2.17</cell></row><row><cell></cell><cell></cell><cell>DiffFlow (FID) [13]</cell><cell></cell><cell>?36M</cell><cell>3.04 14.14</cell></row><row><cell></cell><cell>Flow -based</cell><cell>INDM (FID) INDM (NLL)</cell><cell></cell><cell>118M 121M</cell><cell>3.09 2.28 2.97 4.79</cell></row><row><cell></cell><cell></cell><cell>INDM (NLL) + ST</cell><cell></cell><cell>75M</cell><cell>3.01 3.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on CelebA 64 ? 64.</figDesc><table><row><cell>Model</cell><cell>NLL</cell><cell>NELBO</cell><cell>Gap</cell><cell>FID</cell></row><row><cell>UNCSN++ [26]</cell><cell>1.93</cell><cell>-</cell><cell>-</cell><cell>1.92</cell></row><row><cell>DDGM [29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.92</cell></row><row><cell>Efficient-VDVAE [30]</cell><cell>-</cell><cell>1.83</cell><cell>-</cell><cell>-</cell></row><row><cell>CR-NVAE [31]</cell><cell>-</cell><cell>1.86</cell><cell>-</cell><cell>-</cell></row><row><cell>DenseFlow-74-10 [4]</cell><cell>1.99</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>StyleFormer [70]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.66</cell></row><row><cell>NCSN++ (VE, FID)</cell><cell>3.41</cell><cell>3.42</cell><cell>0.01</cell><cell>3.95</cell></row><row><cell>INDM (VE, FID)</cell><cell>2.31</cell><cell>2.33</cell><cell>0.02</cell><cell>2.54</cell></row><row><cell>DDPM++ (VP, FID)</cell><cell>2.14</cell><cell>2.21</cell><cell>0.07</cell><cell>2.32</cell></row><row><cell>INDM (VP, FID)</cell><cell>2.27</cell><cell>2.31</cell><cell>0.04</cell><cell>1.75</cell></row><row><cell>DDPM++ (VP, NLL)</cell><cell>2.00</cell><cell>2.09</cell><cell>0.09</cell><cell>3.95</cell></row><row><cell>INDM (VP, NLL)</cell><cell>2.05</cell><cell>2.05</cell><cell>0.00</cell><cell>3.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Effect of Pretraining.</figDesc><table><row><cell>Model</cell><cell>NLL</cell><cell>FID</cell></row><row><cell>DDPM++</cell><cell>3.03</cell><cell>6.70</cell></row><row><cell>INDM (w/ pre)</cell><cell>2.98</cell><cell>6.01</cell></row><row><cell>INDM (w/o pre)</cell><cell>2.98</cell><cell>8.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>investigates how the flow training affects to performances, under the various linear diffusions and weighting functions. It compares the pre-trained NCSN++/DDPM++ with INDM, of which these pre-trained models initialize the score network of INDM. Experiments inTable 4presents that INDM improves NELBO in any setting, and minimizes the variational gap to zero if we train the score network with the likelihood weighting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Data and Latent Diffusion Processes . . . . . . . . . . . . . . . . . . . . . . . . . 3 4.2 Model Training and Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Full Statement of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.2 Geometric Interpretation of Latent Diffusion . . . . . . . . . . . . . . . . . . . . . 24 Latent Score-based Generative Model (LSGM) . . . . . . . . . . . . . . . . . . . 27 D.2 Diffusion Normalizing Flow (DiffFlow) . . . . . . . . . . . . . . . . . . . . . . . 28 D.3 Schr?dinger Bridge Problem (SBP) . . . . . . . . . . . . . . . . . . . . . . . . . . 32 NLL Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.5 Calculating the Residual Term . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 F Experimental Details and Additional Results 37 F.1 Model Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 F.2 Experimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 F.2.1 Variance Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 F.2.2 Sampling Tricks to Improve FID . . . . . . . . . . . . . . . . . . . . . . . 39 F.2.3 Interpolation Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 F.3 Effect of Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 F.4 Training Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 F.5 Visualization of Latent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42</figDesc><table><row><cell>Contents</cell><cell></cell></row><row><cell>1 Introduction</cell><cell>1</cell></row><row><cell>2 Preliminary</cell><cell>2</cell></row><row><cell>3 Motivation of Nonlinear Diffusion Process</cell><cell>3</cell></row><row><cell>4 Implicit Nonlinear Diffusion Model</cell><cell>3</cell></row><row><cell>(173):135-150, 1986. 4.1 5 Related Work</cell><cell>Mathematics of computation, 46 5</cell></row><row><cell>6 Discussion</cell><cell>6</cell></row><row><cell>8 Conclusion</cell><cell>10</cell></row><row><cell>A Derivations</cell><cell>18</cell></row><row><cell>C Details on Section 6.2</cell><cell>24</cell></row><row><cell>C.1 D Related Work</cell><cell>27</cell></row><row><cell>D.1</cell><cell></cell></row></table><note>[61] Alexia Jolicoeur-Martineau, R?mi Pich?-Taillefer, Ioannis Mitliagkas, and Remi Tachet des Combes. Adversarial score matching and improved sampling for image generation. In Interna- tional Conference on Learning Representations, 2020.[62] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.[63] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation, 2022.[64] Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the monge-kantorovich mass transfer problem. Numerische Mathematik, 84(3):375-393, 2000.[65] C?dric Villani. Optimal transport: old and new, volume 338. Springer, 2009.[66] Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Sch?lkopf, and Gert RG Lanckriet. Hilbert space embeddings and metrics on probability measures. The Journal of Machine Learning Research, 11:1517-1561, 2010.[67] Lawrence C Evans. Partial differential equations. Graduate studies in mathematics, 19(2), 1998.[68] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. arXiv preprint arXiv:2208.05314, 2022.[69] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 12104-12114. Curran Associates, Inc., 2020.[70] Jeeseung Park and Younggeun Kim. Styleformer: Transformer based generative adversarial networks with style vector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8983-8992, 2022.[71] Abdul Fatir Ansari, Ming Liang Ang, and Harold Soh. Refining deep generative models via discriminator gradient flow. In International Conference on Learning Representations, 2020.[72] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make one strong gan, and that can scale up. Advances in Neural Information Processing Systems, 34, 2021.[73] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International Conference on Machine Learning, pages 1747-1756. PMLR, 2016.[74] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.6.1 Benefit of INDM in Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 6.2 Benefit of INDM in Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 7 Experiments 8 7.1 Correction on Likelihood Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 8 7.2 Quantitative Results on Image Generation . . . . . . . . . . . . . . . . . . . . . . 9 7.3 Application Task: Dataset Interpolation . . . . . . . . . . . . . . . . . . . . . . . 10A.1 Derivation of Variational Bound for Nonlinear Diffusion . . . . . . . . . . . . . . 18 A.2 Derivation of Nonlinear Drift and Volatility Terms for INDM . . . . . . . . . . . . 19 B Details on Section 6.1 20 B.1 Restricting Search Space of s ? into S div . . . . . . . . . . . . . . . . . . . . . . . 23E Correction of Density Estimation Metrics of Diffusion Models with Time Truncation 33 E.1 Equivalent Reverse SDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 E.2 Log-Likelihood for Diffusion Models with Time Truncation . . . . . . . . . . . . 34 E.3 NELBO Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 E.4F.5.1 Visualization of 2d Latent Manifold . . . . . . . . . . . . . . . . . . . . . 42 F.5.2 Visualization of High-dimensional Latent Vector on Benchmark Datasets . 42 F.6 Nonlinear Diffusion Coefficient . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 F.7 Relative Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 F.8 Full Quantitative Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 F.9 Random samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 G Proofs of Theorems and Propositions 43</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Statistics of data variable and latent variable on CIFAR-10. All statistics are averaged by dimension.Mean Variance Min Max</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>LSGM training fails when using the variance weighting function.</figDesc><table><row><cell></cell><cell>NLL</cell><cell>NELBO</cell><cell>FID</cell></row><row><cell>LSGM (VP, FID)</cell><cell>NaN</cell><cell>NaN</cell><cell>NaN</cell></row><row><cell>INDM (VP, FID)</cell><cell>3.23</cell><cell>3.17</cell><cell>2.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparison of latent dimension of INDM and LSGM.</figDesc><table><row><cell>Datset</cell><cell>Data Dimension</cell><cell>Latent Dimension of INDM</cell><cell>Latent Dimension of INDM</cell></row><row><cell>MNIST</cell><cell>784</cell><cell>784</cell><cell>2,560</cell></row><row><cell>CIFAR-10</cell><cell>3,072</cell><cell>3,072</cell><cell>46,080</cell></row><row><cell>CelebA-HQ 256</cell><cell>196,608</cell><cell>196,608</cell><cell>819,200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>This reformulated Eq. (24) estimates L DiffFlow with a single sample path from the Markov chain of {x EM ti } N t=1 , so it requires O(N ) flow evaluations to estimate L DiffFlow (?, ?). Therefore, DiffFlow takes O(N ) computational complexity in total for every optimization step. There are five differences between DiffFlow and INDM. Basically, these differences arise from the different usage of the flow transformation between DiffFlow and INDM. First, INDM enables to train the continuous diffusion model without the sacrifice on training time, while DiffFlow is limited on the discrete diffusion model at the expense of slower training time. DiffFlow approximates the forward nonlinear SDE with a finite Markov chain. Suppose x EM</figDesc><table /><note>t to be the continuous-time random variable defined by x EM t</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>The variance ratio between the variances of the analytic transition probability-based estimation of Eq. (23) and the sample-based estimation of Eq.<ref type="bibr" target="#b23">(24)</ref>.</figDesc><table><row><cell></cell><cell cols="5">Number of Random Variables (N )</cell></row><row><cell></cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>1000</cell><cell>10000</cell></row><row><cell>Estimation Variance Ratio</cell><cell>1.00</cell><cell>1.02</cell><cell>2.08</cell><cell>16.68</cell><cell>76.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Despite of our implementation is built deeply based on Song et al.<ref type="bibr" target="#b0">[1]</ref>, our pytorch implementation and the jax implementation of Song et al.<ref type="bibr" target="#b10">[11]</ref> differs in their final performances.</figDesc><table><row><cell>SDE</cell><cell>Model</cell><cell>after</cell><cell>NLL before</cell><cell cols="2">NELBO w/ residual w/o residual</cell><cell>after</cell><cell>Gap before</cell><cell>FID ODE</cell></row><row><cell></cell><cell>DDPM++ (NLL, reported) [11]</cell><cell>-</cell><cell>2.95</cell><cell>-</cell><cell>3.08</cell><cell>-</cell><cell>0.13</cell><cell>6.03</cell></row><row><cell>VP</cell><cell>DDPM++ (NLL, ours)</cell><cell>3.03</cell><cell>2.97</cell><cell>3.13</cell><cell>3.11</cell><cell>0.10</cell><cell>0.14</cell><cell>6.70</cell></row><row><cell></cell><cell>INDM (NLL)</cell><cell>2.98</cell><cell>2.95</cell><cell>2.98</cell><cell>2.97</cell><cell>0.00</cell><cell>0.02</cell><cell>6.01</cell></row><row><cell cols="5">F Experimental Details and Additional Results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">F.1 Model Architecture</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Ablation study on the stopping sampling time trained on DDPM++ (VP) in CIFAR-10.</figDesc><table><row><cell>Model</cell><cell>FID (tmin = 10 ?3 )</cell><cell>FID (tmin = 10 ?4 )</cell><cell>FID (tmin = 10 ?5 )</cell></row><row><cell>INDM (deep, VP, NLL)</cell><cell>5.94</cell><cell>5.74</cell><cell>5.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>Ablation study on the SNR trained on NCSN++ (VE) in CIFAR-10. The performances are FID-5k scores.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Signal-to-Natio Ratio (SNR)</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.13</cell><cell>0.14</cell><cell>0.15</cell><cell>0.16</cell><cell>0.17</cell></row><row><cell></cell><cell>INDM (VE, FID)</cell><cell>7.24</cell><cell>7.12</cell><cell>7.20</cell><cell>7.25</cell><cell>7.34</cell></row><row><cell cols="7">for T = 1 and = 10 ?5 . The solution for the inverse CDF in Eq. (40) becomes</cell></row><row><cell>e</cell><cell cols="2">t 0 ?(s) ds = 1 + exp (Zu + F( ))</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 :</head><label>15</label><figDesc>Ablation study on the temperature for PC sampler trained on DDPM++ (VP) in CIFAR-10. The performances are FID scores. Contrary to Kingma and Dhariwal<ref type="bibr" target="#b54">[55]</ref>, the temperature bigger than 1 works the best.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Temperature</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>1.03</cell><cell>1.04</cell><cell>1.05</cell><cell>1.1</cell><cell>1.2</cell></row><row><cell>INDM (VP, FID)</cell><cell>2.92</cell><cell>2.91</cell><cell>2.90</cell><cell>2.90</cell><cell>2.91</cell><cell>3.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 18 :</head><label>18</label><figDesc>Elapsed time per a training step by discretization.</figDesc><table><row><cell>Model</cell><cell>Complexity</cell><cell>N = 100</cell><cell>N = 1, 000</cell><cell>N = ?</cell></row><row><cell>DDPM</cell><cell>O(1)</cell><cell>0.27</cell><cell>0.27</cell><cell>0.27</cell></row><row><cell>SBP (w/o experience memory)</cell><cell>O(N )</cell><cell>2.83</cell><cell>23.3</cell><cell>?</cell></row><row><cell>SBP (w/ experience memory)</cell><cell>O(N )</cell><cell>0.52</cell><cell>2.39</cell><cell>?</cell></row><row><cell>DiffFlow</cell><cell>O(N )</cell><cell>18.45</cell><cell>180.88</cell><cell>?</cell></row><row><cell>INDM</cell><cell>O(1)</cell><cell>1.69</cell><cell>1.69</cell><cell>1.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 19 :</head><label>19</label><figDesc>Total training time in a single GPU days.</figDesc><table><row><cell>Model</cell><cell>Total Training Time (GPU Days)</cell><cell>Training Steps</cell><cell>GPU Spec</cell><cell>#GPUs</cell><cell>NLL</cell><cell>FID</cell></row><row><cell>DDPM++</cell><cell>5</cell><cell>500k</cell><cell>P40</cell><cell>1</cell><cell>3.03</cell><cell>6.70</cell></row><row><cell>LSGM</cell><cell>44</cell><cell>450k</cell><cell>RTX 3090</cell><cell>8</cell><cell>2.87</cell><cell>6.89</cell></row><row><cell>SBP</cell><cell>3</cell><cell>260k</cell><cell>RTX 3090</cell><cell>-</cell><cell>2.98</cell><cell>3.18</cell></row><row><cell>DiffFlow</cell><cell>32</cell><cell>100k</cell><cell>RTX 2080</cell><cell>8</cell><cell>3.04</cell><cell>14.14</cell></row><row><cell>INDM (including pre-training time)</cell><cell>25</cell><cell>700k</cell><cell>P40</cell><cell>4</cell><cell>2.98</cell><cell>6.01</cell></row><row><cell>INDM (w/o pre-training)</cell><cell>60</cell><cell>600k</cell><cell>P40</cell><cell>4</cell><cell>2.98</cell><cell>8.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 18</head><label>18</label><figDesc>presents the elapsed time per a training step by the number of discretization on CIFAR-10. In contrast to INDM which is invariant on the choice of N , the training time of SBP and DiffFlow is not scalable for their O(N ) complexities. The training time is measured under the identical computing resource (1x NVIDIA RTX 3090/Intel I7 3.8GHz) and the same batch size (32) to compare INDM with baselines in a fair setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 19 compares</head><label>19</label><figDesc>INDM with baselines with respect to a single GPU-time for the total training time on CIFAR-10. The remaining columns including training steps, GPU Spec, NLL, and FID are reported for the reference. For DiffFlow, we present the reported GPU days in the paper. For LSGM and SBP, we estimate the elapsed time with the released training configuration in their papers and GitHub repositories. For DDPM++ and INDM, we report the elapsed time from our own experiments. From the table, the overall training time of INDM/DiffFlow/LSGM remains at a similar scale. SBP is the fastest algorithm because of the experience replay memory technique. Note that a completely fair comparison between algorithms is infeasible because the training setup (e.g. #GPUs, training steps, network size . . . ) varies by algorithms. Also, P40 is strictly slower than RTX series GPUs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 20 :</head><label>20</label><figDesc>Relative Energy.Each flow network parameter constructs a different latent trajectory, so training the flow network has the effect of shifting the diffusion bridge.To check if learning the flow network is helpful for the transportation cost or not, recall the Benamou-Brenier formula [64, 65], which is a dual formulation of the Wasserstein distance [65, 66] that the optimal transportation cost is the least kinetic energy out of all admissible transportation plans: W 2 2 (p, q) = inf {pt,vt}t K({p t , v t } t );</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell>Relative Energy</cell></row><row><cell></cell><cell></cell><cell>DDPM++</cell><cell>1.60</cell></row><row><cell></cell><cell></cell><cell>INDM</cell><cell>1.23</cell></row><row><cell>?p t ?t</cell><cell cols="3">+ div(p t v t ) = 0 , p 0 = p, p T = q</cell><cell>, where</cell></row><row><cell></cell><cell>continuity equation</cell><cell cols="2">boundary conditions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 22 :</head><label>22</label><figDesc>Performance comparison on CIFAR-10.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NLL (?)</cell><cell cols="2">NELBO (?)</cell><cell></cell><cell>Gap (?)</cell><cell cols="2">FID (?)</cell></row><row><cell>Class</cell><cell>SDE</cell><cell>Type</cell><cell>Model</cell><cell>after correction</cell><cell>before correction</cell><cell>w/ residual (after)</cell><cell>w/o residual (before)</cell><cell cols="2">(=NELBO-NLL) after before</cell><cell>ODE</cell><cell>PC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>StyleGAN2 + ADA [69]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.92</cell></row><row><cell>GAN</cell><cell></cell><cell></cell><cell>StyleFormer [70] SNGAN + DGflow [71]</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>2.82 9.62</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TransGAN [72]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9.26</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PixcelCNN [73]</cell><cell>3.14</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.9</cell></row><row><cell>Autoregressive</cell><cell></cell><cell></cell><cell>PixcelRNN [73]</cell><cell>3.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sparse Transformer [74]</cell><cell>2.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Glow [55]</cell><cell>3.35</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Residual Flow [23]</cell><cell>3.28</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46.4</cell></row><row><cell>Flow</cell><cell></cell><cell></cell><cell>Flow++ [25] Wolf [24]</cell><cell>3.28 3.27</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>46.4 37.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VFlow [75]</cell><cell>2.98</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DenseFlow-74-10 [4]</cell><cell>2.98</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>34.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>NVAE [6]</cell><cell>-</cell><cell>-</cell><cell>2.91</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Very Deep VAE [76]</cell><cell>-</cell><cell>-</cell><cell>2.87</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VAE</cell><cell></cell><cell></cell><cell>?-VAE [77]</cell><cell>-</cell><cell>-</cell><cell>2.83</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DCVAE [78]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>17.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CR-NVAE [31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DDPM [8]</cell><cell>-</cell><cell>-</cell><cell>3.75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell>NCSNv2 [7]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">10.87</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DDIM [79]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.04</cell></row><row><cell></cell><cell></cell><cell></cell><cell>IDDPM [59]</cell><cell>3.37</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.90</cell></row><row><cell></cell><cell>Linear</cell><cell></cell><cell>VDM [28]</cell><cell>2.65</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7.41</cell></row><row><cell></cell><cell></cell><cell></cell><cell>NCSN++ (FID) [1]</cell><cell>4.85</cell><cell>3.45</cell><cell>4.86</cell><cell>4.43</cell><cell>0.01</cell><cell>0.98</cell><cell>-</cell><cell>2.20</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DDPM++ (FID) [1]</cell><cell>3.19</cell><cell>3.13</cell><cell>3.32</cell><cell>3.29</cell><cell>0.13</cell><cell>0.16</cell><cell>3.69</cell><cell>2.64</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DDPM++ (NLL) [11]</cell><cell>3.01</cell><cell>2.95</cell><cell>3.11</cell><cell>3.09</cell><cell>0.10</cell><cell>0.14</cell><cell>6.43</cell><cell>4.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CLD-SGM [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.31</cell><cell>-</cell><cell>-</cell><cell>2.25</cell><cell>-</cell></row><row><cell>Diffusion</cell><cell></cell><cell>SBP</cell><cell>SB-FBSDE [16]</cell><cell>-</cell><cell>2.98</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.18</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LSGM (FID) [9]</cell><cell>-</cell><cell>-</cell><cell>3.45</cell><cell>3.43</cell><cell>-</cell><cell>-</cell><cell>2.10</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>VAE -based</cell><cell>LSGM (NLL)-269M LSGM (NLL) LSGM (balanced)-109M</cell><cell>---</cell><cell>---</cell><cell>-2.87 -</cell><cell>2.97 2.87 2.96</cell><cell>---</cell><cell>---</cell><cell>6.15 6.89 4.60</cell><cell>---</cell></row><row><cell></cell><cell>Nonlinear</cell><cell></cell><cell>LSGM (balanced)</cell><cell>-</cell><cell>-</cell><cell>2.98</cell><cell>2.95</cell><cell>-</cell><cell>-</cell><cell>2.17</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DiffFlow (FID) [13]</cell><cell>-</cell><cell>-</cell><cell>3.04</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>14.14</cell></row><row><cell></cell><cell></cell><cell>Flow -based</cell><cell>INDM (FID) INDM (NLL)</cell><cell>3.13 2.97</cell><cell>3.03 2.94</cell><cell>3.14 2.97</cell><cell>3.10 2.96</cell><cell>0.01 0.00</cell><cell>0.07 0.02</cell><cell>-5.71</cell><cell>2.28 4.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell>INDM (ST)</cell><cell>3.01</cell><cell>2.98</cell><cell>3.02</cell><cell>3.01</cell><cell>0.01</cell><cell>0.03</cell><cell>3.88</cell><cell>3.25</cell></row></table><note>? 2T M 2 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 23 :</head><label>23</label><figDesc>Performance comparison on CelebA 64 ? 64.</figDesc><table><row><cell>Model</cell><cell cols="8">NLL (?) after before w/ res-w/o res-after before ODE PC NELBO (?) Gap (?) FID (?)</cell></row><row><cell>UNCSN++ [26]</cell><cell>-</cell><cell>1.93</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.92</cell></row><row><cell>DDGM [29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.92</cell></row><row><cell>Efficient-VDVAE [30]</cell><cell>-</cell><cell>1.83</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CR-NVAE [31]</cell><cell>-</cell><cell>-</cell><cell>1.86</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DenseFlow-74-10 [4]</cell><cell>1.99</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>StyleFormer [70]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>3.66</cell></row><row><cell>NCSN++ (VE)</cell><cell>3.41</cell><cell>2.37</cell><cell>3.42</cell><cell>3.96</cell><cell>0.01</cell><cell>1.59</cell><cell>-</cell><cell>3.95</cell></row><row><cell>INDM (VE, FID)</cell><cell>2.31</cell><cell>1.95</cell><cell>2.33</cell><cell>2.17</cell><cell>0.02</cell><cell>0.22</cell><cell>-</cell><cell>2.54</cell></row><row><cell>DDPM++ (VP, FID)</cell><cell>2.14</cell><cell>2.07</cell><cell>2.21</cell><cell>2.22</cell><cell>0.06</cell><cell>0.14</cell><cell cols="2">2.32 3.03</cell></row><row><cell>INDM (VP, FID)</cell><cell>2.27</cell><cell>2.13</cell><cell>2.31</cell><cell>2.20</cell><cell>0.04</cell><cell>0.07</cell><cell cols="2">1.75 2.32</cell></row><row><cell>DDPM++ (VP, NLL)</cell><cell>2.00</cell><cell>1.93</cell><cell>2.09</cell><cell>2.09</cell><cell>0.09</cell><cell>0.16</cell><cell cols="2">3.95 5.31</cell></row><row><cell>INDM (VP, NLL)</cell><cell>2.05</cell><cell>1.97</cell><cell>2.05</cell><cell>2.00</cell><cell>0.00</cell><cell>0.03</cell><cell cols="2">3.06 5.14</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The covariance is d dt E x t+dt |x t [(x t+dt ? xt ? f dt)(x t+dt ? xt ? f dt) T ] = G(xt, t)G T (xt, t).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/yang-song/score_sde_pytorch 3 https://tfhub.dev/tensorflow/tfgan/eval/inception/1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t (z t ) q ? t (z t ) .(47)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data(IITP) funded by the Ministry of Science and ICT(2022-0-00077). Also, this work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government(MSIT) (NRF-2019R1A5A1028324).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>t is the marginal distribution of the path measure of the forward SDE given by dz t = f (z t , t) dt + g(t) dw t ,</p><p>then the corresponding Fokker-Planck equation becomes</p><p>Combining Eq. <ref type="bibr" target="#b46">(47)</ref> with Eq. <ref type="bibr" target="#b47">(48)</ref> and using the integration by parts, the derivative of the KL divergence becomes</p><p>Integrating the above derivative, we get the KL divergence of</p><p>Also, from Eq. (44), we have</p><p>By subtracting Eq. (49) from Eq. (50), we get the desired result:</p><p>Proof of Theorem 2. (?) Suppose the variational gap is zero. Then, as the support of p ? t is the whole space of R d , Theorem 1 implies that s ? (z t , t) = ? log q ? t (z t ) almost everywhere, for any t &gt; 0. To check if s ? (z 0 , 0) = ? log q ? 0 (z 0 ) at t = 0, suppose s ? (z 0 , 0) = ? log q ? 0 (z 0 ) on a set of positive measure. Then, from the continuity of s ? and ? log q ? t , we have s ? (z s , s)</p><p>Proposition 5. Let U be the discrete random variable which takes the values 1, ?1 each with probability 1/2. Then ( T 2 (A ? A T ) 1 ) 2 is the unbiased estimator of A ? A T 2 F . Moreover, U is the unique random variable amongst zero-mean random variables for which the estimator is an unbiased estimator, and attains a minimum variance.</p><p>Proof of Proposition 5. A random variable U 2 has strictly positive variance if U 2 attains more than two values on a nonzero measure. To make Var(U 2 ) = 0, the random variable should be a discrete variable which takes the values 1, -1 each with probability 1/2. Theorem 4 (De Bortoli et al. <ref type="bibr" target="#b14">[15]</ref> and Guth et al. <ref type="bibr" target="#b21">[22]</ref>). Assume that there exists M ? 0 such that for any t ? [0, T ] and z ? R d , the score estimation is close enough to the forward score by </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Densely connected normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Grci?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Grubi?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sini?a</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12438" to="12448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Brian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum likelihood training of score-based diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A variational perspective on diffusionbased generative models and score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diffusion normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Solving schr?dinger bridges via maximum likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Thodoroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austen</forename><surname>Lamacraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1134</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion schr?dinger bridge with applications to score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Valentin De Bortoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Likelihood training of schr?dinger bridge using forward-backward SDEs theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Horng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Theodorou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stochastic differential equations: an introduction with applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Oksendal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Score-based generative modeling with critically-damped langevin diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hopf-cole transformation via generalized schr?dinger bridge problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavien</forename><surname>L?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuchen</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Differential Equations</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page" from="788" to="827" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wavelet score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentin</forename><surname>Guth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Coste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">De</forename><surname>Bortoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Residual flows for invertible generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupling global and local representations via invertible generative flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11201" to="11228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A family of embedded runge-kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07582</idno>
		<title level="m">Non gaussian denoising diffusion models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient-vdvae: Less is more</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louay</forename><surname>Hazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayhane</forename><surname>Mama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragavan</forename><surname>Thurairatnam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13751</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Consistency regularization for variational auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adji</forename><surname>Bousso Dieng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Palette: Image-to-image diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unit-ddpm: Unpaired image translation with denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05358</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Applied stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>S?rkk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Solin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Lecture notes for statistics 311/electrical engineering 377</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<ptr target="https://stanford.edu/class/stats311/Lectures/full_notes.pdf.Lastvisitedon" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Principles of mathematical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Importance weighted autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Helmholtz decomposition and rotation potentials in ndimensional cartesian coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Gl?tzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Richters</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13157</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="574" to="584" />
			<date type="published" when="2020" />
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1059" to="1076" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Matplotlib: A 2d graphics environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2007.55</idno>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90" to="95" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Foundations of data science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravindran</forename><surname>Kannan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pot: Python optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mokhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lie</forename><surname>Alaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Boisbunon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laetitia</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Corenflos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemo</forename><surname>Fatras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?o</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><forename type="middle">T H</forename><surname>Gautheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hicham</forename><surname>Gayraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Janati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ievgen</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Redko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antony</forename><surname>Rolet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Schutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Seguy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titouan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">78</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Understanding ddpm latent codes through optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07477</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Topology and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glen E Bredon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An algorithmic introduction to numerical simulation of stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Desmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Higham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="525" to="546" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the relation between optimal transport and schr?dinger bridges: A stochastic control viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tryphon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pavon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="671" to="691" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Convergence of the iterative proportional fitting procedure. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludger</forename><surname>Ruschendorf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1160" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithin</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7537" to="7547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Vflow: More expressive generative flows with variational data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqi</forename><surname>Chenli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Very deep vaes generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Preventing posterior collapse with delta-vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dual contradistinctive generative autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="823" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
