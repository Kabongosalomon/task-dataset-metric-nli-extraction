<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Human-Centered Machine-Learning Approach for Muscle-Tendon Junction Tracking in Ultrasound Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Leitner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jarolim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Englmair</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annika</forename><surname>Kruse</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">Andrea</forename><surname>Lara Hernandez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Konrad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Schr?ttner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">A</forename><surname>Kelly</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><forename type="middle">A</forename><surname>Lichtwark</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Tilp</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Baumgartner</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Medical Devices</orgName>
								<orgName type="institution" key="instit1">Institute of Health Care Engineering with European Testing</orgName>
								<orgName type="institution" key="instit2">Graz University of Technology</orgName>
								<address>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Physics</orgName>
								<orgName type="institution">University of Graz</orgName>
								<address>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedi-cal Engineering</orgName>
								<orgName type="institution">Galileo University</orgName>
								<address>
									<settlement>Guatemala City</settlement>
									<country key="GT">Guatemala</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Graz</orgName>
								<address>
									<postCode>8010</postCode>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Human Movement and Nutrition Sciences</orgName>
								<orgName type="institution">University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Human-Centered Machine-Learning Approach for Muscle-Tendon Junction Tracking in Ultrasound Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>PREPRINT 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Attention Mechanism</term>
					<term>Anatomical Land- mark Detection</term>
					<term>Convolutional Neural Network</term>
					<term>Domain Generalization</term>
					<term>Feature Extraction</term>
					<term>Label Noise</term>
					<term>Locomo-</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Biomechanical and clinical gait research observes muscles and tendons in limbs to study their functions and behaviour. Therefore, movements of distinct anatomical landmarks, such as muscle-tendon junctions, are frequently measured. We propose a reliable and time efficient machine-learning approach to track these junctions in ultrasound videos and support clinical biomechanists in gait analysis. In order to facilitate this process, a method based on deep-learning was introduced. We gathered an extensive dataset, covering 3 functional movements, 2 muscles, collected on 123 healthy and 38 impaired subjects with 3 different ultrasound systems, and providing a total of 66864 annotated ultrasound images in our network training. Furthermore, we used data collected across independent laboratories and curated by researchers with varying levels of experience. For the evaluation of our method a diverse test-set was selected that is independently verified by four specialists. We show that our model achieves similar performance scores to the four human specialists in identifying the muscle-tendon junction position. Our method provides time-efficient tracking of muscle-tendon junctions, with prediction times of up to 0.078 seconds per frame (approx. 100 times faster than manual labeling). All our codes, trained models and test-set were made publicly available and our model is provided as a free-to-use online service on https://deepmtj.org/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Three examples of the MTJ in the medial gastrocnemius (MG) muscle-tendon unit, recorded with three different instruments. The MTJ is indicated by a red cross. We specify contrast-to-noise ratios (CNR) for all considered instruments. The video frame in Figure a was collected with an Aixplorer V6 US system (Aixplorer). This figure also shows the embedding (yellow color) of the MTJ in the triceps surae muscle-tendon unit (MG, lateral gastrocnemius (LG, not shown), Soleus and Achilles Tendon (AT)). The white arrow in <ref type="figure">Figure b</ref> indicates the direction of principal movement of the MTJ from distal to proximal in the x,y-coordinate system. This video frame was collected with an Esaote MyLab 60 US system (Esaote). <ref type="figure">Figure c</ref> shows an image of the MTJ collected with a Telemed ArtUs US system (Telemed). D URING human locomotion, muscle-tendon complexes of lower limbs are under cyclic concentric and eccentric stress <ref type="bibr" target="#b6">[7]</ref>. Within these units, muscles and tendons have different properties <ref type="bibr" target="#b7">[8]</ref>, contribute differently to external loading <ref type="bibr" target="#b8">[9]</ref> arXiv:2202.05199v1 [cs.CV] 10 Feb 2022 and adapt differently to stimuli <ref type="bibr" target="#b9">[10]</ref>. For instance, imbalances in muscle strength or tendon stiffness may impede efficient interplay during locomotion <ref type="bibr" target="#b10">[11]</ref> or lead to injuries <ref type="bibr" target="#b11">[12]</ref>. In clinical populations, knowledge of alterations in muscles and tendons due to short or long-term treatments (e.g., physical therapy or surgeries) is crucial for developing efficient therapeutic strategies <ref type="bibr" target="#b12">[13]</ref>.</p><p>To investigate tissue behaviour in lower limbs (e.g., the triceps surae muscle tendon unit) and to distinguish between individual contributions of muscles and tendons, their junctions are usually visualized using ultrasound (US) imaging ( <ref type="figure">Fig. 1</ref>) while their displacements are tracked with various methods <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref>. The triceps surae ( <ref type="figure">Fig. 1. a)</ref> is a major contributor to human locomotion. It consists of three heads: the medial (MG) and lateral (LG) gastrocnemius as well as the soleus (SO) muscle. Each individual head is connected via a muscle-tendon junction (MTJ) to the Achilles tendon (AT). Thus, the MTJ provides a form and force-locked interconnection between contracting muscles and passively acting tendons <ref type="bibr" target="#b18">[19]</ref>. In US images, the MTJ is clearly visible due to the change of acoustic impedance in muscles and tendons. Moreover, due to its definable maximum displacement and primary longitudinal (distal to proximal) travel direction ( <ref type="figure">Fig. 1. b</ref>), the area of this anatomical feature is covered by standard-sized linear US arrays <ref type="bibr" target="#b19">[20]</ref>. Therefore, this method is widely used and it improves general understanding of muscle-tendon properties and their behaviour in healthy <ref type="bibr" target="#b20">[21]</ref> and impaired subjects <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>However, musculoskeletal US imaging depends on operators <ref type="bibr" target="#b22">[23]</ref>. In particular, image interpretation requires trained specialists. Moreover, investigating displacements of the MTJ in US images typically needs handcrafted labeling. For this reason, several semi-automatic and automatic methods to track MTJs have been proposed <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Image analysis in biomechanical and clinical US studies relies largely on computer vision algorithms <ref type="bibr" target="#b24">[25]</ref>. Applied on noisy, real-world US motion data, these optical-flow or matching based methods are prone to errors <ref type="bibr" target="#b25">[26]</ref>. This is often due to low frame-rate recordings or poor image qualities of standard medical US systems <ref type="bibr" target="#b19">[20]</ref>. One common parameter to quantify US image quality is the contrast-to-noise-ratio (CNR) <ref type="bibr" target="#b26">[27]</ref>.</p><p>Recently, machine learning solutions for automatic detection and tracking of musculoskeletal features in biomechanical applications have been developed <ref type="bibr" target="#b27">[28]</ref>. These methods improve performance because they can learn to extract salient features, such as anatomical landmarks, directly from annotated input images. Therefore, a neural network is trained to find a mapping between input images and manually set labels <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. With a sufficiently large dataset, neural networks can successfully map novel data (generalization). During training with real world images, the network learns to neglect noise or instrumental errors, yielding robust and accurate results compared with classical computer vision applications <ref type="bibr" target="#b30">[31]</ref>. Leitner et al. <ref type="bibr" target="#b16">[17]</ref> for example, used data from an Esaote US system (Esaote SpA, Genoa, Italy) and a ResNet model architecture <ref type="bibr" target="#b31">[32]</ref> with an attention mechanism <ref type="bibr" target="#b32">[33]</ref> to investigate MTJ predictions on 107 subjects using 7200 manually annotated labels. They found that an inclusion of healthy and impaired patients into the training dataset improved overall performance of their model. Krupenevich et al. <ref type="bibr" target="#b17">[18]</ref> focused their work on the trackability of MTJs across several isometric movements and complex functional tasks such as walking. They trained a MobileNetV2 <ref type="bibr" target="#b33">[34]</ref> architecture on 1200 manually annotated ground truth labels, collected from 15 subjects that were walking, with a Telemed US system (Telemed UAB, Vilnius, Lithuania).</p><p>These newly emerging machine-learning applications for MTJ tracking show that deep neural networks provide strong performance in identifying the exact MTJ positions in US images, even for small training datasets, and independent of subjects and movements. However, there is still lack of evidence on how these algorithms perform on noisy interlaboratory, inter-observer data and may be generalized to diverse settings. Furthermore, previous MTJ tracking neural network models were evaluated on inaccessible test-sets and labels of test-set and training dataset were identified by the same person. This neglects unavoidable positional variations of different observers and introduces potential bias. In particular, machine learning benchmarks need to include more than one clinical specialist to generate reliable reference test-set labels <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref> with low noise <ref type="bibr" target="#b37">[38]</ref>. Moreover, predictions across multiple-domains (e.g. data collected from different instruments) are key in generalizing machine learning algorithms <ref type="bibr" target="#b38">[39]</ref>. For example, deep-learning has shown excellent performance if training and test-set data are drawn from the same underlying distribution. However, large domain shifts in data (e.g. using data from machines of different vendors) often cause significant performance impairments. In case of the proposed MTJ tracker by Leitner et al. <ref type="bibr" target="#b16">[17]</ref> and Krupenevich et al. <ref type="bibr" target="#b17">[18]</ref>, evaluation and training dataset come from the same US instrument. Therefore, these networks might fail to provide similar performance on datasets obtained from other US machine types.</p><p>In this work, we present a novel deep-learning approach for the detection of MTJs in ultrasound images. We curate a large and diverse training dataset, in order to provide a universal MTJ detection method, independent of the used US instrument, movement, muscle region or noise coming from experimental setups (Sect. II-A). We use a deep neural network with U-Net architecture and attention mechanism to predict the position of the MTJ as a probability density function (Sect. II-B). An objective test-set was created and curated by four independent specialist to evaluate the average deviation of our model from specialist labels (Sect. III). In addition, we estimate the generalization to novel datasets and discuss the capabilities of our method (Sect. III).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Labeling</head><p>We used five different datasets in this study <ref type="table" target="#tab_0">(Table I)</ref>. Data were collected at the University of Graz, the Graz University of Technology and the University of Queensland between 2014 and 2020 on 123 healthy and 38 impaired individuals. Our research was approved by the responsible ethics committees, and their approval numbers are given in <ref type="table" target="#tab_0">Table I</ref>. With 1590 recordings, the isometric maximum voluntary contractions (MVC) and passive torque movements (PT) on the medial gastrocnemius (MG) had the largest share in the dataset. A smaller amount of data was collected on the MG during running (48 recordings). The measurements on the lateral gastrocnemius (LG) consist of 109 recordings. The complete and fully anonymous dataset holds 1747 video recordings with a mean length of 19.84 seconds per video. Sequences were captured at frame-rates of 30 frames per second (fps) for studies with an Aixplorer V6 (SuperSonic Imagine, Aix-en-Provence, France) US system (Aixplorer, <ref type="figure">Fig. 1</ref>. a), 25 fps for studies with the Esaote MyLab60 system (Esaote, <ref type="figure">Fig. 1. b)</ref>, and 30-80 fps for the Telemed ArtUs US (Telemed, <ref type="figure">Fig. 1</ref>. c), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Dataset Labels</head><p>The position of the MTJ was identified as the most distal insertion of the muscle into the free tendon ( <ref type="figure">Fig. 1</ref>). Datasets 1, 2, 4 and 5 <ref type="table" target="#tab_0">(Table I)</ref> were annotated by senior investigators (&gt;3 years of research experience, conducted &gt;2 subject trials investigating the MTJ in the past 2 years). Three cycles of reviews on the labels were conducted by the same annotator, with a minimum interval of 2 days between reviews. Dataset 3 <ref type="table" target="#tab_0">(Table I)</ref> was annotated by a junior investigator who was carefully trained to annotate US data of the MTJ. Labels were reviewed by a senior investigator (&gt;10 years of research experience, conducted &gt;4 subject trials investigating the MTJ in the past 2 years). <ref type="table" target="#tab_0">Table II</ref> shows a detailed distribution of all labels over included instruments, muscles and movements. A labeling frequency of 10 frames per video was chosen for dataset 1. Every 5 th frame was annotated in dataset 2-5. These sampling frequencies prevent strong temporal correlations. In total, our training dataset holds 66864 ground truth labels, covering 3 different movements, collected on two muscles from 123 healthy and 38 impaired subjects, and were recorded on three US systems from different vendors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test-set Labels</head><p>For model testing, we first randomly selected 12 participants from our dataset and then completely separated all related 107 recordings into an independent test data pool. From there we randomly excluded 13 MVCs and 14 PT movements and cut each video file to a length of 10 seconds covering the movement. Consequently, we excluded every 5 th frame of each individual time-series. This resulted in a number of 1360 frames in the test-set. We refrained from including the running movement in the test-set because diversity (collected on 3 subjects) and the overall share (7.3%) was low. In total, our test-set holds 1360 ground truth labels, covering 2 different movements, collected on 2 muscles from 12 individual subjects, and were recorded on 3 US systems from different vendors.</p><p>The test-set was then annotated by N = 4 specialists S = {1, 2, 3, 4}. Specialists had 2-10 years of experience in biomechanical and clinical research investigating muscles and tendons in 2-9 US studies in the past 2 years. The data was labeled using a web-based annotation tool (Labelbox Inc., San Francisco, CA, USA). The manual annotation of the test-set took on average 2.76 ? 1.11 hours. In a we show pre-processing steps applied to data such as cropping, resizing and rescaling. Furthermore, we have included image augmentation to account for noise occurring in the experimental data collection. b shows a U-Net architecture with 4-layers and attention mechanism. Our model processes probability maps with soft labels. Therefore, small positional uncertainties about the MTJ did not cause large losses. Figure c demonstrates that we applied a 2D Gauss fit on predicted maps and evaluated filtered noise coming from our algorithm or large inter-specialist variability. For our data analyses depicted in <ref type="figure">Figure d</ref>, we compared soft labels set by our model (red overlay) with individual specialist labels (blue dots with white edges). The 95% confidence interval for our model label pixel position is indicated with the white dotted ellipse. We defined the specialist mean position (blue cross) as the target reference label for our performance evaluation. Figure e shows an example of a complete time-series reconstruction of one PT movement included in the test-set. We have indicated specialists (gray color traces), reference labels (blue trace), and the prediction trace of our model (red color).</p><p>To evaluate specialist performances, we used leave-oneout cross validation. We computed Euclidean distances d k between label positions P k x,y of each individual specialist k ? S, and mean label positionsP j x,y of the remaining three specialists j ? S. Mean deviations of specialistsd S and their standard deviations ? S were computed over all k-folds, and used for model evaluations (Sect. II-D):</p><formula xml:id="formula_0">d S = 1 N j =k (P k x ?P j x ) 2 + (P k y ?P j y ) 2<label>(1)</label></formula><p>Absolute standard deviation of specialists? S is used as a supporting distance metric for our data analysis (Sect. II-D) and filtering (Sect. II-C). From the four specialist labels P k x,y , with k ? S, we computed average label positionsP S x,y , in order to obtain the most likely positions of the MTJ. These average specialist positions were used as a reference for model evaluations (Sect. II-D) and were referred to as reference labels.</p><p>To account for label noise among specialists, the interclass correlation coefficient (ICC) for inter-rater analysis was used. Our calculated ICC is based on a mean-rating (k * = 3) and absolute-agreement in a 2-way mixed-effects model. Furthermore, we calculated standard error of mean (SEM) and rootmean-square errors (RMSE) over all k-folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model</head><p>Data Pre-processing <ref type="figure" target="#fig_1">Fig. 2a</ref> (left) shows image sizes for each instrument output (Aixplorer, Esaote, Telemed). All images were cropped by an aspect ratio of 2 : 1. Crop rectangles are indicated by dashed lines and crop rectangle sizes (x-coordinate, y-coordinate, width, height) by pixels for each instrument. Origins were set at upper left corners. All frames were resized to 256 by 128 pixels by a third order transformation ( <ref type="figure" target="#fig_1">Fig. 2a center)</ref>.</p><p>For our neural network training, we applied additional image augmentation, to counterbalance differences and errors in the experimental setups. During experimental trials using bulky standard medical US probes, or in special environments (e.g. data collection on impaired subjects) transducers are often displaced <ref type="bibr" target="#b19">[20]</ref>. This causes image distortions and can also lead to out-of-plane movements of transducers. Furthermore, images of the same tissue can be mirrored in consecutive trials of the same participant due to changed probe orientations after probe reattachment. For our training, we artificially created similar image distortions by random rotations in ranges of ?20 degrees, zooming images by scale factors of 0.7 to 1.3, with shearing to maximum angles of 0.2, randomly shifting in x-and y-directions by up to 10% of image sizes and randomly flipping images vertically and horizontally (Appx. I, <ref type="figure" target="#fig_9">Fig.8</ref>). All images were normalized for zero mean and unit standard deviation before used as input for the neural network (contrast normalization as described in Goodfellow et al. <ref type="bibr" target="#b28">[29]</ref>). From this pre-processing, we assume that our model is more robust to dissimilarities because of experimental setups and movement artifacts (see Sect. III).</p><p>Manually set labels in the training dataset denote exact pixel positions of estimated MTJ positions in the image. For our training, we used soft labeling, where we assigned probability values to each image pixel (cf., Mathis et al. <ref type="bibr" target="#b39">[40]</ref>). We used probability maps at the same resolution as original images, where we modeled positions of the MTJ by a 2D normal distribution with a covariance of 100 pixels at positions of specialist labels. Therefore, model training is more tolerant in terms of small positional uncertainties, in agreement with uncertainties of manual labels. This also accounts for US images where the MTJ is not visible due to out-of-plane movements of transducers, by assigning no MTJ position in the image. For each probability map, we applied the same random image augmentation and resizing to 256x128 pixels as for input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>A convolutional neural network (CNN) was used to predict a probability map of the MTJ position for a given input image. We built upon a U-Net type architecture <ref type="bibr" target="#b40">[41]</ref> that encodes the input image into a feature representation using consecutive convolutional and max-pooling layers. These layers reduced spatial dimensions while increasing network depth. This allowed the network to extract salient features while taking global contexts of images into account (e.g., the characteristic Y shape of the muscle-tendon structure). The feature representation was decoded into the probability map by the same number of consecutive upsampling and convolutional layers as for decoding. Between encoding and decoding of the features, we applied skip connections to correlate spatial information with the extracted image features <ref type="bibr" target="#b40">[41]</ref>. We assume that only a fraction of extracted features in the encoder is relevant to determine exact MTJ positions. Consequently, we followed the approach by Oktay et al. <ref type="bibr" target="#b41">[42]</ref> and added an attention mechanism to the skip connections of our model. Here the encoded features were used to create attention maps that enabled the network to focus on more important regions (see <ref type="figure" target="#fig_1">Fig. 2b</ref>). We used a network depth of 4, with a starting filter number of 64 that we increased by factor 2 after each max pooling layer and decreased by factor 2 after each upsampling layer. The final probability map was obtained from the final convolutional layer with 1 channel and a sigmoid activation function, such that each pixel in the input image was labeled with values within range [0, 1], where 1 denotes high probability for the position of the MTJ at the corresponding pixel position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>For our model training, we used the full training dataset of 66864 manually annotated images. We used binary crossentropy as a loss function, where we weighted the 0 class with 0.1, in order to counteract class imbalance. The Adam optimizer was used for parameter optimization with a learning rate of 0.0001, and decay rates ? 1 = 0.9 and ? 2 = 0.999.</p><p>To account for domain generalization on different instrumental data, a sequential learning strategy was used. Within three independent training steps, we sequentially added data from the individual instruments. Here we followed the sequence: 1. {Telemed}, 2. {Telemed, Esaote} and 3. {Telemed, Esaote, Aixplorer}, in correspondence with the size of the individual datasets (largest to smallest). We trained our model for 100 epochs per step and kept the pre-trained model weights from previous steps. After each step, model weights were saved, in order to assess the influence of additional data diversity on the model performance (Sect. III). For all other evaluations, we used the final model after the third training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-processing</head><p>From the obtained probability maps, we estimated MTJ positions by fitting a 2D normal distribution to the output of the neural network. For optimization, 2D Gaussian kernels were fitted for each image by employing a trust region reflective algorithm <ref type="bibr" target="#b42">[43]</ref>. To set initial conditions for optimization, we first identified points with the highest probability in US imagesP x,y of our network <ref type="figure" target="#fig_1">(Fig. 2d</ref>). We found that although the training was performed with a pixelwise loss, the neural network provided similar probability maps to the ground-truth labels. In most cases, this allows for an unambiguous identification of the MTJ (error-cases are discusses in Sect. II-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Label Noise Filter</head><p>To account for label noise introduced by our algorithm, as well as by inter-observer variability, we identified three errorcase scenarios. Images which are classified in either one of these categories were excluded from the test-set. The first two error-cases are related to predictions of our network. We excluded predictions where labels were either located at image borders which is the case if </p><p>or where the labels are within a 10 pixel padding around borders, and probability scores are low (max p M &lt; 25%). Such cases are given by:</p><formula xml:id="formula_2">P M x / ? min P M x + 10px, max P M x ? 10px or P M y / ? min P M y + 10px, max P M y ? 10px and max p M &lt; 25%.<label>(3)</label></formula><p>For the first two cases, we excluded 11 images in total (Appx. I, <ref type="figure">Fig. 9</ref>). For these cases, our neural network was not able to identify the position of the MTJ position within the image. This erroneous behaviour occurred in 0.8 % of cases.</p><p>The third error-case relates to inconsistent labels among specialists. 5 images of the test-set (Appx. I, <ref type="figure" target="#fig_10">Fig.10</ref>) were excluded because MTJ labels of at least three specialist showed distances of 20 ?? S from reference labelsP S x,y . This reduced the test-set by a total of 16 images (1.17 %). Thus, 1344 video frames were used for further performance analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Analyses</head><p>We calculated Euclidean distances d M between the reference label positionsP S x,y obtained from specialists (Sect. II-A) and label positions from our model P M x,y , and compared them to mean deviations of specialistsd S . To evaluate labeling performances between our network and specialists RMSEs, SEMs, as well as mean absolute errors were computed. We assessed the agreement between our model and the reference labels in x-and y-coordinates ( <ref type="figure">Fig. 1 b)</ref> using Bland-Altman plots. These comparisons are illustrated by differences between pairs of measurements (d M x , d M y ) as a function of the normalized mean measures (normalization to instrument image sizes). Furthermore, we introduced a tolerance distance given as a multiple n * of mean standard deviations? S from the reference labelsP S x,y . We estimate the number of correctly identified samples by individual specialists and our model with increasing distance form the reference labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with other MTJ trackers</head><p>We compared our model to recent MTJ tracking algorithms by computing RMSE for each method based on our test-set. We considered the semi-supervised computer vision algorithm from Cenni et al. <ref type="bibr" target="#b15">[16]</ref> and deep-learning approaches from Leitner et al. <ref type="bibr" target="#b16">[17]</ref> and Krupenevich et al. <ref type="bibr" target="#b17">[18]</ref>. For the algorithm of Cenni et al. MTJ positions in the first frame of each video are needed to start predictions. Therefore, we used the reference labelsP S x1,i,y1,i of the first video frame, from the i th video file in the test-set, to mark starting positions for optical flow calculations. MTJ trackers from Leitner et al. <ref type="bibr" target="#b16">[17]</ref> and Krupenevich et al. <ref type="bibr" target="#b17">[18]</ref> are based on CNN architectures. We used their provided trained models for predictions on our test-set. For these tracking solutions, evaluation and training datasets come from the same US instruments. To evaluate differences in image qualities of US machine types, and to estimate possible domain specificity of models, CNRs <ref type="figure">(Fig.  1)</ref> were calculated with the method described in <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Open Source Cloud Deployment</head><p>We implemented a cloud deployment (Google LLC, Mountain View, CA, USA) of our model and provide public access to our software-as-a-service via a web application. To comply with strict data security standards, each platform user receives a private and personal 128-bit Universally Unique Identifier (UUID) to access and interact with the uploaded data. Video files are temporarily stored on cloud infrastructure for prediction. User data is not stored on servers beyond calculations and deleted after job completion or in case of lost connections. Our web application is entirely open-source and publicly available at https://deepmtj.org. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS AND DISCUSSION</head><p>In this section, we first investigate inter-rate reliability's and than compare our model predictions to the reference labels. Moreover, we assess performance of other recent, and opensource, available MTJ trackers on our test-set and compare them with our model results. <ref type="table" target="#tab_0">In Table III</ref> an overview of model results is presented. Results are obtained from the full test-set, which includes data from all considered instruments (Aixplorer, Esaote and Telemed), two muscles (MG and LG) and two movements (MVC and PT). Among the considered automated methods, our approach shows the best performance both in terms of RMSE (RMSE M = 4.89 mm) and SEM (SEM M = 0.10 mm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of specialists</head><p>Violin plots in <ref type="figure" target="#fig_4">Fig. 3</ref> show labeling results for each specialist. We plot Euclidean distances d k between the Specialist label positions P k</p><p>x,y and the mean label positions of the remaining three specialists P j x,y . Median and SEM values are comparable between the specialists. Few larger errors of up to 35.43 mm were found for annotations of specialists 1-3. The inter-rater assessment using ICC confirms good inter-rater reliability for specialists (ICC = 0.88, 95% confidence interval = [0.87, 0.89]). <ref type="table" target="#tab_0">Table III</ref> shows that our trained network (RMSE M = 4.89 mm) is within the range of the performance level of specialists (RMSE S = 5.05?0.62 mm) on the test-set. The SEM suggests that our model produces consistent results, comparable to the individual specialists. Furthermore, it takes 107.4 s in total (tested on a NVIDIA GeForce RTX2070 graphical processing unit (GPU)) to predict all MTJ positions of the test-set using our algorithm whereas individual specialists need on average 2.76 ? 1.11 hours to label the same dataset. This shows that our method can accelerate data analyses times by a factor of 100 on a low cost GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to specialists</head><p>Bland-Altman plots in <ref type="figure" target="#fig_5">Fig. 4</ref> show that our model has no substantial bias in x-(2.1 mm) and y-direction (0.28 mm) of the detected MTJ position. Larger deviations in the xcoordinate ( <ref type="figure" target="#fig_5">Fig. 4 a)</ref> result from the leading travel direction <ref type="figure">(Fig. 1 b)</ref> of the MTJ. We assume that image quality (i.e., CNR) influences the estimation of MTJ positions for Esaote and Aixplorer, causing larger scattering in x direction. None of the measured biases were found to be proportional to averaged values, indicating that manual and automatic analyses agree equally through the range of measurements.</p><p>In <ref type="figure" target="#fig_6">Fig. 5</ref>, we plot the percentage of correct MTJ detections and correct specialist labels as function of tolerance distance. Tolerance distance is depicted as a multiple n * of absolute standard deviation? S from reference label positionsP S x,y . Results show that the number of correct samples is similar among our model and specialists. A slightly higher performance is achieved by specialists for small n * . The neural network appears to be more robust in general, as can be seen from the close to 100% correct detections with a tolerance  Line plots (our model in red color and specialists in blue color) show valid numbers of frames in percent (y-axis) with increasing tolerance distance (x-axis). Tolerance distance is depicted as a multiple n * of absolute standard deviation? S from reference label positions. P S x,y . distance &gt; 2 ?? S . In other words, the network shows no random false detections, that can occur during manual labeling (e.g., due to attention loss in monotonous tasks <ref type="bibr" target="#b44">[45]</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on different muscles and movements</head><p>We decompose our test-set with respect to included muscles and movements, and then compare results of our algorithm d M to mean deviations of specialistsd S for each category.</p><p>In <ref type="figure" target="#fig_7">Figure 6a</ref>, we show performances on images of LG and MG muscle-tendon units. In <ref type="figure" target="#fig_7">Figure 6b</ref>, we highlight all video frames for our considered movements (MVC and PT). RMSEs shown in <ref type="table" target="#tab_0">Table IV</ref> and kernel densities in <ref type="figure" target="#fig_7">Fig. 6a</ref> and 6b are in good agreement with larger deviations for predictions on the LG. In case of the LG, the share of labeled ground truth data in comparison to the total data volume was small (9.4%). In addition, LG data of the test-set was recorded by the US system (Aixplorer) with the least amount of training data compared to other included devices. Therefore, we assume that performance shortcomings stem from the low training and test dataset volume. Furthermore, varying morphologies of gastrocnemii heads around the muscle tendon junction could also influence prediction qualities of our model. In terms of movements, the two dataset sizes were balanced. The results show that our model has no preference for a specific movement. Comparisons with manual tracking suggests that MTJs, during controlled passive and active contractions, can be reliably and accurately tracked using our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with other MTJ trackers</head><p>We evaluated the performance of recently proposed MTJ tracking approaches on our test-set <ref type="table" target="#tab_0">(Table III</ref>) and found that neither could reach the level of specialists. In particular, the machine learning approaches were trained on machine specific data collected with US systems of one vendor. The approach by Leitner et al. <ref type="bibr" target="#b16">[17]</ref> was trained on Esaote data only, while the method by Krupenevich et al. <ref type="bibr" target="#b17">[18]</ref> is based entirely on Telemed data. When evaluating these algorithms on our test-set data from the respective machine type, the methods by Leitner et al. <ref type="bibr" target="#b16">[17]</ref> and Krupenevich et al. <ref type="bibr" target="#b17">[18]</ref> achieve RMSEs of 5.57 mm and 12.06 mm on Esaote and Telemed data, respectively. Therefore, we conclude that these two machine-learning approaches are domain specific and provide poor performance on our diverse test-set. We associate the lack in performance with the lack of data diversity during training. Hence, it seems that the diversity of instruments, inter-laboratory data and inter-observer settings, as represented in our test-set impacts prediction qualities of previously proposed algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Generalization</head><p>We observe that image qualities differ across US machines of different vendors <ref type="figure">(Fig. 1</ref>). This can originate in the US image formation (beamforming), filtering and other signal processing techniques (e.g. time-gain compensation) implemented on US systems themselves or preset by operators. Image appearances may also vary due to different applications of US systems in experimental setups. Leitner et al. <ref type="bibr" target="#b19">[20]</ref> have shown that, for example, the Telemed system is widely used to collect data during movement with increased frame-rates at the cost of decreasing image qualities. Other devices, such as the Esaote system, offer transducers with wide probe heads to collect data on movements where larger scale length changes occur, decreasing frame-rates. To counteract sensitivity for noise coming from experimental setups and applications we have added data augmentation methods (as depicted in Sect. II-B). Furthermore, the evaluation of methods by Leitner et al. <ref type="bibr" target="#b16">[17]</ref>  and Krupenevich et al. <ref type="bibr" target="#b17">[18]</ref> on our multi-instrumental test-set has demonstrated that the performance of a neural network strongly depends on instrument types used for training and testing <ref type="table" target="#tab_0">(Table III)</ref>. Therefore, our model was trained with US images from three of the four most common US instruments in biomechanical research <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>. In order to analyse the generalization of our network to novel instrumental data, we sequentially introduced datasets from the individual instruments during our model training (Sect. II-B). <ref type="figure" target="#fig_8">Fig. 7</ref> shows the mean absolute deviations of the two intermediate models (trained with images from {Telemed} and {Telemed, Esaote}) and the final model (trained with images from all instruments {Telemed, Esaote, Aixplorer}). For the model that was trained with Telemed data only, we can observe excellent performance for the Telemed images, but the model fails for the other two machine types, similar to the studies by Leitner et al. <ref type="bibr" target="#b16">[17]</ref> and Krupenevich et al. <ref type="bibr" target="#b17">[18]</ref>.</p><p>By adding the Esaote data to the model training, we obtain a significant performance increase for Esaote frames as well as for Aixplorer samples (see <ref type="table" target="#tab_4">Table V</ref>). The model trained with solely Telemed and Esaote images, achieves a high performance for Aixplorer data (RMSE of 5.11), although it was not included in the training set. This suggests a successful generalization to MTJ tracking in US imagery. Thus, we expect that the final training stage of our model {Telemed, Esaote, Aixplorer} provides a similar performance on novel US instrumental data. Moreover, results in <ref type="table" target="#tab_4">Table  V</ref> show that including images from multiple instruments in the model training does not decrease performance for the individual instruments. Hence, models trained with images from {Telemed}, {Telemed, Esaote} or {Telemed, Esaote, Aixplorer} achieve similar performances on the Telemed testset.</p><p>From our multi-instrument training approach and the applied data augmentation, we expect that our model can be applied to a larger variety of different experimental setups (e.g., instrument types, transducer rotations and shifts, image distortions due to movement,...) with less susceptibility to errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this work, we presented a data-driven deep-learning method for the estimation of MTJ positions in US images. Our approach is based on a CNN trained to infer MTJ positions across a variety of US systems from different vendors, collected in independent laboratories from diverse observers, on distinct muscles and movements. We used data augmentation methods and soft labeling to counteract label noise introduced by the experimental setups and inter-observer variability, respectively. A diverse test-set was created and labeled by four independent specialists, in order to provide more objective estimates of MTJ positions. Our method was able to identifiy MTJ positions in 99.2% of cases, with a root-mean-square deviation of 4.89 mm. This error is in the same range as the variation of human specialists (RMSE S = 5.05 ? 0.62 mm). Therefore, our method provides human-like performance on a divers test-set and requires only a fraction of manual labeling times (approx. 100 times faster). We demonstrated that our approach is applicable to generalize to data collected on different US machine types. We made all our codes, trained models and test-set (1344 labeled ultrasound images) publicly available and provide our model as a free-to-use online service under https://deepmtj.org/.</p><p>ACKNOWLEDGMENT C. Leitner would like to acknowledge Martin Sust (University of Graz) for the support of his research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I</head><p>In this section we provide additional figures supporting our work. <ref type="figure" target="#fig_9">Fig. 8</ref> showcases three augmented frames of the same image (MTJ collected on an Esaote system) using our settings for data augmentation. In <ref type="figure">Fig. 9</ref> and <ref type="figure" target="#fig_10">Fig. 10</ref>, we show identified and excluded images from error-cases 1 and 2 as well as error-case 3, respectively.  <ref type="figure">Figure shows</ref> three augmented frames of the same image. We used the ImageDataGenerator implemented in TensorFlow <ref type="bibr" target="#b45">[46]</ref> to additionally generate random data with the following properties: rotation-range=20, horizontal-flip=True, vertical-flip=True, zoom-range=0.3, width-shift-range=0.1, height-shift-range=0.1, shear-range=0.2, fill-mode=reflect. <ref type="figure">Fig. 9</ref>. Excluded images for error-case 1 and 2. Blue crosses indicate pixel positions of reference labels. MTJ probability densities generated by our model are given in shades of red color. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. Three examples of the MTJ in the medial gastrocnemius (MG) muscle-tendon unit, recorded with three different instruments. The MTJ is indicated by a red cross. We specify contrast-to-noise ratios (CNR) for all considered instruments. The video frame in Figure a was collected with an Aixplorer V6 US system (Aixplorer). This figure also shows the embedding (yellow color) of the MTJ in the triceps surae muscle-tendon unit (MG, lateral gastrocnemius (LG, not shown), Soleus and Achilles Tendon (AT)). The white arrow in Figure b indicates the direction of principal movement of the MTJ from distal to proximal in the x,y-coordinate system. This video frame was collected with an Esaote MyLab 60 US system (Esaote). Figure c shows an image of the MTJ collected with a Telemed ArtUs US system (Telemed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>This figure demonstrates the processing workflow of our machine-learning approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>= arg max(p M ) and then set starting conditions for the Gaussian fit as follows: A = max(p M ) ? 2 * SD (amplitude), (x 0 , y 0 ) =P M x,y (origin), ? x = ? y = 5 * ? (standard deviation in x and y direction), ? = d = 0 (angle and offset). Pixel positions with highest kernel density were identified as MTJ positions P M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>? min P M y , max (P M y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Violin plots show the labeling performance of specialists on the test-set. We plot Euclidean distances d k between the Specialist label positions P k x,y and the mean label positions of the remaining three specialists P j x,y . Violin widths are normalized to the total number of included labels in the test-set. Boxplots and whiskers are indicated in gray color. The median is depicted as white circle inside the box. Outliers are shown as filled dots. The interclass correlation coefficient (ICC) at the top is based on a mean-rating (k * = 3) and absolute-agreement in a 2-way mixed-effects model. SEMs for individual specialists are shown horizontally underneath the graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Bland-Altman plots for inter-rater agreement analysis (1344 test-set samples). In figure a and b we show results for MTJ x-and ycoordinates, respectively. Comparisons are illustrated by differences between pairs of measurements (d M x , d M y ) as a function of the normalized mean measures (normalization to instrument image size). Dashed and dotted lines depict bias and 95% limits of agreement, respectively. The color facets show the distribution of instruments: Esaote -red, Telemed -blue, Aixplorer -yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Line plots (our model in red color and specialists in blue color) show valid numbers of frames in percent (y-axis) with increasing tolerance distance (x-axis). Tolerance distance is depicted as a multiple n * of absolute standard deviation? S from reference label positions. P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Breakdown of our model results and specialist deviations in terms of a muscles and b movements. We plot Euclidean distances d M between our model label positions P M x,y and reference label positions P S x,y (red half-violin, negative side) as well as mean specialist deviationsd S (blue half-violin, positive side). Violin widths are normalized to the total number of included labels in the test-set. Boxplots and whiskers are indicated in gray color. The median is depicted as white circle inside the box. Outliers are shown as filled, jittered dots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>The line plots show absolute Euclidean distances d M (y-axis) between our model label positions P M x,y and reference label positions P S x,y for the full test-set (solid line), Aixplorer test-set data (dashed line), Esaote test-set data (dash-dotted line) and Telemed test-set data (dotted line). On the x-axis we denoted the training sequence: 1. training on Telemed (Te), 2. training on Telemed and Esaote (Te &amp; E) and 3. training on our final model with Telemed, Esaote and Aixplorer (Te &amp; E &amp; Aix)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Figure shows three augmented frames of the same image. We used the ImageDataGenerator implemented in TensorFlow [46] to additionally generate random data with the following properties: rotation-range=20, horizontal-flip=True, vertical-flip=True, zoom-range=0.3, width-shift-range=0.1, height-shift-range=0.1, shear-range=0.2, fill-mode=reflect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Excluded images for error-case 3. Blue crosses indicate pixel positions of reference labels. MTJ positions chosen by individual specialists are depicted as blue points with white edges. Gaussian density distributions for MTJ positions generated by our model are given in red color shading.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SPECIFICATIONS</head><label>I</label><figDesc>OF INCLUDED DATASETS. MVC ... isometric maximum voluntary contraction, PT ... passive torque movement, RUN ... running, MG ... medial gastrocnemius, LG ... lateral gastrocnemius, ? Healthy/Impaired, unpub. ... unpublished. Ethics commissions (approval numbers): University of Graz Ethics Commission (GZ. 39/2/63 ex 2011/12 ? ), The University of Queensland Human Research Ethics Committee (2018000525 , 2018000856 ), University of Graz Ethics Commission (GZ. 39/56/63 ex 2018/19 ? ), Medical University of Graz (EK-Nr. 21-362 ex 09/10 ? )</figDesc><table><row><cell>Nr.</cell><cell>Study</cell><cell>Ultrasound System</cell><cell>Transducer</cell><cell cols="2">C. Freq. Frame-rate [MHz] [Hz]</cell><cell>Movement</cell><cell cols="2">Muscle Subjects (H / I  ? )</cell><cell>Tot. Nr. Recordings</cell></row><row><cell>1  ?</cell><cell cols="2">[1]-[3] Esaote MyLab60</cell><cell>LA923</cell><cell>7</cell><cell>25</cell><cell>MVC, PT</cell><cell>MG</cell><cell>66 (66 / 0)</cell><cell>797</cell></row><row><cell>2  ?</cell><cell cols="2">[4]-[6] Esaote MyLab60</cell><cell>LA923</cell><cell>7</cell><cell>25</cell><cell>MVC, PT</cell><cell>MG</cell><cell>41 (13 / 28)</cell><cell>309</cell></row><row><cell>3</cell><cell>unpub.</cell><cell cols="2">Telemed ArtUs LV8-5N60-A2</cell><cell>8</cell><cell>30-34</cell><cell>MVC</cell><cell>MG</cell><cell>9 (9 / 0)</cell><cell>113</cell></row><row><cell>4</cell><cell>unpub.</cell><cell cols="2">Telemed ArtUs LV8-5N60-A2</cell><cell>8</cell><cell>34</cell><cell>MVC</cell><cell>MG</cell><cell>10 (0 / 10)</cell><cell>51</cell></row><row><cell cols="2">5  ? this study</cell><cell cols="2">Telemed ArtUs LV8-5N60-A2 Aixplorer V6 SL10-2</cell><cell>8 9</cell><cell>60-80 25</cell><cell cols="2">MVC, PT, RUN MG, LG MVC, PT</cell><cell>35 (35 / 0)</cell><cell>326 151</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TOTAL: 161 (123 / 38)</cell><cell>1747</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DISTRIBUTION</head><label>II</label><figDesc>OF TRAINING SET LABELS MG (LG) Aixplorer ? Esaote ? ? Telemed ? ? ? TOTAL ? ? ... Esaote MyLab 60, ? ? ? ... Telemed ArtUs, MVC ... isometric maximum voluntary contraction, PT ... passive torque movement, RUN ... running, MG ... medial gastrocnemius, LG ... lateral gastrocnemius.</figDesc><table><row><cell>MVC</cell><cell cols="3">2020 (1216) 6295 (0) 3094 (282)</cell><cell>12907</cell></row><row><cell>PT</cell><cell cols="3">2184 (1868) 3826 (0) 38274 (2900)</cell><cell>49052</cell></row><row><cell>RUN</cell><cell>0 (0)</cell><cell>0 (0)</cell><cell>4905 (0)</cell><cell>4905</cell></row><row><cell cols="4">TOTAL 4204 (3084) 10121 (0) 46273 (3182)</cell><cell>66864</cell></row><row><cell cols="2">? ... Aixplorer V6,</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>SUMMARY OF RESULTS RMSE ... root mean square error, RMSE DS ... domain specific root mean square error, SEM ... standard error of mean, CV ... computer vision, ML ... machine learning ? values are given as mean and standard deviation over all N = 4 specialists.</figDesc><table><row><cell>Annotator</cell><cell>Type</cell><cell>RMSE (RMSE DS ) [mm]</cell><cell>SEM [mm]</cell><cell>time * [min]</cell></row><row><cell>Specialist  ?</cell><cell>Human</cell><cell>5.02 ? 0.62</cell><cell cols="2">0.11 ? 0.02 165.6 ? 66.6</cell></row><row><cell>Cenni et al. [16]</cell><cell>CV</cell><cell>10.62</cell><cell>0.18</cell><cell>8.01</cell></row><row><cell>Leitner et al. [17]</cell><cell>ML</cell><cell>7.92 (5.57  ? )</cell><cell>0.14</cell><cell>2.58</cell></row><row><cell>Krupenevich et al. [18]</cell><cell>ML</cell><cell>26.55 (12.06  ? ? )</cell><cell>0.40</cell><cell>3.78</cell></row><row><cell>This work:</cell><cell>ML</cell><cell>4.89</cell><cell>0.10</cell><cell>1.79</cell></row></table><note>? only Esaote test-set data considered.? ? only Telemed test-set data considered.* models were tested on a NVIDIA GeForce RTX 2070 graphical processing unit.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV FUNCTIONAL</head><label>IV</label><figDesc>RMSE M ... root-mean-square-error of our model, RMSE S ... root-meansquare-error of specialists, LG ... lateral gastrocnemius, MG ... medial gastrocnemius, MVC ... isometric maximum voluntary contraction, PT ...</figDesc><table><row><cell cols="3">ASSESSMENT OF THE DATASET</cell></row><row><cell>Category</cell><cell>RMSE M [mm]</cell><cell>RMSE S [mm]</cell></row><row><cell>LG</cell><cell>6.03</cell><cell>2.26 ? 0.14</cell></row><row><cell>MG</cell><cell>4.79</cell><cell>5.15 ? 0.65</cell></row><row><cell>PT</cell><cell>5.02</cell><cell>5.15 ? 1.14</cell></row><row><cell>MVC</cell><cell>4.74</cell><cell>4.47 ? 0.46</cell></row></table><note>passive torque movement.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V DISTRIBUTION</head><label>V</label><figDesc></figDesc><table><row><cell cols="4">OF ERRORS ON INSTRUMENTAL DATA DURING</cell></row><row><cell cols="3">SEQUENTIAL TRAINING STEPS</cell><cell></cell></row><row><cell>Training Sequence</cell><cell cols="3">RMSE M  ? [mm] Aixplorer Esaote Telemed</cell></row><row><cell>{Telemed}</cell><cell>41.32</cell><cell>48.66</cell><cell>3.32</cell></row><row><cell>{Telemed, Esaote}</cell><cell>5.11</cell><cell>6.31</cell><cell>4.43</cell></row><row><cell>{Telemed, Esaote, Aixplorer}</cell><cell>5.17</cell><cell>5.09</cell><cell>3.60</cell></row><row><cell cols="4">? RMSEs are calculated for data of particular instruments during each</cell></row><row><cell>training sequence.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Increased range of motion after static stretching is not due to changes in muscle and tendon structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tilp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Biomechanics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="636" to="642" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Effect of PNF stretching training on the properties of human muscle and tendon structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Medicine and Science in Sports</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="355" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effects of ballistic streching training on the properties of human muscle and tendon structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tilp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Physiology</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mechanical muscle and tendon properties of the plantar flexors are altered even in highly functional children with spastic cerebral palsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Biomechanics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Muscle and tendon morphology alterations in children and adolescents with mild forms of spastic cerebral palsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Pediatrics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The effect of functional home-based strength training programs on the mechano-morphological properties of the plantar flexor muscle-tendon unit in children with spastic cerebral palsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatric Exercise Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stretch-shortening cycle: a powerful model to study normal and fatigued muscle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Komi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomechanics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1197" to="1206" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Energy-saving mechanisms in walking and running</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Biology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactions between the human gastrocnemius muscle and the Achilles tendon during incline, level and decline locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Lichtwark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Biology</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="4379" to="4388" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human tendon behaviour and adaptation, in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Magnusson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">586</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="81" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Muscular force in running turkeys: the economy of minimizing work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="issue">5303</biblScope>
			<biblScope unit="page" from="1113" to="1115" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Individualized Muscle-Tendon Assessment and Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Physiology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Impact of Altered Gastrocnemius Morphometrics and Fascicle Behavior on Walking Patterns in Children With Spastic Cerebral Palsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?sl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Physiology -www.frontiersin.org</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">518134</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An algorithm for automated analysis of ultrasound images to measure tendon excursion in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Biomechanics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="82" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic Myotendinous Junction Tracking in Ultrasound Images with Phase-Based Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed Research International</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-automatic methods for tracking the medial gastrocnemius muscle-tendon junction using ultrasound: a validation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cenni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Physiology</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="131" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic tracking of the muscle tendon junction in healthy and impaired subjects using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leitner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 42nd Conferences of the IEEE Engineering in Medicine and Biology Society</title>
		<meeting>42nd Conferences of the IEEE Engineering in Medicine and Biology Society</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">07</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated Analysis of Medial Gastrocnemius Muscle-Tendon Junction Displacements in Heathy Young Adults During Isolated Contractions and Walking Using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Krupenevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The development of the myotendinous junction. A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Charvet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ligaments and Tendons Journal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="63" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ultrasound as a Tool to Study Muscle-Tendon Functions during Locomotion: A Systematic Review of Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leitner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effect of training-induced changes in Achilles tendon stiffness on muscle-tendon behavior during landing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Werkhausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Physiology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Medial gastrocnemius and soleus muscle-tendon unit, fascicle, and tendon interaction during walking in children with cerebral palsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Medicine and Child Neurology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="843" to="851" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Is musculoskeletal ultrasonography an operatordependent method or a fast and reliably teachable diagnostic tool? Interreader agreements of three ultrasonographers with different training levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ohrndorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Rheumatology</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Quantifying mechanical loading and elastic strain energy of the human Achilles tendon 1 during walking and running</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kharazi</surname></persName>
		</author>
		<idno>09.05.284182</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">bioRxiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ultrasound imaging to assess skeletal muscle architecture during movements: a systematic review of methods, reliability, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hooren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Appl Physiol</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="978" to="999" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optical flow estimation using high frame rate sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sukhwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2001 International Conference on Image Processing</title>
		<meeting>2001 International Conference on Image Processing<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Continuing Education in Anaesthesia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Swanevelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical Care and Pain</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="186" to="192" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Resolution in ultrasound imaging</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fully automated analysis of muscle architecture from B-mode ultrasound images with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cronin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arxiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved Tracking of Muscle Tendon Junctions in Ultrasound Images Using Speckle Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Englmair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in health technology and informatics</title>
		<imprint>
			<biblScope unit="volume">271</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>NLM (Medline)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-12-12" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learn To Pay Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep learning in spatiotemporal cardiac imaging: A review of methodologies and clinical usability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hernandez</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arxiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automated deep-neural-network surveillance of cranial images for acute neurologic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Titano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1337" to="1341" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning with noisy labels: Exploring techniques and remedies in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101759</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Zech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Medicine</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DeepLabCut: markerless pose estimation of userdefined body parts with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1281" to="1289" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<title level="m">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<title level="m">Attention U-Net: Learning Where to Look for the Pancreas</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Subspace, Interior, and Conjugate Gradient Method for Large-scale Bound-constrained Minimization Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Branch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ustemg: an ultrasound transparent tattoo-based semg system for unobtrusive parallel acquisitions of muscle electromechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leitner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 43nd Conferences of the IEEE Engineering in Medicine and Biology Society</title>
		<meeting>43nd Conferences of the IEEE Engineering in Medicine and Biology Society</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Vigilant attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>O&amp;apos;connell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Attention and Time</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>software available from tensorflow.org. [Online</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
