<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ZUNAIR, HAMZA: MASKED SUPERVISED LEARNING FOR SEGMENTATION Masked Supervised Learning for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasib</forename><surname>Zunair</surname></persName>
							<email>hasibzunair@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Ben</forename><surname>Hamza</surname></persName>
							<email>hamza@ciise.concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University Montreal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ZUNAIR, HAMZA: MASKED SUPERVISED LEARNING FOR SEGMENTATION Masked Supervised Learning for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-attention is of vital importance in semantic segmentation as it enables modeling of long-range context, which translates into improved performance. We argue that it is equally important to model short-range context, especially to tackle cases where not only the regions of interest are small and ambiguous, but also when there exists an imbalance between the semantic classes. To this end, we propose Masked Supervised Learning (MaskSup), an effective single-stage learning paradigm that models both shortand long-range context, capturing the contextual relationships between pixels via random masking. Experimental results demonstrate the competitive performance of MaskSup against strong baselines in both binary and multi-class segmentation tasks on three standard benchmark datasets, particularly at handling ambiguous regions and retaining better segmentation of minority classes with no added inference cost. In addition to segmenting target regions even when large portions of the input are masked, MaskSup is also generic and can be easily integrated into a variety of semantic segmentation methods. We also show that the proposed method is computationally efficient, yielding an improved performance by 10% on the mean intersection-over-union (mIoU) while requiring 3? less learnable parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The basic goal of semantic segmentation, or simply segmentation, is to classify each pixel in an image into one of the pre-defined semantic categories or classes. Its real-world applications are abound, ranging from medical image analysis <ref type="bibr" target="#b18">[19]</ref> to robotics <ref type="bibr" target="#b11">[12]</ref>. In medical imaging, for instance, semantic segmentation can enable physicians to analyze regions of interests (ROIs) more effectively and efficiently for morphological analysis in cancer treatment, especially in high-resolution images <ref type="bibr" target="#b18">[19]</ref>, and information retrieval in diagnosis and surgery <ref type="bibr" target="#b9">[10]</ref>. It also extends to visual scene understanding, which disentangles a scene into objects (e.g. chair), surfaces (e.g. wall) and their relations for robotic object recognition, navigation, manipulation and interaction <ref type="bibr" target="#b11">[12]</ref>.</p><p>Previous works on semantic segmentation include FCN <ref type="bibr" target="#b14">[15]</ref> and U-Net <ref type="bibr" target="#b17">[18]</ref>, which are convolutional-based encoder-decoder networks, and have been further extended for improved performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Some recent works have also demonstrated that modeling long-range context, typically via self-attention mechanism <ref type="bibr" target="#b23">[24]</ref>, translates into better segmentation performance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. Despite the effectiveness of self-attentive models, in this paper we argue that semantic image segmentation is still a challenging problem due to a number of reasons. First, there is diversity in the size and texture of the ROIs <ref type="bibr" target="#b18">[19]</ref>. Second, the same type of ROIs may have different sizes and colors due to the label acquisition protocol. Moreover, there are cases of ambiguity, where the boundary between the ROI and the background cannot easily be distinguished <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. Third, in the case of natural scenes, there are multiple class instances (i.e. objects and surfaces are cluttered) and there exists imbalance in the semantic classes (e.g. pixels in an image associated with the class wall are more than the class chair), as well as different lighting conditions, making the task much more difficult <ref type="bibr" target="#b11">[12]</ref>. Examples of these challenging images are shown in <ref type="figure" target="#fig_0">Figure 1</ref>.  <ref type="bibr" target="#b18">[19]</ref> show the variation in appearance; middle two from CVC-ClinicDB <ref type="bibr" target="#b1">[2]</ref> show difference in scale and ambiguous (ROI); last two from NYU Depth V2 <ref type="bibr" target="#b15">[16]</ref> show many classes with heavy imbalance under different lighting conditions. While powerful, most of these self-attentive methods for semantic segmentation tend to over-segment ROIs, output noisy and discontinuous predictions, fail to accurately predict the boundary regions, and poorly segment minority classes. Moreover, they tend to yield misclassification in multi-class image segmentation due in part to the imbalance that exists between the semantic classes and the large number of semantic classes. We argue that the short-range context is equally important to predict small ROIs in medical images, as well as to accurately segment ROIs and reduce misclassification of minority classes in images of natural scenes, where the class instances are dense and cluttered.</p><p>To address the above limitations, we propose Masked Supervised Learning (MaskSup), a novel single-stage learning paradigm for semantic segmentation to effectively learn rich and discriminative representations. MaskSup follows a Siamese style network <ref type="bibr" target="#b2">[3]</ref>, where the two branches are identical and share weights. Given an image and its randomly masked version, MaskSup models short-range context among neighboring pixels as the context branch is tasked with predicting the semantic class of masked pixels; thereby leveraging information from non-masked pixels. MaskSup also models global or long-range context by a task similarity constraint, where the similarity of the outputs of the two branches is maximized in order to better learn the shape of class instances, and we find is especially useful in multiclass settings. The main contributions of this work can be summarized as follows:</p><p>? We propose a learning paradigm that aims to model both short-and long-range context via random masking for image segmentation without incurring any additional inference cost.</p><p>? We show through experimental results and ablation studies for binary and multi-class semantic segmentation tasks on three public datasets that MaskSup yields competitive performance in comparison with single and multi-task learning baselines.</p><p>? We demonstrate that MaskSup is robust to large masked corruptions and is computationally efficient, especially in multi-class segmentation as it improves by over 10% mIoU while at the same time requires 3? less learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Single-task semantic segmentation. Most state-of-the-art segmentation methods usually follow an encoder-decoder network structure, where the image is first downsampled by the encoder subnetwork to a latent representation and then the decoder subnetwork is used to semantically project the latent representation into a pixel space for precise localization.</p><p>Convolutional-based methods include FCN <ref type="bibr" target="#b14">[15]</ref> and U-Net <ref type="bibr" target="#b17">[18]</ref>, and their variants such as U-Net++ <ref type="bibr" target="#b29">[30]</ref>, ResU-Net <ref type="bibr" target="#b26">[27]</ref>, ResU-Net++ <ref type="bibr" target="#b8">[9]</ref>. Due to the inability of convolutional-based methods to model long-range context <ref type="bibr" target="#b24">[25]</ref>, self-attention <ref type="bibr" target="#b23">[24]</ref> has become a core building block in various attention-based methods such as Attention U-Net <ref type="bibr" target="#b16">[17]</ref> and Axial Attention U-Net <ref type="bibr" target="#b24">[25]</ref>. To better segment ROIs at boundaries, Selective Feature Aggregation (SFA) <ref type="bibr" target="#b6">[7]</ref> employs area-boundary constraints for polyp segmentation. KiU-Net <ref type="bibr" target="#b21">[22]</ref> leverage overcomplete convolutional architectures to better segment very small ROIs and distinguish between ROI and background accurately. More recently, the advent of Vision Transformers <ref type="bibr" target="#b5">[6]</ref> has accelerated research in the direction of transformer-based segmentation methods, which also build upon self-attention <ref type="bibr" target="#b23">[24]</ref>. These transformer-based methods include MedT <ref type="bibr" target="#b20">[21]</ref> and LeViT-UNet <ref type="bibr" target="#b28">[29]</ref>, which aim to learn long-range context. Our proposed framework differs from previous work in that it captures both short-and long-range context, while learning fewer parameters without compromising performance. In fact, masking enables the base segmentation model to learn short-range context among nearby pixels, as the model is tasked to make a pixel-level prediction for a masked input. This forces the network to leverage information from the nearby pixels in order to make a prediction.</p><p>Multi-task semantic segmentation. Semantic segmentation can be jointly optimized with other visual scene understanding tasks such as depth estimation and edge detection. Hybrid-Net A2 [13] is a multi-task learning method, which employs a hybrid convolutional neural network to jointly tackle the task of image segmentation and depth estimation using a single network. PAD-Net [28] is a multi-task learning and distillation based network, which jointly predicts a segmentation, depth, surface normal and edge map by multi-modal data fusion. This is extended in MTI-Net <ref type="bibr" target="#b22">[23]</ref>, where interactions between segmentation and depth estimation are captured at multiple scales when distilling information based on multi-modal distillation, in which the tasks mutually benefit from each other. Unlike multi-task learning methods, our method does not require additional training data, and hence reduces the need for intense manual labeling of additional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>We consider the problem of learning an encoder-decoder network f ? that classifies each pixel of an image I into its semantic class category. The output is an image M p = f ? (I) of the same size as the input image. A gland segmentation task, for example, can be thought of as a binary segmentation problem with two semantic classes: gland and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Masked Supervised Learning</head><p>We present a masked supervised learning framework for effectively learning rich and discriminative representations for semantic segmentation. The proposed MaskSup method follows a Siamese network <ref type="bibr" target="#b2">[3]</ref> style architecture, in which the segmentation branch (SB) and context branch (CB) are identical and share weights. Given an image I and its randomly masked version I masked , we first employ a base segmentation network f ? to output the predictions M p and M pm , respectively, followed by computing an overall loss function. We use a loss function L context to learn short-range context among neighboring pixels, as this branch predicts the semantic class of masked pixels by leveraging pixel information from the non-masked parts in I masked . We also use a loss term L tasksim to learn long-range context, as the similarity of the two outputs M p and M pm is maximized, and hence enables us to better learn the shape of the semantic classes. Overall, MaskSup enables better representation learning for semantic segmentation by capturing the contextual relationships between pixels by predicting a segmentation map for the masked version of the input. MaskSup can tackle cases where the ROIs are ambiguous at boundary, various scales, shape, appearance and also for images that have multiple class instances with imbalance, resulting in fewer pixel-level misclassification of minority classes.</p><p>After training, we employ the network f ? to infer a new image I that outputs a prediction M p . It is important to mention that at test time the images are not masked, and random masking is only used during training. The overall framework is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Context Branch. The goal of the context branch (CB) is to learn short-range context among nearby pixels as this branch outputs predictions for masked pixels by leveraging information from the non-masked pixels in I masked . We take inspiration from the idea of image inpainting, which refers to the task of filling holes in an image, and is commonly used in image editing in order to remove image content such as text in videos or objects <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>. We follow the masking procedure in <ref type="bibr" target="#b13">[14]</ref> to construct a masked image I masked from I using masks of random streaks and holes of arbitrary shapes</p><formula xml:id="formula_0">I masked = I M holes (1)</formula><p>where denotes element-wise multiplication and M holes is a binary mask of random streaks and holes of arbitrary shapes. Intuitively, I masked has a similar layout as I, but randomly removes (i.e. pixel values set to 0) roughly over 50% of the pixels in the image. Given an input image I and the masked image I masked , we train an encoder-decoder network f ? to predict the output of the segmentation branch M p and the context branch (CB) M pm . Note that f ? is a Siamese network <ref type="bibr" target="#b2">[3]</ref> like architecture where the branches are identical and share weights. In most of our experiments, f ? is either a LeViT-UNet-384 <ref type="bibr" target="#b28">[29]</ref> for gland and polyp segmentation or U-Net++ <ref type="bibr" target="#b29">[30]</ref> for indoor scene segmentation. We train f ? by minimizing the cross-entropy loss of both the segmentation branch and the context branch for all samples in the training set. The context branch loss is given by</p><formula xml:id="formula_1">L CB = L seg (M p , M gt ) + L context (M pm , M gt )<label>(2)</label></formula><p>where L seg and L context are cross-entropy losses between the output and ground truth. It is worth mentioning that other application-specific loss functions such as the focal Tversky loss <ref type="bibr" target="#b0">[1]</ref> and distance-based losses <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref> can also be used.</p><p>Task Similarity Constraint. The goal of the task similarity constraint is to model longrange context by maximizing the similarity between the output from the segmentation branch and the output from context branch in order to predict the semantic classes of masked pixels; thereby enabling us to better learn the shape of the class instances (e.g. gland, polyp, wall etc.). More specifically, we aim to maximize the similarity between the predictions made by the segmentation branch output M p and the context branch output M pm by minimizing the L 2 error L tasksim = M p ? M pm 2 . Therefore, the overall loss function of MaskSup is a weighted sum of the segmentation, context and task similarity loss terms</p><formula xml:id="formula_2">L total = ? 1 L seg (M p , M gt ) + ? 2 L context (M pm , M gt ) + ? 3 L tasksim (M p , M pm )<label>(3)</label></formula><p>where ? 1 , ? 2 and ? 3 are nonnegative regularization hyper-parameters, which control the contribution of each loss term. In our experiments, we empirically set them to 1.</p><p>During training, the total loss L total is minimized for several epochs to learn the parameters of f ? using a labeled training set D = {(I 1 , M 1 ), . . . , (I n , M n )}, where M i is the ground truth segmentation mask of the input image I i . During testing, the network f ? is used for semantic segmentation, outputting a segmentation mask prediction M p given an input image I. The architecture and the different loss terms of MaskSup are illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present our experimental setup and results in comparison with competing single and multi-task learning baselines for semantic segmentation. Details on datasets, implementation, architecture, training, and additional results are included in the supplementary material. Code is available at: https://github.com/hasibzunair/masksup-segmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We demonstrate and analyze the performance of our method on Gland Segmentation (GLaS) <ref type="bibr" target="#b18">[19]</ref>, Kvasir <ref type="bibr" target="#b9">[10]</ref> &amp; CVC-ClinicDB <ref type="bibr" target="#b1">[2]</ref> and NYUDv2 <ref type="bibr" target="#b15">[16]</ref> datasets. While GLaS and Kvasir &amp; CVC-ClinicDB are for medical image segmentation tasks, NYUDv2 is for indoor scene segmentation tasks. These datasets cover a wide range of challenges in semantic segmentation, and they represent both binary and multi-class segmentation. They also cover both natural and medical image modalities. In medical images, ROIs are usually very small compared to background. In addition, these datasets have their own challenges such as variation in appearance, scale, ambiguous ROIs, and many class instances with imbalance that are densely cluttered.</p><p>Baselines. We evaluate the performance of our method against several state-of-the-art convolutional-based methods including FCN <ref type="bibr" target="#b14">[15]</ref>, U-Net <ref type="bibr" target="#b17">[18]</ref>, U-Net++ <ref type="bibr" target="#b29">[30]</ref>, ResU-Net <ref type="bibr" target="#b26">[27]</ref>, ResU-Net++ <ref type="bibr" target="#b8">[9]</ref>, SFA <ref type="bibr" target="#b6">[7]</ref>, KiU-Net <ref type="bibr" target="#b21">[22]</ref> and attention-based methods such as Attention U-Net <ref type="bibr" target="#b16">[17]</ref>, Axial Attention U-Net <ref type="bibr" target="#b24">[25]</ref>. We also compare with more recent transformerbased methods MedT <ref type="bibr" target="#b20">[21]</ref> and LeViT-UNet <ref type="bibr" target="#b28">[29]</ref>, and multi-task learning methods Hybrid-Net A2 <ref type="bibr" target="#b12">[13]</ref> and PAD-Net <ref type="bibr" target="#b27">[28]</ref> and MTI-Net <ref type="bibr" target="#b22">[23]</ref> with HRNet-18 <ref type="bibr" target="#b25">[26]</ref> as backbone.</p><p>Evaluation Metric. We report results using the Mean Intersection-Over-Union (mIoU), which is a commonly used metric in semantic segmentation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. The values of mIoU range from 0 to 1, with 1 indicating perfect match between the true and predicted labels, while 0 indicates a complete mismatch between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Comparison with State-Of-The-Art. We compare the performance of MaskSup against several state-of-the-art methods and report the results in <ref type="table">Table 1</ref>. All mIoU scores are averaged over 3 runs. As can be seen, MaskSup consistently outperforms all baselines, achieving relative improvements of 1.26%, 3.45% and 4.85% over the strongest baseline in terms of mIoU on GLaS, Kvasir &amp; CVC-ClinicDB and NYUDv2 datasets, respectively.</p><p>MaskSup yields significant relative improvements of 18.7% over Axial Attention U-Net <ref type="bibr" target="#b24">[25]</ref> and 2.8% over KiU-Net <ref type="bibr" target="#b21">[22]</ref> on GLaS. MaskSup also outperforms transformerbased methods such as MedT <ref type="bibr" target="#b20">[21]</ref> and LeViT-UNets <ref type="bibr" target="#b28">[29]</ref> with relative improvements of 1.26% and 3.45% over LeViT-UNet-384 <ref type="bibr" target="#b28">[29]</ref> on GLaS and Kvasir &amp; CVC-ClinicDB, respectively. MaskSup performs better than multi-task learning methods PAD-Net <ref type="bibr" target="#b27">[28]</ref> and HybridNet A2 <ref type="bibr" target="#b12">[13]</ref> with relative improvements of 18.76% and 14.6%. In addition, MaskSup outperforms MTI-Net <ref type="bibr" target="#b22">[23]</ref> with a relative improvement of 4.85% on NYUDv2. This improvement is significant because MTI-Net [23] is a multi-task learning method that jointly learns four different tasks (i.e. semantic segmentation, depth estimation, edge detection and surface normal estimation), and hence requires additional annotated data for training. In contrast, MaskSup only requires images and the pixel level annotations (i.e. segmentation masks); thereby reducing the need for intense manual labeling of additional data. The results demonstrate the effectiveness and capability of MaskSup in modeling short-and long-range context, yielding improved segmentation.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, we report the performance comparison results of MaskSup and masked autoencoders (MAE) <ref type="bibr" target="#b7">[8]</ref>. For MAE, we pre-train for 800 epochs on the images and fine-tune for 50 epochs on images and labels, while MaskSup only requires a single stage of training for 200 epochs. MAE uses patch-based masking to predict visual tokens similar to image inpainting, whereas MaskSup outputs a prediction label and not the full inpainted image.</p><p>Qualitative Results. In <ref type="figure" target="#fig_3">Figures 3 and 4</ref>, we visually compare MaskSup predictions against the baselines U-Net <ref type="bibr" target="#b17">[18]</ref>, U-Net++ <ref type="bibr" target="#b29">[30]</ref> and LeViT-UNets <ref type="bibr" target="#b28">[29]</ref> on GLaS, Kvasir &amp; CVC-ClinicDB and NYUDv2. In the first row of <ref type="figure" target="#fig_3">Figure 3</ref>, when there is a change in the overall shape and appearance of the glands, the baseline methods tend to over-segment the regions <ref type="table">Table 1</ref>: Performance comparison of MaskSup and baselines on GLaS, Kvasir &amp; CVC-ClinicDB and NYUDv2 test sets using mIoU. Boldface numbers indicate the best performance, whereas the best baselines are underlined. indicates a multi-task learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>GLaS, mIoU (?) CVC-Clinic-DB, mIoU (?) NYUDv2 (?) U-Net <ref type="bibr" target="#b17">[18]</ref> 67.41 69.74 33.60 FCN <ref type="bibr" target="#b14">[15]</ref> 50.84 -29.20 U-Net++ <ref type="bibr" target="#b29">[30]</ref> 69.10 72.90 34.74 HRNet-18 <ref type="bibr" target="#b25">[26]</ref> --33.18 ResU-Net <ref type="bibr" target="#b26">[27]</ref> 65.95 --ResU-Net++ <ref type="bibr" target="#b8">[9]</ref> -79.60 -SFA <ref type="bibr" target="#b6">[7]</ref> -60.70 -Attention U-Net <ref type="bibr" target="#b16">[17]</ref> -82.70 -Axial Attention U-Net <ref type="bibr" target="#b24">[25]</ref> 63.03 --MedT <ref type="bibr" target="#b20">[21]</ref> 69.61 --KiU-Net <ref type="bibr" target="#b21">[22]</ref> 72.78 --LeViT-UNet-128 <ref type="bibr" target="#b28">[29]</ref> 70.45 --LeViT-UNet-192 <ref type="bibr" target="#b28">[29]</ref> 71.83 79.16 -LeViT-UNet-384 <ref type="bibr" target="#b28">[29]</ref> 73  and also produce noisy outputs as they fail to capture the global structure and semantics of the glands. The second row of <ref type="figure" target="#fig_3">Figure 3</ref> shows the case of ambiguous ROIs of polyps, where the baselines fail to accurately segment the ROI. This is largely attributed to the limited capability of the learned representations used in the baselines. Interestingly the LeViT-UNets baseline fail to segment ROIs that are ambiguous at boundaries and vary in size and color, albeit transformers are quite strong in modeling long-range context <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>. Self-attention can be regarded as a form of the non-local means <ref type="bibr" target="#b3">[4]</ref>, and it captures long-range dependencies, resulting in over-segmentation as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. <ref type="figure" target="#fig_5">Figure 4</ref> shows that the baselines fail to accurately segment multiple class instances, output discontinuous predictions and misclassify instances. In the last row of <ref type="figure" target="#fig_5">Figure 4</ref>, we can see that the baselines fail to segment the minority class (i.e. pillow). Overall, the baselines fail to capture context of target regions, resulting in over-segmentation, noisy and discontinuous predictions as well as misclassification of instances, leading to unsatisfactory predictions.</p><p>By comparison, MaskSup is able to better capture the shape and appearance of instances due, in large part, to the context branch, which models short-range context among pixels, resulting in better representation learning. Moreover, the task similarity constraint leads to long-range context invariance, enabling MaskSup to better learn the shape of the ROI, which in turn translates into better output predictions (see <ref type="figure" target="#fig_7">Figure 6</ref> for more comparative results). Overall, learning with the context branch and task similarity constraint helps in cases of segmenting ambiguous ROIs at varying size and color, and also better segment minority class instances in cases of multi-class segmentation.    MaskSup is able to output better predictions for minority classes (e.g. pillows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>Effectiveness of Context Branch. <ref type="figure" target="#fig_6">Figure 5</ref> illustrates the benefit of using the the context branch on both convolutional and transformer-based methods. Using the the context branch leads to better modeling of local semantics, as it is tasked to output pixel-wise predictions for masked regions in the input; thereby leveraging information from neighboring pixels. We can see that the context branch improves performance of different segmentation methods across the three datasets. This shows that MaskSup is generic and can be easily integrated into existing image segmentation methods. However, it is important to mention that a higher performance improvement is observed when the architecture is a transformer-based method due to its key characteristic of modeling long-range context <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Effectiveness of Task Similarity Constraint. <ref type="figure" target="#fig_6">Figure 5</ref> shows the benefit of using the task similarity constraint. We observe that the task similarity constraint further improves performance of both convolutional and transformer-based methods. The use of the task similarity constraint results in learning long-range context invariant representations, which help capture the shape of the ROI. This, in turn, leads to accurate predictions even in cases of different shapes and appearance of instances, ambiguous ROIs at different sizes, and imbalance among multiple class instances in multi-class segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U-Net</head><p>LeViT  Amount of Masking. We performed an ablation study of high-and low-masked pixels regions during MaskSup training, and the results are reported in <ref type="table" target="#tab_3">Table 3</ref>, which shows that masking the images heavily during training yields better performance of MaskSup across all three datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Robustness to Masked Corruptions. <ref type="figure" target="#fig_7">Figure 6</ref> shows the robustness of MaskSup to masked corruptions. As can be seen, MaskSup is able to better predict the ROI even when a large portion of the image is masked, demonstrating its capability in modeling short-and longrange context. Using both the context branch and task similarity constraint, MaskSup is able to learn context invariant representations to better segment and preserve the ROI shape.</p><p>Computational Efficiency. In <ref type="table" target="#tab_4">Table 4</ref>, we report the number of parameters in millions (M), as well as mIoU for MaskSup and baseline methods. MaskSup with LeViT-192 network outperforms LeViT-384 <ref type="bibr" target="#b28">[29]</ref> on both GLaS and Kvasir &amp; CVC-ClinicDB, while having almost 2.6? fewer learnable parameters. MaskSup with U-Net also outperforms U-Net++ <ref type="bibr" target="#b29">[30]</ref>, which has almost 3? more learnable parameters on NYUDv2, with a relative improvement of 10.91% in terms of mIoU. Hence, there is no trade-off between segmentation accuracy and computational efficiency when using MaskSup in comparison with scaled versions of the networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Masked Image Baseline MaskSup Ground Truth  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced a new learning paradigm, called Masked Supervised Learning, for semantic segmentation. By constructing a randomly masked version of the input image, we first make predictions using a base segmentation network on the two inputs. Then, we maximize the predictions between the two outputs to model both short-and long-range context. MaskSup can be easily integrated into any existing semantic segmentation method. We show that MaskSup achieves better performance than state-of-the-art single and multi-task learning baselines in both binary and multi-class semantic segmentation tasks, especially in tackling small, ambiguous regions and minority class instances. In addition, MaskSup is robust to masked corruptions and is computationally efficient without compromising performance.</p><p>For future work, we aim to investigate what type of masking strategies works best in MaskSup. Since MaskSup is a generic paradigm, we plan to adapt it to other computer vision tasks such as multi-label recognition, object detection and human pose estimation. We also plan to explore high-resolution segmentation (e.g. Cityscapes, ADE20K datasets) using MaskSup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples demonstrating challenges in semantic segmentation. Left to right: First two from GLaS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of MaskSup training: joint prediction architecture with context branch and task similarity constraint for semantic segmentation, where f ? is a base segmentation network. The segmentation and context branches are identical and share weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visual comparison of MaskSup and baselines on the GLaS and Kvasir &amp; CVC-ClinicDB test sets. MaskSup outputs better predictions in cases of variation in overall appearance and also very small and ambiguous ROIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparison of MaskSup and baselines on the NYU Depth V2 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Ablation study of different modules of MaskSup on GLaS, Kvasir &amp; CVC-ClinicDB and NYU Depth V2 test sets. MaskSup (CB &amp; TS) consistently improves performance of various baselines in both binary and multi-class image segmentation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Visual comparison of predictions made by MaskSup and baseline for images with masked regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of MaskSup and MAE. MaskSup is efficient and achieves better performance.</figDesc><table><row><cell>Method</cell><cell cols="3">GLaS, mIoU (?) CVC-Clinic-DB, mIoU (?) NYUDv2 (?)</cell></row><row><cell>MAE [8]</cell><cell>75.04</cell><cell>82.50</cell><cell>37.42</cell></row><row><cell>MaskSup (Ours)</cell><cell>76.06</cell><cell>84.02</cell><cell>39.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of high-and low-masked pixels regions during MaskSup training.</figDesc><table><row><cell cols="4">Masking GLaS, mIoU (?) CVC-Clinic-DB, mIoU (?) NYUDv2 (?)</cell></row><row><cell>Low</cell><cell>75.65</cell><cell>81.80</cell><cell>35.33</cell></row><row><cell>High</cell><cell>76.06</cell><cell>84.02</cell><cell>39.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of MaskSup and baselines on GLaS, Kvasir &amp; CVC-ClinicDB and NYUDv2 test sets. MaskSup is computationally efficient and achieves superior performance with fewer parameters. Boldface numbers indicate better performance.</figDesc><table><row><cell>Method</cell><cell cols="4">Params (M) (?) GLaS, mIoU (?) CVC-Clinic-DB, mIoU (?) NYUDv2 (?)</cell></row><row><cell>LeViT-384 [29]</cell><cell>51</cell><cell>73.88</cell><cell>81.38</cell><cell>-</cell></row><row><cell>MaskSup (LeViT-192)</cell><cell>19(2.6x)</cell><cell>74.44(+0.75)</cell><cell>82.17(+0.97)</cell><cell>-</cell></row><row><cell>U-Net++ [30]</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>34.74</cell></row><row><cell>MaskSup (U-Net)</cell><cell>3(3x)</cell><cell>-</cell><cell>-</cell><cell>38.54(+10.91)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel focal Tversky loss function with improved attention U-Net for lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabila</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naimul Mefraz</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Symposium on Biomedical Imaging</title>
		<meeting>IEEE International Symposium on Biomedical Imaging</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="683" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gloria</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debora</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Vilari?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Distance map loss penalty term for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Caliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Iriondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><forename type="middle">Morales</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharmila</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Pedoia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03679</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selective feature aggregation network with area-boundary constraints for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Yu</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ResUNet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">De</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?vard D</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Symposium on Multimedia</title>
		<meeting>IEEE International Symposium on Multimedia</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Thomas De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?vard D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reducing the hausdorff distance in medical image segmentation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davood</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Septimiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salcudean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="499" to="513" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multiview RGB-D object dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Robotics and Automation</title>
		<meeting>IEEE Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth estimation and semantic segmentation from a single RGB image using a hybrid convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalila</forename><surname>S?nchez-Escobedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pard?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1795</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><forename type="middle">Le</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunari</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensaku</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention U-Net: Learning where to look for the pancreas</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: The GLAS challenge contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korsuk</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Josien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><forename type="middle">Bo</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Yang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urko</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Resolution-robust large mask inpainting with fourier convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Suvorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizaveta</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Mashikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Remizova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Silvestrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naejin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshith</forename><surname>Goka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwoong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2149" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Medical transformer: Gated axial-attention for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeya Maria Jose</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poojan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilker</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">KiU-Net: Overcomplete convolutional architectures for biomedical image and volumetric segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeya Maria Jose</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilker</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MTI-Net: Multi-scale task interaction networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="527" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Axial-DeepLab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weighted Res-UNet for highquality retina vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Information Technology in Medicine and Education</title>
		<meeting>Information Technology in Medicine and Education</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PAD-Net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">LeViT-UNet: Make faster encoders with transformer for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08623</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">UNet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Mahfuzur Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sharp U-Net: Depthwise convolutional network for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasib</forename><surname>Zunair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page">104699</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
