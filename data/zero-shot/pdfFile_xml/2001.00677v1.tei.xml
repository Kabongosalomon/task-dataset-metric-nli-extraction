<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMPROVE UNSUPERVISED DOMAIN ADAPTATION WITH MIXUP TRAINING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxiang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lincan</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">MI ? Bosch Research North America</orgName>
								<address>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IMPROVE UNSUPERVISED DOMAIN ADAPTATION WITH MIXUP TRAINING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Unsupervised domain adaptation</term>
					<term>mixup</term>
					<term>image classification</term>
					<term>human activity recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation studies the problem of utilizing a relevant source domain with abundant labels to build predictive modeling for an unannotated target domain. Recent work observe that the popular adversarial approach of learning domain-invariant features is insufficient to achieve desirable target domain performance and thus introduce additional training constraints, e.g. cluster assumption. However, these approaches impose the constraints on source and target domains individually, ignoring the important interplay between them. In this work, we propose to enforce training constraints across domains using mixup formulation to directly address the generalization performance for target data. In order to tackle potentially huge domain discrepancy, we further propose a feature-level consistency regularizer to facilitate the inter-domain constraint. When adding intra-domain mixup and domain adversarial learning, our general framework significantly improves state-of-the-art performance on several important tasks from both image classification and human activity recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Despite the success of deep learning based approaches in visual understanding and time-series analysis, they typically rely on abundant data and extensive human labeling. During deployment in real-world scenarios, they often face critical challenges when domain shifts occur and the labels under novel distributions are scarce or unavailable. It is crucial to develop highly effective domain adaptation scheme to transfer existing model trained on large-scale labeled data (source domain) to the related domain (target domain). In this work, we address the challenging unsupervised domain adaptation (UDA) problem, where the target domain is completely unannotated.</p><p>A popular approach in UDA is learning indistinguishable representations between source and target domains through adversarial training. For example, the seminal DANN framework <ref type="bibr" target="#b0">[1]</ref> demonstrates that training with a domain discriminator Work done as an intern at Bosch Research North America. directly minimizes the H-divergence between domains <ref type="bibr" target="#b1">[2]</ref> and hence induces domain-invariant representations. While several other existing work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> differ on the network and training paradigms, the overarching assumption is: when the domain discrepancy is addressed at the representation level, the trained source classifier is able to automatically achieve good performance on the target domain. However, recent research suggest that a classifier that performs well on both domains may be non-existent <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and under this circumstance, solely relying on source classifier can lead to significant misclassifications in the target domain. This challenge motivates state-of-the-art approaches to seek additional training constraints during the adversarial learning process. VADA <ref type="bibr" target="#b5">[6]</ref> is the first approach proposed to minimize the conditional entropy of target domain, based on the cluster assumption commonly utilized in semisupervised learning. On each domain individually, VADA further adds virtual adversarial training (VAT). In JDDA <ref type="bibr" target="#b6">[7]</ref>, the authors propose metric-learning style losses on the source domain. By learning more compact and separable source features, it indirectly encourages more discriminative target domain features through domain alignment.</p><p>We observe that the aforementioned studies employ training constraints in the chosen domain(s) independently, not jointly. This leaves the important interplay between the two domains unexplored and may significantly limit the potential of the training constraints. In this work, through the lens of the simple yet effective mixup training <ref type="bibr" target="#b7">[8]</ref>, we demonstrate that introducing training constraints across domains can significantly improve the model adaptation performance. Denoting a pair of sample-label tuples as (x i , y i ), (x j , y j ), mixup generates augmented tuples as <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_0">x = ?x i + (1 ? ?)x j y = ?y i + (1 ? ?)y j</formula><p>where ? ? [0, 1]. By using the constructed (x , y ) for training, mixup encourages linear behavior of the model where linear interpolation in the raw data leads to linear interpolation of predictions.</p><p>Inspired by the recent advances in semi-supervised learning <ref type="bibr" target="#b8">[9]</ref>, we achieve mixup across domains through inferred labels on the target data. In this way, as opposed to training the classifier only with source labels, we are able to provide  additional supervision also with interpolated (virtual) labels between domains. As the mixup training and domain adversarial training both progress, the model infers virtual labels with increased accuracy. This procedure can be critical to directly improve the generalization of the classifier when applied to target domain. Furthermore, to effectively enforce the linearity constraint under very large domain discrepancy, we develop a feature-level consistency regularizer to better facilitate the mixup training. Besides the inter-domain constraints, mixup can also be applied within each domain. The interand intra-domain mixup training constitute the proposed IIMT framework for enforcing multifaceted constraints to improve target domain performance. Our extensive evaluation on both visual recognition and human activity recognition demonstrate that IIMT significantly outperforms state-of-the-art methods on several important tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED APPROACH</head><p>The overview of IIMT framework is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. We denote the labeled source domain as set</p><formula xml:id="formula_1">{(x s i , y s i )} ms i=1 ? S and unlabeled target domain as set {x t i } mt i=1 ? T .</formula><p>Here y i denotes one-hot labels. The overall classification model is denoted as h ? : S ? C with the parameterization by ?. Following prominent approaches in UDA <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, we consider the classification model as the composite of an embedding encoder f ? and an embedding classifier g ? : h = f ?g. Note that encoder is shared by the two domains. The core component in our framework is mixup, imposed both across domains (Inter-domain in <ref type="figure" target="#fig_1">Figure  1</ref>) and within each domain (Intra-domain (source) and Intradomain (target) in <ref type="figure" target="#fig_1">Figure 1</ref>). All mixup training losses and the domain adversarial loss are trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Inter-domain Mixup Training</head><p>The key component in our proposed framework is the mixup training between source and target domains. This can be achieved after the target domain labels are inferred based on current classifier. In the training of h, mixup provides interpolated labels to enforce linear prediction behavior of the classifier across domains. Compared with training with source labels alone, this induces a simplistic inductive bias that can directly improve the generalization capability of the classifier for the target domain.</p><p>Mixup training requires sample labels in order to perform interpolation. We utilizes inferred labels as weak supervision for the target domain. Similar ideas have been shown to be highly effective in exploiting relevant unlabeled data in semisupervised learning setting <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref>. We follow <ref type="bibr" target="#b8">[9]</ref> by integrating data augmentation and entropy reduction into the virtual label generation process. First, we perform K task-dependent stochastic augmentations (e.g. random crop and flipping for images and random scaling for time series) on each data sample to obtain transformed samples {x i,k } K k=1 . Then, the target domain virtual labels are calculated as:</p><formula xml:id="formula_2">q i = 1 K K k=1 h ? (x i,k )</formula><p>, and further normalized as:</p><formula xml:id="formula_3">q i =q 1 T i / cq 1 T i,c .</formula><p>Here T denotes the softmax temperature and c is the class index. That is, the class predictions over the K augmented inputs are first averaged to constituteq i and further sharpened to form the virtual labels q i . Using smaller value T &lt; 1 produces sharper predicted distributions and helps to reduce the conditional entropy when q i is used in training.</p><p>Given a pair of source and target samples, (x s i , x t i ), taken from their corresponding batches, label-level mixup to enforce the linearity consistency across domains can be defined as:</p><formula xml:id="formula_4">x st i = ? x s i + (1 ? ? )x t i (1) q st i = ? y s i + (1 ? ? )q t i (2) L q = 1 B i H(q st i , h ? (x st i ))<label>(3)</label></formula><p>where B denotes the batch size, H denotes the cross-entropy loss and the mixup weighting parameter is selected according to: ? ? Beta(?, ?) and ? = max(?, 1 ? ?). Here Beta refers to beta distribution with the shared shape parameter ?. When ? is set to closer to 1, there is a larger probability to choose medium value of ? from range [0, 1], leading to higher level of interpolation between the two domains. Note that ? is always over 0.5 to ensure the source domain is dominant. Similarly the mixup dominated by the target domain can be generated via switching x s and x t in Eq. (1), which forms (x ts , q ts ).</p><p>With (x ts , q ts ) we utilize the mean square error (MSE) loss as it is more tolerant to false virtual labels in the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Consistency Regularizer</head><p>In the scenario of very large domain discrepancy, the linearity constraint imposed by inter-domain mixup could be less effective. Specifically, when the heterogeneous raw inputs are interpolated in Eq. (1), forcing the model h to produce correspondingly interpolated predictions becomes significantly harder. At the same time, the joint training with domain adversarial loss (details in next section) for feature-level domain confusion can add to the training difficulty.</p><p>These challenges motivate us to design a consistency regularizer for the latent features to better facilitate inter-domain mixup training. Denoting Z as the embedding space induced by f and z ? Z, we define the regularizer term as:</p><formula xml:id="formula_5">z st i = ? f ? (x s i ) + (1 ? ? )f ? (x t i ) (4) L z = 1 B i z st i ? f ? (x st i ) 2 2<label>(5)</label></formula><p>where x st i is from Eq. (1). That is, we push the mixed feature closer to the feature of the mixed input through MSE loss between the two vectors. In this way, we impose the linearity constraint to be enforced also at the feature level. The efficacy of this regularizer is that when Eq. (5) is enforced and z st i , f ? (x st i ) are passed through the shallow classifier g, the linearity in the model prediction becomes much easier to be satisfied. Note that similar to the handling of Eq. (1), x s i and x t i can also be switched in Eq. (4) to form z ts i . We omitted the loss term on that for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Domain Adversarial Training</head><p>Our last component in inter-domain mixup is employing standard domain adversarial training to reduce the domain discrepancy. Here we restrict our implementation to the more fundamental DANN framework <ref type="bibr" target="#b0">[1]</ref>, as an attempt to focus on evaluating the mixup linearity constraints. In DANN, a domain discriminator and the shared embedding encoder (generator) are trained under the adversarial objective such that the encoder learns to generate domain-invariant features. Denote the domain discriminator as D : Z ? (0, 1), where 0/1 annotates binary domain label. We define the domain adversarial loss on the mixed source and target samples:</p><formula xml:id="formula_6">L d = 1 B i ln D(f ? (x st i )) + ln(1 ? D(f ? (x st i )))<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Intra-domain Mixup Training</head><p>Given the source labels and target virtual labels, mixup training can be performed within each domain too. Since samples in the same domain follow the similar distribution, there is no need to apply feature-level linearity. We therefore employ only label-level mixup training for both domains and define their corresponding losses:</p><formula xml:id="formula_7">x s i = ? x s i + (1 ? ? )x s j y s i = ? y s i + (1 ? ? )y s j L s = 1 B i H(y s i , h ? (x s i ))<label>(7)</label></formula><p>x</p><formula xml:id="formula_8">t i = ? x t i + (1 ? ? )x t j q t i = ? q t i + (1 ? ? )q t j L t = 1 B i q t i ? h ? (x t i ) 2 2<label>(8)</label></formula><p>Although within-domain mixup is intuitive as a data augmentation strategy, it is particularly useful for UDA. As discussed in <ref type="bibr" target="#b5">[6]</ref>, the minimization of conditional entropy without locally-Lipschitz constraints can results in abrupt prediction changes in the vicinity of data samples. In <ref type="bibr" target="#b5">[6]</ref>, virtual adversarial training <ref type="bibr" target="#b9">[10]</ref> is utilized to enforce prediction smoothness in neighborhood. Differently, we find that intra-domain mixup training is able to achieve the same objective. We provide more empirical details in the experiments to demonstrate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Overall Architecture</head><p>We illustrate the overall architecture in <ref type="figure" target="#fig_1">Figure 1</ref>. Summarizing all previous components, we arrive at the final loss:</p><formula xml:id="formula_9">L = w q L q + w d L d + w z L z + w s L s + w t L t<label>(9)</label></formula><p>Since L t only involves virtual labels, it could be easily affected by the uncertainty in target domain. We set a linear schedule for w t in training, from 0 to a predefined maximum value. From initial experiments, we observe that the algorithm is robust to other weighting parameters. Therefore we only search w t while simply fixing all other weightings to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL RESULTS</head><p>Our evaluation consists of image classification and human activity recognition (HAR). For the first part, we consider visual domain adaptation experiments commonly used in the UDA literature. They are constructed on MNIST, MNIST-M, Street View House Numbers (SVHN), Synthetic Digits (SYN DIGITS), CIFAR-10 and STL-10. For HAR experiments, we evaluate on OPPORTUNITY <ref type="bibr" target="#b10">[11]</ref> and WiFi <ref type="bibr" target="#b11">[12]</ref> datasets. We use A ? B to denote the domain adaptation task with source domain A and target domain B. The architecture of the embedding encoder f is taskspecific: we use similar architectures to <ref type="bibr" target="#b5">[6]</ref> for visual recognition and WiFi datasets; we employ state-of-the-art Deep-ConvLSTM architecture <ref type="bibr" target="#b10">[11]</ref> for OPPORTUNITY. For the domain discriminator D, we forward the extracted feature to a simple fully-connected layer (x ? 128 ? ReLU ? 1). We adopt shallow fully-connected networks for embedding classifier g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation on Visual Recognition</head><p>Our benchmarking results on common visual domain adaptation tasks are summarize in <ref type="table" target="#tab_0">Table 1</ref>. For digits classification UDA tasks, our proposed approach outperforms the state-ofthe-art results achieved by DIRT-T for MNIST ? MNISTM and SYN DIGITS ? SVHN. Note that DIRT-T accuracy is close to 100% and further improvement is very valuable. For SVHN ? MNIST, our performance is on par with VADA while only scoring lower than DIRT-T.</p><p>For object recognition UDA tasks CIFAR ?STL, the two adaptation directions have different degrees of difficulty. Since  <ref type="table">Table 2</ref>. Test set weighted F1 score on OPPORTUNITY.</p><p>STL is much smaller, a model trained on it without any adaptation performs badly on the target domain. For both directions, our algorithm significantly outperforms state-of-the-art methods: in CIFAR ? STL, we achieve 3.1% margin-ofimprovement; for STL ? CIFAR, we achieve 8.1% and 6.3% margin-of-improvement on VADA and DIRT-T respectively. Note that DIRT-T had a remarkable improvement of 11% over ?-model for STL ? CIFAR. The capability to achieve significant further improvement demonstrates that, the proposed approach is highly effective in exploiting the target domain unlabeled data, even when source domain labels are limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation on Human Activity Recognition</head><p>In building HAR system for fitness monitoring and assisted living <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, acquiring training data requires careful system setup, long-term human subjects involvement and laborious labeling efforts. Since repeating such procedure for new sensing environment is prohibitive in practice, UDA becomes crucial in the deployment of practical HAR systems. We conduct experiments on this problem with both IMU sensor and WiFi based HAR datasets. Note that domain refers to human subject in the first application and corresponds to sensing room in the second problem. Sensor-based HAR is performed on the public OPPORTU-NITY repository containing sensor signals of sporadic gestures. Based on the first 3 subjects, we construct 6 cross-subject UDA experiments to explicitly investigate the influence of user (domain) change to the classifier accuracy. The sensor signals are segmented using a 0.8s moving window with half overlapping. Due to the imbalanced class distribution (caused by the null class), we use the weighted F1 score as the evaluation metric. The experimental results are summarized in <ref type="table">Table 2</ref>. First, we observe that without domain adaptation, all classifiers have   inferior performance on the target subject, thus evidencing the necessity of UDA in HAR. Second, possibly due to the high class imbalance, VADA performance is only slightly better than conventional adversarial training. On the other hand, our proposed method significantly outperforms both approaches, achieving an averaged improvement over 0.04.</p><p>We conduct experiments on the same WiFi activity recognition dataset <ref type="bibr" target="#b11">[12]</ref> used in <ref type="bibr" target="#b5">[6]</ref>. In here the CSI from commercial WiFi systems was collected in 2 different rooms. We conduct the adaptation experiment from Room A to B in recognizing the 7 physical activities such as walk, fall, pickup etc. The 90-dimensional CSI time series are temporally segmented with a 1s moving window and 0.2s stride. As can be seen from the experimental results in <ref type="table" target="#tab_2">Table 3</ref>, our approach significantly outperforms VADA/DIRT-T.</p><p>Finally, we perform an ablation analysis in order to study the contribution of each mixup component and the results are listed in <ref type="table" target="#tab_3">Table 4</ref>. We observe that each proposed component is effective in achieving performance gain. Importantly, the performance with intra-domain mixup training is already slightly better than VADA (see <ref type="table" target="#tab_0">Table 1</ref> and 2). This demonstrates that intra-domain mixup is as effective as VAT <ref type="bibr" target="#b5">[6]</ref> in enforcing the locally-Lipschitz constraint. While <ref type="bibr" target="#b5">[6]</ref> utilizes VAT on each domain, we additionally perform cross-domain mixup. The two inter-domain components in <ref type="table" target="#tab_3">Table 4</ref> gain collective improvement of 6% for STL ? CIFAR and 0.02 for 1 ? 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>This paper studies imposing cross-domain training constraints for domain adaptation through the lens of mixup linearity. A consistency regularizer is proposed to facilitate inter-domain mixup training in the presence of large domain discrepancy. The general IIMT framework incorporating both inter-and intra-domain mixup training outperforms state-of-the-art methods in diverse application areas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the proposed architecture highlighting mixup training. Note z s and z t come from the output of f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Test set accuracy (%) on visual UDA benchmarks.</figDesc><table><row><cell>Source</cell><cell>MNIST</cell><cell>SVHN</cell><cell>SYN</cell><cell>CIFAR</cell><cell></cell><cell>STL</cell></row><row><cell>Target</cell><cell cols="3">MNISTM MNIST SVHN</cell><cell>STL</cell><cell cols="2">CIFAR</cell></row><row><cell>DAN [13]</cell><cell>76.9</cell><cell>71.1</cell><cell>88.0</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>DANN [1]</cell><cell>81.5</cell><cell>71.1</cell><cell>90.3</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>DRCN [14]</cell><cell>-</cell><cell>82.0</cell><cell>-</cell><cell>66.4</cell><cell></cell><cell>58.7</cell></row><row><cell>ATT [15]</cell><cell>94.2</cell><cell>86.2</cell><cell>92.9</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>?-model [16]</cell><cell>-</cell><cell>92.0</cell><cell>94.2</cell><cell>76.3</cell><cell></cell><cell>64.2</cell></row><row><cell>JDDA [7]</cell><cell>88.4</cell><cell>94.2</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Source-Only</cell><cell>62.5</cell><cell>72.6</cell><cell>88.1</cell><cell>75.9</cell><cell></cell><cell>61.8</cell></row><row><cell>VADA [6]</cell><cell>97.7</cell><cell>97.9</cell><cell>94.8</cell><cell>80.0</cell><cell></cell><cell>73.5</cell></row><row><cell>DIRT-T [6]</cell><cell>98.9</cell><cell>99.4</cell><cell>96.1</cell><cell>80.0</cell><cell></cell><cell>75.3</cell></row><row><cell>IIMT</cell><cell>99.5</cell><cell>97.3</cell><cell>97.0</cell><cell>83.1</cell><cell></cell><cell>81.6</cell></row><row><cell>Method</cell><cell cols="5">1 ? 2 1 ? 3 2 ? 1 2 ? 3 3 ? 1 3 ? 2</cell><cell>Avg</cell></row><row><cell cols="7">Source-Only 0.652 0.640 0.696 0.637 0.659 0.631 0.652</cell></row><row><cell>DANN</cell><cell cols="6">0.768 0.731 0.785 0.694 0.746 0.725 0.741</cell></row><row><cell>VADA</cell><cell cols="6">0.776 0.747 0.797 0.734 0.726 0.720 0.750</cell></row><row><cell>IIMT</cell><cell cols="6">0.809 0.780 0.813 0.745 0.831 0.787 0.794</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Test set accuracy (%) on WiFi HAR UDA task.</figDesc><table><row><cell>Method</cell><cell cols="2">STL ? CIFAR 1 ? 2</cell></row><row><cell>Source-Only</cell><cell>61.8</cell><cell>0.652</cell></row><row><cell>DANN</cell><cell>63.4</cell><cell>0.768</cell></row><row><cell>Add intra-domain L s , L t</cell><cell>75.7</cell><cell>0.787</cell></row><row><cell>Add inter-domain L q</cell><cell>79.2</cell><cell>0.798</cell></row><row><cell>Add inter-domain L z</cell><cell>81.6</cell><cell>0.809</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation experiments for STL ? CIFAR and OP-PORTUNITY 1 ? 2 tasks. Each row corresponds to adding the specified component(s) to the previous row.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1994" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th International Conference on Learning Representations</title>
		<meeting>6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint domain alignment and discriminative feature learning for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3296" to="3303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep convolutional and lstm recurrent neural networks for multimodal wearable activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Ord??ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Roggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on behavior recognition using wifi channel state information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankalp</forename><surname>Dayal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahrokh</forename><surname>Valaee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="98" to="104" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep reconstructionclassification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1607.03516</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Personal sensing: understanding mental health using ubiquitous sensors and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of clinical psychology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="23" to="47" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimizing kernel machines using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spanias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5528" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
