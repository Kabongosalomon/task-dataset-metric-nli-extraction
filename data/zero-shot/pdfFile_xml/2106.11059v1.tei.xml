<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Multi-Modal Learning with Uni-Modal Teachers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenzhuang</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingle</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Wen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">UIBE</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Hua</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
							<email>hangzhao@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Qi Zhi Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Multi-Modal Learning with Uni-Modal Teachers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning multi-modal representations is an essential step towards real-world robotic applications, and various multi-modal fusion models have been developed for this purpose. However, we observe that existing models, whose objectives are mostly based on joint training, often suffer from learning inferior representations of each modality. We name this problem Modality Failure, and hypothesize that the imbalance of modalities and the implicit bias of common objectives in fusion method prevent encoders of each modality from sufficient feature learning. To this end, we propose a new multi-modal learning method, Uni-Modal Teacher, which combines the fusion objective and uni-modal distillation to tackle the modality failure problem. We show that our method not only drastically improves the representation of each modality, but also improves the overall multi-modal task performance. Our method can be effectively generalized to most multi-modal fusion approaches. We achieve more than 3% improvement on the VGGSound audio-visual classification task, as well as improving performance on the NYU depth V2 RGB-D image segmentation task. * Equal Contribution.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal signals, e.g., vision, sound, text, are ubiquitous in our daily life, allowing us to perceive the world through multiple sensory systems. Inspired by the crucial role that multi-modalities play in human perception and decision <ref type="bibr" target="#b41">[42]</ref>, substantial efforts have been made to build effective and reliable multi-modal systems in fields like multimedia computing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">48]</ref>, representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref> and robotics <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Much current research on multi-modal fusion mainly revolves around the design of model architectures, such as middle fusion <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45]</ref>, late fusion <ref type="bibr" target="#b43">[44]</ref> and attention-based fusion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41]</ref>. However, simply combining multiple modalities often results in unsatisfactory performance. <ref type="bibr" target="#b43">[44]</ref> observed that in the video classification task, the best vision-only uni-modal network can achieve similar or even better performance than its multi-modal counterpart. They empirically examined the effects of the optimization process of different modalities and showed how the conflicts of joint multi-modal training can negatively affect the final accuracy. Such conflicts are almost inevitable when optimizing the naive joint-training objective. The inharmony of the modalities creates difficulties in multi-modal learning.</p><p>In this work, we identify a significant but widely neglected weakness in previous fusion-based methods, which is termed as modality failure. Concretely, in fusion-based methods, the weaker modality is significantly under-trained, even when the training of the fusion model has already converged. We use the naive fusion architecture as our motivating example, which simply concatenates features extracted from all modalities (see <ref type="figure" target="#fig_0">Figure 1</ref> left). As our experiments on encoder evaluation in <ref type="table" target="#tab_0">Table 1</ref> show, even when the multi-modal classifier has already achieved 99.9% training accuracy, the video encoder can only achieve 35% accuracy over the training data with linear evaluation. It is worth noting that a uni-modal network trained solely on video modality can easily achieve over 99% training accuracy. Therefore, we hypothesize that modality failure is a main cause for the inferior performance of fusion networks.</p><p>Inspired by the recent theoretical progress of <ref type="bibr" target="#b2">[3]</ref>, we give a theoretical interpretation of the modality failure problem: the imbalance of modalities and the implicit bias of the fusion objective together lead to the insufficient feature learning of the weaker modality. In light of this interpretation, we propose Uni-Modal Teacher (UMT), a distillation method whose objective is to distill the pre-trained unimodal features to the multi-modal networks. When combined with the original fusion objective, the multi-modal networks achieve significant performance gains. With UMT, we obtain state-of-the-art results in various multi-modal tasks, including audio-visual video classification and RGB-D semantic segmentation.</p><p>Contributions. We summarize our key contributions as follows:</p><p>? We identify an optimization problem in multi-modal training methods called modality failure, and link it to the insufficient feature learning caused by naive joint training. ? To tackle this problem, we introduce a distillation method, called Uni-Modal Teacher (UMT), to alleviate modality failure and improve multi-modal performance during testing. ? We provide abundant analysis on modality failure and UMT, and demonstrate the effectiveness of UMT on various multi-modal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we aim to describe our findings and our proposed method UMT. First, we describe the modality failure problem of fusion-based multi-modal learning in ?2.1. In ?2.2, we introduce Uni-Modal Teacher (UMT) which effectively solves the modality failure issue and achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Observations of Modality Failure</head><p>The typical approach in multi-modal learning is to combine features from encoders of different modalities to tackle a given task. Such methods are called fusion-based methods. Usually, the encoder networks are simultaneously updated by the gradients obtained from the training objective, i.e., the loss function <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b7">8]</ref>. However, as we shall describe below, the contributions to the task from different modalities are uneven, which leads to the failure of individual modalities.</p><p>What is modality failure in fusion-based methods. We shall illustrate that fusion-based methods cannot fully exploit the potentials of multi-modal learning, especially the weaker modality which contributes less to the task in terms of testing accuracy. We call it modality failure, where the weaker modality, in our case the video modality, is significantly under-trained, even when the training of the fusion model has already converged. Experimental evidences are presented below: ? As shown in <ref type="figure">Figure 2</ref>, among the classes in which the audio network trained over uni-modal data can achieve good accuracy, the video network trained by the naive fusion method falls behind its uni-modal counterpart. Specifically, the mean accuracy on these classes in <ref type="figure">Figure 2</ref> of four types of video encoder are 33.79%, 36.92%, 49.05%, and 53.13% respectively, where the gap between naive fusion video and uni-video is 15.26%. ? Moreover, as <ref type="table" target="#tab_0">Table 1</ref> shows, the naively trained video encoder achieves less than 35% accuracy over the training data, and less than 16% accuracy over the testing data, which are significantly lower than the performance of the video encoder trained over uni-modal data.</p><p>In order to solve the modality failure problem, we propose our Uni-Modal Teacher method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Uni-Modal Teacher</head><p>Before describing our UMT method, we formally define the naive fusion approach as a base method, which we present below.</p><p>Naive fusion methods. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (left), naive fusion can be described as follows: * for the k-classification problem, given a training set Z = {X i , y i } i?[n] , where the inputs X i = (X m1 i , X m2 i ) are composed of two modalities m 1 and m 2 and y i ? [k], we use two neural network encoders ? 1 (W 1 , ?) and ? 2 (W 2 , ?) to map the inputs to X m1</p><formula xml:id="formula_0">i ?1 ?? ? 1 (W 1 , X m1 i ) ? R d? 1 and X m2 i ?2 ?? ? 2 (W 2 , X m2 i ) ? R d? 2 .</formula><p>Here W 1 , W 2 are the weights of ? 1 and ? 2 respectively. Now by denoting the final linear classifier as ? = (? 1 , . . . , ? k ) ? R k?(d? 1 +d? 2 ) , we aim to solve the following optimization problem using SGD (or other variants of the gradient method):</p><formula xml:id="formula_1">min ?,W1,W2 L(?, W 1 , W 2 ) = 1 n i?[n]</formula><p>? log e Fy(X)</p><formula xml:id="formula_2">j?[k] e Fj (X)<label>(1)</label></formula><p>where the function F y (X) := ? y , (? 1 (W 1 , X m1 i ), ? 2 (W 2 , X m2 i )) . Nevertheless, as we have shown in ?2.1, such naive fusion methods will result in modality failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uni-Modal Teacher (UMT)</head><p>The essential idea here is to distill features from well-trained unimodal encoders to under-trained multi-modal encoders. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (right), UMT involves an initial stage of uni-modal pre-training followed by a stage of distillation and multi-modal fusion. In UMT, we assume our teacher encoders ? * 1 (?), ? * 2 (?) are pre-trained as follows: letting s ? [2] indicate the modality, we solve the following uni-modal learning problem:</p><formula xml:id="formula_3">mi? ?s,?s L(? s , ? s ) = 1 n i?[n] ? log e F s y (X) j?[k] e F s j (X)<label>(2)</label></formula><p>* Without loss of generality, we describe classification with two modalities for the simplicity of exposition.  <ref type="figure">Figure 2</ref>: Video modality failure in multi-modal fusion training. We first select the top 20 test accuracy classes from uni-audio training, then evaluate different video encoders on these classes. It can be seen that the video encoder in naive fusion setting is worse than that in uni-video setting over about 16 classes, indicating that modality failure occurs in naive fusion training. This problem also happens in Gradient-Blending <ref type="bibr" target="#b43">[44]</ref>, but is considerably alleviated by our UMT method.</p><p>where F s j (?) := ? j,s , ? s (?) is the uni-modal learner. After obtaining the pre-trained ? * s , we distill their outputs to the randomly initialized encoders (i.e., ? 1 (W 1 , ?) and ? 2 (W 2 , ?)) in the fusion model. More precisely, we use</p><formula xml:id="formula_4">a 2 -objective L distill (? * s (X i ), ? s (W s , X i )) := ? * s (X i ) ? ? s (W s , X i ) 2 2</formula><p>to capture the discrepancies between the features of our fusion encoders and pretrained encoders, and we add them to the final objective as follows:</p><formula xml:id="formula_5">L UMT := L(?, W 1 , W 2 ) + ? E (Xi,yi)?Z [ s?[2] L distill (? * s (X i ), ? s (W s , X i ))]<label>(3)</label></formula><p>where the fused classification loss L(?, W 1 , W 2 ) is the same as (1), ? is a hyper-parameter that weights the distillation loss. We update the parameters of the encoders via SGD (or other variants of the gradient method). As shown in <ref type="table" target="#tab_0">Table 1</ref>, via such distillation procedure, the performance of the weaker modality can be significantly improved which results in the overall enhancement of the multi-modal evaluation. We have also compared UMT with other alternative approaches ( <ref type="table">Table 2)</ref> and evaluated the UMT method in different tasks <ref type="table" target="#tab_3">(Table 3)</ref> in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inspiration from Theory</head><p>In this section, we elaborate how the multi-view structure in <ref type="bibr" target="#b2">[3]</ref> can be analogous to the multi-modal learning tasks, and how they inspire us to solve modality failure via distillation from well-trained uni-modal encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Understanding Multi-modal Data Through Multi-view Structure</head><p>The work in <ref type="bibr" target="#b2">[3]</ref> explains how multi-view structure in the input can lead to insufficient feature learning. They formally define their multi-view data ((X, y) ? D m ) as follows: in a k-classification problem, they assume each class y ? [k] has two class-indicating features ? v y,1 and v y,2 (i.e., the class index is dependent on both of them). Let ? &gt; 0 be a small constant. For about 1 ? ? portion of the population data X in class y, X contains both features v y,1 and v y,2 , and the rest of the population only has one feature v y,i , i ? <ref type="bibr" target="#b1">[2]</ref>. Over such multi-view data distribution, their theorem can be stated as follows: (the F y (?) here is their neural network output, similar to ours) Theorem 1 (Theorem 1 in <ref type="bibr" target="#b2">[3]</ref>, sketched). For some k-classification problems, over certain finite dataset sampled from the population of multi-view data, even if the training accuracy is 100% (meaning all training data are correctly classified), with high probability it holds that</p><p>Pr (X,y)?Dm (y = arg max y F y (X)) ? 0.49? ? In <ref type="bibr" target="#b2">[3]</ref>, they claim the two-features setting can be easily generalize to data with more features, but the proof would be much more involved and non-illustrative. <ref type="table">Table 2</ref>: Top-1 accuracy (in %) under UMT and baseline methods on VGGSound dataset, where the best result is shown in bold. "Dropout" denotes naive fusion with 0.5 dropout ratio; "Pre-train + Fine-tune" represents first pre-training uni-modal encoders, then fine-turning a classifier over them; "Modality Dropout" that randomly drops (with probability 1/3) the outputs from one modality in every iteration; "Self Distillation" means distilling a pre-trained naive fusion model to a new one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top- where ? is the proportion of the single-view data, i.e., data with only one class-indicating feature. This means that the learner F (?) fails to learn one of the class-indicating features in most classes.</p><p>It's straightforward to link our multi-modality model to the multi-view structure. In our two modality settings, we present the following hypothesis below. Note that the features in our setting should be formulated as similar to those in <ref type="bibr" target="#b2">[3]</ref>. Hypothesis 1. Let ? &gt; 0 be some small constant. We assume for each multi-modal data sample (X m1 i , X m2 i ), y i in the training dataset, with probability 1 ? ? the class-indicating features are contained in both X m1 i , X m2 i , and with probability ? only one of X m1 i , X m2 i contains the classindicating feature. Moreover, we consider imbalanced multi-modal data, that is, for each class, the feature in single-feature data mostly (e.g., 1 ? o(1) fraction of such feature) comes from the stronger modality (e.g., the audio modality in our practice).</p><p>Given this hypothesis, we immediately obtain a corollary to Theorem 1. We shall give an informal analysis to explain what happens to the learning processes involved below. Corollary 1. Under similar learner architecture in <ref type="bibr" target="#b2">[3]</ref> (where different classes use different encoders), if Hypothesis 1 holds, then we have the same generalization results for our multi-modal learner. That is, for each class, the learner will inevitably fail to learn the features of one of the modalities (usually the weaker modality).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why Naive Fusion Methods Lead to Modality Failure.</head><p>Given the fusion objective, a straightforward observation is that the learning processes of two modalities are rather separated. Take modality m 1 for example, let y ? [k] be a class index, we break ? y into ? j = (? y,1 , ? y,2 ) so that we can rewrite the output as</p><formula xml:id="formula_6">F y (X) = ? y,1 , ? 1 (W 1 , X m1 i ) + ? y,2 , ? 2 (W 2 , X m1 i )</formula><p>. which is a linear combination of two predictors from different modalities. So the update rule of our parameters can be written as follows: denoting logit y (F, X) := e Fy (X) j?[k] e F j (X) , at each iteration t ? 0, with learning rate ? &gt; 0, our parameters are updated as (using GD)</p><formula xml:id="formula_7">? (t+1) y,1 = ? (t) y,1 ? ? E (Xi,yi)?Z (1 yi=y ? logit y (F, X i )) ? ? 1 (W (t) 1 , X m1 i ) , W (t+1) 1 = W (t) 1 ? ? E (Xi,yi)?Z (1 yi=y ? logit y (F, X i )) ? ? W1 ? (t) y,1 , ? 1 (W (t) 1 , X m1 i ) ,</formula><p>and similarly for ? . We can easily see that updates of ? y,1 and W 1 have no correlation to those of different modality (that is ? y,2 and W 2 ) other than from logit y (F, X i ), i.e., from the training loss/accuracy. More specifically, under the imbalance assumption in Hypothesis 1, the stronger modality can achieve close to 100% accuracy over the training data even when the weaker modality is under-trained. When the training of all data {X i , y i } i?[n] are close to convergence, we have  That is, the algorithm can already converge, even when the encoder of the weaker modality (say ? 2 ) is largely under-trained. This is indeed what happens in our experiments. As shown by <ref type="table" target="#tab_0">Table 1</ref>, the video encoders trained by the naive fusion methods and Gradient-Blending <ref type="bibr" target="#b43">[44]</ref> cannot achieve more than 40% training accuracy in single-modal evaluation. Such under-training is due to the inherent bias of the fusion methods and the imbalance of contributions of different modalities. Nevertheless, the success of knowledge distillation in <ref type="bibr" target="#b2">[3]</ref> inspires us to propose our UMT method.</p><formula xml:id="formula_8">1 yi=y ? logit y (F, X i ) ? 0 for all i ? [n] =? ? ? L , ? W1 L , ? W2 L ? 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">How Distillation Helps Multi-modal Learning.</head><p>We first describe how knowledge distillation helps in multi-view setting, then we discuss how they inspire the UMT method. The statement on knowledge distillation in <ref type="bibr" target="#b2">[3]</ref> can be sketched as follows:</p><p>Theorem 2 (Theorem 3 in <ref type="bibr" target="#b2">[3]</ref>, informal). Over multi-view data, single model can learn all the class-indicating features by matching the soft labels of ensembles of ?(1) many single-models.</p><p>What happens in Theorem 2 can be sketched as follows: the soft-labels of the ensemble contain information of the neglected features in each class, thus matching the soft labels can help the single model to learn a complete set of features.</p><p>Yet in our setting, matching the fusion output to soft labels of well-trained uni-modal networks may not help, since both networks have already achieved 100% training accuracy. We instead sought a different approach: in UMT, we distill the features (i.e., the inputs of the last layer) from well-trained uni-modal networks to the corresponding encoders in naive fusion methods. We conjecture the following statement that inspired our method:</p><p>Conjecture 1. By carefully picking the ? parameter, the encoders trained by the UMT method can at least learn all the features learned by the corresponding teacher encoders. Furthermore, we also conjecture that UMT can help the encoders to extract features that can only be learned by multi-modal training, outperforming the pre-trained uni-modal encoders in linear evaluations.</p><p>Indeed, by setting ? ? ?, we can obtain a trivial solution, that is ? m = ? * m , for m ? <ref type="bibr" target="#b1">[2]</ref>. Then the learning problem reduces to train a linear classifier over pre-trained features. In this way UMT has enlarged the feature sets to incorporate both modalities, so that it can outperform uni-modal networks. We leave the proof of this conjecture as a future direction. ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we test the effectiveness of UMT on two standard multi-modal tasks: audio-visual classification and RGB-Depth semantic segmentation. In ?4.1, we verify our method on audio-visual classification and report the performance. we also compare different solutions for modality failure. In ?4.2, we demonstrate UMT in middle fusion task, i.e., semantic segmentation. In each sub-section, we first introduce the dataset, then describe the implementation details on how to apply our method to a specific task. We also provide further empirical demonstrations of modality failure through evaluating the performance of different encoders, as well as showing how UMT tackles this issue.</p><p>Here, all the networks were built and trained using PyTorch <ref type="bibr" target="#b34">[35]</ref> and all experiments were done by using one NVIDIA GeForce RTX 3090 GPU. ? If we are using similar learner architecture as in <ref type="bibr" target="#b2">[3]</ref> (where predictor of different classes use different encoders), then it is hopeful to use the techniques in <ref type="bibr" target="#b2">[3]</ref> to prove our conjecture, since their results are proven for one-hidden-layer networks and their soft labels are similar to the features in our setting. Dataset. VGGSound dataset <ref type="bibr" target="#b10">[11]</ref>, which contains over 200k video clips for 309 different sound classes, is used for evaluating our method. It is an audio-visual dataset in the wild where each object that emits sound is also visible in the corresponding video clip, making it suitable for scene classification tasks. Please note that some clips in the dataset are no longer available on YouTube, and we actually use about 175k videos for training and 15k for testing, but the number of sound classes remains the same.</p><p>Implementation details. Two ResNet18 <ref type="bibr" target="#b19">[20]</ref> backbones are employed as our video and audio encoders respectively (whether 3D CNN is needed depends on the input), aiming at extracting visual and acoustic features simultaneously. Then we apply the late fusion, i.e., fusing the visual and acoustic features before the linear classifier, to incorporate video and audio information. We design a preprocessing paradigm to improve training efficiency as follows: (1) each video is interpolated to 256?256 and saved as stacked images; (2) each audio is first converted to 16 kHz and 32-bit precision in the floating-point PCM format, then randomly cropped or tiled to a fixed duration of 10s. For video input, 32 frames are uniformly sampled from each clip before feeding to the video encoder. While for the audio input, a 1024-point discrete Fourier transform is performed using nnAudio <ref type="bibr" target="#b11">[12]</ref>, with 64 ms frame length and 32 ms frame-shift. And we only feed the magnitude spectrogram to the audio encoder. Please note that we do not use any kinds of data augmentation for both video and audio input. Besides, all the models are trained with a batch size of 24 and an initial learning rate of 1e-3 for 20 epochs, using the Adam optimizer <ref type="bibr" target="#b25">[26]</ref>. We also apply a fixed learning rate scheduler, i.e., decay the learning rate by 0.1 for every 5 epochs and 10 epochs on UMT and baseline methods respectively. We use MSE loss as our distillation objective and we set the ? parameter in Equation (3) to 50. Unless otherwise stated, all distillations were performed on feature-level.</p><p>Baseline methods. We compare UMT with other baseline methods, e.g., naive fusion, Gradient-Blending <ref type="bibr" target="#b43">[44]</ref>, fine-tuning a multi-modal classifier from encoders pre-trained on uni-modal data, dropout <ref type="bibr" target="#b42">[43]</ref>, self distillation <ref type="bibr" target="#b46">[47]</ref> and modality dropout. Besides, we also implement two other methods, i.e., video-only distillation and audio-only distillation, to further support our hypothesis.</p><p>Results. From the results presented in <ref type="table">Table 2</ref>, it is clear that UMT outperforms all the baseline methods by a large margin, suggesting that distillation could circumvent modality failure in joint training. In addition to showing that UMT can greatly improve performance, we also compare different methods to help us understand the fundamental issues of multi-modal learning:</p><p>? Fine-tuning a multi-modal classifier over pre-trained uni-modal encoders can outperform naive fusion. This implies the modality failure problem is critical in fusion-based methods.</p><p>? UMT outperforms finetuning over pre-trained uni-modal encoders, showing that multi-modal learning can help the encoders to learn features that are unique to corresponding multi-modal tasks, since UMT differs from uni-modal pre-training by the multi-modal objective.</p><p>? We compare the results of vanilla Dropout <ref type="bibr" target="#b42">[43]</ref> with Modality Dropout. Our results show that modality-wise dropout is more effective in multi-modal learning. Also, self-distillation performing worse than UMT also implies we should pay more attention to modality-wise features.</p><p>? Distilling from only one modality can deteriorate the performance of other modalities. The results in <ref type="table" target="#tab_0">Table 1</ref> and our analysis in Section 3 show that modality failure is caused by the learning process of strong modalities. Thus, only distilling from one modality is not able to avoid modality failure.</p><p>Encoder evaluation. We also do another experiment, i.e., fix the encoder from different training methods and fine-tune a classifier, which allows us to have a better understanding of why UMT outperforms baseline methods. As we can see in <ref type="table" target="#tab_0">Table 1</ref>, UMT achieves the best results on both audio and video modalities, manifesting that it could keep the representation power of each modality. It is worth noting that, while performing distillation on a single modality, the representation of the corresponding modality will be maintained, but that of the other modality will be significantly degraded. Hence, we further demonstrate that the inferior performance of naive fusion is because the stronger modal encoder would rapidly learn a powerful feature to fit the samples, while the weaker encoder cannot learn enough, which leads to modality failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RGB-Depth Semantic Segmentation Experiment</head><p>Dataset. We evaluate our approach on the commonly used RGB-D multi-class indoor semantic segmentation dataset, namely NYUv2 dataset, which contains 1449 indoor RGB-Depth data totally and we use 40-class label setting. The number of training set and testing set is 795 and 654 respectively.</p><p>UMT in semantic segmentation. In contrast to the late fusion classification task, the RGB-Depth semantic segmentation belongs to middle fusion. Since features generated by each layer matter, we distill multi-scale depth feature maps using the MSE loss. For feature maps from the RGB encoder, however, since they are generated by fusing RGB and depth modalities, we cannot distill RGB feature maps directly like depth feature maps. To mitigate this effect, we curate predictors, namely 2 layers CNNs, aiming to facilitate the fused feature maps to predict the RGB feature maps trained by the RGB modality before distillation. The full schematic diagram is presented in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Implementation details. Our segmentation UMT is based on a state-of-the-art method, i.e., ESANet <ref type="bibr" target="#b40">[41]</ref>. For RGB modality, the ResNet34 backbone, downsampling method, and contextual module are employed following <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b48">49]</ref>. When it comes to the decoder, it receives skip connections from the encoder, which is akin to U-Net <ref type="bibr" target="#b38">[39]</ref>. For Depth modality, we leverage another encoder that focuses on extracting geometric information and fuse it with RGB feature maps at the five different scales using the attention mechanism. We keep all hyper-parameters (e.g., learning rate, optimization schemes, dropout ratio, etc.) the same as the official implementation ? .</p><p>Results. It is common to use ImageNet pre-trained parameters as the initialization weights to achieve higher performance <ref type="bibr" target="#b30">[31]</ref>. To this end, we train the network on both scenarios: (1) training from scratch on NYUv2; (2) pre-training on ImageNet followed by fine-tuning on NYUv2. As shown in <ref type="table" target="#tab_3">Table 3</ref>, UMT improves the mean Intersection over Union (mIoU) metric on both settings, demonstrating its effectiveness. Furthermore, the increment discrepancy on training from scratch is more apparent than that on fine-tuning, manifesting that each encoder in multi-modal architecture could learn better general-purpose representations <ref type="bibr" target="#b23">[24]</ref> after pre-training.</p><p>Encoder evaluation. To explore whether modality failure also exists in RGB-Depth segmentation, we evaluate the depth encoder from ESANet <ref type="bibr" target="#b40">[41]</ref> by freezing its parameters and fine-tuning a new decoder. We compare it with uni-depth setting. Please note that only the depth encoder is applicable for this experiment since the RGB encoder has been fused with the depth information. As shown in <ref type="table" target="#tab_4">Table 4</ref>, it turns out the encoder from RGB-Depth yields worse performance on both types than uni-depth setting, which verifies our hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Multi-modal fusion. There are several different fusion methods, including middle fusion <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>, late fusion <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b4">5]</ref> and attention-based fusion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41]</ref>. Specifically, ESANet <ref type="bibr" target="#b40">[41]</ref>  Knowledge distillation. Knowledge distillation <ref type="bibr" target="#b6">[7]</ref> was first introduced to compress the knowledge from an ensemble into a smaller and faster model but still preserve competitive generalization power. Hinton et al. <ref type="bibr" target="#b20">[21]</ref> proposed to use a temperature in the softmax outputs to represent smaller probabilities, as opposed to <ref type="bibr" target="#b6">[7]</ref> that just matches the output logits. Romero et al. <ref type="bibr" target="#b37">[38]</ref> further extended distillation from the output labels to intermediate representation.</p><p>Audio-visual learning. One of the most popular multi-modal data pair is vision and audio, as they are naturally co-occurred and are recorded by video cameras simultaneously. Researchers have explored various tasks utilizing both of them, such as representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, scene classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>, vision-assisted speech recognition <ref type="bibr" target="#b0">[1]</ref>, audio-visual source separation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40]</ref>, audio source grounding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b47">48]</ref>, audio spatialization <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>, emotion recognition <ref type="bibr" target="#b1">[2]</ref> and audio-visual navigation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>RGB-Depth fusion. Depth provides complementary geometric information to RGB images in a lot of tasks, such as semantic segmentation. Most state-of-the-art methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref> for RGB-Depth semantic segmentation leveraged an RGB encoder and a depth encoder respectively, and then applied middle fusion between them to better incorporate low-level (texture, geometry) and high-level (semantic) features. In the field of autonomous driving, there have also been great efforts on fusing camera frames and LiDAR scans for better object detection <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we identify a serious phenomenon called modality failure in multi-modal training. To tackle this issue, we propose Uni-Modal Teacher (UMT), which significantly improves the performances on both the audio-visual scene classification task and the RGB-Depth semantic segmentation task. Our experiments suggest several avenues for future works. For example, can we alleviate the modality failure by carefully designing the encoders, and whether a similar problem also exists in uni-modal learning? In addition, generalizing our method to other tasks including multi-modal object detection and generation will benefit many realistic applications. We hope our findings will shed new light on multi-modal learning research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Broader impact</head><p>Potential benefits. Our UMT method improves multi-modal recognition and perception systems. In addition, our method enables multi-modal training with lower computational footprints, which can be a crucial step towards environment friendly AI. Finally, we provide theoretical analysis of our method, which tries to explain the current black-box multi-modal models, to facilitate transparency and equality in current AI research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Model architecture of naive late fusion (left) and Uni-Modal Teacher (UMT) (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Model architecture of UMT for RGB (left) and depth (right) modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Modality Failure: we study how well the encoders of each modality perform from various training methods. For the audio encoder, the best results are shown in bold and the worst results are underlined. "Audio Test" denotes the top 1 test accuracy (in %), which is evaluated using the fixed audio encoder with a fine-tuned classifier, and similarly for the video encoders in the other column categories. Details of our experiments are in Section 4.</figDesc><table><row><cell>Method</cell><cell cols="4">Evaluation Method Audio Test Video Test Audio Training Video Training</cell></row><row><cell>Naive Fusion</cell><cell>43.48</cell><cell>15.86</cell><cell>88.58</cell><cell>34.69</cell></row><row><cell>Audio-only Training</cell><cell>46</cell><cell>/</cell><cell>99.95</cell><cell>/</cell></row><row><cell>Video-only Training</cell><cell>/</cell><cell>23.78</cell><cell>/</cell><cell>99.47</cell></row><row><cell>Gradient-Blending [44]</cell><cell>44.42</cell><cell>16.85</cell><cell>95.03</cell><cell>36.9</cell></row><row><cell>Audio-only Distillation</cell><cell>46.28</cell><cell>8.96</cell><cell>95.72</cell><cell>16.45</cell></row><row><cell>Video-only Distillation</cell><cell>40.76</cell><cell>24.5</cell><cell>70.95</cell><cell>82.7</cell></row><row><cell>Uni-Modal Teacher</cell><cell>46.66</cell><cell>24.84</cell><cell>86.21</cell><cell>61.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Predictor Fused RGB Feature Maps Predicted RGB Feature Maps Pre-trained RGB Feature Maps ? distill Depth Feature Maps Pre-trained Depth Feature Maps</head><label></label><figDesc></figDesc><table><row><cell>? distill</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Model performance comparison under UMT and ESANet on NYU-DepthV2 RGB-Depth semantic segmentation task.</figDesc><table><row><cell>Initialization</cell><cell cols="2">Training Setting ESANet [41] UMT</cell><cell>Improve</cell></row><row><cell>From Scratch</cell><cell>38.59</cell><cell>40.45</cell><cell>1.86</cell></row><row><cell>ImageNet Pre-train</cell><cell>48.48</cell><cell>49.14</cell><cell>0.66</cell></row><row><cell cols="2">4.1 Audio-visual Classification Experiment</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on RGB-Depth semantic segmentation setting. "Initialization" indicates how weights are initialized for the network, "from Depth" represents end-to-end training with a depth-only segmentation network, and "from RGB+depth" refers to freezing the depth encoder from ESANet<ref type="bibr" target="#b40">[41]</ref> then fine-tuning with a new decoder.</figDesc><table><row><cell>Initialization</cell><cell cols="2">Training Setting from Depth from RGB+Depth</cell><cell>Drop</cell></row><row><cell>From Scratch</cell><cell>32.69</cell><cell>28.53</cell><cell>-4.16</cell></row><row><cell>ImageNet Pre-train</cell><cell>39.45</cell><cell>34.73</cell><cell>-4.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>fed depth feature maps into RGB encoder at multiple scales; CEN<ref type="bibr" target="#b44">[45]</ref> exchanged different modalities' channels in the middle of the encoders; Hori et al.<ref type="bibr" target="#b21">[22]</ref> proposed an attention-based fusion architecture for video captioning. MFAS<ref type="bibr" target="#b35">[36]</ref>, on the other hand, posed multi-modal fusion as a neural architecture search problem. More recently, Wang et al. proposed Gradient-Blending<ref type="bibr" target="#b43">[44]</ref>, which leveraged concatenated features from two modalities without any aggregation in the intermediate stage, but introduced additional weighted losses for better training.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? https://github.com/TUI-NICR/ESANet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotion recognition in speech using cross-modal transfer in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Samuel Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards understanding ensemble, knowledge distillation and self-distillation in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09816</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multimodal fusion for multimedia analysis: a survey. Multimedia systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradeep K Atrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulmotaleb</forename><forename type="middle">El</forename><surname>Anwar Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Saddik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kankanhalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How much does audio matter to recognize egocentric object interactions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Cartas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petia</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Segura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariella</forename><surname>Dimiccoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of the conference of Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Soundspaces: Audio-visual navigation in 3d environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unnat</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Schissler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastia</forename><surname>Vicenc Amengual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziad</forename><surname>Gari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Vamsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ithapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Localizing visual sounds the hard way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vggsound: A large-scale audio-visual dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kat Agres, and Dorien Herremans. nnaudio: An on-the-fly gpu audio to spectrogram conversion toolbox using 1d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin</forename><forename type="middle">Wai</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="161981" to="162003" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinatan</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Music gesture for visual sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Look, listen, and act: Towards audio-visual embodied navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2.5 d visual sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jointly discovering visual objects and spoken words from raw sensory input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D?dac</forename><surname>Sur?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng-Yok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bret</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiko</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4193" to="4202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<title level="m">What makes imagenet good for transfer learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rednet: Residual encoder-decoder network for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01054</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Choosing smartly: Adaptive multimodal fusion for object detection in changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-supervised generation of spatial audio for 360 video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Langlois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12607" to="12616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="801" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ki-Sang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4980" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<title level="m">An imperative style, high-performance deep learning library</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mfas: Multimodal fusion architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>P?rez-R?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6966" to="6975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09224</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selfsupervised audio-visual co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2357" to="2361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient rgb-d semantic segmentation for indoor scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst-Michael</forename><surname>Gross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06961</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The development of embodied cognition: Six lessons from babies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial life</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">What makes training multi-modal classification networks hard?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Telling left from right: Learning spatial correspondence of sight and sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karren</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9932" to="9941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The sound of pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
