<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SAINT+: Integrating Temporal Features for EdNet Correctness Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Shin</surname></persName>
							<email>dm.shin@riiid.co</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeun</forename><surname>Shim</surname></persName>
							<email>yugeun.shim@riiid.co</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangyeol</forename><surname>Yu</surname></persName>
							<email>hangyeol.yu@riiid.co</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seewoo</forename><surname>Lee</surname></persName>
							<email>seewoo.lee@riiid.co</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Kim</surname></persName>
							<email>byungsoo.kim@riiid.co</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
							<email>youngduck.choi@riiid.co</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Riiid! AI Research Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Riiid! AI Research Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Riiid! AI Research Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Riiid! AI Research Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Riiid! AI Research Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Riiid! AI Research Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<addrLine>12-16</addrLine>
									<postCode>2021</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SAINT+: Integrating Temporal Features for EdNet Correctness Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3448139.3448188</idno>
					<note>ACM Reference Format: Dongmin Shin, Yugeun Shim, Hangyeol Yu, Seewoo Lee, Byungsoo Kim, and Youngduck Choi. 2021. SAINT+: Integrating Temporal Features for Ed-Net Correctness Prediction. In LAK21: 11th International Learning Analytics/21/04. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Artificial intelligence</term>
					<term>Neu- ral networks</term>
					<term>? Applied computing ? Interactive learning environments</term>
					<term>? Social and professional topics ? Student as- sessment KEYWORDS Education, Personalized Learning, Knowledge Tracing, Deep Learn- ing, Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose SAINT+, a successor of SAINT which is a Transformer based knowledge tracing model that separately processes exercise information and student response information. Following the architecture of SAINT, SAINT+ has an encoder-decoder structure where the encoder applies self-attention layers to a stream of exercise embeddings, and the decoder alternately applies self-attention layers and encoder-decoder attention layers to streams of response embeddings and encoder output. Moreover, SAINT+ incorporates two temporal feature embeddings into the response embeddings: elapsed time, the time taken for a student to answer, and lag time, the time interval between adjacent learning activities. We empirically evaluate the effectiveness of SAINT+ on EdNet, the largest publicly available benchmark dataset in the education domain. Experimental results show that SAINT+ achieves state-of-the-art performance in knowledge tracing with an improvement of 1.25% in area under receiver operating characteristic curve compared to SAINT, the current state-of-the-art model in EdNet dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The recent COVID-19 pandemic has raised needs for social distancing, leading many organizations to develop virtual and remote services to prevent widespread infection of the disease. Accordingly, educational systems have developed remote learning environments including Massive Open Online Courses and Intelligent Tutoring Systems (ITSs). Knowledge tracing, modeling a student's knowledge state based on the history of their learning activities records, is a fundamental task for creating ITS that aims to provide personalized learning experiences to each student. Traditionally, Bayesian Knowledge Tracing <ref type="bibr">[4, 5, 10, 15-17, 19, 20, 24, 26]</ref> and Collaborative Filtering based models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> were common approaches for knowledge tracing. However, with the widespread adoption of Deep Learning (DL) in many machine learning problems, DL based models <ref type="bibr">[2, 6, 9, 12-14, 18, 21, 23, 25, 27]</ref> have become the de facto standard for knowledge tracing by capturing complex relations among interactions in students' learning activities records. The strength of the DL based models has become amplified with the advent of large scale public benchmark dataset in Artificial Intelligence in Education. EdNet <ref type="bibr" target="#b2">[3]</ref> is such a publicly available dataset which is the largest in scale with more than 131M learning activities records from around 780K students.</p><p>In this paper, we propose SAINT+, a successor of SAINT <ref type="bibr" target="#b1">[2]</ref> which enhances knowledge tracing with temporal feature embeddings, and empirically verify the effectiveness of the model on EdNet dataset. SAINT is a Transformer <ref type="bibr" target="#b24">[25]</ref> based knowledge tracing model that separately processes information of exercise and student response. Specifically, a stream of exercises that a student consumes is fed to an encoder, and a decoder gets a corresponding response sequence and encoder output sequence, computing the final output sequence of the model. SAINT+ augments SAINT by integrating two temporal feature embeddings: elapsed time, the time taken for a student to answer, and lag time, the time interval between adjacent learning activities. Empirical evaluations conducted on EdNet dataset show that SAINT+ improves SAINT, the current state-of-the-art knowledge tracing model on EdNet dataset, by 1.25% in area under receiver operating characteristic curve (AUC). Also, the experimental results show that incorporating the temporal features into the decoder input achieves the best arXiv:2010.12042v2 [cs.CY] 1 Feb 2021 AUC compared to incorporating them into the encoder input, and both the encoder and decoder input, verifying the hypothesis that separately processing exercise information and student response information is appropriate for knowledge tracing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Knowledge tracing is a fundamental task for many computer-aided educational applications and has been studied extensively in the field of AIEd. Traditional approaches addressed knowledge tracing based on Bayesian Knowledge Tracing (BKT) <ref type="bibr">[4, 5, 10, 15-17, 19, 20, 24, 26]</ref> and Collaborative Filtering (CF) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref>. Basically, BKT is the Hidden Markov Model where a latent variable represents evolving student knowledge. BKT assumes the latent student knowledge as a set of binary variables: either the student mastered the knowledge or not. Each latent variable is updated based on observations of the student correctly applying the knowledge which are also binary: either the student correctly or incorrectly answered a given exercise. On the other hand, CF based approaches model students and exercises as low-rank matrices. Each vector in the student matrix and exercise matrix represents latent traits of each student and latent knowledge required for each exercise, respectively. The probability of a student correctly answers to an exercise is calculated by applying the sigmoid function to the dot product between the corresponding student and exercise vectors.</p><p>The advances of Deep Learning (DL) have given rise to neural network based knowledge tracing models. DKT <ref type="bibr" target="#b17">[18]</ref> is the first DL based knowledge tracing model. DKT models students' evolving knowledge state through Recurrent Neural Network (RNN) which compresses their past learning activities in a hidden layer. Like many RNN based models that commonly leverage attention mechanism, NPA <ref type="bibr" target="#b11">[12]</ref> models student knowledge through Bidirectional Long-Short Term Memory (Bi-LSTM) network equipped with an attention layer that weighs more importance to relevant parts of their learning history for prediction. EKT <ref type="bibr" target="#b8">[9]</ref> is also a Bi-LSTM knowledge tracing model with an attention layer. However, EKT addresses the cold start problem in knowledge tracing by exploiting not only students' learning activities records but also text descriptions of exercises. Not all DL based knowledge tracing models are based on RNN architecture. DKVMN <ref type="bibr" target="#b26">[27]</ref> is a memory-augmented neural network knowledge tracing model where the key matrix stores knowledge concepts and the value matrix stores students' mastery levels of corresponding concepts. CKT <ref type="bibr" target="#b20">[21]</ref> is a knowledge tracing model that applies hierarchical convolutional operations to extract learning rate features from student's learning activities history. Applying self-attention mechanism in Transformer <ref type="bibr" target="#b24">[25]</ref> architecture, which is de facto standard to many sequential prediction tasks, to knowledge tracing is also an actively studied area. SAKT <ref type="bibr" target="#b13">[14]</ref> is the first knowledge tracing model with self-attention layers. In each self-attention layer of SAKT, each query is an exercise embedding vector, and key and value are interaction embedding vectors. SAINT <ref type="bibr" target="#b1">[2]</ref> is the first Transformer based knowledge tracing model which leverages encoder-decoder architecture composed of stacked self-attention layers. Unlike SAKT, SAINT gets separated streams of exercises and responses as inputs where a sequence of exercises are fed to the encoder, and a sequence of encoder outputs and responses are fed to the decoder. AKT <ref type="bibr" target="#b5">[6]</ref> also adopts self-attention layers for knowledge tracing. The attention weights in AKT are decayed exponentially based on the context-aware relative distance measure. Moreover, AKT uses the Rasch model based exercise and exercise-response embeddings to avoid overparameterization and overfitting. Recently, several works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> attempt to incorporate graph structure to the knowledge tracing model. <ref type="bibr" target="#b12">[13]</ref> formulate knowledge tracing as a time series node-level classification task in graph structure and proposes GKT which extracts representation of each knowledge concept by aggregating representations of neighboring concepts. HGKT <ref type="bibr" target="#b22">[23]</ref> applies graph neural network to get hierarchical exercise graph which better represent groups of similar exercises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KNOWLEDGE TRACING</head><p>We formulate knowledge tracing as a task of predicting the probability of a student's answer being correct to a particular exercise given their previous interaction histories. Formally, the student's learning activity is recorded as an interaction sequence 1 , . . . , . Each interaction = ( , ) is a tuple of exercise information , the -th exercise given to the student with related metadata, such as the type of the exercise, and response information , the student's response to the exercise along with related metadata including the correctness of the response, the duration of time the student took to respond and the time interval between the current and previous interactions. The student's response correctness ? {0, 1} is equal to 1 if the student answered the -th exercise correctly and 0 if not. Thus, knowledge tracing aims to estimate the probability,</p><formula xml:id="formula_0">P[ = 1| 1 , 2 , . . . , ?1 , ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SAINT+ 4.1 SAINT: Separated Self-Attentive Neural Knowledge Tracing</head><p>In this subsection, we give a brief review of SAINT, a Separated Self-AttentIve Neural Knowledge Tracing. We refer the paper <ref type="bibr" target="#b1">[2]</ref> for those who want to lean detailed aspects of SAINT. SAINT is a knowledge tracing model based on Transformer <ref type="bibr" target="#b24">[25]</ref> architecture which consists of an encoder and a decoder. It separates a stream of student interactions into two sequences: an exercise sequence and a response sequence. Then, the encoder takes a sequence of exercise embeddings = [ 1 , 2 , . . . , ] as input and pass an output sequence = [ 1 , 2 , . . . , ] to the decoder. The decoder additionally takes a shifted response embedding sequence = [ , 1 , 2 , . . . , ?1 ] as input, whose first element is a start token embedding, to produce the final output sequenc? = [?1,?2, . . . ,?]. Each?is an estimated probability of the student's answer to the -th exercise being correct given the current exercise information and the past interactions 1 , 2 , . . . , ?1 .</p><p>The most fundamental part of SAINT is a multi-head attention layer. Let ? be the number of heads. When input query matrix , key matrix , and value matrix are given, we compute = , = , and = for each 1 ? ? ?, where , , and are weight matrices of query, key and value, respectively. Then, the scaled dot-product attention computes each head matrix,</p><formula xml:id="formula_1">t 1 t 2 t 3 t 4 t 5</formula><p>Elapsed Time  and the final output is a linear transformation of concatenated head matrices,</p><formula xml:id="formula_2">Multihead( , , ) = Concat(head 1 , head 2 , . . . , head ? ) where head = Softmax Mask ? ,</formula><p>where is a dimension of the query and key vectors, and is a weight matrix. Note that the masking mechanism overwrites the region above the diagonal of the matrix with ?? so that the corresponding region of the softmax output becomes zero. This prevents the current position from attending to subsequent positions. In other words, SAINT uses no future information from the sequence while training.</p><p>The encoder block consists of sequentially aligned copies of encoder layers. A single encoder layer is a multi-headed selfattention layer with an upper triangular mask followed by a feed forward network (FFN) which is defined by</p><formula xml:id="formula_3">FFN( ) = ReLU( 1 + 1 ) 2 + 2 ,</formula><p>where 1 , 2 and 1 , 2 are weights and biases, respectively. Suppose is given as input to an encoder layer. Then, the output sequence is computed as follows:</p><formula xml:id="formula_4">= + Multihead(LayerNorm( , , )) = + FFN(LayerNorm( )).</formula><p>Here, the input sequence for the foremost encoder layer is an exercise embedding sequence while each subsequent layer takes the feed forward output of the previous layer. Note that we apply layer normalization <ref type="bibr" target="#b0">[1]</ref> and skip connection <ref type="bibr" target="#b6">[7]</ref> to every sub-layer.</p><p>The decoder is a sequence of identical decoder layers, which consists of two multi-head attention layers with upper triangular masks followed by a feed forward network as well. Suppose is an input sequence to a decoder layer. If the layer is the foremost, is the response embedding sequence . Otherwise, it is the output sequence from the previous decoder layer. The first layer is a multiheaded self-attention layer which only takes . Then, its output 1 serves as queries for the second attention whose keys and values are the encoder output . The computation can be summarized as follows:</p><formula xml:id="formula_5">1 = + Multihead(LayerNorm( , , )) 2 = 1 + Multihead(LayerNorm( 1 ,<label>, ))</label></formula><p>= 2 + FFN(LayerNorm <ref type="formula">( 2 )</ref>).</p><p>The output of the last decoder layer is passed to a fully connected layer to produce the final output of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Enhancing Knowledge Tracing with Temporal Feature Embeddings</head><p>SAINT takes sequences of exercise embeddings and response embeddings, where each entry in the exercise embedding sequence is the sum of vectors of an exercise ID, an exercise category and the position, and each entry in the response embedding sequence is the sum of vectors of a response correctness and the position. SAINT+ augments response embeddings with two temporal feature embeddings: elapsed time, the duration of time a student took to respond, and lag time, the time interval between the current and previous interactions <ref type="figure" target="#fig_0">(Figure 1</ref>). The embedding vectors for elapsed time and lag time are added to the response embeddings ( <ref type="figure" target="#fig_1">Figure 2</ref>). In the following subsections, we provide detailed explanations of the two temporal feature embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Elapsed Time.</head><p>Elapsed time is an amount of time that a student spent on solving a given exercise. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>,</p><formula xml:id="formula_6">et 1 = 2 ? 1 (resp. et 2 = 4 ? 3 )</formula><p>is the elapsed time for the exercise 1 (resp. exercise 2). If the student does not have enough knowledge and skills for the exercise, it would be hard to respond correctly within the recommended time limit. Hence, elapsed time provides strong evidence for a student's proficiency in knowledge and skills, and student's understanding of concepts associated with the exercise. We propose two different approaches to embed elapsed times as latent vectors: continuous embedding and categorical embedding. In continuous embedding, a latent embedding vector for an elapsed time et is computed as v et = et?w elapsed_time , where w elapsed_time is a single learnable vector. For categorical embedding, unique latent vectors are assigned to each integer seconds. We set the maximum elapsed time as 300 seconds and any time more than that is capped off to 300 seconds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Attention with Upper Triangular Mask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Lag Time.</head><p>Lag time is the time gap between interactions, an important factor that affects complex phenomena occurring in students' learning process. For instance, students tend to forget what they have learned as time passes. If a lot of time passed after a student answers an exercise about certain concepts, it would be hard to respond to similar exercises correctly, even if they provided the correct answer to the exercise before. On the other hand, students need time to refresh. By taking a rest, their brains organize and arrange what they have learned, and prepare for the next learning session. We define lag time as the time interval between the moment a student encountered the current exercise and the moment the student consumed the previous exercise. For instance, in <ref type="figure" target="#fig_0">Figure 1</ref>, lag time for the exercise 2 (resp. exercise 3) is lt 2 = 3 ? 2 (resp. lt 3 = 5 ? 4 ). Similar to the elapsed time embedding, we use continuous embedding and categorical embedding for lag time. In continuous embedding, a latent embedding vector for a lag time lt is computed as v lt = lt ? w lag_time , where w lag_time is a trainable vector. For categorical embedding, lag times are discretized as integer minutes 0, 1, 2, 3, 4, 5, 10, 20, 30, . . . , 1440. As a result, there are a total of 150 trainable unique latent vectors assigned to each integer minute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Dataset</head><p>EdNet <ref type="bibr" target="#b2">[3]</ref> is the largest publicly available benchmark dataset in education domain consisting of interaction logs collected by Santa 1 . We conduct experiments on an updated version of EdNet-KT1, which contains problem-solving logs from January 1st, 2019 to June 1st, 2020. Details of the dataset statistics are provided in <ref type="table" target="#tab_1">Table 1</ref>. Also, the distributions of elapsed time and lag time of the dataset are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We use interaction logs of the most recent 100K students as test set and 80% (resp. 20%) of the remaining dataset are used as training (resp. validation) set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Models</head><p>We compare SAINT+ against benchmark knowledge tracing models, DKT <ref type="bibr" target="#b17">[18]</ref>, DKVMN <ref type="bibr" target="#b26">[27]</ref>, SAKT <ref type="bibr" target="#b13">[14]</ref> and SAINT <ref type="bibr" target="#b1">[2]</ref>:</p><p>? DKT is a simple RNN-based model that uses concept and response correctness as input features, and models student's knowledge status as RNN's hidden state vectors. We choose an LSTM <ref type="bibr" target="#b7">[8]</ref> architecture. Also, we consider each unique exercise id as a concept associated with the exercise. ? DKVMN is a memory augmented neural network based model where the key matrix stores knowledge concepts and the value matrix stores students' mastery levels of the corresponding concepts. We use each exercise id as a concept of the corresponding exercise. ? SAKT is the first knowledge tracing model that utilizes Transformer's self-attention architecture. It is a single encoderbased model where exercise ids are used as queries and interaction ids are used as keys and values. ? SAINT is the first Transformer-based knowledge tracing model which leverages an encoder-decoder structure to process information of question and student response separately. The encoder takes a sequence of question embeddings and the decoder gets a sequence of encoder outputs and response embeddings to compute the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Training and Evaluation</head><p>We use the accuracy (ACC) and the area under the receiver operating characteristic curve (AUC) as the performance metric. We pick the weight with the best validation AUC and evaluate them on the test set. For SAINT+ and SAINT, we use Adam optimizer with = 0.001, 1 = 0.9, 2 = 0.999 and = 1 ? 8, and set warmup steps to 4000. The window size, number of layers, dimension of the model, dropout rate and batch size is set to 100, 4, 512, 0.0 and 64, respectively. For other benchmark models, both embedding and hidden dimensions of DKT, DKVMN and SAKT are searched over <ref type="bibr">[50,</ref><ref type="bibr">100,</ref><ref type="bibr">150,</ref><ref type="bibr">200,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>, and the best results are reported on the test dataset. Also, we set the number of latent concepts as 64 for DKVMN. <ref type="table" target="#tab_2">Table 2</ref> shows the performance comparison of SAINT+ with the benchmark knowledge tracing models. Compared to the benchmark models, SAINT+ achieves increases of ACC and AUC maximally up to 2.72% and 3.61%, respectively. Also, SAINT+ improves SAINT with 1.03% and 1.25% gain in ACC and AUC, respectively, demonstrating the effectiveness of integrating the temporal features for knowledge tracing. We compare different approaches for embedding temporal features: continuous and categorical. Modeling elapsed time in continuous (resp. categorical) fashion assumes that the relationship between a student's knowledge status and the difficulty of a question for the student is a smooth (resp. stepwise) function of time. Similarly, for lag time, continuous (resp. categorical) modeling addresses various aspects in a students' learning process including forgetting, re-organizing concepts and improvement change smoothly (resp. discretely) over time. <ref type="table">Table 3</ref> show that the best result is obtained when using continuous embedding for elapsed time and categorical embedding for lag time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Elapsed Time vs. Lag Time.</head><p>We verify the contribution of each temporal feature by comparing performance improvements of two variants of SAINT: 1) SAINT with elapsed time only, and 2) SAINT with lag time only. The results are described in <ref type="table">Table  Table 3</ref>  The temporal information from the interaction logs, elapsed time and lag time, are provided as decoder features in SAINT+ as described in <ref type="figure" target="#fig_1">Figure 2</ref>. Since both elapsed time and lag time arise as a result of student response, this approach is naturally aligned with the core idea of SAINT that providing exercise information to the encoder and response information to the decoder is appropriate for knowledge tracing. We compare SAINT+ (decoder only) with other two variants: feeding the temporal features to 1) the encoder only, and 2) both the encoder and the decoder. <ref type="table" target="#tab_5">Table 5</ref> summarizes the results. As expected, SAINT+ shows the best performance among the variants. Also, SAINT+ and the variants show better results than SAINT, demonstrating that the temporal features provide useful information for estimating knowledge status. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we proposed SAINT+, a Transformer based knowledge tracing model that processes exercise information and student response information separately, and integrates two temporal feature embeddings into the response embeddings: elapsed time and lag time. Experiments conducted on EdNet dataset show that SAINT+ improves SAINT, the former state-of-the-art knowledge tracing model, in both ACC and AUC. Furthermore, the best result was obtained by incorporating the temporal features into the decoder input, verifying the hypothesis that separately processing exercise information and student response information is appropriate for knowledge tracing. Avenues of future work include 1) modeling not only students' problem-solving records, but also various learning activities, such as watching lectures and studying explanations for each exercise, 2) exploring architectures for knowledge tracing models other than Transformer based encoder-decoder model that separately processes exercise information and student response information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Descriptions of elapsed time and lag time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Model architecture of SAINT+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Distributions of elapsed time and lag time in an updated version of EdNet-KT1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of an updated version of EdNet-KT1 dataset.</figDesc><table><row><cell cols="2"># of interactions 60294498</cell></row><row><cell># of students</cell><cell>678128</cell></row><row><cell># of exercises</cell><cell>14418</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of ACC and AUC between benchmark knowledge tracing models and SAINT+. Temporal Feature Embedding: Continuous vs. Categorical.</figDesc><table><row><cell>Model</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell>DKT</cell><cell cols="2">0.7060 0.7638</cell></row><row><cell cols="3">DKVMN 0.7079 0.7668</cell></row><row><cell>SAKT</cell><cell cols="2">0.7073 0.7663</cell></row><row><cell>SAINT</cell><cell cols="2">0.7178 0.7816</cell></row><row><cell cols="3">SAINT+ 0.7252 0.7914</cell></row><row><cell>5.5 Ablation Test</cell><cell></cell><cell></cell></row><row><cell>5.5.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>: Comparison of different approaches for temporal feature embeddings: continuous and categorical for elapsed time and lag time.</figDesc><table><row><cell>Elapsed time</cell><cell>Lag time</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell cols="4">Continuous Continuous 0.7239 0.7898</cell></row><row><cell cols="4">Continuous Categorical 0.7252 0.7914</cell></row><row><cell cols="4">Categorical Continuous 0.7245 0.7911</cell></row><row><cell cols="4">Categorical Categorical 0.7235 0.7900</cell></row><row><cell cols="4">4. When used alone, lag time increases performance more than</cell></row><row><cell cols="4">elapsed time. Moreover, regardless of the temporal features used,</cell></row><row><cell cols="4">utilizing only one feature is not good as using both features together</cell></row><row><cell cols="4">(SAINT+), while certain improvements are obtained compared to</cell></row><row><cell>SAINT.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Contribution of each temporal feature. ET (resp. LT) stands for elapsed time (resp. lag time) and SAINT+ET (resp. SAINT+LT) only uses elapsed time (resp. lag time). Integrating Temporal Features: Encoder vs. Decoder.</figDesc><table><row><cell></cell><cell>ACC</cell><cell>AUC</cell></row><row><cell>SAINT</cell><cell cols="2">0.7178 0.7816</cell></row><row><cell>SAINT + ET</cell><cell cols="2">0.7213 0.7858</cell></row><row><cell>SAINT + LT</cell><cell cols="2">0.7239 0.7898</cell></row><row><cell cols="3">SAINT + ET + LT 0.7252 0.7914</cell></row><row><cell>5.5.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of methods for integrating temporal features: encoder only (Enc), decoder only (Dec) and both encoder and decoder (Enc+Dec).</figDesc><table><row><cell></cell><cell>ACC</cell><cell>AUC</cell></row><row><cell>SAINT</cell><cell cols="2">0.7178 0.7816</cell></row><row><cell>Enc</cell><cell cols="2">0.7216 0.7866</cell></row><row><cell>Dec</cell><cell cols="2">0.7252 0.7914</cell></row><row><cell cols="3">Enc + Dec 0.7221 0.7872</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">LAK21, April 12-16, 2021, Irvine, CA, USA Dongmin, et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://aitutorsanta.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngnam</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jineon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongmin</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewe</forename><surname>Heo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07033</idno>
		<title level="m">Towards an Appropriate Query, Key, and Value Computation for Knowledge Tracing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ednet: A large-scale hierarchical dataset in education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngnam</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seoyon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seewoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jineon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewe</forename><surname>Heo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence in Education</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John R</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="253" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">More accurate student modeling through contextual estimation of slip and guess probabilities in bayesian knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">T</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aleven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent tutoring systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="406" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context-aware attentive knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew S</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2330" to="2339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EKT: Exercise-aware Knowledge Tracing for Student Performance Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating programming knowledge with Bayesian knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jussi</forename><surname>Kasurinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uolevi</forename><surname>Nikula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCSE Bulletin</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="313" to="317" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Machine Learning Approaches for Learning Analytics: Collaborative Filtering Or Regression With Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongmin</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changho</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngnam</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngduck</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunbin</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanyou</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongku</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10910</idno>
		<title level="m">Creating A Neural Pedagogical Agent by Jointly Learning to Review and Assess</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph-based Knowledge Tracing: Modeling Student Proficiency Using Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiromi</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Iwasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/WIC/ACM International Conference on Web Intelligence (WI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06837</idno>
		<title level="m">A Self-Attentive model for Knowledge Tracing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Navigating the parameter space of Bayesian Knowledge Tracing models: Visualizations of the convergence of the Expectation Maximization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Data Mining</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adapting Bayesian Knowledge Tracing to a Massive Open Online Course in edX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Zachary A Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Seaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pritchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EDM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="137" to="144" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling individualization in a bayesian networks implementation of knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil T</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on User Modeling, Adaptation, and Personalization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="255" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Piech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Does Time Matter? Modeling the Effect of Time with Bayesian Knowledge Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingmei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">A</forename><surname>Pardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil T</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Incorporating scaffolding and tutor context into bayesian knowledge tracing to predict inquiry skill acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Michael Sao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janice</forename><surname>Gobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Data Mining</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional Knowledge Tracing: Modeling Individualization in Student Learning Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuanghong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1857" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recommender system for predicting student performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Nguyen Thai-Nghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artus</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Krohn-Grimberghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2811" to="2819" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">HGKT: Introducing Problem Schema with Hierarchical Exercise Graph for Knowledge Tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanshuang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16915</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Properties of the bayesian knowledge tracing model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brett Van De Sande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Data Mining</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Individualized bayesian knowledge tracing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael V Yudelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Kenneth R Koedinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence in education</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic keyvalue memory networks for knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th international conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
