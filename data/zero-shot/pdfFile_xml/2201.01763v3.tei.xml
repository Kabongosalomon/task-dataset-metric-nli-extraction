<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Self-Supervised Audio-Visual Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Shi</surname></persName>
							<email>bshi@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
							<email>wnhsu@fb.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Meta</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Self-Supervised Audio-Visual Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: audio-visual speech recognition</term>
					<term>self-supervised learning</term>
					<term>representation learning</term>
					<term>robust speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Audio-based automatic speech recognition (ASR) degrades significantly in noisy environments and is particularly vulnerable to interfering speech, as the model cannot determine which speaker to transcribe. Audio-visual speech recognition (AVSR) systems improve robustness by complementing the audio stream with the visual information that is invariant to noise and helps the model focus on the desired speaker. However, previous AVSR work focused solely on the supervised learning setup; hence the progress was hindered by the amount of labeled data available. In this work, we present a self-supervised AVSR framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-theart audio-visual speech representation learning model. On the largest available AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by ? 50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in the presence of babble noise, while reducing the WER of an audio-based model by over 75% (25.8% vs. 5.8%) on average 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the recent development of supervised neural models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, the performance of automatic speech recognition (ASR) systems has improved significantly, achieving human parity <ref type="bibr" target="#b2">[3]</ref> or even outperforming humans on several clean speech benchmarks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. However, ASR systems are vulnerable to noise and may degrade drastically when speech recordings are corrupted with noise <ref type="bibr" target="#b5">[6]</ref>. To make ASR more reliable in various scenarios, research on noise robustness <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> has received increasing attention in recent years.</p><p>An active research direction on noise robustness combines the audio and visual streams of the speaker to utilize the noiseinvariant lip movement information. Audio-visual speech recognition (AVSR) models, which combine these two modalities, bring AI systems one step closer to how humans perceive speech <ref type="bibr" target="#b9">[10]</ref> and provide better performance for a broad range of application scenarios <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> where both audio and visual streams are accessible, e.g., video meetings, talks, interviews.</p><p>Although early studies of audio-visual speech recognition (AVSR) appeared more than 60 years ago <ref type="bibr" target="#b12">[13]</ref>, recent developments on novel model architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> and large-scale data collection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> have brought AVSR performance to new heights. Nonetheless, while modern neural architectures are hungry for large training data, existing research AVSR efforts are fully-supervised, requiring costly labeled data. This limitation hinders the application of modern AVSR systems in low-resource settings, which is the case for most of the ?7,000 spoken languages <ref type="bibr">[17]</ref>.</p><p>This paper presents a self-supervised framework for robust AVSR, which is based upon the recently introduced Audio-Visual HuBERT (AV-HuBERT) pre-training appraoch <ref type="bibr" target="#b16">[18]</ref>. First, large quantities of unlabeled audio-visual speech data are used to pretrain our model to capture the nuanced correlations between sounds and associated lip movements, then only a tiny amount of transcribed audio-visual speech data is used for fine-tuning the model for best AVSR performance. The efficacy of our framework is demonstrated on low-resource (30h) and mid-resource (433h) setups showing WER reductions of up to 50% compared to previous SOTA models. Furthermore, we investigate the robustness of the proposed approach and audio-only systems against different types of noises, which have not been studied in prior work but are essential for practical applications. For example, an AVSR system deployed in meeting scenarios is subject to babble noise, while one used in a home environment naturally encounters music, cooking, or vacuums machine noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>In this section, we present our methodology for audio-visual speech recognition. First, we introduce the Audio-Visual Hu-BERT (AV-HuBERT) pre-training approach, which we use for unsupervised learning of joint representations over audio and visual streams. We then describe how we adopt AV-HuBERT for robust audio-visual speech recognition.   AV-HuBERT <ref type="bibr" target="#b16">[18]</ref> is a self-supervised approach for learning joint speech representations from audio and lip-movement in-formation in video recordings, which extends the HuBERT <ref type="bibr" target="#b17">[19]</ref> speech representation learning framework to multimodal inputs. AV-HuBERT consumes frame-level synchronous audio and video streams as input to produce contextualized audiovisual representations for each frame. AV-HuBERT pretraining iterates over two steps: feature clustering and masked prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">AV-HuBERT for Audio-Visual Speech Recognition</head><p>Feature clustering creates discrete frame-level targets for the subsequent masked prediction step. Audio-based mel-frequency cepstral coefficients (MFCC) features are always used for cluster generation in the first iteration. For multi-iteration pretraining, the learned audio-visual features extracted from the latest AV-HuBERT transformer network are used for cluster generation in all subsequent iterations. Inspired by the BERT pretraining widely used for text data <ref type="bibr" target="#b18">[20]</ref> and deep cluster for visual data <ref type="bibr" target="#b19">[21]</ref>, The masked prediction loss drives training of the AV-HuBERT model by predicting the cluster assignments of the masked frames given a corrupted video signal with randomly masked segments. To finetune it for a downstream task, the cluster prediction head of the pretrained model is removed. Depending on the desired architecture of the final model, either a linear layer is added for an encoder-only model or a randomly initialized decoder module with cross attention over the pretrained encoder is used for a sequence-to-sequence model. Some or all layers may be updated during finetuning.</p><p>Unlike the prior work in <ref type="bibr" target="#b16">[18]</ref>, which utilizes the pretrained AV-HuBERT encoder for unimodal downstream scenarios like lip-reading and ASR, this paper examines the effectiveness of the multimodal learned representations of AV-HuBERT for the multimodal audio-visual speech recognition (AVSR) task that aims to transcribe speech videos using audio and visual streams. Given a pre-trained AV-HuBERT model, we keep both its audio and video frontends during finetuning. We use a sequence-tosequence model for AVSR, where AV-HuBERT serves as the encoder module. In contrast to pretraining, we do not apply input masking or modality dropout during finetuning. Also, we froze the pretrained AV-HuBERT encoder for a certain number of training steps, after which we updated all model weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Noise-Augmented AV-HuBERT</head><p>AVSR systems leverage the visual modality during noisy conditions <ref type="bibr" target="#b13">[14]</ref>; however, A recognizer trained on clean conditions may rely overly on the audio stream since a model can predict with audio more effortlessly, thus leading to failure of leveraging visual information in adverse auditory conditions at test time. A typical solution adopted by prior work is noise-augmented supervised training <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, which adds noise sampled from a separate noise dataset to the clean audio at a fixed or sampled signal-to-noise ratio (SNR). We adopt this strategy during the finetuning stage and refer to it as noise-augmented finetuning to emphasize the stage noise is employed.</p><p>To further boost our model's robustness to acoustic noise, we extended noise augmentation to AV-HuBERT pretraining by randomly adding different types of noise to the audio input, making AV-HuBERT more suitable for AVSR applications. We refer to it as noise-augmented pretraining. Incorporating noise in the pretraining phase benefits the model by closing the domain gap between pretraining, finetuning, and testing. We still use the cluster assignment inferred from clean audio-visual speech because phonetic information, which is highly correlated with the clusters, should be invariant to noise.</p><p>A concurrent work, WavLM <ref type="bibr" target="#b20">[22]</ref>, proposes utterance mixing, which is a similar technique to ours but applied for audio-only speech representation learning. Utterance mixing augments input audio by randomly sampling speech utterances from the same minibatch. We use more diverse sources in our noise-augmented pretraining, including both speech and non-speech noise, e.g., ambient and babble noise. Additionally, since WavLM targets audio-only self-supervised learning, the intersection between the secondary and the primary utterances needs to be fewer than 50% to signify which utterance in the mixture is the main one. Our approach is unconstrained and more flexible on mixing noise because the accompanying visual stream disambiguates the primary and secondary utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data and Experimental Setup</head><p>Our experiments are conducted on LRS3 <ref type="bibr" target="#b15">[16]</ref> with around 433 hours of audio-visual speech from over 5000 speakers, which is the largest publicly available labeled audio-visual speech recognition dataset. VoxCeleb2 <ref type="bibr" target="#b21">[23]</ref>, a large-scale audio-visual speech dataset that was initially proposed for the speaker recognition task is used for our self-supervised pre-training. VoxCeleb2 has around 2,442 hours of videos from over 6,000 speakers and contains utterances from multiple languages. We follow the preprocessing steps in <ref type="bibr" target="#b16">[18]</ref> to select the "English" portion, which amounts to 1,326 hours of videos.</p><p>We augment input samples using many noise categories. The total duration of noise in each category is shown in table 1. The noise audio clips in the categories of "natural", "music" and "babble" are sampled from MUSAN dataset <ref type="bibr" target="#b22">[24]</ref>, while the overlapping "speech" noise samples are drawn from LRS3. In creating "speech" and "babble" noise sets, we ensured there are no speaker overlap among different partitions. We follow the protocol of <ref type="bibr" target="#b16">[18]</ref> to create two settings for finetuning the model; a low-resource setting using 30h of labeled videos and a mid-resource setting using 433h of labels. Unless otherwise specified, we use AV-HuBERT LARGE as the default model architecture for all our experiments. The model has 24 transformer blocks, where each block has 16 attention heads and 1024/4096 embedding/feedforward dimensions. We add a 9-layer randomly initialized transformer decoder with similar embedding/feedforward dimensions during finetuning.</p><p>During training, we first select one noise category and sample a noise audio clip from its training partition. We randomly mix the sampled noise at 0dB SNR with a probability of 0.25, following <ref type="bibr" target="#b14">[15]</ref>. At test time, we evaluate the model separately for each noise type. The testing noise clips are added at five SNR levels: {?10, ?5, 0, 5, 10}dB. The performance on the original clean test set is also reported for comparison. By default, noise clips are added during both pre-training and finetuning. We follow the training pipeline in <ref type="bibr" target="#b16">[18]</ref>, where the model is trained for five iterations in total. To save the computation time, we always use the smaller BASE model architecutre <ref type="bibr" target="#b16">[18]</ref> in all iterations except the last one, where we use a LARGE model. Video samples are batched together not to exceed 1000 image frames per GPU. The model is pre-trained with 600K steps using 64 V100-GPUs and finetuned for 30K/100K steps respectively in 30h/433h setting. <ref type="table">Table 2</ref> compares the performance of our proposed noiseaugmented AV-HuBERT approach under different settings versus existing supervised AVSR models. In the clean audio setting, our best model outperforms the best model from Ma et al. <ref type="bibr" target="#b24">[26]</ref> by 39.0% (2.3%?1.4%) while using fewer labeled data. To enable direct comparison with previous research work which focus primarily on babble noise, we follow <ref type="bibr" target="#b14">[15]</ref> to synthesize babble noise by randomly mixing 30 audio clips from LRS3 2 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Main Results</head><p>As shown in the "Babble" column, with only 30 hours of labeled data, our model outperforms <ref type="bibr" target="#b13">[14]</ref> by 80.4% (42.5%?8.3%) and <ref type="bibr" target="#b14">[15]</ref> by 67.4% (25.5%?8.3%) at 0dB SNR. Compared to the former SOTA <ref type="bibr" target="#b14">[15]</ref>, we achieve 49.6% lower WER (28.0% ? 14.1%) on average across different SNR ratios with 10 times fewer labels. When using all 433 hours labeled data for finetuning, the relative improvement is further increased to 55.7% (28.0% ? 12.4%). Note that the babble noise we use for training our model is synthesized from MUSAN, which has potential domain mismatch from the babble noise synthesized from LRS3 used at test time; however, our approach significantly improves over prior work and estableshes the new SOTA.</p><p>When the noise type is extended beyond babble noise, our proposed audio-visual model consistently improves over its audio-only ASR counterpart with is more than 70% relative WER reduction. The reduction varies depending on noise type and SNR; hence, we analyze how the model performs in different noise conditions and how each component of our approach contributes to such improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis</head><p>To examine the impact of pre-training (no pre-training, pretraining with clean audio, or noise-augmented pre-training) and input modality (audio or audio-visual), we experimented with the six setups covering the cross product of these conditions. For setups with audio-only input during finetuning, we follow <ref type="bibr" target="#b16">[18]</ref> by replacing the visual features in the pre-trained AV-HuBERT model with a dummy zero vector at each frame. Performances of these six setups are shown in table 3. <ref type="figure" target="#fig_3">Figure 2</ref> shows a more detailed performance breakdown over SNR ratios and noise types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Effect of the visual modality</head><p>We first examine the performance of AVSR models against audioonly ASR models under low-resource and mid-resource conditions by comparing the blue and yellow bars of the same shading pattern in each group in <ref type="figure" target="#fig_3">figure 2</ref> and audio-only vs. audio-visual columns in <ref type="table" target="#tab_2">Table 3</ref>. AVSR consistently outperforms audio-only ASR under all settings regardless of the SNR and the type of noise, except for the setup where the model is trained on only 30 hours of labeled data from scratch.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, the benefit of incorporating the visual stream is more apparent in challenging scenarios, where the WER degradation relative to the clean condition is large. Specifically, these scenarios include low SNR conditions where the volume of the noise is higher and noisy environments with speech or babble noise where the interfering signal is similar to the target speech. Averaged over different pre-training configurations, the AVSR model achieves 53.0% (42.6% ? 20.0%) and 70.8% (31.3% ? 9.2%) relative WER reduction over audio-only ASR under noisy settings using 30 hours and 433 hours of labeled data, respectively.</p><p>It is worth noting that our AVSR model enjoys its largest gain over the audio-only model under speech noise settings, where a secondary speech utterance is randomly mixed into the primary one. When using overlapping speech noise under the mid-resource setting, the WER went from 48.1% to 7.7% by AVSR on average across different pre-training configurations, while the WER is reduced from 25.8% to 9.6% in the other three noise categories. These results suggest that the paired visual stream provides an effective audio source separation, where the audio-only recognizer can not distinguish two audio tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Effect of pre-training</head><p>The performance of fine-tuning an AV-HuBERT model is compared against directly optimizing a model from scratch on labeled audio-visual speech in (AV, PT=Clean) vs. (AV, PT=None) bars of <ref type="figure" target="#fig_3">Figure 2</ref> and (Clean vs. None) rows (i.e., (b) vs. (a) and (e) vs. (d)) in <ref type="table" target="#tab_2">Table 3</ref>. Note that the AV-HuBERT model pre-trained on clean audio (PT=clean) is identical to the one used in <ref type="bibr" target="#b16">[18]</ref>.</p><p>On average, AV-HuBERT pre-training brings substantial relative improvements of 78.3% (42.9%?9.3%) and 53.4% (14.8%?6.9%) when using 30h and 433h of labeled data, respectively. The model achieves bigger gains in the low-resource setting, which confirms the impact of the self-supervised audiovisual representations learned by the AV-HuBERT model. <ref type="table">Table 2</ref>: WER (%) of our models and prior work on the LRS3 dataset. "Mode" denotes whether a model uses audio-visual input (AV) or only audio as input (A). "Hr" denotes the amount of labeled audio-visual speech data used in each system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Mode Hr Babble, SNR= Speech, SNR= Music+Natural, SNR= Clean -10 -5 0 5 10 avg -10 -5 0 5 10 avg -10 -5 0  </p><formula xml:id="formula_0">5 10 avg ? Makino et al. [25] AV 31K - - - - - - - - - - - - - - - - - - 4.5 Ma et al. [26] AV 595 - - - - - - - - - - - - - - - - - - 2.3 Afouras et al. [14] AV 1.4K - - 42.5 - - - - - - - - - - - - - - - 7.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Effect of noise-augmented pre-training</head><p>The impact of incorporating noise in pre-training is presented in (AV, PT=Noisy) vs. (AV, PT=Clean) bars from <ref type="figure" target="#fig_3">Figure 2</ref> and (Noisy vs. Clean) rows in <ref type="table" target="#tab_2">Table 3</ref> (i.e., (b) vs. (c) and (e) vs. (f)).</p><p>Overall the noise-augmented pre-training improves the result in noisy settings. The WER is reduced by 16.1% (9.3%?7.8%) / 15.9% (6.9%?5.8%) on average in low-resource (30h) and midresource (433h) settings compared to pre-training on clean data. Compared to an audio-visual model trained from scratch, the noise-augmented pre-training approach reduces recognition error by 81.8% (42.9%?7.8%) / 60.8% (14.8%?5.8%) in low/midresource settings. Concerning SNR, the WER is reduced the most in low SNR settings, i.e., high noise, as is shown in <ref type="figure" target="#fig_3">Figure 2</ref>. This observation matches our hypothesis made in section 3.3.2 about the domain discrepancy between pre-training and finetuning. Introducing noise during the pre-training stage bridges the domain gap and makes the model more resilient to noise at test time.</p><p>One key takeaway from this work is that noise augmentation is needed during pre-training and finetuning phases to achieve the best AVSR performance in adverse acoustic conditions. Compared to training from scratch, the gain of noise-augmented pre-training peaks around 0dB SNR ratio, which matches the SNR used in training.</p><p>Compared to "babble", "music" and "natural" noise types, noise-augmented AV-HuBERT pre-training is more effective in overlapping "speech" noise as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Consistent with previous findings when comparing audio-visual and audio-only recognizers trained from scratch, the visual modality provides a strong clue to "choose" the target speech track. AV-HuBERT effectively learns visual representation from paired audio-visual data, leading to significant gains in speech separation.</p><p>The gains from noise-augmented pre-training generalize across model architectures, as shown in table 4. In addition, regardless of the finetuning strategy, our proposed pre-training approach is helpful, as is shown by row (b) vs. row (c) and row (e) vs. row (f) in N-WER in <ref type="table" target="#tab_3">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>This paper presented a new state-of-the-art audio-visual speech recognition (AVSR) model based on the AV-HuBERT approach for multimodal speech representation learning. To our knowledge, this is the first attempt towards building an AVSR model using a large volume of unlabeled audio-visual speech data. Our audio-visual speech recognizer achieves high recognition accuracy and is robust to different noise categories even with a few hours of labeled data. With less than 10% of labeled data, our model outperforms prior SOTA by ? 50%. Our future work includes applying audio-visual speech recognition in real-world low-resource and multilingual settings.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>AV-HuBERT for audio-visual speech recognition. X: mask; blue waveform: original audio; orange waveform: noise; Cn: audio-visual clusters. Dashed box: the pre-trained part</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of models using different inputs and pretraining methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Total duration in hours of noise samples in different categories</figDesc><table><row><cell>Partition</cell><cell cols="4">natural music babble speech</cell></row><row><cell>train</cell><cell>6</cell><cell>35</cell><cell>20</cell><cell>50</cell></row><row><cell>validation</cell><cell>1</cell><cell>4</cell><cell>2</cell><cell>6</cell></row><row><cell>test</cell><cell>1</cell><cell>4</cell><cell>2</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison among models with different pre-training configurations and input modalities. C: clean audio, N: noisy audio. The N-WER is averaged over 4 noise types and 5 SNRs.</figDesc><table><row><cell>Model</cell><cell>PT</cell><cell>FT</cell><cell cols="2">Audio-only</cell><cell cols="2">Audio-visual</cell></row><row><cell>Size</cell><cell>Type</cell><cell cols="5">Data C-WER N-WER C-WER N-WER</cell></row><row><cell cols="2">(a). LARGE None</cell><cell>30h</cell><cell>20.6</cell><cell>59.2</cell><cell>20.8</cell><cell>42.9</cell></row><row><cell cols="2">(b). LARGE Clean</cell><cell>30h</cell><cell>4.3</cell><cell>39.8</cell><cell>3.3</cell><cell>9.3</cell></row><row><cell cols="2">(c). LARGE Noisy</cell><cell>30h</cell><cell>3.8</cell><cell>28.7</cell><cell>3.3</cell><cell>7.8</cell></row><row><cell cols="3">(d). LARGE None 433h</cell><cell>4.7</cell><cell>39.2</cell><cell>3.5</cell><cell>14.8</cell></row><row><cell cols="3">(e). LARGE Clean 433h</cell><cell>1.5</cell><cell>29.1</cell><cell>1.4</cell><cell>6.9</cell></row><row><cell cols="3">(f). LARGE Noisy 433h</cell><cell>1.6</cell><cell>25.8</cell><cell>1.4</cell><cell>5.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison among BASE models with different pretraining configurations and input modalities. The model is finetuned with 30h labeled data. C: clean audio, N: noisy audio. The N-WER is averaged over 4 noise types and 5 SNR ratios.</figDesc><table><row><cell>Model</cell><cell>PT</cell><cell>FT</cell><cell cols="2">Audio-only</cell><cell>Audio-visual</cell></row><row><cell>Size</cell><cell>Type</cell><cell>Type</cell><cell cols="3">C-WER N-WER C-WER N-WER</cell></row><row><cell cols="3">(a). BASE None Clean</cell><cell>24.6</cell><cell>79.8</cell><cell>22.0</cell><cell>70.9</cell></row><row><cell cols="3">(b). BASE Clean Clean</cell><cell>4.6</cell><cell>46.3</cell><cell>4.0</cell><cell>28.2</cell></row><row><cell cols="3">(c). BASE Noisy Clean</cell><cell>4.4</cell><cell>33.8</cell><cell>4.1</cell><cell>12.5</cell></row><row><cell cols="3">(d). BASE None Noisy</cell><cell>16.9</cell><cell>55.4</cell><cell>17.2</cell><cell>39.5</cell></row><row><cell cols="3">(e). BASE Clean Noisy</cell><cell>4.8</cell><cell>37.3</cell><cell>4.2</cell><cell>13.1</cell></row><row><cell cols="3">(f). BASE Noisy Noisy</cell><cell>4.4</cell><cell>33.3</cell><cell>4.1</cell><cell>10.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Test WER (%) of LARGE AV-HuBERT under different levels and types of noise. Lower is better. B: babble, S: speech, M: music, N: natural noise.</figDesc><table><row><cell>SNR (dB)</cell><cell></cell><cell cols="2">PT: None</cell><cell></cell><cell></cell><cell cols="2">PT: Clean</cell><cell></cell><cell></cell><cell cols="2">PT: Noisy</cell><cell></cell></row><row><cell>A,30h</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell></row><row><cell>-10</cell><cell cols="12">103.1 95.7 83.4 77.8 100.7 101.9 66.1 58.2 99.6 77.3 50.5 45.2</cell></row><row><cell>-5</cell><cell cols="4">90.3 86.9 65.9 61.0</cell><cell>87.5</cell><cell cols="7">91.4 39.4 36.0 69.3 51.2 21.5 21.5</cell></row><row><cell>0</cell><cell cols="4">63.9 70.5 46.4 46.6</cell><cell>37.6</cell><cell cols="5">63.9 17.4 15.9 21.9 32.0</cell><cell>9.0</cell><cell>9.3</cell></row><row><cell>5</cell><cell cols="4">43.2 52.2 35.2 34.3</cell><cell>11.8</cell><cell>25.4</cell><cell>8.2</cell><cell>7.6</cell><cell cols="2">9.0 19.7</cell><cell>5.9</cell><cell>5.8</cell></row><row><cell>10</cell><cell cols="4">32.1 39.9 28.0 27.8</cell><cell>6.7</cell><cell>9.5</cell><cell>5.9</cell><cell>5.5</cell><cell cols="2">5.6 10.8</cell><cell>4.9</cell><cell>4.6</cell></row><row><cell>clean</cell><cell></cell><cell>20.6</cell><cell></cell><cell></cell><cell></cell><cell>4.3</cell><cell></cell><cell></cell><cell></cell><cell>3.8</cell><cell></cell><cell></cell></row><row><cell>A,433h</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell></row><row><cell>-10</cell><cell cols="4">100.7 95.3 64.7 55.6</cell><cell>98.2</cell><cell cols="7">94.3 47.4 39.3 97.5 81.7 40.8 36.5</cell></row><row><cell>-5</cell><cell cols="4">82.3 79.7 38.2 34.0</cell><cell>65.6</cell><cell cols="7">73.8 18.7 17.2 62.3 56.2 15.3 14.9</cell></row><row><cell>0</cell><cell cols="4">39.2 52.8 18.8 17.7</cell><cell>17.0</cell><cell>46.3</cell><cell>6.5</cell><cell cols="3">6.4 15.7 37.3</cell><cell>5.7</cell><cell>5.6</cell></row><row><cell>5</cell><cell cols="4">17 28.4 10.7 10.6</cell><cell>5.3</cell><cell>22.9</cell><cell>3.0</cell><cell>3.4</cell><cell cols="2">5.1 19.0</cell><cell>3.1</cell><cell>3.1</cell></row><row><cell>10</cell><cell cols="2">8.4 15.7</cell><cell>7.5</cell><cell>7.3</cell><cell>2.7</cell><cell>9.7</cell><cell>2.0</cell><cell>2.2</cell><cell>2.6</cell><cell>8.3</cell><cell>2.3</cell><cell>2.3</cell></row><row><cell>clean</cell><cell></cell><cell>4.7</cell><cell></cell><cell></cell><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell>1.6</cell><cell></cell><cell></cell></row><row><cell>AV,30h</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell></row><row><cell>-10</cell><cell cols="4">80.3 62.7 60.3 55.6</cell><cell>32.2</cell><cell cols="7">18.1 16.4 13.2 30.7 11.5 12.5 11.4</cell></row><row><cell>-5</cell><cell cols="4">63.1 52.1 46.8 44.9</cell><cell>18.5</cell><cell>10.4</cell><cell>9.3</cell><cell cols="2">8.0 15.9</cell><cell>6.8</cell><cell>7.3</cell><cell>6.6</cell></row><row><cell>0</cell><cell cols="4">45.1 42.7 36.2 35.0</cell><cell>8.7</cell><cell>6.6</cell><cell>5.6</cell><cell>5.2</cell><cell>7.3</cell><cell>5.0</cell><cell>4.9</cell><cell>4.7</cell></row><row><cell>5</cell><cell cols="4">33.3 34.8 29.5 28.5</cell><cell>4.8</cell><cell>4.8</cell><cell>4.3</cell><cell>4.1</cell><cell>4.4</cell><cell>4.2</cell><cell>4.1</cell><cell>4.0</cell></row><row><cell>10</cell><cell cols="4">27.2 29.4 25.4 25.2</cell><cell>3.7</cell><cell>4.0</cell><cell>3.7</cell><cell>3.7</cell><cell>3.9</cell><cell>3.9</cell><cell>3.6</cell><cell>3.7</cell></row><row><cell>clean</cell><cell></cell><cell>20.8</cell><cell></cell><cell></cell><cell></cell><cell>3.3</cell><cell></cell><cell></cell><cell></cell><cell>3.3</cell><cell></cell><cell></cell></row><row><cell>AV,433h</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell></row><row><cell>-10</cell><cell cols="4">60.2 26.5 29.5 24.0</cell><cell>30.0</cell><cell cols="5">15.9 13.8 10.3 28.4 11.4</cell><cell>9.9</cell><cell>9.4</cell></row><row><cell>-5</cell><cell cols="4">33.1 15.1 14.9 12.4</cell><cell>15.2</cell><cell>7.5</cell><cell>6.4</cell><cell cols="2">5.4 13.4</cell><cell>4.6</cell><cell>4.8</cell><cell>4.6</cell></row><row><cell>0</cell><cell>14</cell><cell>8.8</cell><cell>7.9</cell><cell>8.0</cell><cell>5.9</cell><cell>3.9</cell><cell>3.3</cell><cell>2.9</cell><cell>5.0</cell><cell>2.9</cell><cell>2.5</cell><cell>2.5</cell></row><row><cell>5</cell><cell>6.8</cell><cell>6.2</cell><cell>5.0</cell><cell>5.3</cell><cell>2.7</cell><cell>2.4</cell><cell>2.1</cell><cell>2.2</cell><cell>2.6</cell><cell>2.2</cell><cell>1.9</cell><cell>1.9</cell></row><row><cell>10</cell><cell>4.6</cell><cell>5.1</cell><cell>4.0</cell><cell>4.3</cell><cell>1.9</cell><cell>1.9</cell><cell>1.7</cell><cell>1.8</cell><cell>1.9</cell><cell>1.8</cell><cell>1.8</cell><cell>1.7</cell></row><row><cell>clean</cell><cell></cell><cell>3.5</cell><cell></cell><cell></cell><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell>1.4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Test WER (%) of BASE AV-HuBERT fine-tuned with 30 hours of labeled data under different levels and types of noise. Lower is better. B: babble, S: speech, M: music, N: natural noise.</figDesc><table><row><cell>SNR (dB)</cell><cell></cell><cell>PT: None</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PT: Clean</cell><cell></cell><cell></cell><cell cols="2">PT: Noisy</cell><cell></cell></row><row><cell>A,30h (Clean-FT)</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell></row><row><cell>-10</cell><cell cols="4">111.7 101.1 93.8 91.3</cell><cell cols="8">99.4 99.5 74.5 65.7 96.4 88.1 53.6 47.7</cell></row><row><cell>-5</cell><cell>110.5</cell><cell cols="3">98.0 88.3 84.1</cell><cell cols="8">92.9 91.5 51.9 45.1 74.8 66.0 27.5 26.5</cell></row><row><cell>0</cell><cell>99.8</cell><cell cols="3">92.4 76.7 72.7</cell><cell cols="8">57.3 74.2 25.5 23.7 29.7 48.0 12.3 12.6</cell></row><row><cell>5</cell><cell>79.3</cell><cell cols="3">81.3 58.7 55.6</cell><cell cols="6">21.2 38.7 11.2 12.4 12.1 31.4</cell><cell>7.7</cell><cell>7.5</cell></row><row><cell>10</cell><cell>52.6</cell><cell cols="3">60.8 43.5 44.0</cell><cell cols="2">9.4 17.6</cell><cell>7.4</cell><cell>7.5</cell><cell cols="2">7.1 15.3</cell><cell>6.1</cell><cell>5.9</cell></row><row><cell>clean</cell><cell></cell><cell>24.6</cell><cell></cell><cell></cell><cell></cell><cell>4.6</cell><cell></cell><cell></cell><cell></cell><cell>4.4</cell><cell></cell><cell></cell></row><row><cell>A,30h (Noisy-FT)</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell></row><row><cell>-10</cell><cell>103.1</cell><cell cols="11">94.3 80.2 75.3 107.0 92.2 65.6 55.8 98.6 87.5 53.2 46.8</cell></row><row><cell>-5</cell><cell>89.4</cell><cell cols="3">86.3 61.7 55.9</cell><cell cols="8">81.2 74.9 38.9 31.8 74.5 65.1 26.6 24.5</cell></row><row><cell>0</cell><cell>59.2</cell><cell cols="3">68.0 40.7 39.5</cell><cell cols="8">35.8 43.6 17.3 16.3 28.5 47.1 11.8 11.9</cell></row><row><cell>5</cell><cell>38.3</cell><cell cols="3">48.6 28.8 29.0</cell><cell cols="2">13.8 20.7</cell><cell>9.1</cell><cell cols="3">9.8 11.3 31.1</cell><cell>7.1</cell><cell>7.5</cell></row><row><cell>10</cell><cell>26.6</cell><cell cols="3">35.3 22.9 24.8</cell><cell cols="2">7.5 10.3</cell><cell>6.8</cell><cell>6.7</cell><cell cols="2">6.7 14.8</cell><cell>5.8</cell><cell>5.6</cell></row><row><cell>clean</cell><cell></cell><cell>16.9</cell><cell></cell><cell></cell><cell></cell><cell>4.8</cell><cell></cell><cell></cell><cell></cell><cell>4.4</cell><cell></cell><cell></cell></row><row><cell>AV,30h (Clean-FT)</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell></row><row><cell>-10</cell><cell>103.5</cell><cell cols="3">97.0 89.6 85.8</cell><cell cols="8">84.8 92.4 44.7 33.2 48.5 29.3 21.3 17.5</cell></row><row><cell>-5</cell><cell>99.7</cell><cell cols="3">92.4 81.3 76.5</cell><cell cols="8">49.6 75.0 24.0 18.0 24.7 12.9 11.1 10.1</cell></row><row><cell>0</cell><cell>86.6</cell><cell cols="3">83.5 66.2 60.6</cell><cell cols="3">19.7 41.1 11.2</cell><cell cols="2">9.3 11.4</cell><cell>7.7</cell><cell>6.9</cell><cell>6.7</cell></row><row><cell>5</cell><cell>65.8</cell><cell cols="3">69.0 49.2 45.9</cell><cell cols="2">8.7 15.6</cell><cell>6.5</cell><cell>6.5</cell><cell>6.3</cell><cell>5.8</cell><cell>5.1</cell><cell>5.1</cell></row><row><cell>10</cell><cell>42.2</cell><cell cols="3">50.5 35.7 36.1</cell><cell>5.5</cell><cell>7.8</cell><cell>5.0</cell><cell>5.2</cell><cell>4.7</cell><cell>5.0</cell><cell>4.4</cell><cell>4.6</cell></row><row><cell>clean</cell><cell></cell><cell>22.0</cell><cell></cell><cell></cell><cell></cell><cell>4.0</cell><cell></cell><cell></cell><cell></cell><cell>4.1</cell><cell></cell><cell></cell></row><row><cell>AV,30h (Noisy-FT)</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell><cell>B</cell><cell>S</cell><cell>M</cell><cell>N</cell></row><row><cell>-10</cell><cell>79.4</cell><cell cols="3">61.1 57.5 54.7</cell><cell cols="8">42.1 27.2 23.6 18.3 38.5 15.8 17.9 15.1</cell></row><row><cell>-5</cell><cell>60.6</cell><cell cols="3">49.4 42.9 40.0</cell><cell cols="5">25.4 17.0 13.3 11.1 21.2</cell><cell cols="2">9.6 10.0</cell><cell>8.9</cell></row><row><cell>0</cell><cell>42.0</cell><cell cols="3">38.8 32.4 30.7</cell><cell cols="2">12.8 10.0</cell><cell>7.9</cell><cell>7.4</cell><cell>9.9</cell><cell>6.7</cell><cell>6.5</cell><cell>6.2</cell></row><row><cell>5</cell><cell>28.9</cell><cell cols="3">30.3 25.6 25.1</cell><cell>7.0</cell><cell>6.8</cell><cell>5.8</cell><cell>5.6</cell><cell>5.9</cell><cell>5.5</cell><cell>5.0</cell><cell>5.2</cell></row><row><cell>10</cell><cell>22.8</cell><cell cols="3">24.4 21.4 21.3</cell><cell>5.3</cell><cell>5.2</cell><cell>4.9</cell><cell>4.8</cell><cell>4.8</cell><cell>4.8</cell><cell>4.3</cell><cell>4.5</cell></row><row><cell>clean</cell><cell></cell><cell>17.2</cell><cell></cell><cell></cell><cell></cell><cell>4.2</cell><cell></cell><cell></cell><cell></cell><cell>4.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b13">[14]</ref> uses audios from LRS2<ref type="bibr" target="#b13">[14]</ref>, which has restricted access and we are unable to obtain it.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Kingsbury</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep speech 2 : End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Achieving human parity in conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Single headed attention based sequence-to-sequence model for state-ofthe-art results on switchboard-300</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>T?ske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Super-human performance in online low-latency recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stueker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Waibel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<idno>abs/2004.09249</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of environment, microphone and data simulation mismatches in robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving noise robust automatic speech recognition with single-channel time-domain enhancement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ochiai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macdoald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple cameras audio visual speech recognition using active appearance model visual features in car environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A mobile command input through vowel lip shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oharada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sawada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shizuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>HCI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual contribution to speech intelligibility in noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Sumby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="212" to="215" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative multi-modality speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lrs3-ted: a large-scale dataset for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning audio-visual speech representation by masked multimodal cluster prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hubert: Self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wavlm: Large-scale selfsupervised pre-training for full stack speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno>abs/2110.13900</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Musan: A music, speech, and noise corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<idno>abs/1510.08484</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Recurrent neural network transducer for audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Braga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Siohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end audio-visual speech recognition with conformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP, 2021. 6 shows the WERs of LARGE and BASE AV-HuBERT models under various noise types and SNR levels</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
