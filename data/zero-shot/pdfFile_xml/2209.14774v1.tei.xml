<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RECALL: Rehearsal-free Continual Learning for Object Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Knauer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Denninger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Triebel</surname></persName>
						</author>
						<title level="a" type="main">RECALL: Rehearsal-free Continual Learning for Object Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks show remarkable results in classification but struggle with learning new things on the fly. We present a novel rehearsal-free approach, where a deep neural network is continually learning new unseen object categories without saving any data of prior sequences. Our approach is called RECALL, as the network recalls categories by calculating logits for old categories before training new ones. These are then used during training to avoid changing the old categories. For each new sequence, a new head is added to accommodate the new categories. To mitigate forgetting, we present a regularization strategy where we replace the classification with a regression. Moreover, for the known categories, we propose a Mahalanobis loss that includes the variances to account for the changing densities between known and unknown categories. Finally, we present a novel dataset for continual learning (HOWS-CL-25), especially suited for object recognition on a mobile robot, including 150,795 synthetic images of 25 household object categories. Our approach RECALL outperforms the current state of the art on CORe50 and iCIFAR-100 and reaches the best performance on HOWS-CL-25.</p><p>? Proposal of RECALL: A method to mitigate forgetting in a rehearsal-free continual learning environment.</p><p>? Analysis of the logit output distribution discrepancies of RECALL, which are caused by learning over different sequences. Evaluation and proposal of different solutions to cope with these discrepancies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Humans are remarkable in extracting new knowledge about unknown things continuously throughout their lifetime. Thus, lifelong learning is a crucial capability in our daily life. Deep neural networks have shown excellent results on many problems, from recognition to reconstruction tasks in computer vision and more <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Typically, these algorithms apply batch-wise training to large datasets, e.g., ImageNet by Deng et al. <ref type="bibr" target="#b4">[5]</ref>, and then need many iterations over the whole dataset to obtain satisfactory performance. In contrast to humans, neural networks rely on a static dataset, which has to be fixed before the training starts. For new categories that have not been part of the original training set, the network has to be trained again, while the original training set has to be kept in memory (rehearsal) to prevent forgetting. It is not practical to keep relearning previously seen categories as it is time and memory-consuming. So in this work, we present a novel approach to do Rehearsalfree Continual Learning called RECALL, which can be used to enable a network to learn new categories on-demand without the need of retraining the whole network. Therefore, the experiments are designed for a class-incremental learning (Class-IL) <ref type="bibr" target="#b5">[6]</ref>, schematized in <ref type="figure" target="#fig_1">Fig. 1</ref>, where each sequence contains a collection of images of a non-reoccurring fixed amount of categories. This means that a category is only present in one sequence. In order to get closer to a lifelong learning approach, we propose a rehearsal-free approach, <ref type="bibr" target="#b0">1</ref> Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Oberpfaffenhofen, Germany first.last@dlr.de  which does not require storing any visual history like most other solutions in literature <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The main contributions of this work are: sequences, which are then frozen after training. In contrast to that, in our work, all weights are trained at the same time, enabling the network to learn those differences and prevent it from forgetting.</p><p>In literature, learning without forgetting (LwF) <ref type="bibr" target="#b10">[11]</ref> is often used for comparison <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>. They propose a regularization strategy to stabilize the model accuracy on old tasks using knowledge distillation, proposed by Hinton et al. <ref type="bibr" target="#b14">[15]</ref>. Here, the logits of the previous and current sequences are encouraged to be similar when applied to data from the new sequence. Further, they propose a form of knowledge distillation, where they record a set of label probabilities for each training image on the previous network weights. In LWF, they compensate for the distribution shift in the predicted probability distribution of different sequences by first freezing the old neurons and only training the new neurons in a warm-up step. Whereas in our work, we record the logits directly instead of modeling them through a probability distribution. Further, we propose to use a regression loss to compensate for distribution shifts, as it removes them completely.</p><p>Another regularization strategy is Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b15">[16]</ref>, proposed by Kirkpatrick et al., which tries to protect important weights from changing by using a Fisher weight importance matrix. Another one is Synaptic Intelligence (SI) <ref type="bibr" target="#b12">[13]</ref>, by <ref type="bibr">Zenke et al.,</ref> where they propose calculating the weight importance on the fly, using Stochastic Gradient Descent. Both approaches are rooted in neural science, as they argue that a biological synapse accumulates task-relevant information over time and stores new memories without forgetting old ones. In this work, we change all weights on purpose to give the network the ability to learn all kinds of separations, and we tackle the forgetting by enforcing that the network's outputs don't change.</p><p>The most similar approach to our work is AR1 by Maltoni and Lomonaco <ref type="bibr" target="#b11">[12]</ref>. It is a combination of an architectural strategy (CWR+) and a regularization strategy (SI). AR1 uses importance weighting and therefore needs the entire backbone during training. They also zero-init their last layer and propose to use mean-shifting to improve their continual learning performance. In RECALL, on the other hand, only the backbone is frozen and used to generate the features beforehand, thus decreasing training time. Furthermore, each sequence gets its own head to improve the overall capacity of the network and its accuracy instead of just one single layer in AR1. Lastly, AR1 uses a classification loss instead of a regression loss.</p><p>All previously mentioned approaches are rehearsal-free and therefore comparable to ours, as no examples of previous tasks are available here. Another way of continual learning is given by rehearsal strategies, where previous training examples or representations of them are stored and used in later training steps <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Two relevant methods are Incremental Classifier and Representation Learning (iCaRL), by Rebuffi et al. <ref type="bibr" target="#b6">[7]</ref>, and Gradient Episodic Memory (GEM) from Lopez-Patz and Ranzato <ref type="bibr" target="#b7">[8]</ref>. In iCaRL, they propose a class-incremental algorithm to learn new classes over time.</p><p>It relies on a regularization strategy in a nearest-mean-ofexemplar classification and a rehearsal strategy by saving feature representations in each sequence. Like LWF, iCaRL uses a combination of knowledge distillation and classification loss to train the network. GEM uses the same strategies, but while iCaRL is designed to fill the whole memory after every batch, GEM uses a fixed amount of memory for each batch, a so-called episodic memory. Instead of keeping the predictions of past sequences invariant by using distillation, GEM uses the losses as an inequality constraint. This avoids their increase but allows their decrease in order to make the positive backward transfer possible. Like Rebuffi et al. <ref type="bibr" target="#b6">[7]</ref>, our approach focuses on keeping the predictions of past sequences invariant and does not consider the possibility of a positive backward transfer proposed in GEM. Another approach is Persistent Anytime Learning of Objects from Unseen Classes (PAL) by Denninger et al. <ref type="bibr" target="#b19">[20]</ref>. Similar to us, they also use CNNs for feature extraction, but instead of fully connected layers, they use a random forest classifier, which also relies on saving previously seen data samples. In their online learning scenario, they found that the removal of trees leads to catastrophic forgetting and that the performance of learning new categories decreases over time.</p><p>These approaches reduce forgetting by saving samples from previous sequences. However, our overarching goal in this work is to get closer to a system that is able to learn continually without depending on storing previously seen examples. A lifelong learning strategy should not contain the requirement to store everything it has ever seen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RECALL</head><p>This chapter introduces RECALL, our algorithm for rehearsal-free object classification on 2D images for continual learning. We selected ResNet50 as a feature extractor and backbone, which is pre-trained on Imagenet <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architectural Strategies</head><p>Due to the continual procedure, where new categories are shown in sequences, we propose to let the network grow in each sequence. This is depicted in <ref type="figure" target="#fig_1">Fig. 1</ref> and <ref type="figure" target="#fig_10">Fig. 2</ref>. Here, the first sequence is learned as in a classical CNN. For each following sequence, we propose to add a new head. Each newly added head has two fully-connected layers, called fc[s, 0] and fc[s, 1] with the sequence number s. Whereas each fc[s, 1] has the same amount of outputs as categories C s are in sequence s. During the training of this new sequence, all weights of all added heads are trained, and only the backbone is kept frozen. The input to each head is the same 2048 feature vector. By doing this, the network can better address the feature separation in specific sequences. This also avoids that the training of the categories of a new sequence changes any of the weights of prior sequences heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Regularization Strategies</head><p>The next step is to tackle catastrophic forgetting, which describes the decreasing performance on the categories of prior sequences when new sequences are trained. Our goal is now to prevent the logits of the categories of previous sequences from changing. 1) Recall label: In the first sequence, image-label pairs (x, y) are used for training, where the label y represents the one-hot encoded category of an object. In the following sequences, however, these one-hot encoded categories might lead to catastrophic forgetting, as the ground truth labels for categories, which are no longer present, are zero. To reach this zero value during training, the categories logit values of the network have to continually move into the negative, as they can only reach zero in the softmax if x ? ?? in e x . This then changes the previously learned categories by forcing them to predict zeros regardless of the input. The logit value here is the input to the final softmax. In order to prevent this forgetting, we propose another label for already learned categories. As a label represents a value, which the network's output should target, we propose to use the original logit output on the training image. We call this value recall label r. This is similar to the approach from Li et al. <ref type="bibr" target="#b10">[11]</ref>, but they save the final softmax output and not the logit itself. We argue that the softmax output already has too much influence from the other classes making the decoupling harder.</p><p>But, the labels y? from any previous sequence? are no longer present in the next sequence s. Thus, we only have the image-label pairs (x s , y s ). Therefore, we propose replacing all zeros in the current y s , which stand for the previous categories' labels with our recall label r s . This recall label r s is then compared to the output of fc[0, 1] at the current sequence s. The training procedure for this is shown in Algorithm 1, where the calculation of the recall label happens in line three. This procedure is also depicted in <ref type="figure" target="#fig_10">Fig. 2</ref>.</p><p>Given the model f (x) from our method, sequences S ? N 0 and categories C ? N 0 , where each sequence s ? S has its own categories C s ? C, where C si ?C sj = ? and s i = s j for s i , s j ? S and its network weights ? s , we do:</p><p>? In the first step, the model f with its weights ? 0 is trained using the image-class pairs (x 0 , y 0 ) of sequence s = 0 (see Algorithm 1, line six). ? For each new sequence s ? S, the recall labels r s are calculated for each image of the current sequence s, using the weights ?? of the previous sequence (see <ref type="bibr">Algorithm</ref>  if s = 0 then 3:</p><formula xml:id="formula_0">rs = [f.predictWithoutSoftmax(X = i) for i in x] 4:</formula><p>ys = concatenate (rs, ys) <ref type="bibr" target="#b4">5</ref>:</p><formula xml:id="formula_1">EXPAND NETWORK(f, |(Cs)|) 6: f.train(X = xs, Y = ys) 7: return f (x)</formula><p>2) Loss function: In order to enable the network to learn how to reconstruct the previous network outputs and simultaneously learn new categories, we have to adapt the loss function. So, we define a new loss, as we are now using recall labels r s for the categories of previous sequences and onehot encoded classification labels for the current categories. Our loss function consists out of three parts: a) 1. Loss on the previous categories L recall : The aim is to force the logit output space of the categories of previous sequences C prev s to only slightly adapt to the categories of the current sequence C s , but to mainly stay the same, as those previous categories are not represented in the training anymore.</p><formula xml:id="formula_2">L recall i,s = 1 |C prev s | c ? C prev s (o i [c] ? r i [c]) 2<label>(1)</label></formula><p>Equation <ref type="formula" target="#formula_2">(1)</ref> shows the loss on previous categories L recall i,s , where a regression loss is used instead of a classification loss. Namely, an L2-norm is calculated between the recall label r and the network's logit output o for a given training example i and a given category c. This is done for all c in C prev s :</p><formula xml:id="formula_3">C prev s = ? ? ? ? ? ? if s = 0 [0,...,?] s C s else<label>(2)</label></formula><p>With this loss function, previously shown categories are now prevented from being forgotten. Nevertheless, it is still necessary to ensure that the network learns new categories, which is tackled next. b) 2. Loss on the current categories L curr :: In order to learn the categories of the current sequence, a cross-entropy loss with softmax is used on the newly generated heads, which is possible as classification labels are used in a onehot encoding style for the current sequence. The loss on new categories for one training example in the current sequence L curr , can be described as:</p><formula xml:id="formula_4">L curr i,s = 1 |C s | c ? Cs 1 i [c] log(p i [c])<label>(3)</label></formula><p>With this loss L curr , our network is able to learn new categories, where p[c] is the softmax output for a class c.</p><p>To balance those two-loss parts, a third loss is defined next. c) 3. Loss over all categories L all :: As the goal is to have a complete classifier for all categories, we now have to find a solution for the decoupling of the previous categories from the new ones. Here, also the cross entropy loss with softmax as in Eq. (3) is used, but this time on all logit outputs (all previous and new categories)</p><formula xml:id="formula_5">C s = [0,...,s] s C s , where s is the current sequence: L all i,s = 1 |C s | c ? C s 1 i [c] log(p i [c])<label>(4)</label></formula><p>Important to note here is that this loss part again uses classification labels and a classification loss. Therefore, this loss function tries to lower the logit response of previous categories, as they are represented with a zero value in the labels, which might lead to forgetting. The recall loss L recall prevents this. Combining the three formula parts, our loss function on all training examples of the current sequence s is defined in Eq. <ref type="bibr" target="#b4">(5)</ref>. The loss in the first sequence only uses Eq. (3) of the loss function as there are no previous categories given.</p><formula xml:id="formula_6">L s = L curr i,s if s = 0 L recall i,s + L curr i,s + L all i,s else<label>(5)</label></formula><p>3) Discrepancy in the output distribution: Continually training the network sequence after sequence causes the logit outputs o to rise steadily, which leads to forgetting. See the blue line in <ref type="figure" target="#fig_12">Fig. 3</ref>. This is caused as the previous categories try to keep their logit values through the L recall , which means that to learn new categories, the logit values of those categories have to be higher than before. As the softmax uses all categories, this then increases the variance of each sequence. Caused through this unequal comparison inside of the softmax, the older sequences categories will be increasingly forgotten over time. We solve this problem of distribution discrepancy within already learned categories by dividing the difference between the output and the recall label by the variance per category.</p><formula xml:id="formula_7">?[c] = E[(o[c] ? ?[c]) 2 ], ?c ? C prev s<label>(6)</label></formula><p>In Eq. <ref type="formula" target="#formula_7">(6)</ref>  Sequences RECALL reg. RECALL var. reg. <ref type="figure" target="#fig_12">Fig. 3</ref>: Logarithmic variance over all logits per sequence and method. In blue, the output variance for the standard RE-CALL with catastrophic forgetting is shown. In red RECALL using dividing by variance, in black RECALL using full regression loss and in green RECALL using full regression loss in combination with dividing by variance is depicted. CORe50 <ref type="bibr" target="#b9">[10]</ref> is used here.</p><p>for one class c. So, the loss on the previous categories from Eq. (1) is changed, resulting in a modified lossL recall , shown in Eq. </p><formula xml:id="formula_8">L recall i,s = 1 |C prev s | c ? C prev s o i [c] ? r i [c] ?[c] 2<label>(7)</label></formula><p>The red line in <ref type="figure" target="#fig_12">Fig. 3</ref> shows that this technique smooths the variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Full regression loss:</head><p>The division by the variance, however, does not solve the problem entirely as it only tries to patch up the problems caused by the softmax and crossentropy.</p><p>As this is the root cause, we propose to replace the softmax and cross-entropy with a regression loss and clamp the output of the last layer to the range of zero to one, which is shown in Eq. <ref type="bibr" target="#b7">(8)</ref>.?</p><formula xml:id="formula_9">i [c] = max(0, min(o i [c], 1))<label>(8)</label></formula><p>After that, we need to adapt L curr and L all by replacing the cross entropy with an L2 loss, see Eq. (9) and Eq. <ref type="bibr" target="#b9">(10)</ref>.</p><formula xml:id="formula_10">L curr i,s = 1 |Cs| c ? Cs (? i [c] ? 1 i [c]) 2<label>(9)</label></formula><formula xml:id="formula_11">L all i,s = 1 |C s | c ? C s (? i [c] ? 1 i [c]) 2<label>(10)</label></formula><p>Resulting inL curr andL all , which are now used in Eq. (5). The black line in <ref type="figure" target="#fig_12">Fig. 3</ref> depicts that our proposed full regression loss smooths the output distribution best. Nevertheless, we lose the advantage that the sum of all values is one. Finally, we also try using the variance here to shift the focus to categories, which have a high range of values in their corresponding logit output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HOWS-CL-25 DATASET</head><p>In order to better show the strength of our approach, we present a novel synthetic dataset for object classification for continual learning, created with BlenderProc, called HOWS-CL-25 (Household Objects Within Simulation dataset for Continual Learning). BlenderProc, by Denninger et al. <ref type="bibr" target="#b20">[21]</ref>, is a procedural pipeline to generate images for deep learning. Our dataset contains 150,795 unique synthetic images using 25 different household categories with 925 3D models in total, see <ref type="figure" target="#fig_5">Fig. 4</ref>. We achieved that by creating a room with randomly textured floors, walls, and a light source with randomly chosen light intensity and color. After that, a 3D model is placed in the resulting room. This object gets customized by randomly assigning materials including different textures in order to achieve a diverse dataset. Moreover, each object might be deformed with a random displacement texture. For each RGB-D image, we also provide the corresponding segmentation map and normal image.</p><p>The images are organized in five sequences, containing five categories each. We also provide a long-version with twelve sequences containing three categories in the first and two in the following sequences. At the end, ten percent of the images are used for validation, whereas an object instance can either be in the training or in the validation set. This avoids that the network learns to recognize instances of certain categories. We created this dataset by taking 774 3D models from the ShapeNet dataset <ref type="bibr" target="#b21">[22]</ref> and the others models from various sites. The dataset and the code are available online: https://github.com/DLR-RM/RECALL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset comparison</head><p>In our opinion, one of the most relevant applications for online learning is robotics, where a dataset consisting of household objects is more relevant than out of cars, planes, boats and dogs. This is the most important feature setting HOWS-CL-25 apart from iCIFAR-100 and ImageNet1K, making it similar to CORe50 but with a pronounced focus on category than on instance learning. Compared with CORe50, our dataset contains two and a half times more categories and over 18.5 times more instances, as well as a wider variety of backgrounds, lighting conditions, and camera positions. Furthermore, our dataset is non-handheld, as a CNN could learn the category of the object based on the grasp, which is not ideal if, after training, the approach is used in a general setting <ref type="bibr" target="#b24">[25]</ref>. In Tab. I, we show datasets that other continual learning papers have used. This table shows that we provide the widest variety of objects and sessions from all datasets, and we are the only ones providing a segmentation map and Rebuffi et al. <ref type="bibr" target="#b6">[7]</ref> introduce another dataset that is often used in continual learning, called iCIFAR-100, which is an incremental version of the CIFAR-100 dataset proposed by Krizhevsky et al. <ref type="bibr" target="#b25">[26]</ref> with 100 categories. This dataset splits the categories equally into different task sets (sequences). The advantage of this dataset is that it contains more categories than CORe50 or our dataset, but the problem is that it is easier for a network, which is pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> as those contain similar or even the same categories. On top of that, it is not a household dataset.</p><p>OpenLORIS is a robotic vision dataset proposed by She et al. <ref type="bibr" target="#b23">[24]</ref>, which contains household objects recorded by a robot. The difference in our dataset is that OpenLORIS focus on instance learning, as each sequence contains the same 69 objects but in different conditions (occlusion, view change, and more), and additionally, this dataset uses the same instances and sessions for training and testing.</p><p>One of the best advantages of a synthetic dataset is that it is only a question of computation time to create an even more extensive dataset, whereas expanding the number of categories or instances in CORe50 is much more timeintensive, as these images are recorded manually. One might be concerned that using synthetic data will not generalize to real-world images. But, as shown from Hoda? et al. <ref type="bibr" target="#b26">[27]</ref> and Denninger et al. <ref type="bibr" target="#b20">[21]</ref>, a generalization from synthetic to real-world images is possible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>We evaluate our approach on iCIFAR-100 created by Rebuffi et al. <ref type="bibr" target="#b6">[7]</ref>, on CORe50, proposed by Lomonaco et al. <ref type="bibr" target="#b9">[10]</ref>, and finally on both versions of HOWS-CL-25, our dataset created with BlenderProc. In addition to the rehearsal-free methods (AR1, LwF, EWC, SI), we also compare our approach with iCaRL and A-GEM (see Sec. II) except for the CORe50 dataset, which is not supported out of the box for those approaches. All evaluated approaches use the same features generated by a frozen ResNet50 backbone on RGB input data. This ResNet50 is pre-trained on ImageNet. The only exception is AR1 as this approach actively changes the backbone while training, so using the features is impossible. For AR1, we use the code provided by the authors, which uses a MobileNet as backbone <ref type="bibr" target="#b27">[28]</ref>. To improve comparability we also tested our approach with the same MobileNet backbone, shown in Tab. III. For generating the results on LwF, EWC, SI, iCaRL and A-GEM we use code provided by Van de Ven et al. <ref type="bibr" target="#b5">[6]</ref> and adapt it so that all approaches us the same features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion</head><p>The results of our experiments are depicted in Tab. II and <ref type="figure" target="#fig_16">Fig. 5</ref>. Our approach reaches the best accuracy of all rehearsal-free methods with 71.45% on CORe50, as shown in the top plot of <ref type="figure" target="#fig_16">Fig. 5</ref>. RECALL even outperforms AR1, which is to the best of our knowledge state-of-the-art for rehearsal-free continual learning on this dataset.</p><p>Based on the results, it is quite clear that LwF suffers from catastrophic forgetting. The distribution shift in LwF still seems to be a problem, as the network forgets quite fast even though they use the final output of the softmax as target values. RECALL solves this by using recall labels r s and a full regression loss. The bad performance of SI and EWC shows that only using an importance matrix seems insufficient. AR1, which combines SI with an architectural strategy, improves the performance by a decent margin.</p><p>On iCIFAR-100, RECALL reaches 61.15% in contrast to 42.39% with AR1, even though the implementation provided by the authors of AR1 reaches a better performance on iCIFAR-100 than the approx. 31% reported in their paper <ref type="bibr" target="#b11">[12]</ref>. In comparison to the other rehearsal-free learning approaches, we are able to learn new categories without strong forgetting.</p><p>On HOWS-CL-25, our approach is also the best performing rehearsal-free method with 57.83% by a big margin, see third plot of <ref type="figure" target="#fig_16">Fig. 5</ref>. For the long version of HOWS-CL-25, the best performing RECALL version is the one with full regression loss and dividing by variance with 40.65% compared to the standard one with 36.85%. This shows that replacing the probability distribution in the full regression loss mode works better on more complex tasks. However, in such a scenario with more sequences and complex categories, the limit of our approach is shown, as forgetting can't be prevented entirely, see the last plot in <ref type="figure" target="#fig_16">Fig. 5</ref>.</p><p>The experiments on the HOWS-CL-25 dataset show the advantage of rehearsal strategies, where training data is kept for later sequences. As HOWS-CL-25 contains more objects than CORe50 or iCIFAR-100, which seems to be harder for all rehearsal-free methods. This observation is even more vital for the challenging long version of HOWS-CL-25, see the bottom plot of <ref type="figure" target="#fig_16">Fig. 5</ref>. Here, it is shown that each rehearsal-free method forgets almost everything in the last sequence. On less challenging datasets like iCIFAR-100, RECALL even performs better than the rehearsal strategies, see the second plot of <ref type="figure" target="#fig_16">Fig. 5</ref>. It is interesting how iCaRL performs on this dataset, as it first suffers from forgetting but is able to relearn most of the categories in the last sequence. This is only possible because it saves previously seen training examples. Remarkably, RECALL performs almost as well as the rehearsal strategies on both versions of HOWS-CL-25, which shows that using the recall labels r s helps the network to recall previous categories without using any memory.</p><p>The contrast in performance between RECALL and the other methods gets even bigger if one focuses solely on categorical continual learning tasks. Here, the method has to understand the semantic meaning of an object rather than recognizing one specific instance. This is visible for AR1, which is not able to learn the categories of HOWS-CL-25, as the category instances used for testing are different from the ones used for training. By testing RECALL with MobileNet features we find that this is not caused by the used backbone of AR1, see Tab. III. For CORe50, where the instances stay the same, the performance difference of AR1 is smaller.</p><p>In addition, RECALL is faster than AR1 as we can directly work on the backbone features. These are saved in a TFRecord file, so one complete run (training and validation) takes roughly three minutes 2 . The one-time conversion of all training images of CORe50 to ResNet50 features takes roughly 6 minutes 2 . In comparison, AR1 adapts the backbone and takes roughly 55 min. for each run of CORe50. For detailed sequence analysis, we refer to the appendix.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation</head><p>As RECALL is fast to train, it was possible to do several ablation studies. In the following, we present hyperparameters, which have a strong influence on the results. a) Feature extraction network: We evaluated the influence of the backbone on the performance, see Tab. III. We use ResNet50 as our default backbone in order to achieve better comparability over all datasets, even though Inception-ResNetV2, proposed by Szegedy et al. <ref type="bibr" target="#b30">[31]</ref>, performs better on the HOWS-CL-25 dataset. As HOWS-CL-25 is a category classification dataset, we assume that more advanced CNNs have an improved accuracy in contrast to CORe50, which is an instance-classification dataset. It can also be seen that ResNet50 proposed by He et al. <ref type="bibr" target="#b0">[1]</ref> works better than the second version ResNet50V2 also proposed by He et al. <ref type="bibr" target="#b28">[29]</ref>. On the CORe50 dataset, ResNet50 outperforms every other tested backbone. Other papers like Maltoni et al. <ref type="bibr" target="#b11">[12]</ref> have partly confirmed these results, where they also found that ResNet50 outperformed GoogLeNet, proposed by Szegedy et al. <ref type="bibr" target="#b33">[34]</ref> on the CORe50 dataset. b) Using different heads per sequence: In Tab. III it can be seen that using a different head per sequence instead of only changing the last layer improves the results. Especially for the CORe50 dataset, this can be explained by the fact that separating different sequences reduces the possible influence of new output values on the first layer during training. c) Activation function: In our approach, different activation functions are tested, see Tab. III. SIREN proposed by Sitzmann et al. <ref type="bibr" target="#b32">[33]</ref> performs best for both datasets. As SIREN has been successfully used in reconstruction tasks, it is interesting to see that it also works best in a classification scenario <ref type="bibr" target="#b31">[32]</ref>. But, as highlighted in their paper, it highly depends on its hyperparameter ? 0 . In the case of the standard version of HOWS-CL-25, ReLU <ref type="bibr" target="#b31">[32]</ref> performs better.</p><p>VI. CONCLUSION This paper shows that the challenging task of online learning can be efficiently solved by combining several architectural and regularization techniques. We demonstrate SOTA performance on the two datasets CORe50 and iCIFAR-100, in a rehearsal-free setting. Our approach RECALL is able to adapt to new categories by adding a new head per sequence. In order to prevent forgetting, we introduce recall labels. However, the usage of those might lead to a discrepancy in the output distribution, for which we propose two different solutions. First, we propose to normalize the logits outputs by dividing with the variance per category and show top performance on iCIFAR-100. In the second solution, we replace the classification with a regression and outperform AR1 on CORe50. Further, we present a novel dataset for continual learning, especially suited for object recognition in a mobile robot environment, called HOWS-CL-25. On this, we show a strong improvement in comparison to all other rehearsal-free learning methods. LwF EWC SI iCaRL A-GEM RECALL <ref type="figure" target="#fig_16">Fig. 5</ref>: Results on four datasets. iCaRL and A-GEM are not rehearsal-free and are here to show how strong our approach is. RECALL is SOTA for rehearsal-free learning in all plots. We always use the best run here.For the exact values, see the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With RECALL, we introduce a novel algorithm for rehearsal-free object classification on 2D images for continual learning. This supplemental document includes the quantitative results of the graphs, which are shown in the result section of the paper. For more details, see Sec. 2. In Sec. 3, a study on catastrophic forgetting is provided. Furthermore, we give a more detailed overview of our HOWS-CL-25 dataset in Sec. 4. More ablation study results are presented in Sec. 5. And finally, we also discuss the release of our used hyperparameters in Sec. 6. In addition to this document, a video is provided, making the approach more accessible. For more details for downloading HOWS-CL-25 and running the code, check the accompanying code and README file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Quantitative results</head><p>In this section, we provide the quantitative results of our experiments on the different datasets from <ref type="figure" target="#fig_16">Fig.  5</ref> of the paper. <ref type="table" target="#tab_0">Table 1</ref> shows the accuracy of each tested approach on the CORe50 dataset. In Tab.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Qualitative sequence specific results</head><p>In the paper, we examine the average performance of the variants of RECALL over all sequences. In this section, we want to further investigate the performance for the categories of each sequence in each sequence. We therefore provide plots for all four different versions of RECALL in combination with CORe50 <ref type="figure" target="#fig_10">(Fig. 2)</ref>, iCIFAR-100 ( <ref type="figure" target="#fig_12">Fig. 3)</ref>, HOWS-CL-25 <ref type="figure" target="#fig_5">(Fig. 4)</ref>, and the long version of the HOWS-CL-25 dataset <ref type="figure" target="#fig_16">(Fig. 5)</ref>. In each plot, it can be seen how the categories of a particular sequence perform in the following sequences. For example, the categories' performance of the first sequence is shown in red during the training. In the second sequence, the green categories are added and tracked throughout the training process. The same is true for the remaining sequences.     (c) iCIFAR-100 with RECALL var.         </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HOWS-CL-25</head><p>The HOWS-CL-25 dataset has two different versions: the standard one with five sequences and a long version with twelve sequences. Both versions contain the same images and categories. In <ref type="figure" target="#fig_17">Fig. 6</ref>, the standard version is shown. Here in each sequence five new categories are introduced to the method. These categories are only available during this one sequence and are not stored for later usage. Furthermore, we use different instances of the same category for testing and training, requiring the method to understand the semantic meaning of a category, rather than just recognizing a certain instance of a category.</p><p>In the long version of the HOWS-CL-25 dataset, we start with three categories in the first sequence and then go down to two categories per sequence for the remaining sequences. This is depicted in <ref type="figure" target="#fig_18">Fig. 7</ref>. The long version contains twelve sequences in total, which is the highest amount of sequences of all datasets for continual learning, as far as the authors of this work are aware of. Making the learning even more challenging as it firstly contains more training steps, which means more risk to suffer from catastrophic forgetting. On top of that, the comparison in each sequence between the different categories is limited, as each sequence now only contains two different categories. RECALL tries to combat this with our socalled recall label r.</p><p>In contrast to existing datasets like CORe50, the network can not learn the instances by heart or even rely on the scene's background to get information about the object. This is especially difficult as the HOWS-CL-25 even contains categories like fork and spoon, which are similar by design. If one compares the images in sequence eight and eleven of the spoon and fork, one will see that this is particularly challenging. The design decision behind this was to lift continual learning to the next level, overcoming the datasets, where a network could learn based on the environment or a specific instance the matching category, as mentioned in the paper.</p><p>Finally, synthetic data makes it easier to randomize the environments, which removes the previous mentioned factor in the evaluation. This is achieved by using over 1000 different materials, which are randomly assigned to the room's floor and walls. These materials also have randomized physical properties, meaning that the specularity and other factors of the materials are randomly changed even to further increase the randomness of the environment.</p><p>For each RGB we also provide a depth, normal, and segmentation map. Please find an example in <ref type="figure" target="#fig_1">Fig. 1</ref>. A complete list of the different categories used in HOWS-CL-25 is shown in Tab. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Additional ablation studies 5.1 Number of layers</head><p>In Tab. 6 the resulting accuracies on CORe50 and HOWS-CL-25 dataset are shown, when different amounts of fully-connected-layers are used. It is shown that the best results for both datasets are achieved when one fully-connected layer is used, which is our default option of RECALL, as more layers do not further improve the results. We speculate the reason for this is that the features from the ResNet50 are already separated well enough that further processing only hurts the overall performance.     6 Hyperparameter and source code</p><p>The exact hyperparameter used to achieve all the reported results of the different RECALL versions and the implementation of our approach are available in our GitHub repository: https://github.com/ DLR-RM/RECALL. The same is true for our HOWS-CL-25 dataset. Which can also be downloaded here: https://zenodo.org/record/7054171.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Continual learning without using previously seen examples. In each sequence, the model has to learn new categories while also ensuring not to forget previous ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, the calculation of the variance per category ?[c] is shown, where ?[c] is the mean over the logit outputs o during one sequence s over all the image-label pairs (x s , y s )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b6">(7)</ref>.Inspired by a Mahalanobis distance, this equation shows the network output difference o i [c] and the recall label r i [c] of each neuron being divided by the variance of the respective category ?[c]. This reduces the discrepancy in the output of the neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>All 25 categories used in the HOWS-CL-25 dataset are suited for mobile robotics as shown here. For training and testing, different instances of these categories are used. normal image to each RGB-D image. Here, a session is a specific environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>on HOWS-CL-25 long version AR1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>2, the results of the iCIFAR dataset are printed. The result of the different approaches on our HOWS-CL-25 dataset are shown in Tab. 3. Finally, the long version of the HOWS-CL-25 dataset is depicted in Tab. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 :</head><label>1</label><figDesc>Each RGB image of an object in the HOWS-CL-25 dataset (left) has a corresponding segmentation map (second from left), a normal image (third from left), and a depth image (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>for each validation sequence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 :</head><label>2</label><figDesc>The results on the CORe50 dataset with our four methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>on iCIFAR-100 for each validation sequence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 3 :</head><label>3</label><figDesc>The results on the iCIFAR-100 dataset with our four methods. HOWS-CL-25 with RECALL reg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>HOWS-CL-25 with RECALL var.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>HOWS-CL-25 with RECALL var. reg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 4 :</head><label>4</label><figDesc>The results on the HOWS-CL-25 dataset with our four methods. on HOWS-CL-25 for each validation sequence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 5 :</head><label>5</label><figDesc>The results on the HOWS-CL-25 long dataset with our four methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 6 :</head><label>6</label><figDesc>Standard version of HOWS-CL-25 dataset with five sequences. time Seq 0 Seq 1 Seq 2 Seq 3 Seq 4 Seq 5 Seq 6 Seq 7 Seq 8 Seq 9 Seq 10 Seq 11 . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 7 :</head><label>7</label><figDesc>Long version of HOWS-CL-25 dataset with twelve sequences. A particular set of categories is introduced to the network in each sequence, whereas each category contains a set of instances. For example, in the first sequence, the categories camera, ball, and bowl are introduced. Different instances of the same category are used for testing, and these categories are only available in the first sequence.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1 .</head><label>1</label><figDesc>Prediction of the recall labels r s before training a new sequence.</figDesc><table><row><cell></cell><cell>frozen ResNet50</cell><cell></cell><cell></cell><cell>r s</cell><cell></cell></row><row><cell>Training images of current sequence s (224?224?3)</cell><cell>Output (2048)</cell><cell cols="3">fc[0,0] (2048) previous network f? fc[0,1] (5) Weights of the</cell><cell>Use r s</cell></row><row><cell cols="5">2. Training of the current sequence after expanding the network.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>L recall s</cell><cell></cell></row><row><cell></cell><cell>frozen ResNet50</cell><cell>fc[0,0] (2048)</cell><cell>fc[0,1] (5)</cell><cell></cell><cell></cell></row><row><cell>(224?224?3) current sequence s Training images of</cell><cell>Output (2048)</cell><cell>fc[1,0] (2048)</cell><cell>(3) fc[1,1]</cell><cell>L curr s</cell><cell>y s</cell></row><row><cell cols="6">Fig. 2: Calculation of the recall label r: First, we calculate the logits for each training example of the current sequence s. After that, a new head is added (fc[1, 0], fc[1, 1]) and the recall labels r</cell></row></table><note>s , are used for training in sequence s.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1, line three). Now, the model f s can be trained.</figDesc><table><row><cell>? We create new network weights ? s by adding a new head f? (fc[1, 0], fc[1, 1]) according to the number of categories in the current sequence |C s |.</cell></row><row><cell>Algorithm 1 Network training procedure</cell></row></table><note>?1: procedure TRAIN NETWORK(Network f , Sequence s, Cate- gories C, Images xs, Labels ys)2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Compared to other datasets used for continual learning, HOWS-CL-25 has the most objects, sessions and sequences.</figDesc><table><row><cell>Dataset permuted MNIST [23], [16] OpenLORIS [24] CORe50 [10] iCIFAR-100 [7]</cell><cell>Type Category Instance Instance Category</cell><cell>Imgs. 70000 1106424 164866 60000</cell><cell>Cat. 10 19 10 100</cell><cell>Obj. -69 50 600</cell><cell>Sess. 1 7 11 600</cell><cell>Seq. 10 9 9 10</cell><cell>Format grayscale RGB-D RGB-D RGB</cell><cell>Setting hand written robot camera hand hold mixed</cell></row><row><cell>HOWS-CL-25</cell><cell>Category</cell><cell>150795</cell><cell>25</cell><cell>925</cell><cell>50265</cell><cell>5 or 12</cell><cell>RGB-D, segmap, normal</cell><cell>synthetic</cell></row><row><cell cols="2">V. RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>RECALL performs best on various datasets, compared to other rehearsal-free approaches, measured by the accuracy over all categories after the last sequence. This average accuracy and std. deviation is calculated over 40 runs.</figDesc><table><row><cell>Approach/Dataset LwF [11] EWC [16] SI [13] AR1 [12] RECALL RECALL var. RECALL reg. RECALL var. reg.</cell><cell>CORe50 (instance) iCIFAR-100 (category) HOWS-CL-25 (category) HOWS-CL-25 long (category) 34.14 27.93 25.13 10.29 43.29 9.67 17.99 7.01 26.77 9.62 16.25 6.96 69.48 1 42.39 8.59 8.41 64.57(?0.79) 61.15(?0.29) 57.83(?0.18) 36.85(?0.23) 50.36(?1.06) 61.09(?0.31) 55.68(?1.27) 36.48(?0.22) 63.13(?2.46) 56.52(?0.23) 57.05(?0.53) 39.19(?1.67) 71.45(?0.43) 56.46(?0.24) 56.82(?0.95) 40.65(?1.37)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>The accuracies over all categories after the last sequence are depicted for various feature-extractors, strategies, and activation functions. The used hyperparameters in RECALL are marked with a star.</figDesc><table><row><cell>Dataset CORe50 HOWS-CL-25</cell><cell>Mobile-Net[28] 71.85 44.47</cell><cell>ResNet50* [1] 72.00 59.17</cell><cell>Feature-Extractor ResNet50V2 Inceptionv3 [29] [30] 64.04 61.87 59.51 67.66</cell><cell>Inception-ResNetV2 [31] 51.55 69.19</cell><cell>Strategy Exp. last Adding layer head* 64.44 72.00 55.15 58.31</cell><cell>Activation function ReLU SIREN* [32] [33] 59.35 72.00 59.17 58.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Comparison of RECALL and other approaches on CORe50 dataset, shown in Fig. 5 of the paper</figDesc><table><row><cell>Approach</cell><cell cols="5">Seq 0 Seq 1 Seq 2 Seq 3 Seq 4 Seq 5 Seq 6 Seq 7 Seq 8</cell></row><row><cell>LwF</cell><cell cols="5">97.85 85.44 77.22 68.23 58.97 50.78 44.46 38.14 34.14</cell></row><row><cell>EWC</cell><cell cols="5">97.64 75.93 69.85 64.93 57.66 52.07 48.63 46.99 43.29</cell></row><row><cell>SI</cell><cell cols="5">97.71 75.95 62.20 47.79 42.33 36.41 32.76 29.65 26.77</cell></row><row><cell>AR1</cell><cell cols="5">95.01 84.48 76.89 72.95 65.26 61.37 59.52 58.59 57.67</cell></row><row><cell>AR1 from [18]</cell><cell cols="5">97.34 87.64 84.62 80.84 78.23 74.71 72.24 70.85 69.48</cell></row><row><cell>RECALL</cell><cell cols="5">97.78 91.11 86.15 78.94 72.98 68.19 64.76 66.22 67.01</cell></row><row><cell>RECALL var.</cell><cell>97.56</cell><cell>93.0</cell><cell>86.22</cell><cell>75.0</cell><cell>66.67 60.44 51.81 51.53 53.01</cell></row><row><cell>RECALL reg.</cell><cell cols="5">97.78 90.67 82.52 79.11 74.49 71.15 68.44 67.44 67.33</cell></row><row><cell cols="6">RECALL var. reg. 96.89 90.44 86.44 81.61 79.47 75.52 73.68 72.97 72.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Comparison of RECALL and other approaches on iCIFAR-100 dataset, shown in Fig. 5 of the paper</figDesc><table><row><cell>Approach</cell><cell cols="4">Seq 0 Seq 1 Seq 2 Seq 3 Seq 4 Seq 5 Seq 6 Seq 7 Seq 8 Seq 9</cell></row><row><cell>LwF</cell><cell cols="4">95.00 62.05 49.00 42.80 40.04 36.95 33.87 31.36 29.87 27.93</cell></row><row><cell>EWC</cell><cell cols="3">95.00 47.05 31.47 23.58 18.88 15.53 13.30 11.59 10.60</cell><cell>9.67</cell></row><row><cell>SI</cell><cell cols="3">95.00 47.00 31.70 23.48 18.80 15.13 13.24 11.61 10.55</cell><cell>9.62</cell></row><row><cell>AR1</cell><cell cols="4">76.20 66.35 61.46 56.45 53.20 49.90 46.11 44.40 43.50 42.39</cell></row><row><cell>RECALL</cell><cell>95.90 85.75 79.03 75.77 73.26 69.93</cell><cell>68.3</cell><cell cols="2">66.12 64.68 62.04</cell></row><row><cell>RECALL var.</cell><cell cols="4">94.90 85.15 78.77 75.25 72.80 69.50 68.04 65.89 64.38 61.76</cell></row><row><cell>RECALL reg.</cell><cell>95.20 85.50 78.87 74.27 69.72 64.78</cell><cell>63.3</cell><cell cols="2">61.33 59.56 57.30</cell></row><row><cell cols="5">RECALL var. reg. 94.80 85.90 78.73 74.25 69.68 64.55 63.17 61.26 59.64 57.39</cell></row><row><cell>A-GEM</cell><cell cols="4">95.00 71.85 56.50 54.90 44.90 44.40 37.93 36.94 34.85 25.83</cell></row><row><cell>iCarl</cell><cell cols="4">91.00 83.15 77.07 62.25 44.50 35.40 30.90 21.77 24.61 49.68</cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Comparison of RECALL and other approaches on HOWS-CL-25 dataset, shown in Fig. 5 of the paper.</figDesc><table><row><cell>Approach</cell><cell cols="2">Seq 0 Seq 1 Seq 2 Seq 3 Seq 4</cell></row><row><cell>LwF</cell><cell cols="2">96.91 51.91 34.49 27.91 25.13</cell></row><row><cell>EWC</cell><cell cols="2">96.90 57.72 34.91 24.90 17.99</cell></row><row><cell>SI</cell><cell cols="2">96.91 48.83 31.42 23.12 16.25</cell></row><row><cell>AR1</cell><cell>28.54 22.33 14.97 11.09</cell><cell>8.59</cell></row><row><cell>RECALL</cell><cell cols="2">95.71 87.62 78.58 69.73 58.21</cell></row><row><cell>RECALL var.</cell><cell cols="2">97.07 86.15 76.06 68.10 58.34</cell></row><row><cell>RECALL reg.</cell><cell cols="2">97.05 84.01 73.92 66.85 58.13</cell></row><row><cell cols="3">RECALL var. reg. 96.73 84.57 74.12 66.94 58.31</cell></row><row><cell>A-GEM</cell><cell cols="2">96.91 87.00 82.60 73.78 61.18</cell></row><row><cell>iCarl</cell><cell cols="2">95.71 92.98 89.09 82.66 73.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Comparison of RECALL and other approaches on HOWS-CL-25 long dataset, shown in Fig. 5 of the paper Approach Seq 0 Seq 1 Seq 2 Seq 3 Seq 4 Seq 5 Seq 6 Seq 7 Seq 8 Seq 9 Seq 10 Seq 11 LwF 97.82 49.90 33.41 25.04 20.11 16.98 14.30 12.29 10.85 11.38 11.82 10.29 EWC 97.82 49.61 33.33 24.84 19.98 16.62 13.00 12.48 10.63 67.41 54.51 58.10 57.96 58.14 50.16 50.74 46.55 42.90 40.30 37.10 RECALL reg. 98.30 71.40 73.36 72.71 66.79 66.47 56.97 58.23 53.33 49.06 46.78 42.90 RECALL var. reg. 98.04 81.59 79.10 78.77 69.49 68.47 63.25 62.30 57.09 51.70 48.68 44.62 A-GEM 98.72 91.50 33.30 84.29 75.76 82.22 67.02 63.80 73.36 65.08 43.22 59.46 iCarl 97.48 95.42 95.12 91.87 81.80 65.45 50.42 47.32 45.65 50.38</figDesc><table><row><cell>9.62</cell><cell>9.03</cell><cell>7.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Overview of all categories of our RECALL dataset</figDesc><table><row><cell>Category</cell><cell cols="2">HOWS-CL-25 HOWS-CL-25 long</cell></row><row><cell>Apple</cell><cell>seq. 0</cell><cell>seq. 0</cell></row><row><cell>Bag</cell><cell>seq. 0</cell><cell>seq. 0</cell></row><row><cell>Ball</cell><cell>seq. 0</cell><cell>seq. 0</cell></row><row><cell>Banana</cell><cell>seq. 0</cell><cell>seq. 1</cell></row><row><cell>Bowl</cell><cell>seq. 0</cell><cell>seq. 1</cell></row><row><cell>Bread</cell><cell>seq. 1</cell><cell>seq. 2</cell></row><row><cell>Camera</cell><cell>seq. 1</cell><cell>seq. 2</cell></row><row><cell>Can</cell><cell>seq. 1</cell><cell>seq. 3</cell></row><row><cell>Cap</cell><cell>seq. 1</cell><cell>seq. 3</cell></row><row><cell>Computer keyboard</cell><cell>seq. 1</cell><cell>seq. 4</cell></row><row><cell>Egg</cell><cell>seq. 2</cell><cell>seq. 4</cell></row><row><cell>Fork</cell><cell>seq. 2</cell><cell>seq. 5</cell></row><row><cell>Glass bottle</cell><cell>seq. 2</cell><cell>seq. 5</cell></row><row><cell>Glasses</cell><cell>seq. 2</cell><cell>seq. 6</cell></row><row><cell>Headset</cell><cell>seq. 2</cell><cell>seq. 6</cell></row><row><cell>Knife</cell><cell>seq. 3</cell><cell>seq. 7</cell></row><row><cell>Milk canister</cell><cell>seq. 3</cell><cell>seq. 7</cell></row><row><cell>Mobile phone</cell><cell>seq. 3</cell><cell>seq. 8</cell></row><row><cell>Mug</cell><cell>seq. 3</cell><cell>seq. 8</cell></row><row><cell>Pan</cell><cell>seq. 3</cell><cell>seq. 9</cell></row><row><cell>Pear</cell><cell>seq. 4</cell><cell>seq. 9</cell></row><row><cell>Pen</cell><cell>seq. 4</cell><cell>seq. 10</cell></row><row><cell>Scissors</cell><cell>seq. 4</cell><cell>seq. 10</cell></row><row><cell>Spoon</cell><cell>seq. 4</cell><cell>seq. 11</cell></row><row><cell>Teddy bear</cell><cell>seq. 4</cell><cell>seq. 11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Results of RECALL on different datasets when using different amounts of fully-connected layers. The default option is marked with a star.</figDesc><table><row><cell cols="3">Layer amount CORe50 HOWS-CL-25</cell></row><row><cell>1*</cell><cell>72.02</cell><cell>58.31</cell></row><row><cell>2</cell><cell>71.65</cell><cell>52.11</cell></row><row><cell>3</cell><cell>70.32</cell><cell>52.21</cell></row><row><cell>4</cell><cell>69.95</cell><cell>53.56</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This value is from<ref type="bibr" target="#b11">[12]</ref>, in our experiments we reach 57.67%.<ref type="bibr" target="#b1">2</ref> Using a NVIDIA GeForce RTX 2080 Ti</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning less is more-6d camera localization via 3d surface regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Three scenarios for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Tolias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07734</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6467" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Progressive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Core50: a new dataset and benchmark for continuous object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Continuous learning in singleincremental-task scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lomonaco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">3987</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incremental object learning from contiguous views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dhanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Il2m: Class incremental learning with dual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belouadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Storing encoded episodes as concepts for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ayub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eec: Learning to encode and regenerate images for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ayub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Persistent anytime learning of objects from unseen classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4075" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blenderproc: Reducing the reality gap with photorealisitc rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denninger</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Winkelbauer</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Olefir</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoda?</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zidan</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elbadrawy</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Knauer</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Katam</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lodhi</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics: Science and Systems</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Openloris-object: A robotic vision dataset and benchmark for lifelong deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Visual concepts and compositional voting</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photorealistic image synthesis for object instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hanzelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Urbina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
