<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from Single RGB Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Argus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from Single RGB Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 3D hand pose from single RGB images is a highly ambiguous problem that relies on an unbiased training dataset. In this paper, we analyze cross-dataset generalization when training on existing datasets. We find that approaches perform well on the datasets they are trained on, but do not generalize to other datasets or in-the-wild scenarios. As a consequence, we introduce the first large-scale, multi-view hand dataset that is accompanied by both 3D hand pose and shape annotations. For annotating this realworld dataset, we propose an iterative, semi-automated 'human-in-the-loop' approach, which includes hand fitting optimization to infer both the 3D pose and shape for each sample. We show that methods trained on our dataset consistently perform well when tested on other datasets. Moreover, the dataset allows us to train a network that predicts the full articulated hand shape from a single RGB image. The evaluation set can serve as a benchmark for articulated hand shape estimation. arXiv:1909.04349v3 [cs.CV] 13 Sep 2019 Training Set Evaluation Set Figure 2: Examples from our proposed dataset showing images (top row) and hand shape annotations (bottom row). The training set contains composited images from green screen recordings, whereas the evaluation set contains images recorded indoors and outdoors. The dataset features several subjects as well as object interactions.</p><p>the key aspects is that we record synchronized images from multiple views, an idea already used previously in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">27]</ref>. The multiple views remove many ambiguities and ease both the manual annotation and automated fitting. The second key aspect of our approach is a semi-automated humanin-the-loop labeling procedure with a strong bootstrapping component. Starting from a sparse set of 2D keypoint annotations (e.g., finger tip annotations) and semi-automatically generated segmentation masks, we propose a hand fitting method that fits a deformable hand model [25] to a set of multi-view input. This fitting yields both 3D hand pose and shape annotation for each view. We then train a multi-view 3D hand pose estimation network using these annotations. This network predicts the 3D hand pose for unlabeled samples in our dataset along with a confidence measure. By verifying confident predictions and annotating least-confident samples in an iterative procedure, we acquire 11592 annotations with moderate manual effort by a human annotator.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D hand pose and shape estimation from a single RGB image has a variety of applications in gesture recognition, robotics, and AR. Various deep learning methods have approached this problem, but the quality of their results depends on the availability of training data. Such data is created either by rendering synthetic datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b36">38]</ref> or by capturing real datasets under controlled settings typically with little variation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b30">32]</ref>. Both approaches have limitations, discussed in our related work section.</p><p>Synthetic datasets use deformable hand models with texture information and render this model under varying pose configurations. As with all rendered datasets, it is difficult to model the wide set of characteristics of real images, such as varying illumination, camera lens distortion, motion blur, depth of field and debayering. Even more importantly, ren-</p><formula xml:id="formula_0">Figure 1:</formula><p>We create a hand dataset via a novel iterative procedure that utilizes multiple views and sparse annotation followed by verification. This results in a large scale real world data set with pose and shape labels, which can be used to train single-view networks that have superior crossdataset generalization performance on pose and shape estimation.</p><p>dering of hands requires samples from the true distribution of feasible and realistic hand poses. In contrast to human pose, such distributional data does not exist to the same extent. Consequently, synthetic datasets are either limited in the variety of poses or sample many unrealistic poses.</p><p>Capturing a dataset of real human hands requires annotation in a post-processing stage. In single images, manual annotation is difficult and cannot be easily crowd sourced due to occlusions and ambiguities. Moreover, collecting and annotating a large scale dataset is a respectable effort.</p><p>In this paper, we analyze how these limitations affect the ability of single-view hand pose estimation to generalize across datasets and to in-the-wild real application scenarios. We find that datasets show excellent performance on the respective evaluation split, but have rather poor performance on other datasets, i.e., we see a classical dataset bias.</p><p>As a remedy to the dataset bias problem, we created a new large-scale dataset by increasing variation between samples. We collect a real-world dataset and develop a methodology that allows us to automate large parts of the labeling procedure, while manually ensuring very highfidelity annotations of 3D pose and 3D hand shape. One of The dataset spans 32 different people and features fully articulated hand shapes, a high variation in hand poses and also includes interaction with objects. Part of the dataset, which we mark as training set, is captured against a green screen. Thus, samples can easily be composed with varying background images. The test set consists of recordings in different indoor and outdoor environments; see <ref type="figure">Figure 2</ref> for sample images and the corresponding annotation.</p><p>Training on this dataset clearly improves cross-dataset generalization compared to training on existing datasets. Moreover, we are able to train a network for full 3D hand shape estimation from a single RGB image. For this task, there is not yet any publicly available data, neither for training nor for benchmarking. Our dataset is available on our project page and therefore can serve both as training and benchmarking dataset for future research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Since datasets are crucial for the success of 3D hand pose and shape estimation, there has been much effort on acquiring such data.</p><p>In the context of hand shape estimation, the majority of methods fall into the category of model-based techniques. These approaches were developed in a strictly controlled environment and utilize either depth data directly <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b31">33]</ref> or use multi-view stereo methods for reconstruction <ref type="bibr" target="#b1">[2]</ref>. More related to our work are approaches that fit statistical human shape models to observations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref> from in-thewild color images as input. Such methods require semiautomatic methods to acquire annotations such as keypoints or segmentation masks for each input image to guide the fitting process.</p><p>Historically, acquisition methods often incorporated markers onto the hand that allow for an easy way to estimate pose. Common choices are infrared markers <ref type="bibr" target="#b10">[11]</ref>, color coded gloves <ref type="bibr" target="#b32">[34]</ref>, or electrical sensing equipment <ref type="bibr" target="#b35">[37]</ref>. This alters hand appearance and, hence, makes the data less valuable for training discriminative methods.</p><p>Annotations can also be provided manually on hand images <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b33">35]</ref>. However, the annotation is limited to visible regions of the hand. Thus, either the subject is required to retain from complex hand poses that result in severe selfocclusions, or only a subset of hand joints can be annotated.</p><p>To avoid occlusions and annotate data at larger scale, Simon et al. <ref type="bibr" target="#b25">[27]</ref> leveraged a multi-view recording setup. They proposed an iterative bootstrapping approach to detect hand keypoints in each view and triangulate them to generate 3D point hypotheses. While the spirit of our data collection strategy is similar, we directly incorporate the multiview information into a neural network for predicting 3D keypoints and our dataset consists of both pose and shape annotations.  <ref type="bibr" target="#b36">[38]</ref>. Each row represents the training set used and each column the evaluation set. The last column shows the average rank each training set achieved across the different evaluation sets. The top-three ranking training sets for each evaluation set are marked as follows: first, second or third. Note that the evaluation set of HO-3D was not available at time of submission, therefore one table entry is missing and the other entries within the respective column report numbers calculated on the training set.</p><p>Since capturing real data comes with an expensive annotation setup and process, more methods rather deployed synthetic datasets recently <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b36">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis of Existing Datasets</head><p>We thoroughly analyze state-of-the-art datasets used for 3D hand pose estimation from single RGB images by testing their ability to generalize to unseen data. We identify seven state-of-the-art datasets that provide samples in the form of an RGB image and the accompanying 3D keypoint information as shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Considered Datasets</head><p>Stereo Tracking Benchmark (STB) <ref type="bibr" target="#b33">[35]</ref> dataset is one of the first and most commonly used datasets to report performance of 3D keypoint estimation from a single RGB image. The annotations are acquired manually limiting the setup to hand poses where most regions of the hands are visible. Thus, the dataset shows a unique subject posing in a frontal pose with different background scenarios and without objects.</p><p>The Panoptic (PAN) dataset <ref type="bibr" target="#b14">[15]</ref> was created using a dense multi-view capture setup consisting of 10 RGB-D sensors, 480 VGA and 31 HD cameras. It shows humans performing different tasks and interacting with each other. There are 83 sequences publicy available and 12 of them have hand annotation. We select 171204 pose3 to serve as evaluation set and use the remaining 11 sequences from the range motion, haggling and tools categories for training.</p><p>Garcia et al. <ref type="bibr" target="#b5">[6]</ref> proposed the First-person hand action benchmark (FPA), a large dataset that is recorded from an egocentric perspective and annotated using magnetic sensors attached to the finger tips of the subjects. Wires run along the fingers of the subject altering the appearance of the hands significantly. 6 DOF sensor measurements are uti-lized in an inverse kinematics optimization of a given hand model to acquire the full hand pose annotations.</p><p>Using the commercial Leap Motion device <ref type="bibr">[22]</ref> for keypoint annotation, Gomez et al. <ref type="bibr" target="#b7">[8]</ref> proposed the Large-scale Multiview 3D Hand Pose Dataset (LSMV). Annotations given by the device are transformed into 4 calibrated cameras that are approximately time synchronized. Due to the limitations of the sensor device, this dataset does not show any hand-object interactions.</p><p>The Rendered Hand Pose Dataset (RHD) proposed by Zimmermann et al. <ref type="bibr" target="#b36">[38]</ref> is a synthetic dataset rendered from 20 characters performing 31 different actions in front of a random background image without hand object interaction.</p><p>Building on the SynthHands <ref type="bibr" target="#b22">[24]</ref> dataset Mueller et al. <ref type="bibr" target="#b21">[23]</ref> presented the GANerated (GAN) dataset. SynthHands was created by retargeting measured human hand articulation to a rigged meshed model in a mixed reality approach. This allowed for hand object interaction to some extend, because the subject could see the rendered scene in real time and pose the hand accordingly. In the following GANerated hand dataset, a CycleGAN approach is used to bridge the synthetic to real domain shift.</p><p>Recently, Hampali et al. <ref type="bibr" target="#b8">[9]</ref> proposed an algorithm for dataset creation deploying an elaborate optimization scheme incorporating temporal and physical consistencies, as well as silhouette and depth information. The resulting dataset is referred to as HO-3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Setup</head><p>We trained a state-of-the-art network architecture <ref type="bibr" target="#b13">[14]</ref> that takes as input an RGB image and predicts 3D keypoints on the training split of each of the datasets and report its performance on the evaluation split of all other datasets. For each dataset, we either use the standard training/evaluation split reported by the authors or create an 80%/20% split dataset num.</p><p>num. real obj-shape labels frames subjects ects STB <ref type="bibr" target="#b33">[35]</ref> 15 k / 3 k 1 manual PAN <ref type="bibr" target="#b36">[38]</ref> 641 k / 34 k &gt; 10 MVBS <ref type="bibr" target="#b25">[27]</ref> FPA <ref type="bibr" target="#b5">[6]</ref> 52 k / 53 k 6 marker LSMV <ref type="bibr" target="#b7">[8]</ref> 117 k / 31 k 21 leapmotion RHD <ref type="bibr" target="#b36">[38]</ref> 41 k / 2.7 k 20 synthetic GAN <ref type="bibr" target="#b21">[23]</ref> 266 k / 66 k synthetic HO-3D <ref type="bibr" target="#b8">[9]</ref> 11 k / -3 automatic [9] Ours 33 k / 4 k 32 hybrid <ref type="table">Table 2</ref>: State-of-the-art datasets for the task of 3D keypoint estimation from a single color image used in our analysis. We report dataset size in number of frames, number of subjects, if it is real or rendered data, regarding hand object interaction, if shape annotation is provided and which method was used for label generation.</p><p>otherwise; see the supplementary material for more details.</p><p>The single-view network takes an RGB image I as input and infers 3D hand pose P = {p k } with each p k ? R 3 , representing a predefined landmark or keypoint situated on the kinematic skeleton of a human hand. Due to scale ambiguity, the problem to estimate real world 3D keypoint coordinates in a camera centered coordinate frame is ill-posed. Hence, we adopt the problem formulation of <ref type="bibr" target="#b13">[14]</ref> to estimate coordinates in a root relative and scale normalized fashion:</p><formula xml:id="formula_1">p k = s ?p k = s ? ? ?x k y k z k ? ? = ? ?x k y k z rel k +? root ? ? ,<label>(1)</label></formula><p>where the normalization factor s is chosen as the length of one reference bone in the hand skeleton,? root is the root depth and? rel k the relative depth of keypoint k. We define the resulting 2.5D representation as:</p><formula xml:id="formula_2">p rel k = x k ,? k ,? rel k T .<label>(2)</label></formula><p>Given scale constraints and 2D projections of the points in a calibrated camera, 3D hand pose P can be recovered from P rel . For details about this procedure we refer to <ref type="bibr" target="#b13">[14]</ref>. We train the single-view network using the same hyperparameter choices as Iqbal et al. <ref type="bibr" target="#b13">[14]</ref>. However, we use only a single stage and reduce the number of channels in the network layers, which leads to a significant speedup in terms of training time at only a marginal decrease in accuracy. We apply standard choices of data augmentation including color, scale and translation augmentation as well as rotation around the optical axis. We apply this augmentation to each of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>It is expected that the network performs the best on the dataset it was trained on, yet it should also provide reasonable predictions for unseen data when being trained on a dataset with sufficient variation (e.g., hand pose, viewpoint, shape, existence of objects, etc.). <ref type="table" target="#tab_0">Table 1</ref> shows for each existing training dataset the network is able to generalize to the respective evaluation split and reaches the best results there. On the other hand, performance drops substantially when the network is tested on other datasets.</p><p>Both GAN and FPA datasets appear to be especially hard to generalize indicating that their data distribution is significantly different from the other datasets. For FPA this stems from the appearance change due to the markers used for annotation purposes. The altered appearance gives the network trained on this dataset strong cues to solve the task that are not present for other datasets at evaluation time. Thus, the network trained on FPA performs poorly when tested on other datasets. Based on visual inspection of the GAN dataset, we hypothesize that subtle changes like missing hand texture and different color distribution are the main reasons for generalization problems. We also observe that while the network trained on STB does not perform well on remaining datasets, the networks trained on other datasets show reasonable performance on the evaluation split of STB. We conclude that a good performance on STB is not a reliable measure for how a method generalizes to unseen data.</p><p>Based on the performance of each network, we compute a cumulative ranking score for each dataset that we report in the last column of <ref type="table" target="#tab_0">Table 1</ref>. To calculate the cumulative rank we assign ranks for each column of the table separately according to the performance the respective training sets achieve. The cumulative rank is then calculated as average over all evaluation sets, i.e. rows of the table. Based on these observations, we conclude that there is a need for a new benchmarking dataset that can provide superior generalization capability.</p><p>We present the FreiHAND Dataset to archieve this goal. It consists of real images, provides sufficient viewpoint and hand pose variation, and shows samples both with and without object interactions. Consequently, the single-view network trained on this dataset achieves a substantial improvement in terms of ranking for cross-dataset generalization. We next describe how we acquired and annotated this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FreiHAND Dataset</head><p>The dataset was captured with the multi-view setup shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. The setup is portable enabling both indoor and outdoor capture. We capture hand poses from 32 subjects of different genders and ethnic backgrounds. Each subject is asked to perform actions with and without objects. To capture hand-object interactions, subjects are given a number of everyday household items that allow for reasonable one-handed manipulation and are asked to demonstrate different grasping techniques. More information is provided in the supplementary material.</p><p>To preserve the realistic appearance of hands, no markers are used during the capture. Instead we resort to postprocessing methods that generate 3D labels. Manual acquisition of 3D annotations is obviously unfeasible. An alternative strategy is to acquire 2D keypoint annotations for each input view and utilize the multi-view camera setup to lift such annotations to 3D similar to Simon et al. <ref type="bibr" target="#b25">[27]</ref>.</p><p>We found after initial experiments that current 2D hand pose estimation methods perform poorly, especially in case of challenging hand poses with self-and object occlusions. Manually annotating all 2D keypoints for each view is prohibitively expensive for large-scale data collection. Annotating all 21 keypoints across multiple-views with a specialized tool takes about 15 minutes for each multi-view set. Furthermore, keypoint annotation alone is not sufficient to obtain shape information.</p><p>We address this problem with a novel bootstrapping procedure (see <ref type="figure">Fig. 4</ref>) composed of a set of automatic methods that utilize sparse 2D annotations. Since our data is captured against a green screen, the foreground can be extracted automatically. Refinement is needed only to coalign the segmentation mask with the hand model's wrist. In addition, a sparse set of six 2D keypoints (finger tips and wrist) is manually annotated. These annotations are relatively cheap to acquire at a reasonably high quality. For example, manually correcting a segmentation mask takes on average 12 seconds, whereas annotating a keypoint takes around 2 seconds. Utilizing this information we fit a deformable hand model to multi-view images using a novel fitting process described in Section 4.1. This yields candidates for both 3D hand pose and shape labels. These can-  <ref type="figure">Figure 4</ref>: The dataset labeling workflow starts from manual annotation followed by the shape fitting process described in 4.1, which yields candidate shape fits for our data samples. Sample fits are manually verified allowing them to be accepted, rejected or queued for further annotation. Alternatively a heuristic can accept samples without human interaction. The initial dataset allows for training the networks involved, which for subsequent iterations of the procedure, can predict information needed for fitting. The labeling process can be bootstrapped, allowing more accepted samples to accumulate in the dataset.</p><p>didates are then manually verified, before being added to a set of labels.</p><p>Given an initial set of labels, we train our proposed network, MVNet, that takes as inputs multi-view images and predicts 3D keypoint locations along with a confidence score, described in Section 4.2. Keypoint predictions can be used in lieu of manually annotated keypoints as input for the fitting process. This bootstrapping procedure is iterated. The least-confident samples are manually annotated (Section 4.3). With this human-in-the-loop process, we quickly obtain a large scale annotated dataset. Next we describe each stage of this procedure in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hand Model Fitting with Sparse Annotations</head><p>Our goal is to fit a deformable hand shape model to observations from multiple views acquired at the same time. We build on the statistical MANO model, proposed by Romero et al. <ref type="bibr" target="#b23">[25]</ref>, which is parameterized by ? ? R 61 . The model parameters ? = (?, ?, ?) T include shape ? ? R 10 , articulation ? ? R 45 as well as global translation and orientation ? ? R 6 . Using keypoint and segmentation information we optimize a multi-term loss,</p><formula xml:id="formula_3">L = L 2D kp + L 3D kp + L seg + L shape + L pose ,<label>(3)</label></formula><p>to estimate the model parameters?, where the tilde indicates variables that are being optimized. We describe each of the terms in <ref type="formula" target="#formula_3">(3)</ref> next. 2D Keypoint Loss L 2D kp : The loss is the sum of distances between the 2D projection ? i of the models' 3D keypoints p k ? R 3 to the 2D annotations q i k over views i and visible keypoints k ? V i :</p><formula xml:id="formula_4">L 2D kp = w 2D kp ? i k?Vi ? q i k ? ? i (p k ) 2 .<label>(4)</label></formula><p>3D keypoint Loss L 3D kp : This loss is defined in a similar manner as (4), but over 3D keypoints. Here, p k denotes the 3D keypoint annotations, whenever such annotations are available (e.g., if predicted by MVNet),</p><formula xml:id="formula_5">L 3D kp = w 3D kp ? i k?Vi p k ?p k 2 .<label>(5)</label></formula><p>Segmentation Loss L seg : For shape optimization we use a sum of l 2 losses between the model dependent maskM i and the manual annotation M i over views i:</p><formula xml:id="formula_6">L seg = w seg ? i ( M i ?M i 2 + EDT(M i ) ?M i 2</formula><p>).</p><p>(6) Additionally, we apply a silhouette term based on the Euclidean Distance Transform (EDT). Specifically, we apply a symmetric EDT to M i , which contains the distance to the closest boundary pixel at every location. Shape Prior L shape : For shape regularization we employ</p><formula xml:id="formula_7">L shape = w shape ? ? 2 ,<label>(7)</label></formula><p>which enforces the predicted shape to stay close to the mean shape of MANO.</p><p>Pose Prior L pose : The pose prior has two terms. The first term applies a regularization on the PCA coefficients a j used to represent the pose? in terms of PCA basis vectors c j (i.e.,? = j? j ? c j ). This regularization enforces predicted poses to stay close to likely poses with respect to the PCA pose space of MANO. The second term regularizes the distance of the current pose?, to the N nearest neighbors of a hand pose dataset acquired from <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_8">L pose = w pose ? j ? j 2 + w nn ? n?N ? n ?? 2 . (8)</formula><p>We implement the fitting process in Tensorflow <ref type="bibr" target="#b0">[1]</ref> and use MANO to implement a differentiable mapping from? to 3D model keypointsp k and 3D model vertex locations V ? R 778?3 . We adopt the Neural Renderer <ref type="bibr" target="#b17">[18]</ref> to render the segmentation masksM i from the hand model vertices V and use the ADAM optimizer <ref type="bibr" target="#b18">[19]</ref> to minimize: <ref type="figure">Figure 5</ref>: MVNet predicts a single hand pose P using images of all 8 views (for simplicity only 2 are shown). Each image is processed separately by a 2D CNN that is shared across views. This yields 2D feature maps f i . These are individually reprojected into a common coordinate frame using the known camera calibration to obtain F i = ? ?1 (f i ).</p><formula xml:id="formula_9">? = arg mi? ? (L(?))<label>(9)</label></formula><p>The F i are aggregated over all views and finally a 3D CNN localizes the 3D keypoints within a voxel representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MVNet: Multiview 3D Keypoint Estimation</head><p>To automate the fitting process, we seek to estimate 3D keypoints automatically. We propose MVNet shown in <ref type="figure">Fig. 5</ref> that aggregates information from all eight camera images I i and predicts a single hand pose P = {p k }. We use a differentiable unprojection operation, similar to Kar et al. <ref type="bibr" target="#b16">[17]</ref>, to aggregate features from each view into a common 3D volume.</p><p>To this end, we formulate the keypoint estimation problem as a voxel-wise regression task:</p><formula xml:id="formula_10">L MVNet = 1 K k S k ?S k 2 ,<label>(10)</label></formula><p>whereS k ? R N ?N ?N represents the prediction of the network for keypoint k and S k is the ground truth estimate we calculate from validated MANO fits. S k is defined as a normalized Gaussian distribution centered at the true keypoint location. The predicted pointp k is extracted as maximal location inS k . Furthermore, we define the confidence c of a prediction as maximum along the spatial and average over the keypoint dimension:</p><formula xml:id="formula_11">c = 1 K k (max i,j,lS k (i, j, l)).<label>(11)</label></formula><p>Additional information can be found in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Iterative Refinement</head><p>In order to generate annotations at large scale, we propose an iterative, human-in-the-loop procedure which is visualized in <ref type="figure">Fig. 4</ref>. For initial bootstrapping we use a set of manual annotations to generate the initial dataset D 0 . In iteration i we use dataset D i , a set of images and the corresponding MANO fits, to train MVNet and HandSegNet <ref type="bibr" target="#b36">[38]</ref>. MVNet makes 3D keypoint predictions along with confidence scores for the remaining unlabeled data and Hand-  <ref type="table">Table 3</ref>: This table shows shape prediction performance on the evaluation split of FreiHAND after alignment. We report two measures: The mean mesh error and the F-score at two different distance thresholds.</p><p>SegNet predicts hand segmentation masks. Using these predictions, we perform the hand shape fitting process of Section 4.1. Subsequently, we perform verification that either accepts, rejects or partially annotates some of these data samples.</p><p>Heuristic Verification. We define a heuristic consisting of three criteria to identify data samples with good MANO fits. First, we require the mean MVNet confidence score to be above 0.8 and all individual keypoint confidences to be at least 0.6, which enforces a minimum level of certainty on the 3D keypoint prediction. Second, we define a minimum threshold for the intersection over union (IoU) between predicted segmentation mask and the mask derived from the MANO fitting result. We set this threshold to be 0.7 on average across all views while also rejecting samples that have more than 2 views with an IoU below 0.5. Third, we require the mean Euclidean distance between predicted 3D keypoints and the keypoints of the fitted MANO to be at most 0.5 cm where no individual keypoint has a Euclidean distance greater than 1 cm. We accept only samples that satisfy all three criteria and add these to the set D h i . Manual Verification and Annotation. The remaining unaccepted samples are sorted based on the confidence score of MVNet and we select samples from the 50 th percentile upwards. We enforce a minimal temporal distance between samples selected to ensure diversity as well as choosing samples for which the current pose estimates are sufficiently different to a flat hand shape as measured by the Euclidean distance in the pose parameters. We ask the annotators to evaluate the quality of the MANO fits for these samples. Any sample that is verified as a good fit is added to the set D m i . For remaining samples, the annotator has the option of either discarding the sample or provide additional annotations (e.g., annotating mislabeled finger tips) to help improve the fit. These additionally annotated samples are added to the set D l i . Joining the samples from all streams yields a larger labeled dataset</p><formula xml:id="formula_12">D i+1 = D i + D h i + D m i + D l i<label>(12)</label></formula><p>which allows us to retrain both HandSegNet and MVNet. We repeated this process 4 times to obtain our final dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Cross-Dataset Generalization of FreiHAND</head><p>To evaluate the cross-dataset generalization capability of our dataset and to compare to the results of <ref type="table" target="#tab_0">Table 1</ref>, we define the following training and evaluation split: there are samples with and without green screen and we chose to use all green screen recordings for training and the remainder for evaluation. Training and evaluation splits contain data from 24 and 11 subjects, respectively, with only 3 subjects shared across splits. The evaluation split is captured in 2 different indoor and 1 outdoor location. We augmented the training set by leveraging the green screen for easy and effective background subtraction and creating composite images using new backgrounds. To avoid green color bleeding at the hand boundaries we applied the image harmonization method of Tsai et al. <ref type="bibr" target="#b29">[31]</ref> and the deep image colorization approach by Zhang et al. <ref type="bibr" target="#b34">[36]</ref> separately to our data. Both the automatic and sampling variant of <ref type="bibr" target="#b34">[36]</ref> were used. With the original samples this quadruples the training set size from 33 k unique to 132 k augmented samples. Examples of resulting images are shown in <ref type="figure">Fig. 2</ref>.</p><p>Given the training and evaluation split, we train the single view 3D pose estimation network on our data and test it across different datasets. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the network achieves strong accuracy across all datasets and ranks first in terms of cross-dataset generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">3D Shape Estimation</head><p>Having both pose and shape annotations, our acquired dataset can be used for training shape estimation models in a fully supervised way. In addition, it serves as the first real dataset that can be utilized for evaluating shape estimation methods. Building on the approach of Kanazawa et al. <ref type="bibr" target="#b15">[16]</ref>, we train a network that takes as input a single RGB image and predicts the MANO parameters? using the following loss:</p><formula xml:id="formula_13">L = w 3D p k ?p k 2 + w 2D ?(p k ) ? ?(p) 2 + w p ? ?? 2 .<label>(13)</label></formula><p>We deploy l 2 losses for 2D and 3D keypoints as well as the model parameters and chose the weighting to w 3D = 1000, w 2D = 10 and w p = 1.</p><p>We also provide two baseline methods, constant mean shape prediction, without accounting for articulation changes, and fits of the MANO model to the 3D keypoints predicted by our single-view network.</p><p>For comparison, we use two scores. The mesh error measures the average Euclidean distance between corresponding mesh vertices in the ground truth and the predicted hand shape. We also evaluate the F -score <ref type="bibr" target="#b19">[20]</ref> which, given a distance threshold, defines the harmonic mean between recall and precision between two sets of points <ref type="bibr" target="#b19">[20]</ref>. In our evaluation, we use two distances: F @5mm and F @15mm to report the accuracy both at fine and coarse scale. In order to decouple shape evaluation from global rotation and translation, we first align the predicted meshes using Procrustes alignment. Results are summarized in <ref type="table">Table 3</ref>. Estimating MANO parameters directly with a CNN performs better across all measures than the baseline methods. The evaluation reveals that the difference in F -score is more pronounced in the high accuracy regime. Qualitative results of our network predictions are provided in <ref type="figure">Fig. 6</ref>. <ref type="figure">Figure 6</ref>: Given a single image (top rows), qualitative results of predicted hand shapes (bottom rows) are shown. Please note that we don't apply any alignment of the predictions with respect to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation of Iterative Labeling</head><p>In the first step of iterative labeling process, we set w 2D kp = 100 and w 2D kp = 0 (since no 3D keypoint annotations are available), w seg = 10.0, w shape = 100.0, w nn = 10.0, and w pose = 0.1. (For subsequent iterations we set w 2D kp = 50 and w 3D kp = 1000.) Given the fitting results, we train MVNet and test it on the remaining dataset. After the first verification step, 302 samples are accepted.</p><p>Validating a sample takes about 5 seconds and we find that the global pose is captured correctly in most cases, but in order to obtain high quality ground truth, even fits with minor inaccuracies are discarded.  <ref type="table">Table 4</ref>: Bootstrapping convergence is evaluated by reporting cross-dataset generalization to RHD and PAN. The measure of performance is AUC, which shows monotonous improvement throughout.</p><formula xml:id="formula_14">Dataset D 0 D 1 D 2 D 3 D</formula><p>We use the additional accepted samples to retrain MVNet and HandSegNet and iterate the process. At the end of the first iteration we are able to increase the dataset to 993 samples, 140 of which are automatically accepted by heuristic, and the remainder from verifying 1000 samples. In the second iteration the total dataset size increases to 1449, 289 of which are automatically accepted and the remainder stems from verifying 500 samples. In subsequent iterations the complete dataset size is increased to 2609 and 4565 samples, where heuristic accept yields 347 and 210 samples respectively. This is the dataset we use for the cross-dataset generalization (see <ref type="table" target="#tab_0">Table 1</ref>) and shape estimation (see <ref type="table">Table 3</ref>) experiments.</p><p>We evaluate the effectiveness of the iterative labeling process by training a single view 3D keypoint estimation network on different iterations of our dataset. For this purpose, we chose two evaluation datasets that reached a good average rank in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="table">Table 4</ref> reports the results and shows a steady increase for both iterations as our dataset grows. More experiments on the iterative procedure are located in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented FreiHAND, the largest RGB dataset with hand pose and shape labels of real images available to date. We capture this dataset using a novel iterative procedure. The dataset allows us improve generalization performance for the task of 3D hand pose estimation from a single image, as well as supervised learning of monocular hand shape estimation.</p><p>To facilitate research on hand shape estimation, we plan to extend our dataset even further to provide the community with a challenging benchmark that takes a big step towards evaluation under realistic in-the-wild conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Cross-dataset generalization</head><p>In this section we provide additional information on the single view pose estimation network used in the experiment and other technical details. The datasets used in the experiment show slight differences regarding the hand model definition: Some provide a keypoint situated at the wrist while others define a keypoint located on the palm instead. To allow a fair comparison, we exclude these keypoints, which leaves 20 keypoints remaining for the evaluation. In the subsequent sections we, first provide implementation details in 7.1, which includes hyperparameters used and the network architecture. Second, we analyze the influence that using a pretrained network has on the outcome of the experiment in 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Implementation details</head><p>We chose our hyper parameters and architecture similar to <ref type="bibr" target="#b13">[14]</ref>. The network consists of an encoder decoder structure with skip connections. For brevity we define the building blocks Block0 (see <ref type="table" target="#tab_6">Table 6</ref>) Block1 (see <ref type="table" target="#tab_7">Table 7</ref>), Block2 (see <ref type="table" target="#tab_8">Table 8</ref>), Block3 (see <ref type="table" target="#tab_9">Table 9</ref>) and Block4 (see <ref type="table" target="#tab_0">Table 10</ref>). Using these, the network is assembled according to <ref type="table" target="#tab_5">Table 5</ref>. All blocks have the same number of channels for all convolutions throughout. An exception is Block4, which has 128 output channels for the first two and 42 for the last convolution. The number 42 arises from 21 keypoints we estimate 2D locations and depth for. Skip connections from Block1 to Block3 always branch off after the last convolution (id 3) of Block1 using the respective block that has the same spatial resolution.</p><p>We train the network for 300 k iterations with a batch size of 16. For optimization we use the Momentum solver with an initial learning rate of 0.001 and momentum of 0.9. Learning rate is lowered to 0.0001 after iteration 150 k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Pretrained network</head><p>We provide an additional version of the proposed crossdataset generalization experiment, which shows the influence of using a pretrained network. For this purpose, we use a ImageNet pretrained ResNet50 backbone, which we train to learn a direct mapping from images to normalized 3D pose. From the original ResNet50 we use the average pooled final features and process them using 2 fully connected layers with 2048 neurons each using ReLU activations and a final linear fully connected layer outputting the 63 parameters (= 21? 3D coordinates). Hyperparameters are identical to 7.1, except for the use of ADAM solver and an initial learning rate of 10 ?5 , which is lowered to 10 ?6 after iteration 150 k. The results are presented in <ref type="table" target="#tab_0">Table 11</ref>, which shows that the average ranks are mostly unchanged compared to the results reported in the main paper. We witness a tendency of lower performance on the respective evaluation set, but better generalization to other datasets.       in the main paper, which we refer to as Scorevolume loss, we used the Softargmax loss formulation <ref type="bibr" target="#b13">[14]</ref>. As reported in literature, we find that Softargmax achieves better results for keypoint estimation, but that the respective score for each prediction is less meaningful as a predictor of the expected error.</p><p>In both cases we define the score c of an prediction that MVNet makes as reported in the main paper and use the latent heat map of the Softargmax loss to calculate c. To analyze the relation between prediction score c and the expected error of the final prediction, we follow the methodlogy described in detail in <ref type="bibr" target="#b12">[13]</ref>, and report sparsification curves in <ref type="figure">Fig. 7</ref>. This plot analyses how the prediction error on a given dataset evolves by gradually removing uncertain predictions, as measured by the prediction score c. If the prediction score is a good proxy for the prediction error the curves should monotonically decrease to zero, because predictions with low score should identify samples with high error. The oracle curve shows the ideal curve for a respective loss and is created by accessing the ground truth error instead of using the prediction score, i.e. one is always removing the predictions with the largest error. <ref type="figure">Fig. 7</ref> shows that score of the Scorevolume loss shows much better behavior, because it stays fairly close to its oracle line. Which is in contrast to the Softargmax loss. We deduct from this experiment, that the scores that arise when training on a Scorevolume represent a more meaningful measure of the algorithms uncertainty and therefore it should be used for our labeling procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Implementation details</head><p>The part of network for 2D feature extraction is initialized with the network presented by Simon et al. <ref type="bibr" target="#b25">[27]</ref>. We use input images of size 224 ? 224, that show hand cropped images. For hand cropping we use a MobileNet architecture <ref type="bibr" target="#b11">[12]</ref> that is trained on Egohands <ref type="bibr" target="#b2">[3]</ref> and finetuned on  <ref type="figure">Figure 7</ref>: Shown is the average predictions 3D error of MVNet on a given dataset over the sparsification rate. When moving along the x-axis from left to right the remaining evaluation set gets smaller and the y-axis reports the error upon the remaining dataset. For each approach the resulting curves are shown, when the prediction score is used as solid lines, and when the ground truth error is used as dashed lines, which we refer to as oracle. A good score yields a line that stays close to the oracle line, which shows that Scorevolume trained networks learn much more meaningful scores than Softargmax. a small, manually labeled, subset of our data. From the 2D CNN we extract the feature encoding f i of dimension 28 ? 28 ? 128 after 12 convolutional layers, which is unprojected into a 64 ? 64 ? 64 ? 128 voxel grid F i of size 0.4 meters. The voxel grid is centered at a 3D point hypothesis that is calculated from triangulating the detected hand bounding box centers we previously used for image cropping.</p><p>For joining the unprojected information from all cameras we average F i over all views i and use a U-Net <ref type="bibr" target="#b24">[26]</ref> like encoder-decoder architecture in 3D. We train the network for 100 k training steps using ADAM solver. Batch size is 8 for the 2D CNNs and 1 for the 3D CNN. Scorevolume loss is used with ground truth Gaussian targets having standard deviation of 2 voxel units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Extended Evaluation of iterative procedure</head><p>In <ref type="figure" target="#fig_2">Fig. 8</ref>   <ref type="bibr" target="#b36">[38]</ref>. In contrast to the table reported in the main paper an ImageNet pretrained ResNet50 network is used for direct regression of normalized 3D pose. Entries are marked if they rank: first, second or third for each dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Image Compositing</head><p>Here we study different methods for post processing our recorded images in order to improve generalization of composite images. Green screen indicates that images are used as is i.e. no additional processing step was used. Cut&amp;Paste refers to blending the original image with a randomly sampled new background image using the foreground segmentation mask as blending alpha channel. Harmonization is a deep network based approach presented by Tsai et al. <ref type="bibr" target="#b29">[31]</ref>, which should improve network performance on composite images. Additionally, we experimented with the deep image colorization approach by Zhang et al. <ref type="bibr" target="#b34">[36]</ref>. For this processing step we convert the composite image of the Cut&amp;Paste method into a grayscale image and input it in the colorization method. Here we have the options Auto, in which the network hallucinates all colors and Sample where we provide the network with the actual colors in 20 randomly chosen sample points on each foreground and background. Examples of images these approaches yield are shown in <ref type="figure" target="#fig_3">Fig. 10</ref>. <ref type="table" target="#tab_0">Table 12</ref> reports results for the network described in 7.1 and <ref type="table" target="#tab_0">Table 13</ref> shows results when the pretrained baseline is used instead. The two tables show, that post processing methods are more important when networks are trained from scratch. In this case, <ref type="table" target="#tab_0">Table 12</ref> shows that using each of the processing options yields roughly the same gain in performance and using all options jointly performs best. This option is chosen for the respective experiments in the main paper. When a pretrained network is used <ref type="table" target="#tab_0">Table 13</ref> reports already good results, when the network is only trained on green screen images. Interestingly, in this scenario the more elaborate post processing methods yield only a minor gain compared to the Cut&amp;Paste strategy. We hypothesize these results are related to a significant level of robustness the pretrained weights possess. Please note that we can't use these algorithms in a similar manner for datasets that don't provide segmentation masks of the foreground object. Only RHD provides segmentation masks, which is why we show the influence the discussed processing methods have on its generalization. <ref type="table" target="#tab_0">Table 14</ref> shows that the discussed methods don't improve performance for RHD trained networks the same way. The results indicate that these strategies should not been seen as general data augmentation, but rather specific processing steps to alleviate the problem of green screen color bleeding we witness on our training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">FreiHAND details</head><p>For the dataset we recorded 32 people and asked them to perform actions in front of the cameras. The set of non object actions included: signs from the american sign language, counting, move their fingers to their kinematic limits. The set of objects contains different types of workshop tools like drills, wrenches, screwdrivers or hammers. Different types of kitchen supply were involved as well, f.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>method eval RHD Ours</head><p>Green Screen 0.246 0.440 Cut&amp;Paste 0.350 0.508 Harmonization <ref type="bibr" target="#b29">[31]</ref> 0.443 0.628 Colorization Auto <ref type="bibr" target="#b34">[36]</ref> 0.458 0.634 Colorization Sample <ref type="bibr" target="#b34">[36]</ref> 0.494 0.643 Joint 0.518 0.678 0.416 0.513 Colorization Auto <ref type="bibr" target="#b34">[36]</ref> 0.368 0.478 Colorization Sample <ref type="bibr" target="#b34">[36]</ref> 0.381 0.496 Joint 0.399 0.523 <ref type="table" target="#tab_0">Table 13</ref>: Instead of the network architecture used in the main paper, we use a ResNet50 Baseline as described in 7.2. When the network is initialized with weights that already show a certain level of robustness the importance of background removal and recombination with post processing is less pronounced, but still improves results substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>method eval RHD Ours</head><p>Original 0.767 0.508 Harmonization <ref type="bibr" target="#b29">[31]</ref> 0.723 0.517 Colorization Auto <ref type="bibr" target="#b34">[36]</ref> 0.726 0.472 Colorization Sample <ref type="bibr" target="#b34">[36]</ref> 0.748 0.501 Joint 0.756 0.514 <ref type="table" target="#tab_0">Table 14</ref>: Network architecture as used in the main paper, but instead of training on our dataset we train on RHD and apply the same post processing methods to it. We chose RHD as reference because it is the only dataset that also provides foreground segmentation masks, which are needed for the processing methods. The   of the recording or hang into our setup and we recorded the process of grabbing the object. Actions that contain interaction with objects include the following items: Hammer, screwdrive, drill, scissors, tweezers, desoldering pump, stapler, wrench, chopsticks, caliper, power plug, pen, spoon, fork, knive, remote control, cream tube, coffee cup, spray can, glue pistol, frisbee, leather cover, cardboard box, multi tool and different types of spheres (f. e. apples, oranges, styrofoam). The action were selected such that all major grasp types were covered including power and precision grasps or spheres, cylinders, cubes and disks as well as more specialized object specific grasps.</p><p>These recordings form the basis we run our iterative labeling procedure on, that created the dataset presented in the main paper. Some examples of it are shown in <ref type="figure" target="#fig_4">Fig. 11</ref>. <ref type="figure">Fig. 9</ref> shows one dataset sample containing 8 images recorded at a unique time step from the 8 different cameras involved in our capture setup. One can see that the cameras capture a broad spectrum of viewpoints around the hand and how for different cameras different fingers are occluded. Our datasets shape annotation is overlayed in half of the views.</p><p>Furthermore, we provide more qualitative examples of our single view shape estimating network in <ref type="figure" target="#fig_5">Fig. 12</ref>.</p><p>Distributions across genders and ethnicity's are reported in <ref type="table" target="#tab_0">Table 16</ref>, whereas <ref type="table" target="#tab_0">Table 15</ref> shows the distribution of labeled samples across gender and object interaction.</p><p>The dataset was recorded using the following hardware: Two Basler acA800-510uc and six Basler acA1300-200uc color cameras that were hardware triggered using the GPIO module by numato. The cameras were equipped with fixed <ref type="figure">Figure 9</ref>: Here we show one sample of our dataset, which consists of 8 images that are recorded at the same time instance. We overlay the shape label found with our method in some of the images. lenses with either 4mm or 6mm focal length. The recording setup is approximately forming a cube of edge length 1m with one of the color cameras being located in each of the corners. The subjects then reached inside the cubicle through one of the cubes' faces, which approximately put their hand at an equal distance to all the cameras. When recording for the evaluation split, we used ambient lighting. To improve lighting during green screen recording there were 4 powerful LED lights as used during recording with a video camcorder. These allowed to vary in terms of lighting power and light temperature during the recordings.  More qualitative examples of predicted hand shapes that our single view network makes. Again, we don't apply any alignment of the predictions with respect to the ground truth, which explains minor miss alignment whereas the hand articulation is captured correctly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Recording setup with 8 calibrated and temporally synchronized RGB cameras located at the corners of a cube. A green screen background can be mounted into the the setup, enabling easier background subtraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>We show performance measured as 3D PCK of MVNet over different iterations of our procedure. It shows that training MVNet only on the Panoptic dataset is not sufficient to generalize to our data an that each iteration improves performance. In brackets the 3D AUC is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 10 :</head><label>10</label><figDesc>Visualizations of our explored post processing options on the same sample. From left to right: Original frame, Cut&amp;Paste, Harmonization<ref type="bibr" target="#b29">[31]</ref>, Colorization Auto<ref type="bibr" target="#b34">[36]</ref>, Colorization: Sample<ref type="bibr" target="#b34">[36]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 11 :</head><label>11</label><figDesc>Examples from our proposed dataset showing images and hand shape annotations. It shows the final images that are used for training of single view methods with the original background replaced and [31] applied for post processing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: More qualitative examples of predicted hand shapes that our single view network makes. Again, we don't apply any alignment of the predictions with respect to the ground truth, which explains minor miss alignment whereas the hand articulation is captured correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>This table shows cross-dataset generalization measured as area under the curve (AUC) of percentage of correct keypoints following</figDesc><table><row><cell>train</cell><cell>eval</cell><cell>STB</cell><cell>RHD</cell><cell>GAN</cell><cell>PAN</cell><cell>LSMV</cell><cell>FPA</cell><cell cols="2">HO-3D Ours</cell><cell>Average Rank</cell></row><row><cell cols="2">STB [35]</cell><cell cols="2">0.783 0.179</cell><cell>0.067</cell><cell>0.141</cell><cell>0.072</cell><cell>0.061</cell><cell>0.138</cell><cell>0.138</cell><cell>6.0</cell></row><row><cell cols="2">RHD [38]</cell><cell cols="3">0.362 0.767 0.184</cell><cell>0.463</cell><cell>0.544</cell><cell>0.101</cell><cell>0.450</cell><cell>0.508</cell><cell>2.9</cell></row><row><cell cols="2">GAN [23]</cell><cell>0.110</cell><cell cols="3">0.103 0.765 0.092</cell><cell>0.206</cell><cell>0.180</cell><cell>0.087</cell><cell>0.183</cell><cell>5.4</cell></row><row><cell cols="2">PAN [15]</cell><cell>0.459</cell><cell>0.316</cell><cell cols="3">0.136 0.870 0.320</cell><cell>0.184</cell><cell>0.351</cell><cell>0.407</cell><cell>3.0</cell></row><row><cell cols="2">LSMV [8]</cell><cell>0.086</cell><cell>0.209</cell><cell>0.152</cell><cell cols="3">0.189 0.717 0.129</cell><cell>0.251</cell><cell>0.276</cell><cell>4.1</cell></row><row><cell cols="2">FPA [6]</cell><cell>0.119</cell><cell>0.095</cell><cell>0.084</cell><cell>0.120</cell><cell cols="2">0.118 0.777</cell><cell>0.106</cell><cell>0.163</cell><cell>6.0</cell></row><row><cell cols="3">HO-3D [9] 0.154</cell><cell>0.130</cell><cell>0.091</cell><cell>0.111</cell><cell>0.149</cell><cell>0.073</cell><cell>-</cell><cell>0.169</cell><cell>6.1</cell></row><row><cell cols="2">Ours</cell><cell>0.473</cell><cell>0.518</cell><cell>0.217</cell><cell>0.562</cell><cell>0.537</cell><cell>0.128</cell><cell cols="2">0.557 0.678</cell><cell>2.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="4">Our single view network architecture used for 3D</cell></row><row><cell cols="4">pose estimation in the cross-dataset generalization experi-</cell></row><row><cell>ment.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>id</cell><cell>Name</cell><cell cols="2">Kernel Stride</cell></row><row><cell cols="3">1 Conv. + ReLU 3 ? 3</cell><cell>1</cell></row><row><cell>2</cell><cell>Avg. Pool</cell><cell>4 ? 4</cell><cell>1</cell></row><row><cell cols="3">3 Conv. + ReLU 3 ? 3</cell><cell>1</cell></row><row><cell>4</cell><cell>Avg. Pool</cell><cell>4 ? 4</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Block0.</figDesc><table><row><cell>id</cell><cell>Name</cell><cell cols="2">Kernel Stride</cell></row><row><cell cols="3">1 Conv. + ReLU 3 ? 3</cell><cell>1</cell></row><row><cell>2</cell><cell>Avg. Pool</cell><cell>4 ? 4</cell><cell>2</cell></row><row><cell cols="3">3 Conv. + ReLU 3 ? 3</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Block1.</figDesc><table><row><cell>id</cell><cell>Name</cell><cell cols="2">Kernel Stride</cell></row><row><cell cols="3">1 Conv. + ReLU 3 ? 3</cell><cell>1</cell></row><row><cell>2</cell><cell>Upconv.</cell><cell>4 ? 4</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Block2.</figDesc><table><row><cell>8. MVNet</cell></row><row><cell>8.1. Training loss</cell></row><row><cell>We experimented with different losses for training</cell></row><row><cell>MVNet and witnessed large differences regarding their ap-</cell></row><row><cell>plicability to our problem. In addition to the loss described</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Block3.</figDesc><table><row><cell>id</cell><cell>Name</cell><cell cols="2">Kernel Stride</cell></row><row><cell cols="3">1 Conv. + ReLU 7 ? 7</cell><cell>1</cell></row><row><cell cols="3">2 Conv. + ReLU 7 ? 7</cell><cell>1</cell></row><row><cell>3</cell><cell>Conv.</cell><cell>7 ? 7</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Block4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>, we show how the 3D keypoint estimation accuracy of MVNet evolves over iterations. For comparison, we also shown how MVNet performs when trained on the Panoptic (PAN) dataset alone. While this gives insufficient performance, joint training on PAN and our dataset yields a large gain in performance in the first iteration and every other iteration provides further improvement.</figDesc><table><row><cell>train</cell><cell>eval</cell><cell>STB</cell><cell>RHD</cell><cell>GAN</cell><cell>PAN</cell><cell>LSMV</cell><cell>FPA</cell><cell cols="2">HO3D Ours</cell><cell>Average Rank</cell></row><row><cell cols="2">STB</cell><cell cols="2">0.687 0.247</cell><cell>0.151</cell><cell>0.263</cell><cell>0.220</cell><cell>0.138</cell><cell>0.207</cell><cell>0.244</cell><cell>5.6</cell></row><row><cell cols="2">RHD</cell><cell cols="3">0.480 0.697 0.200</cell><cell>0.353</cell><cell>0.490</cell><cell cols="3">0.156 0.417 0.403</cell><cell>2.9</cell></row><row><cell cols="2">GAN</cell><cell>0.184</cell><cell cols="3">0.198 0.624 0.217</cell><cell>0.229</cell><cell>0.182</cell><cell>0.188</cell><cell>0.233</cell><cell>5.8</cell></row><row><cell cols="2">PAN</cell><cell>0.447</cell><cell>0.367</cell><cell cols="3">0.221 0.632 0.454</cell><cell>0.205</cell><cell>0.264</cell><cell>0.345</cell><cell>3.0</cell></row><row><cell cols="2">LSMV</cell><cell>0.242</cell><cell>0.286</cell><cell>0.199</cell><cell cols="3">0.226 0.640 0.162</cell><cell>0.283</cell><cell>0.307</cell><cell>4.4</cell></row><row><cell cols="2">FPA</cell><cell>0.186</cell><cell>0.178</cell><cell>0.156</cell><cell>0.206</cell><cell cols="3">0.197 0.705 0.162</cell><cell>0.239</cell><cell>6.6</cell></row><row><cell cols="2">HO3D</cell><cell>0.254</cell><cell>0.311</cell><cell>0.198</cell><cell>0.206</cell><cell>0.338</cell><cell>0.148</cell><cell>-</cell><cell>0.313</cell><cell>5.4</cell></row><row><cell cols="2">Ours</cell><cell>0.520</cell><cell>0.399</cell><cell>0.205</cell><cell>0.395</cell><cell>0.509</cell><cell>0.208</cell><cell cols="2">0.416 0.523</cell><cell>2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>This table shows, cross-dataset generalization measured as area under the curve (AUC) of percentage of correct keypoints following</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Network architecture as used in the main paper. When training networks from scratch, it is important to introduce background variation by inserting random backgrounds into the green screen recordings. Using post processing algorithms improves generalization.</figDesc><table><row><cell>method</cell><cell>eval RHD</cell><cell>Ours</cell></row><row><cell>Green Screen</cell><cell>0.338</cell><cell>0.436</cell></row><row><cell>Cut&amp;Paste</cell><cell>0.386</cell><cell>0.468</cell></row><row><cell>Harmonization [31]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>table shows that none of them yields clear improvements over using the original unaltered RHD dataset for training.chopsticks, cutlery, bottles or BBQ tongs. These objects were either placed into the subjects hand from the beginning</figDesc><table><row><cell>Part</cell><cell>Samples with object</cell><cell>Samples w/o object</cell><cell cols="3">male female total</cell></row><row><cell>Training (green screen)</cell><cell>2580</cell><cell cols="2">1490 2462</cell><cell cols="2">1608 4070</cell></row><row><cell>Evaluation (total)</cell><cell>339</cell><cell>156</cell><cell>290</cell><cell>205</cell><cell>495</cell></row><row><cell>Evaluation (plane space office)</cell><cell>119</cell><cell>40</cell><cell>63</cell><cell>96</cell><cell>159</cell></row><row><cell>Evaluation (outdoor)</cell><cell>114</cell><cell>47</cell><cell>88</cell><cell>73</cell><cell>161</cell></row><row><cell>Evaluation (meeting room)</cell><cell>106</cell><cell>69</cell><cell>139</cell><cell>36</cell><cell>175</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Distribution of labeled samples in our dataset across different aspects.</figDesc><table><row><cell>Ethnicity</cell><cell cols="2">Training Evaluation</cell></row><row><cell>Caucasian (NA, Europe, ...)</cell><cell>14</cell><cell>11</cell></row><row><cell>South Asian (India, Pakistan, ...)</cell><cell>3</cell><cell>2</cell></row><row><cell>East Asia (China, Vietnam, ...)</cell><cell>2</cell><cell>0</cell></row><row><cell>Male subjects</cell><cell>15</cell><cell>6</cell></row><row><cell>Female subjects</cell><cell>9</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Gender and ethnicity distribution of recorded subjects in the dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge funding by the Baden-W?rttemberg Stiftung as part of the RatTrack project. Work was partially done during Christian's internship at Adobe Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Motion capture of hands in action using discriminative salient points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Taneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="640" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Bambach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1949" to="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. Conf. on Computer Vision (ECCV)</title>
		<meeting>of the Europ. Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d hand shape and pose from images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnane</forename><surname>Boukhayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>De Bem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">First-person hand action benchmark with rgb-d videos and 3d hand pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryul</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00812</idno>
		<title level="m">3d hand shape and pose estimation from a single rgb image</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale multiview 3d hand pose dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Gomez-Donoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ho-3d: A multi-user, multi-object dataset for joint 3d hand-object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Hampali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01481</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning joint reconstruction of hands and manipulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Kalevatykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11807" to="11816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inverse kinematic infrared optical finger tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerrit</forename><surname>Hillebrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Achatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gudrun</forename><surname>Klinker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Am</forename><surname>Oferl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Humans and Computers (HC 2006)</title>
		<meeting>the 9th International Conference on Humans and Computers (HC 2006)<address><addrLine>Aizu, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uncertainty estimates and multi-hypotheses networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozgun</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Galesso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osama</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. Conf. on Computer Vision (ECCV)</title>
		<meeting>of the Europ. Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="652" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hand pose estimation via latent 2.5 d heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. Conf. on Computer Vision (ECCV)</title>
		<meeting>of the Europ. Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Neural Information Processing Systems (NIPS)</title>
		<meeting>of Int. Conf. on Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ganerated hands for real-time 3d hand tracking from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time hand tracking under occlusion from an egocentric rgb-d sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV), October 2017</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time joint tracking of a hand manipulating an object from rgbd input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. Conf. on Computer Vision (ECCV)</title>
		<meeting>of the Europ. Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sphere-meshes for real-time hand modeling and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">222</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online generative model personalization for hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">243</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep image harmonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Capturing hands in action using discriminative salient points and physics simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilash</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="193" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Capturing hand motion with an rgb-d sensor, fusing a generative model with salient points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilash</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the German Conf. on Computer Vision (GCPR)</title>
		<meeting>of the German Conf. on Computer Vision (GCPR)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="277" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time hand-tracking with a color glove</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqiong</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07214</idno>
		<title level="m">3d Hand Pose Tracking and Estimation Using Stereo Matching</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Real-time user-guided image colorization with learned deep priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02999</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A hand gesture interface device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaron</forename><surname>Thomas G Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Lanier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harvill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCHI Bulletin</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d hand pose from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.01389.1,3,4" />
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
