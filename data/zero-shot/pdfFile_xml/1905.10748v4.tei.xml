<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Smooth Representation for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Mengchu</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesham</forename><surname>Alhumade</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Die</forename><surname>Hu</surname></persName>
						</author>
						<title level="a" type="main">Learning Smooth Representation for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transfer learning</term>
					<term>unsupervised domain adap- tation</term>
					<term>Lipschitz constraint</term>
					<term>local smooth discrepancy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Typical adversarial-training-based unsupervised domain adaptation methods are vulnerable when the source and target datasets are highly-complex or exhibit a large discrepancy between their data distributions. Recently, several Lipschitzconstraint-based methods have been explored. The satisfaction of Lipschitz continuity guarantees a remarkable performance on a target domain. However, they lack a mathematical analysis of why a Lipschitz constraint is beneficial to unsupervised domain adaptation and usually perform poorly on large-scale datasets. In this paper, we take the principle of utilizing a Lipschitz constraint further by discussing how it affects the error bound of unsupervised domain adaptation. A connection between them is built and an illustration of how Lipschitzness reduces the error bound is presented. A local smooth discrepancy is defined to measure Lipschitzness of a target distribution in a pointwise way. When constructing a deep end-to-end model, to ensure the effectiveness and stability of unsupervised domain adaptation, three critical factors are considered in our proposed optimization strategy, i.e., the sample amount of a target domain, dimension and batchsize of samples. Experimental results demonstrate that our model performs well on several standard benchmarks. Our ablation study shows that the sample amount of a target domain, the dimension and batchsize of samples indeed greatly impact Lipschitz-constraint-based methods' ability to handle large-scale datasets. Code is available at https://github.com/CuthbertCai/ SRDA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>G. <ref type="bibr">Cai</ref>  shift between training and testing distributions <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. With the development of computer vision, advanced tasks, such as self-driving cars and robots, need large-scale annotated data. Due to the labor-intensive process of annotation, using synthetic data and computer-generated annotation becomes popular. However, training samples from simulators are synthesized by 3D rendering models whereas testing samples are real-world scenes. The large discrepancy between training and testing distributions and complexity of image information cause the failures of classical UDA models easily <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>. More powerful and robust UDA algorithms are desiderated to cope with these progressive situations.</p><p>Typical UDA algorithms can be divided into two main categories: homogeneous and heterogeneous. The significant difference between them is that the former assumes that the input space of domains is the same while the latter requires no such assumption. For example, the latter can transfer knowledge from a text dataset to an image one whereas homogeneous methods cannot. In this work, we focus on homogeneous UDA.</p><p>A typical schema of homogeneous UDA was presented in <ref type="bibr" target="#b1">[2]</ref>. The divergence between different distributions is first estimated and then an appropriate optimization strategy is introduced to minimize it. Several UDA methods <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref> are based on Maximum Mean Discrepancy (MMD). Deep kernels were used to estimate MMD between different distributions <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and optimization strategies were proposed to minimize MMD <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Similar to MMD, a correlation matrix was introduced in <ref type="bibr" target="#b13">[14]</ref> to measure the discrepancy between different domains. Using the mutual information as a measurement was introduced in <ref type="bibr" target="#b14">[15]</ref>. Many other UDA methods utilized a Proxy A-distance <ref type="bibr" target="#b1">[2]</ref> to measure the divergence between source and target distributions. A strategy to control the Proxy A-distance is to find a feature space of examples where both source and target domains are as indistinguishable as possible <ref type="bibr" target="#b1">[2]</ref>. To get indistinguishable feature space, an adversarial training strategy <ref type="bibr" target="#b15">[16]</ref> was proposed by <ref type="bibr">Ganin et al.</ref> where an auxiliary network tries to distinguish source and target domains while a main network tries to make domains indistinguishable and classify images. Studies <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> followed an adversarial training strategy and conducted it on a pixel-level to enhance models' performance. However, this schema faces two issues. First, although various estimation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref> is proposed to measure the divergence between source and target distributions, their estimation error becomes larger for more complex distributions in general. Second, a large discrepancy exists when both source and target distributions are complex. It is difficult to design an optimiza-arXiv:1905.10748v4 [cs.CV] <ref type="bibr" target="#b15">16</ref> Aug 2021 tion strategy for reducing such a discrepancy. Neither direct minimization <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref> nor adversarial training <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> have shown stable performance in the cases of complex distributions.</p><p>Another schema <ref type="bibr" target="#b20">[21]</ref> was introduced by virtual adversarial training <ref type="bibr" target="#b21">[22]</ref> as a regularization method to avoid a gradient vanishing problem of domain adversarial training <ref type="bibr" target="#b18">[19]</ref>. Experimental results in <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref> have proved that a local-Lipschitz constraint is effective in UDA and semi-supervised learning. However, they owed its success to a cluster assumption <ref type="bibr" target="#b23">[24]</ref>, i.e., a local-Lipschitz constraint helps input samples get divided into clusters. According to the assumption, input samples in the same cluster come from the same category. They did not analyze mathematically how a local-Lipschitz constraint affects the error bound of a UDA problem. They cannot be applied to large-scale datasets because they ignored the dimension of samples that affects the error bound of a UDA problem. Another problem is that domain adversarial training is still used by them. It is thus possible to cause a gradient vanishing problem <ref type="bibr" target="#b18">[19]</ref>, thereby degrading their performance. The poor performance on large-scale datasets and a gradient vanishing problem prevent previous Lipschitz-constraint-based UDA methods from being applied to real-world scenarios.</p><p>Heterogeneous UDA algorithms face a challenge if the source domain and target domain have different features and distributions, especially in cross-modal applications. The generalization error bound for heterogeneous UDA was analyzed in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Several methods were proposed to align feature space and distributions jointly, such progressive alignment <ref type="bibr" target="#b26">[27]</ref> and nonlinear matrix factorization <ref type="bibr" target="#b27">[28]</ref>. Further, several methods extended heterogeneous UDA to more settings, where an optimal transport theory was introduced in <ref type="bibr" target="#b28">[29]</ref> to tackle semisupervised heterogeneous UDA and fuzzy-relation nets were proposed in <ref type="bibr" target="#b29">[30]</ref> to tackle multi-source heterogeneous UDA.</p><p>In this paper, we focus on homogeneous UDA and intend to answer why a local-Lipschitz constraint is effective in solving UDA problems and analyze several essential factors that prevent previous Lipschitz-constraint-based methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> from working well on large-scale datasets. According to <ref type="bibr" target="#b30">[31]</ref>, the error bound of a UDA problem is determined by probabilistic Lipschitzness and a constant term. A local-Lipschitz constraint is a special case of probabilistic Lipschitzness such that it helps us tackle UDA problems. However, the methods in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> ignored the constant term that would be extremely large when dealing with large-scale datasets. Does such a large term lead to the poor performance of previous Lipschitz-constraint-based methods? This work answers it.</p><p>To expand the application scope of Lipschitz-constraintbased methods, the probabilistic Lipschitzness is further extended by proposing a more concise way to achieve Lipschitz continuity in a target distribution through a newly defined concept called local smooth discrepancy. An optimization strategy that takes constant factors analyzed in <ref type="bibr" target="#b30">[31]</ref> into consideration is established. It enables our model to cope with large-scale datasets efficiently and stably. Our model contains a feature generator and classifier. The latter tries to classify source samples correctly and detect sensitive target samples that break down a Lipschitz constraint. The former is trained to strengthen the Lipschitzness of these sensitive samples. The defined local smooth discrepancy measures the Lipschitzness of a target distribution in a pointwise way. Then two specific methods are introduced to compute it. Utilizing it, a detailed optimization strategy is proposed to tackle a UDA problem by considering the effects of the dimension and batchsize of samples and the sample amount of a target domain. This work aims to make the following contributions to advance the field: 1) A mathematical analysis is for the first time conducted to explain why a local-Lipschitz constraint reduces the error bound of a UDA problem; 2) A novel and concise approach to achieve probabilistic Lipschitzness is proposed. The concept of a local smooth discrepancy is defined and used to measure Lipschitzness; 3) An elaborated optimization strategy that takes the dimension and batchsize of samples and the sample amount of a target domain into consideration is presented. It enables Lipschitz-constraint-based methods to perform effectively and stably on large-scale datasets. In addition, the proposed approach is extensively evaluated on several standard benchmarks. The results demonstrate that our method performs well on all of them. The ablation study is conducted to validate the role of batchsize and dimension of samples and sample amount of a target domain in its performance changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A. Distribution-Matching-Based UDA Theoretical work <ref type="bibr" target="#b1">[2]</ref> confirmed that a discrepancy between source and target distributions causes an invalid model in a target domain. Because the distribution of a domain is difficult to illustrate, an intuitive thought is to match the statistic characteristics of two distributions instead. Maximum mean discrepancy (MMD), which measures expectation difference in reproducing kernel Hilbert space of source and target distributions, has been widely used <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b32">[32]</ref>. Besides MMD-based methods, many other methods utilized different tricks to match distributions. For example, the covariance of features was utilized in <ref type="bibr" target="#b13">[14]</ref> to match different domains. The label information was used in <ref type="bibr" target="#b33">[33]</ref> to enhance the distribution matching between different domains in a shared subspace. Using the mutual information as a measurement was introduced in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adversarial-Training-Based UDA</head><p>Domain-Adversarial Training Network (DANN) was first introduced by Ganin et al. <ref type="bibr" target="#b15">[16]</ref>. An adversarial training strategy to tackle a UDA problem was used in DANN. A domain classifier was introduced to predict which domain a sample is drawn from. Their feature generator was trained to fool its domain classifier such that features from different domains are well-matched. Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b34">[34]</ref> was proposed by Tzeng et al. that followed this strategy <ref type="bibr" target="#b15">[16]</ref> and introduced an asymmetric network architecture to obtain a more discriminative representation. Instead of conducting adversarial training in feature space as DANN and ADDA did, several methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b35">[35]</ref> implemented adversarial training at pixel level. They tried to generate target images from labeled source images. In this way, a classification model can be trained with labeled target images. Specifically, Pixel-level UDA proposed by Bousmalis et al. <ref type="bibr" target="#b17">[18]</ref> followed a training strategy of generative adversarial networks (GANs) <ref type="bibr" target="#b36">[36]</ref> and obtained excellent performance on digits datasets. The methods proposed by Murez et al. <ref type="bibr" target="#b35">[35]</ref> and Liu et al. <ref type="bibr" target="#b16">[17]</ref> introduced a training strategy similar to cycle GANs to improve their models' performance. Besides taking a marginal distribution into consideration, a conditional domain adversarial network (CDAN) proposed by Long et al. <ref type="bibr" target="#b37">[37]</ref> adopted the joint distribution of samples and labels in an adversarial training. The label information was used to enhance adversarial training and achieve remarkable results. Specifically, CDAN exploited discriminative information conveyed in the classifier predictions to assist adversarial training. Based on the domain adaptation theory <ref type="bibr" target="#b1">[2]</ref>, CDAN gave a theoretical guarantee on the generalization error bound and achieved the best results on five benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Large-Margin-Based UDA</head><p>Several UDA methods <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref> assumed that classifiers provide adequate information about different distributions. They enforced robust classifiers with large margins. Asymmetric tri-training <ref type="bibr" target="#b38">[38]</ref> added two auxiliary classifiers to assist in generating valid pseudo labels for target samples. They constructed decision boundaries with large margins for a target domain with the help of two auxiliary classifiers. The critical effect of large enough margins was also explored by Lu et al. <ref type="bibr" target="#b40">[40]</ref>. Maximum Classifier Discrepancy for Domain Adaptation (MCD) <ref type="bibr" target="#b39">[39]</ref> was proposed by Saito et al. and it well answered why the use of matching marginal distributions causes misclassification and why constructing decision boundaries with large margins reduces the error bound of UDA. It also proposed a Siamese-like network and adversarial training strategy to solve a UDA problem. Deep Max-Margin Gaussian Process Approach (GPDA) <ref type="bibr" target="#b41">[41]</ref> adopted MCD's principle and introduced a Gaussian process to enhance GPDA's performance. MCD and GPDA were comparable with each other and superior to previous methods on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Lipschitz-Constraint-Based UDA</head><p>Recently, a local-Lipschitz constraint <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref> was introduced into UDA and semi-supervised learning to avoid an unstable training process faced by some adversarial-trainingbased UDA methods <ref type="bibr" target="#b18">[19]</ref>. Compared with MMD-based methods, they got rid of matching different distributions. When aligning distributions, MMD-based methods need to estimate some statistics of both source and target domains. Two problems are confronted in such alignment. First, none of the statistics can describe a distribution perfectly because they represent only part of the distribution. Second, it is hard to estimate statistics of a high-dimensional distribution precisely. However, Lipschitz-constraint-based methods only need to satisfy a Lipschitz constraint in the target domain such that drawbacks of MMD-based methods are avoided. In detail, the virtual adversarial training <ref type="bibr" target="#b21">[22]</ref> firstly introduced a Lipschitz constraint into semi-supervised learning. Decision-boundary Iterative Refinement Training with a Teacher (DIRT) <ref type="bibr" target="#b20">[21]</ref> explicitly incorporated the virtual adversarial training <ref type="bibr" target="#b21">[22]</ref> and added the loss of it to the objective function as an additional term. It indeed improved the performance on several benchmark datasets. However, DIRT introduced a cluster assumption <ref type="bibr" target="#b23">[24]</ref> to explain its effectiveness instead of rigorous proof. Meanwhile, an adversarial training in its optimization procedure could lead to a vanishing gradient problem during training <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Heterogeneous UDA</head><p>Besides minimizing distribution discrepancy, heterogeneous UDA algorithms have to project cross-modal features into a common feature space. A knowledge transfer theorem and a principal angle-based metric were given in <ref type="bibr" target="#b25">[26]</ref>. A fuzzyrelation net <ref type="bibr" target="#b29">[30]</ref> was introduced by Liu et al. to expand heterogeneous UDA to a multi-source setting. A progressive alignment was introduced in <ref type="bibr" target="#b26">[27]</ref> and was used to optimize both feature discrepancy and distribution divergence in a unified objective function. A nonlinear matrix factorization was proposed in <ref type="bibr" target="#b27">[28]</ref> where nonlinear correlations between features and data instances could be exploited to learn heterogeneous features for different domains. The optimal transport theory was utilized in <ref type="bibr" target="#b28">[29]</ref> to tackle the semi-supervised heterogeneous UDA problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARY</head><p>In this section, we give a brief description of a UDA problem and define several properties relevant to realizing a practical UDA algorithm. Moreover, a basic UDA error bound is derived from these properties <ref type="bibr" target="#b30">[31]</ref>  <ref type="bibr" target="#b0">1</ref> .</p><p>Let (?, ?) be a domain set where ? denotes a set of samples and ? : ? 2 ? R + is a divergence metric over ?. R + is the set of all non-negative real numbers. In a UDA setup, we denote P S and P T as a source and target distribution, respectively. The marginal distributions of P S and P T over ? are denoted by D S and D T and their labeling rules are denoted by l s : ? ? [0, 1] and l t : ? ? [0, 1]. The goal is to learn a function l : ? ? {0, 1} which predicts correct labels for samples in ? with respect to P T over (? ? {0, 1}). Considering that l s and l t are defined with a continuous range while l is defined with a concrete range, a common way to map l s and l t to a concrete range is to set a threshold ?. If l s (x) &gt; ? (l t (x) &gt; ?), then x is assigned with 1 else 0. For any hypothesis h : ? ? {0, 1}, we define the error with respect to P T by E P T (h) = P (x,y)?P T (y = h(x)) where P denotes a probability that the label of x ? P T is different from the output of h(x). Thus, the Bayes optimal error for P T is E * (P T ) := min h?{0,1} ? E P T (h).</p><p>Besides basic notations, in the context of realistic UDA problems, there are some properties expressing several conditions of these distributions that enable a practical UDA algorithm <ref type="bibr" target="#b30">[31]</ref>.</p><p>Definition 1 (Covariate shift <ref type="bibr" target="#b42">[42]</ref>). Source and target distributions satisfy the covariate shift property if they have the same labeling rule, i.e., l s (x) = l t (x). The common labeling function is denoted as l = l s = l t . Definition 2 (Probabilistic Lipschitzness <ref type="bibr" target="#b30">[31]</ref>). Let ? :</p><formula xml:id="formula_0">R ? [0, 1]. f : ? ? R is ?-Lipschitz w.r.t. distribution D over ? if, for all ? &gt; 0: P x?D [?y : |f (x) ? f (y)| &gt; ??(x, y)] ? ?(?)<label>(1)</label></formula><p>This definition generalizes the standard ?-Lipschitz property where a function f :</p><formula xml:id="formula_1">? ? R satisfies |f (x) ? f (y)| ? ??(x, y) for all x, y ? ?.</formula><p>In the standard ?-Lipschitz property, if the labeling function is deterministic, just like the goal function l(x) ? {0, 1} for all x ? ?, the standard property forces ?(x, y) ? 1/? if x and y belong to different category and |f (x) ? f (y)| ? {0, 1} for all x and y. Thus, the standard property is unfit for a concrete range space. However, the probabilistic Lipschitzness just requires the inequality |f (x) ? f (y)| ? ??(x, y) to hold only with some probability. Namely, it does not require all points to hold the inequality. Unlike the standard property that becomes meaningless in a concrete range space because |f (x) ? f (y)| is always 0 or 1, the probabilistic Lipschitzness forms a constraint over P x?D [?y : |f (x) ? f (y)| &gt; ??(x, y)] that is defined with a continuous range space. The more y ? ? that satisfies the inequality, the greater the probability. Such relaxation makes the probabilistic Lipschitzness applicable to the goal function l(x) ? {0, 1}.</p><p>Definition 3 (Weight ratio <ref type="bibr" target="#b30">[31]</ref>). Let B ? 2 ? be a collection of subsets of ? measurable with respect to both D S and D T . For some ? &gt; 0 we define the ?-weight ratio of source and target distributions with respect to B as</p><formula xml:id="formula_2">R B,? (D S , D T ) = inf b?B D T (b)?? D S (b) D T (b) ,<label>(2)</label></formula><p>Further, the weight ratio of source and target distributions with respect to B is defined as</p><formula xml:id="formula_3">R B (D S , D T ) = inf b?B D T (b) =0 D S (b) D T (b) ,<label>(3)</label></formula><p>A basic observation about UDA methods is that they can be infeasible when D S and D T are supported on disjoint domain regions. To guard against such scenarios, it is common to assume R B (D S , D T ) &gt; 0.</p><p>According to Definition 1-3, an error bound was derived in <ref type="bibr" target="#b30">[31]</ref> for a general UDA learning procedure based on the Nearest Neighbor algorithm. Note that ? should be a fixeddimension space and B should be of finite VC-dimension such that the ?-weight ratio can be estimated from finite samples <ref type="bibr" target="#b30">[31]</ref>. In detail, a domain is assumed to be a unit cube ? = [0, 1] d . B denotes a set of axes aligned rectangles in [0, 1] d and, given some ? &gt; 0, let B ? be a class of axes aligned rectangles with sidelength ?.</p><p>Given a labeled sample batch S ? (? ? {0, 1}), S ? denotes the set of samples without labels of S. For any sample x ? S ? , l S (x) denotes the label of x in S and N S (x) denotes the nearest neighbor to x in S, N S (x) = arg min z?S? ?(x, z). Define a hypothesis determined by the Nearest Neighbor algorithm as H(x) = l S (N S (x)) for all x ? ?.</p><p>Theorem 1 (Error bound <ref type="bibr" target="#b30">[31]</ref>). For some domain ? = [0, 1] d , some R &gt; 0 and ? &gt; 0, let W be a class of pairs (P S , P T ) of source and target distributions over (? ? {0, 1}) satisfying the covariate shift assumption, with R B? (D S , D T ) ? R, and their common labeling function l : ? ? {0, 1} satisfying the ?-probabilistic-Lipschitz property w.r.t a target distribution, for some function ?. Then, for all m, and all (P S , P T ) ? W,</p><formula xml:id="formula_4">E S?P m S [E P T (H)] ? 2E * (P T ) + ?(?) + 4? ? d Rm 1 d+1 (4)</formula><p>where m denotes the size of S containing points sampled i.i.d. from ?.</p><p>Theorem 1 offers a sufficiently tight error bound for practical UDA, although the first term of Theorem 1 is twice the Bayes error rate. According to <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b39">[39]</ref>, for a practical UDA problem, E * (P T ) is a constant that is considered sufficiently low to achieve an accurate adaptation with respect to Definition 1. Otherwise, the UDA problem is impractical to solve. The impact of E * (P T ) is minimal while the other two terms affect the error bound obviously. Practically, several UDA methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> based on Theorem 1 show their effectiveness on standard benchmarks. It verifies that Theorem 1's error bound is tight enough for a practical UDA problem. Moreover, considering that the second and third terms affect the error bound heavily, they are further optimized in our method. Two techniques are proposed to ensure their small values, thus making the error bound offered by Theorem 1 tighter in the task we focus on, i.e., an image classification UDA problem.</p><p>Specifically, inspired by Theorem 1 that small values of the second and third terms lead to a tight error bound, we propose a loss function and an optimization strategy. First, to decrease ? in a deep end-to-end model, we define a local smooth discrepancy to measure the probabilistic Lipschitzness with a small number of samples, even with a single sample. It is suitable for deep models, because their parameters are updated with respect to a batch of samples in an iteration. This requires a criterion to measure the probabilistic Lipschitzness with limited samples. Local smooth discrepancy satisfies this requirement. Second, the dimension of samples, i.e., d and the sample count of a target domain, i.e., m, affect the error bound. Small d and large m are preferred. An optimization strategy with small d is proposed in this work, and an analysis of how m and batchsize affect the performance of Lipschitz-based UDA methods is to be given. The two improvements are not formally defined because they specialize in a more complex and practical setting, i.e., image classification. However, the need to decrease the bound in Theorem 1 motivates this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LIMITATION OF PREVIOUS LIPSCHITZ-CONSTRAINT-BASED METHODS</head><p>Previous Lipschitz-constraint-based methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> were inspired by virtual adversarial training <ref type="bibr" target="#b21">[22]</ref>. They explicitly incorporated it and added it to a objective function as a regularization term:</p><formula xml:id="formula_5">L v (?; D S , D T ) = E x?D S [ max ||r||? D K (h ? (x)||h ? (x + r))]+ E x?D T [ max ||r||? D K (h ? (x)||h ? (x + r))]<label>(5)</label></formula><p>where h ? denotes a classifier parameterized by ?, r denotes a vector with the same shape as input sample x and D K is the Kullback-Leibler divergence. The regularization term was proposed to enforce classification consistency within the normball neighborhood of each sample <ref type="bibr" target="#b21">[22]</ref>. DIRT <ref type="bibr" target="#b20">[21]</ref> gave some intuitive explanations for the effectiveness of locally-Lipshitz constraint. It regarded (5) as a constraint to satisfy the cluster assumption <ref type="bibr" target="#b23">[24]</ref>. However, as far as we know, no mathematical analysis has been conducted to connect a local-Lipschitz constraint with the error bound of a UDA problem. In this work, a brief analysis of how a local-Lipschitz constraint affects the error bound of UDA is given. According to Theorem 1 and Definition 2, it is clear that probabilistic Lipschitzness is crucial to the error bound of UDA. If (x + r) and h ? are regarded as y and f , respectively,</p><formula xml:id="formula_6">Ex?D T [max ||r||? D K (h ? (x)||h ? (x + r))]</formula><p>is to estimate the expectation of maximum divergence between f (x) and f (y). Its meaning is closely correlated with Definition 2 except that Definition 2 considers all possible y in D T while (x + r) only denotes a sample within the neighborhood of x. If the expectation of maximum divergence between f (x) and f (y) is small enough, the probability of [?y : |f (x) ? f (y)| &gt; ??(x, y)] is small. Therefore, once minimizing (5), ?(?) in (4) is approximately minimized and the error bound of UDA is also reduced.</p><p>Although a local-Lipschitz constraint helps reduce the error bound of UDA, there are several issues we need to overcome. The first one is that despite Theorem 1 and Definition 2 give us the intuition that local-Lipschitz constraint helps reduce the error bound of UDA, they are not derived from a deep learning model whereas recent UDA methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> are mainly based on deep learning models. An explanation is needed to connect a deep end-to-end model with the local-Lipschitz constraint. Secondly, previous Lipschitz-constraintbased methods use an adversarial training strategy. As shown in <ref type="bibr" target="#b18">[19]</ref>, adversarial training in UDA methods can easily cause a gradient vanishing problem. It thus hurts the stability of such methods. Thirdly, besides ?(?) and E * (P T ), Theorem 1 contains a constant term. Previous methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> all ignored it but it may increase to an extremely large value when a large-scale dataset is processed. This term prevents previous Lipschitz-constraint-based methods from working well on large datasets.</p><p>In this work, we analyze and solve the above three problems. A brief explanation is given to answer why Theorem 1 holds with a deep learning model. The adversarial training strategy is removed in our proposed method and a concise minimization procedure is utilized to maintain stable performance. Another important contribution of our work is that we answer how the constant term affects Lipschitz-constraint-based methods' performance and extend such methods to work well on large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LEARNING SMOOTH REPRESENTATION</head><p>In Theorem 1, the error bound of a general UDA learning algorithm is bounded by three terms. The first one, E * (P T ), refers to a Bayes optimal error for P T . It is a value discussed in the theoretical analysis which is impossible to obtain in practical settings. Therefore, the other two terms are the priorities in this paper. The second term, ?(?), refers to the degree of how target samples satisfy probabilistic Lipschitzness. Because</p><formula xml:id="formula_7">ES?P m S [E P T (H)]</formula><p>is positively related to ?(?), our goal is to decrease ?(?), i.e., strengthen a probabilistic-Lipschitz property with respect to a target distribution. The last term is relevant to multiple factors, for example, the lower bound R for the weight ratio of source and target distributions, the dimension d of domain ? and the size m of S. Note that the weight ratio R B? (D S , D T ) is predetermined for a specific pair (P S , P T ). Therefore, it is impracticable to optimize its lower bound R. To achieve a tighter error bound of a target distribution, it is reasonable to explore appropriate d and m. In summary, two principles, i.e., strengthening probabilistic-Lipschitz property and searching proper d and m, guide us to tackle a UDA problem thoroughly. Similar analyses are deduced from <ref type="bibr" target="#b30">[31]</ref>, while no applicable algorithm is proposed in <ref type="bibr" target="#b30">[31]</ref>. In this paper, we adopt the well-established principles and innovatively extend them to a feasible and effective UDA algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep End-to-End Model</head><p>Although the proposed principles are reasonable, the first problem we should deal with is how to design a deep end-toend model aligned well with them. Deep neural networks with numerous parameters optimized by gradient-based algorithms can well handle large-scale datasets <ref type="bibr" target="#b0">[1]</ref>. An error bound with respect to a deep neural network is derived as follows.</p><p>Given a deep neural network n ? parameterized by ? whose input samples are x ? ?, if the output range space of n ? is defined in [0, 1], n ? satisfies the definition of labeling rules just like l s and l t . For any sample x ? S ? , n ? (x) is used to replace l S (x) to denote the label of x in S and N S (x) denotes the nearest neighbor to x in S, i.e., N S (x) = arg min z?S? ?(x, z). We make a hypothesis determined by the Nearest Neighbor algorithm as H (x) = n ? (N S (x)) for all x ? ?.</p><p>Because n ? meets the definition of labeling rules and Theorem 1 is derived based on the labeling rules, an error bound based on n ? is given as:</p><formula xml:id="formula_8">E S?P m S [E P T (H )] ?2E * (P T ) + ?(?) + 4? ? d Rm 1 d+1<label>(6)</label></formula><p>Thus, the local-Lipschitz constraint can be applied to a deep end-to-end model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local Smooth Discrepancy</head><p>Besides extending Theorem 1 to a deep end-to-end model, how to decrease ?(?) is another crucial problem. According to <ref type="bibr" target="#b5">(6)</ref>, the basic UDA error bound is positively related to</p><formula xml:id="formula_9">?(?). Because ?(?) = sup x?D P x?D [?y : |f (x) ? f (y)| &gt; ??(x, y)], minimizing P x?D [?y : |f (x) ? f (y)| &gt; ??(x, y)]</formula><p>for all x ? D decreases ?, thus, decreases the error bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ ?</head><p>Decision boundary  A naive estimation for P x?D [?y :</p><formula xml:id="formula_10">|f (x)?f (y)| &gt; ??(x, y)] is E x?D [ 1[?y:|f(x)?f(y)|&gt;?u(x,y),y?D]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1[?y:|y?D]</head><p>]. 1[statement] is an indicator function that returns 1 if statement is true, and 0 otherwise. This estimation needs to sample many y ? D to make sure the estimation with low variance. In a deep end-to-end model, if a bunch of y is sampled to calculate |f (x) ? f (y)|, it requires multiple forward propagations for a batch. According to <ref type="bibr" target="#b23">[24]</ref>, a replacement called local Lipschitz property for Lipschitz continuity is proposed. It assumes that every x ? D has a neighborhood U such that f is Lipschitz continuous with respect to x and points in U . According to the local Lipschitz property, we propose a concept called a local smooth discrepancy (LSD) to measure the degree that a sample x breaks down the local Lipschitz property in a pointwise way:</p><formula xml:id="formula_11">L s (x, ?) = D(n ? (x + r), n ? (x)), ||r|| ?<label>(7)</label></formula><p>where r is a random vector with the same shape as x. D(?, ?) is a discrepancy function that measures the divergence between (x + r) and x, and denotes the maximum norm of r. In L s (x, ?), a sample x adds r to detect another sample in x's neighborhood. controls the range of x's neighborhood. As for the choice of D(?, ?), we employ a cross-entropy loss function in all experiments. Although the proposed (7) looks similar to (5), there are two essential different points. Firstly, according to Theorem 1, ?(?) is estimated with respect to a target distribution. Thus, <ref type="bibr" target="#b6">(7)</ref> is only conducted for target samples whereas (5) applies to both source and target distributions. Our proposed LSD avoids introducing noise from a source distribution and reduces computational cost. Moreover, (5) requires to seek a point that causes Kullback-Leibler divergence to be largest in a norm-ball neighborhood of each sample. Its intuition is that if the worst point in the neighborhood is enforced to satisfy Lipschitz constraint, the whole neighborhood satisfies Lipschitz constraint. However, our intuition is that points in the neighborhood of the original target sample are sampled to estimate the probability of n ? breaking down a Lipschitz constraint. Therefore, <ref type="bibr" target="#b6">(7)</ref> involves every possible point in the neighborhood instead of a single point.</p><p>There is another essential point we should pay attention to. Specifically, r is limited only by its norm in <ref type="bibr" target="#b6">(7)</ref> and its direction is ignored. In fact, the goal of adding r includes detecting sensitive samples that do not satisfy a local Lipschitz property. If all x+r belong to the same category as x, it means that the direction of r could not detect sensitive samples. In this condition, sensitive samples are not modified to ensure n ? to be local Lipshitz continuous. To solve this problem, we propose two methods to produce r, an isotropic one and an anisotropic one as shown in <ref type="figure" target="#fig_3">Fig. 2</ref>.</p><p>Isotropic Method: In this method, r is drawn from a Gaussian distribution and normalized to satisfy ||r|| ? . The formula of LSD is modified into:</p><formula xml:id="formula_12">L s (x, ?) = D(n ? (x + r R ), n ? (x)), r R = m ||m|| 2 , m ? N (0, 1)<label>(8)</label></formula><p>where r R denotes a noise vector sampled from a Gaussian distribution.</p><p>Anisotropic Method: This method only looks for r that leads x + r with different labels from x and ignores r in other directions. It can be considered as a biased estimation of the probability that target samples break down a Lipschitz constraint, because the samples that satisfy a Lipschitz constraint are neglected. To implement this method, an insight from an adversarial attack algorithm <ref type="bibr" target="#b43">[43]</ref> is taken. It applies a certain hardly perceptible perturbation, which is found by maximizing the model's prediction error, to an image to cause the model to misclassify <ref type="bibr" target="#b43">[43]</ref>. Similar to its goal, the anisotropic method tries to search for a perturbation to cause a model to output different labels for an image. However, true labels are needed in adversarial attack algorithms. In a UDA problem, true labels for a target domain are not available. Therefore, several modifications are introduced in a traditional adversarial attack method. In detail, it approximates an adversarial perturbation by <ref type="bibr" target="#b43">[43]</ref>:</p><formula xml:id="formula_13">r A = m ||m|| 2 , m = ? x D(n ? (x), y)<label>(9)</label></formula><p>where r A denotes an anisotropic noise that leads to a wrong label. To get rid of the constraint of label information, r A is approximated by:</p><formula xml:id="formula_14">r A ? m ||m|| 2 , m = ? x D(n ? (x), onehot(n ? (x)))<label>(10)</label></formula><p>where onehot denotes a function that transforms the softmax output of n ? to a one-hot vector. (10) computes gradients of x and replaces y with onehot(n ? (x)). These modifications result in a new LSD with anisotropic noise:</p><formula xml:id="formula_15">L s (x, ?) = D(n ? (x + r A ), n ? (x))<label>(11)</label></formula><p>Both the isotropic and anisotropic methods are proposed to detect sensitive samples to estimate the probability of original target samples breaking down a Lipschitz constraint. However, they are different as shown in <ref type="figure" target="#fig_3">Fig. 2</ref>. The former, which is an unbiased estimation method, detects all directions around a target sample. It ensures a robust training procedure. However, the latter attempts to detect more sensitive samples and ignores many points around the original sample. It fits the situation that sensitive samples are hard to seek. Meanwhile, because the softmax output of n ? is regarded as a pseudo label, it can find a wrong direction and result in a vulnerable training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization Strategy</head><p>By optimizing the proposed LSD, probabilistic Lipschitzness can be satisfied. However, how to optimize it and what factors should be concerned to constrain the third term in Theorem 1 need a detailed study.</p><p>Basically, either <ref type="bibr" target="#b7">(8)</ref> or <ref type="formula" target="#formula_0">(11)</ref> can be adopted as a loss function and use a gradient-based algorithm for optimization. However, there is an essential factor deserved to be considered. The dimension d of domain affects the error bound sharply. For a UDA problem on different image domains, d potentially varies over a large range of values because the size of images is totally different. For instance, when tackling a typical task that adapts a model trained on the MNIST dataset <ref type="bibr" target="#b44">[44]</ref> to perform well on USPS dataset <ref type="bibr" target="#b45">[45]</ref>, each image's size is 28 ? 28 while the size of an image in VisDA could be 352 ? 311. Despite setting other hyper-parameters in the two tasks to be the same, the constant term in Theorem 1 varies a lot. Especially, for a large-scale dataset, a large image size results in a large constant term. Considering that a large d leads to a loose error bound, a noise r is generated in feature space instead of an image level such that d is decreased acutely. <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula" target="#formula_0">(11)</ref> are thus revised into:</p><formula xml:id="formula_16">L s (x, ?) = D(C(G(x) + r R ), C(G(x))), r R = m ||m|| 2 , m ? N (0, 1) (12) L s (x, ?) = D(C(G(x) + r A ), C(G(x))) r A ? m ||m|| 2 , m = ? g D(C(g), onehot(C(g))), g = G(x)<label>(13)</label></formula><p>where G and C denote a feature extractor and classifier in n ? , respectively. Their parameters are denoted as ? G and ? C . Then an optimization strategy is proposed in feature space in three steps. Firstly, G and C are trained in a source domain.</p><formula xml:id="formula_17">min G,C L(X s , Y s ), L(X s , Y s ) = E x,y?P S [ K k=1 1[k = y] ? log(C(G(x)))]<label>(14)</label></formula><p>where 1[?] is an indicator function, and K denotes the number of classes in a task. Then, sensitive samples that break down a local Lipschitz property are produced. Note that in Theorem 1, a Lipschitz property constraint is satisfied w.r.t a target distribution such that we focus on target samples in the second step. In our work, a sensitive sample g t is generated in the output space of G:</p><formula xml:id="formula_18">g t = g t + r<label>(15)</label></formula><p>where g t = G(x t ), and r is a general notation for the adding noise. In practice, we set r = r R for an isotropic method or r = r A for an anisotropic method. Finally, G is trained to minimize L s for target samples. Only parameters of G are updated in this step. G is trained to project g t to the same category with g t :</p><formula xml:id="formula_19">min G L s (X t , ? G ), L s (X t , ? G ) = E x?Pt D(C(g t ), C(g t ))<label>(16)</label></formula><p>where ? G denotes the aparameters of G, and D(?, ?) denotes a cross-entropy loss function. <ref type="formula" target="#formula_0">(14)</ref>- <ref type="bibr" target="#b15">(16)</ref> are repeated in the optimization schedule as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. An intuitive understanding of our optimization strategy is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. In detail, n ? , which is well-trained in a source domain, classifies source samples correctly. For a robust model, there is a large margin between a decision boundary and samples. It ensures that a local Lipschitz property is well-satisfied. However, a dataset shift may push some target samples to cross the boundary. They are misclassified and the local Lipschitz constraint is broken down. To obtain n ? that works well in a target domain, G needs to project x ? P T into feature space away from the decision boundary.</p><p>In our strategy, a large margin is formed between target samples and a decision boundary obtained in a source domain. The proposed optimization strategy detects samples close to a boundary and forces G to project them far away from the boundary. It makes the representation of target distribution become "smooth" gradually, implying that a local Lipschitz constraint is ensured. When the optimization procedure ends, a large margin between a target distribution and decision boundary is formed.</p><p>Another factor, i.e., the sample amount of a target domain m also affects the constant term in Theorem 1. Briefly, a large m is recommended. According to Theorem 1, it is clear that when m increases, the constant term 4?</p><formula xml:id="formula_20">? d Rm 1 d+1</formula><p>decreases.</p><p>Thus, the fact that larger m causes smaller 4?</p><formula xml:id="formula_21">? d Rm 1 d+1</formula><p>suggests us to set a large m during the training period. In practice, if there is only a small number of samples in the target domain, Lipschitz-constraint-based methods cannot ensure excellent performance, to be shown in our experiments.</p><p>Besides d and m, the batchsize during training is also a critical factor according to our experimental results. Recent studies <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref> have shown that a large batchsize improves the performance of a deep neural network. They concern about image classification and GAN models. To our knowledge, this work is the first study to conclude that a large batchsize is beneficial to UDA problems. To verify our conclusion, an ablation study is conducted next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS AND DISCUSSION</head><p>To verify the effectiveness of the proposed method, i.e., learning Smooth Representation for unsupervised Domain Adaptation (SRDA), several classification experiments are conducted on standard datasets. First, the datasets of experiments are introduced. Second, SRDA are tested on all datasets to show its outstanding performance in comparison with other UDA methods. Then, an ablation study is conducted to analyze how the sample amount of a target domain, and the dimension and batchsize of samples affect Lipschitz-based methods' performance and discuss the effectiveness and robustness of SRDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Digits: Digits datasets include MNIST <ref type="bibr" target="#b44">[44]</ref>, USPS <ref type="bibr" target="#b45">[45]</ref>, Synthetic Traffic Signs (SYNSIG) <ref type="bibr" target="#b48">[48]</ref>, Street View House Numbers (SVHN) <ref type="bibr" target="#b49">[49]</ref> and German Traffic Signs Recognition Benchmark (GTSRB) <ref type="bibr" target="#b50">[50]</ref>. Specifically, MNIST, USPS and SVHN consist of 10 classes, whereas SYNSIG and GTSRB consist of 43 classes. We set four transfer tasks: SVHN?MNIST, SYNSIG?GTSRB, MNIST?USPS and USPS?MNIST.</p><p>VisDA: VisDA <ref type="bibr" target="#b51">[51]</ref> is a more complex object classification dataset. It contains more than 280K images belonging to 12 categories. These images are divided into training, validation and test sets. There are 152,397 training images synthesized by rendering 3D models. The validation images are collected from MSCOCO <ref type="bibr" target="#b52">[52]</ref> and amount to 55,388 in total. This dataset requires an adaptation from synthetic-object to realobject images. We regard the training set as a source domain and the validation set as a target domain.</p><p>Office-31: Office-31 <ref type="bibr" target="#b53">[53]</ref> is a small-scale dataset that comprises only 4,110 images and 31 categories collected from three domains: AMAZON (A) with 2,817 images, DSLR (D) with 498 images and WEBCAM (W) with 795 images. We focus on the most difficult four tasks: A?D, A?W, D?A and W?A.</p><p>Please note that Office-31 is so small-scale dataset that small m leads to a loose error bound for Office-31. By comparing results on Office-31 with those on VisDA, an analysis of how m affects the performance of Lipschitz-constraint-based methods is conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Detail</head><p>The first practical issue is that how to decide which layer to conduct optimization, because LSD is minimized in feature space in the proposed optimization strategy. In all experiments, a general principle is adopted that the last convolutional layer should be chosen to conduct optimization no matter what the network backbone is. For example, if the network backbone is ResNet50 <ref type="bibr" target="#b57">[57]</ref>, G includes all convolutional layers (49 layers) and C includes a dense layer. Therefore, the number of parameters of G and C varies as the network backbone changes. This principle is reasonable because a small d leads to a low error bound for Lipschitz-constraint-based methods. In the last convolutional layer, d reaches the minimum. The reason for not considering dense layers is that features after dense layers form a cluster if they belong to the same category. A large margin is formed between two different clusters. It is hard to detect sensitive samples because a feature vector with an added noise is classified as the original category.</p><p>Digits: For a fair comparison, we follow the network backbone in MCD <ref type="bibr" target="#b39">[39]</ref> and ADDA <ref type="bibr" target="#b34">[34]</ref>. Hyper-parameter is set to 0.5 and the learning rate is set to 1e ?3 . Batchsize is set to 128 in all tasks and all models are trained for 150 epochs. Both isotropic and anisotropic methods are implemented. For the former, noise is sampled from a standard Gaussian distribution. For the latter, sensitive samples are generated in two different ways. Two classical adversarial attack algorithms are chosen, namely Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b43">[43]</ref> and Virtual Adversarial Training (VAT) <ref type="bibr" target="#b21">[22]</ref>, to produce noise. FGSM <ref type="bibr" target="#b43">[43]</ref> needs true labels to execute a backpropagation to compute gradients. Instead, pseudo labels are used to replace them.</p><p>VisDA: Both isotropic and anisotropic methods are implemented. All models are trained for 15 epochs and batchsize is 32. Learning rate is 10 ?4 and hyper-parameter is set to 0.5. The backbone network is ResNet101 <ref type="bibr" target="#b57">[57]</ref> that is the same with MCD <ref type="bibr" target="#b39">[39]</ref> and GPDA <ref type="bibr" target="#b41">[41]</ref>.</p><p>Office-31: Both the isotropic and anisotropic methods are implemented. For the former, noise is sampled from a standard Gaussian distribution. For the latter, FGSM <ref type="bibr" target="#b43">[43]</ref> is used to produce noise. All models are trained for 50 epochs and batchsize is set to 32. Learning rate is set to 10 ?3 and hyperparameter is set to 0.5 for all four tasks. The backbone network is ResNet101 <ref type="bibr" target="#b57">[57]</ref>.</p><p>C. Model Evaluation 1) Digits Classification: Results of the digits classification experiment are shown in <ref type="table" target="#tab_1">Table I</ref>. We call SRDA models that generate noise from FGSM, VAT and a random Gaussian distribution as SRDA F , SRDA V and SRDA G , respectively. We compare them with other UDA algorithms such as DAN <ref type="bibr" target="#b8">[9]</ref>, DANN <ref type="bibr" target="#b15">[16]</ref>, Domain Separation Network (DSN) <ref type="bibr" target="#b54">[54]</ref>, ADDA <ref type="bibr" target="#b34">[34]</ref>, Coupled Generative Adversarial Network (CoGAN) <ref type="bibr" target="#b16">[17]</ref>, Asymmetric Tri-training for Unsuper-vised Domain Adaptation (ATDA) <ref type="bibr" target="#b38">[38]</ref>, Associative Domain Adaptation (ASSC) <ref type="bibr" target="#b55">[55]</ref>, Deep Reconstruction-Classification Network (DRCN) <ref type="bibr" target="#b56">[56]</ref>, MCD <ref type="bibr" target="#b39">[39]</ref>, Virtual Mixup Training (VMT) <ref type="bibr" target="#b22">[23]</ref>, Virtual Adversarial Domain Adaptation (VADA) <ref type="bibr" target="#b20">[21]</ref> and Decision-boundary Iterative Refinement Traning (DIRT) <ref type="bibr" target="#b20">[21]</ref>. VMT, VADA and DIRT are Lipschitzconstraint-based methods. The parameters of their backbone are more than twice those of other models, and thus they perform much better than others when no adaption is adopted.</p><p>Among all four tasks, SRDA G ranks the third in SVHN?MNIST. However, due to the source-only model of VMT and DIRT performs much better than that of SRDA G , the improvement bought from SRDA G is the largest. SRDA G ranks the fisrt in MNIST?USPS and SRDA F performs the best in USPS?MNIST. Especially, in the most difficult task, i.e., USPS?MNIST, our three models are the top three. Only MCD and the Lipschitz-constraint-based models are comparable to the proposed models and other methods are inferior to ours with a large margin. In SYNSIG?GTSRB, our models do not obtain the best results. We reason that it is caused by relatively satisfying results when no adaptation is implemented. Once a model without adaptation has already formed a large margin between different categories, SRDA is hard to detect enough sensitive samples to optimize G.</p><p>2) VisDA Classification: Results of the VisDA classification experiment are shown in <ref type="table" target="#tab_1">Table II</ref>. We compare SRDA F , SRDA V and SRDA G with several typical methods, such as DAN <ref type="bibr" target="#b8">[9]</ref>, DANN <ref type="bibr" target="#b15">[16]</ref>, MCD <ref type="bibr" target="#b39">[39]</ref>, GPDA <ref type="bibr" target="#b41">[41]</ref>, VMT <ref type="bibr" target="#b22">[23]</ref>, VADA <ref type="bibr" target="#b20">[21]</ref> and DIRT <ref type="bibr" target="#b20">[21]</ref>. Because VMT, VADA and DIRT are not reported on VisDA in their original papers, we modify their codes and test them on VisDA.</p><p>SRDA and GPDA <ref type="bibr" target="#b41">[41]</ref> achieve much better accuracy than other methods. Moreover, SRDA G ranks the first among all  <ref type="table" target="#tab_1">II  CLASSIFICATION ACCURACY PERCENTAGE OF VISDA CLASSIFICATION EXPERIMENT. THE FIRST ROW CORRESPONDS TO THE PERFORMANCE IF NO  ADAPTION IS IMPLEMENTED. COLUMNS IN THE MIDDLE CORRESPOND TO DIFFERENT CATEGORIES AND THE COLUMN ON THE RIGHT REPRESENTS  AVERAGE ACCURACY. WE EVALUATE THREE SRDA MODELS WITH DIFFERENT METHODS FOR ADDING NOISE. SRDA* DENOTES THE MODELS THAT ARE  OPTIMIZED IN AN IMAGE LEVEL. THE NUMBER BEHIND MCD DENOTES DIFFERENT HYPER-PARAMETERS. THE RESULTS ARE CITED FROM EACH</ref>    the models and SRDA F obtains comparable accuracy with MCD <ref type="bibr" target="#b39">[39]</ref>. Besides SRDA, other Lipschitz-constraint-based methods perform poorly on VisDA, because the large size of images in VisDA causes a large d that leads to a loose error bound. In detail, SRDA G achieves the best results in class plane and person, SRDA V achieves the best result in class motorcycle and SRDA F gets the best result in class knife. An interesting phenomenon is that three models of SRDA perform diversely among these categories. For example, in class knife, SRDA F performs much better than the others and SRDA V ranks the first in class motorcycle. Overall, SRDA G performs the best. Different performance of three SRDA models reflects the importance of detecting sensitive samples. A well-defined method that can seek more sensitive samples and a metric that can illustrate smoothness of samples precisely are hopeful to further promote the proposed method.</p><p>3) Office-31 classification: Results of the Office-31 classification experiment are shown in <ref type="table" target="#tab_1">Table III</ref>. We compare SRDA F and SRDA G with several UDA methods, such as Geodesic Flow Kernel for unsupervised domain adaptation (GFK) <ref type="bibr" target="#b58">[58]</ref>, Transfer Component Analysis (TCA) <ref type="bibr" target="#b5">[6]</ref>, Residual Transfer Network (RTN) <ref type="bibr" target="#b12">[13]</ref>, DAN <ref type="bibr" target="#b8">[9]</ref> and DANN <ref type="bibr" target="#b15">[16]</ref>.</p><p>Overall, SRDA G is comparable to RTN and performs worse than DANN. In particular, SRDA F performs poorly in D?A and W?A whereas in A?D, A?W, its performance is acceptable. Considering the extremely small scale of D and W, the results are reasonable. Because of limited samples, SRDA is hard to detect enough sensitive samples to construct a robust decision boundary. In detail, to form a smooth representation space, SRDA seeks sensitive samples and enforces them to hold consistent outputs within their neighbors. An adequate number of such neighbors construct a smooth feature space. Without enough sensitive samples, the local-Lipschitzconstraint is hard to satisfy. This experiment confirms our assumption that a small sample amount of the target domain leads to a loose error bound. D. Discussions 1) Image Level versus Feature Space: In our optimization strategy, LSD is minimized in feature space instead of an image level. This modification is introduced because the dimension d of an image greatly impact the error bound of a UDA problem. If the optimization is conducted in an image level, d would be much larger than in feature space and a loose error bound is formed. To verify it, we assess SRDA that optimizes in an image level on both digits and VisDA datasets. In detail, we adopt (8) and (11) as our optimization goals. Three SRDA models that generate noise from FSGM, VAT and a Gaussian distribution are denoted as SRDA F * , SRDA V * and SRDA G * , respectively.</p><p>Results are shown in <ref type="figure">Fig. 3</ref> and <ref type="table" target="#tab_1">Table II</ref>. Except that SRDA G * and SRDA V * perform slightly better than SRDA G and SRDA V in USPS?MNIST and MNIST?USPS, respectively, all models optimized in feature space obtain much better performance. Particularly, SRDA F * is sensitive to d where its performance drops a lot in almost all tasks and it even cannot converge in SYNSIG?GTSRB. On VisDA, SRDA F * and SRDA V * perform like random guessing, and the accuracy of SRDA G * decreases 15.5%.</p><p>Another evidence is that previous Lipschitz-constraint-based methods, i.e., VMT, VADA and DIRT, perform poorly on VisDA. These methods satisfy the local Lipschitz constraint at an image level. VMT converges to a trivial solution where all images are classified to a category. The accuracy of VADA and DIRT is less than 15%. On the contrary, they perform excellently on Digits whose images are small-scale.</p><p>The experimental results demonstrate that satisfying a Lipschitz constraint in feature space is convenient to apply Lipschitz-based UDA algorithms to large-scale datasets. Large-scale images in VisDA dataset aggravate the influence of d because the difference of d between an image level and feature space is more obvious. Therefore, we conclude that optimization in feature space is necessary to reduce the value of d. Considering that previous Lipschitz-based methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> are all conducted on small-scale datasets, the proposed method gives a promising direction to spread them to more scenarios. In addition, the isotropic method is more robust than the anisotropic one when an SRDA model is optimized in an image level. We speculate that all-direction exploration avoids instability.</p><p>2) Sample Amount of Target Domain: We conclude that a small sample amount of the target domain would lead to a loose error bound for UDA. In this section, it is verified by comparing the performance of SRDA on VisDA and Office-31. In detail, VisDA includes 55,388 samples in the target domain, while three domains in Office-31 includes fewer than 3,000 images. To make a fair comparison, we resize all images to 224?224 and use the same network backbone.</p><p>As the results shown in Tables II-III, SRDA G performs better than DANN and DAN by 16.1% and 12.4% on VisDA. However, on Office-31, SRDA G performs worse than DANN and better than DAN by 2.1%. Similarly to SRDA G , SRDA F performs better than DANN and DAN by 13.4% and 10% on VisDA. On Office-31, SRDA F performs worse than DANN and outperforms DAN by 1.3%. It is obvious that SRDA shows a large advantage over DAN and DANN if the target domain includes a large number of images. On the contrary, if the sample amount of a target domain is small, such as Office-31, the Lipschitz-constraint-based models fail to perform well. The different performance of SRDA on VisDA and Office-31 shows that the sample amount of a target domain is critical for Lipschitz-constraint-based methods' performance.</p><p>3) Visualization of Representation: In <ref type="figure">Figs. 4</ref> and 5, we further analyze the behavior of SRDA and SRDA* by T-SNE embeddings of the feature space where we adopt our optimization strategy. In particular, we visualize embeddings for MNIST?USPS and VisDA classification.</p><p>In <ref type="figure">Fig. 4(f)</ref>, SRDA G not only separates target samples into ten clusters clearly but also aligns source and target distributions well. However, SRDA F and SRDA V only separate target samples into ten clusters as shown in Figs. 4(d) and 4(e). It explains why SRDA G outperforms the other two models in MNIST?USPS. An alignment of different domains enhances the performance of a target domain. In <ref type="figure">Figs. 4(a)</ref>-(c), although target samples are separated into clusters and different domains align well, there exists adhesion among different clusters. Such adhesion impedes a classifier trained on a source domain to classify target samples as their correct categories. This means that a large margin among these clusters is not built. We assume that some sensitive samples belonging to different categories from their neighbors are forced to hold consistent outputs such that the adhesion among different clusters is formed.</p><p>In <ref type="figure">Figs. 5(e)</ref> and 5(f), SRDA G * and SRDA V * separate target samples clearly and align different domains well. Compared to SRDA V , SRDA V * even matches different domains better. It explains why SRDA V * achieves better accuracy than SRDA V in MNIST?USPS. In <ref type="figure">Fig. 5(d)</ref>, SRDA F * performs like SRDA F . They form clear clusters but fail to match source and target domains. In <ref type="figure">Figs. 5(a)</ref>-(c), all clusters assemble together and two domains are separated in all three models. A classifier is hard to work well in this situation. This phenomenon demonstrates that holding a Lipschitz constraint in an image level is unfeasible on a large-scale dataset like VisDA. 4) Batchsize Analysis: Besides the dimension of samples and sample amount of a target domain, we discover that a large batchsize improves SRDA's performance. A large batchsize is recommended in other computer vision problems, such as image generation <ref type="bibr" target="#b46">[46]</ref> and image classification <ref type="bibr" target="#b47">[47]</ref>. However, to our best knowledge, this work is the first one to consider whether a large batchsize can help us solve a UDA problem. To verify how a batchsize influences the performance of SRDA, we evaluate it with m ? {32, 16, 8, 4}. Except for batchsize, other settings follow the VisDA classification experiment.</p><p>Results are shown in <ref type="table" target="#tab_1">Table IV</ref>. For all the three SRDA models, the average accuracy decreases as their batchsize gets smaller. Especially, when the batchsize is set to 4, the performance drops rapidly even lower than 30%. When the batchsize ? 8, the performance drops gradually as the batchsize decreases and three models maintain their accuracy over 60%. The trend shows that the batchsize should be set large enough for optimizing SRDA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Sensitivity Analysis:</head><p>We add a detailed discussion on . We test ? {0.3, 0.4, 0.5, 0.6, 0.7} on the digits datasets. Experimental results indicate that they are not sensitive to ; and thus we do not need to set carefully. We can easily set = 0.5 as a default value. As shown in <ref type="figure">Fig. 6</ref>, three models of SRDA are robust in most settings. In particular, SRDA G 's accuracy fluctuates   no more than 5% among the four tasks. Moreover, except SYNSIG?GTSRB, SRDA G 's accuracy fluctuates no more than 1.5%. SRDA G shows robust performance as varies, thus meaning that there is no need to tune hyper-parameters subtly for it. SRDA V performs stably in USPS?MNIST and SVHN?MNIST whereas its accuracy varies by more than 4.5% in MNIST?USPS and SYNSIG?GTSRB. SRDA F holds stability in USPS?MNIST, MNIST?USPS and SYNSIG?GTSRB. In SVHN?MNIST, its accuracy varies by more than 6%. The experimental results confirm the belief that SRDA is robust in most settings as changes. Especially, SRDA G is the most stable one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Discussion of Local Smooth Discrepancy:</head><p>To verify that LSD reflects the performance of a model, we show the relationship between LSD and a model's accuracy in <ref type="figure" target="#fig_7">Fig. 7</ref>. Three models, i.e., SRDA F , SRDA V and SRDA G , are assessed on VisDA. Because we get an accuracy value every epoch and LSD values are recorded every step, we apply a quadratic interpolation for accuracy values.</p><p>As shown in <ref type="figure" target="#fig_7">Figs. 7(a)</ref>-(c), the accuracy of all three models gradually increases as LSD decreases. Further, SRDA G with the highest accuracy refers to the lowest LSD and SRDA F with the lowest accuracy refers to the highest LSD. The relationship between an accuracy value and LSD indicates that LSD is a reasonable metric to evaluate the performance of a UDA model. This is a remarkable property because adversarial training that conducts a min-max optimization lacks a metric to supervise a training procedure <ref type="bibr" target="#b15">[16]</ref>. Its loss value shows no obvious relationship with its accuracy. Moreover, to prove that LSD is a useful metric to assess the performance of a UDA model, we further test it on MCD. We train 12 MCD models with different accuracy on VisDA by tuning hyper-parameters. We generate adversarial samples with SRDA F * to ensure fairness. To ensure valid results, both classifiers in MCD are tested.</p><p>As shown in <ref type="figure" target="#fig_7">Fig. 7(d)</ref>, LSD and accuracy are negatively correlated. We train 12 MCD models with accuracy percentages belong to {62.42%, 64.83%, 65.33%, 65.86%, 66.66%, 68.89%, 69.79%, 70.46%, 70.60%, 71.76%, 71.78%, 71.82%}. LSD of both classifiers decrease from 0.6 to 0.3 roughly as models' accuracy increases. Because FGSM is based on a gradient descent algorithm, randomness of such algorithm causes some fluctuations that several MCD models with high accuracy show relatively high LSD, such as the model with accuracy of 71.82%. Overall, LSD can be a proper metric to evaluate the performance of a UDA method. 7) Limitation: This work answers why a local-Lipschitz constraint is beneficial to the solution of a UDA problem and proposes a novel Lipschitz-constraint-based method that considers the effect of the sample amount of a target domain, and the dimension and batchsize of samples. A limitation of the proposed method needs to be noted. As shown in the Office-31 classification experiment, both SRDA F and SRDA G get inferior performance. A large amount of the target domain is necessary for Lipschitz-constraint-based methods. Because Office-31 consists of 4,110 images only, the proposed method cannot detect enough sensitive samples to form a large margin between a decision boundary and samples. Thus, it shows poor performance on Office-31. The results demonstrate that the proposed method fails to handle well a small amount of data. To make it work well with such limited data cases, exploring a better method than the proposed one to detect more sensitive samples is our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we have proposed a method for UDA inspired by a probabilistic Lipschitz constraint. Principles analyzed in <ref type="bibr" target="#b30">[31]</ref> are well extended to a deep end-to-end model and a practical optimization strategy is presented. The key to strengthening Lipschitz continuity is to minimize a local smooth discrepancy defined in this paper. To avoid a loose error bound, the proposed optimization strategy is subtly designed by considering the dimension and batchsize of samples, which have been ignored by previous Lipschitz-constraintbased methods <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Experiments demonstrate that a Lipschitz constraint without adversarial training is effective for UDA and factors we discuss are critical to an efficient and stable UDA model. Our future work includes seeking theoretically tighter error bounds and applications of the proposed methods to security, manufacturing and transportation <ref type="bibr" target="#b59">[59]</ref>- <ref type="bibr" target="#b62">[62]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>U</head><label></label><figDesc>NSUPERVISED domain adaptation (UDA) typically tackles the performance drop once there exists a dataset This work was supported in part by Joint Funds of the National Science Foundation of China under Grant U18092006, in part by the Shanghai Municipal Science and Technology Committee of Shanghai Outstanding Academic Leaders Plan under Grant 19XD1434000, in part by the Projects of International Cooperation of Shanghai Municipal Science and Technology Committee under Grant 19490712800, in part by the National Natural Science Foundation of China under Grant 61772369, Grant 61773166, Grant 61771144, in part by National Key R&amp;D Program of China under Grant 2020YFA0711400, in part by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0100), in part Shanghai Municipal Commission of Science and Technology Project(19511132101), in part by the Changjiang Scholars Program of China, in part by the Fundamental Research Funds for the Central Universities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>A visual illustration of how the proposed method achieves adaptation. The decision boundary is determined by learning from a source domain. Then, the high-density region is detected by seeking sensitive samples of a target distribution. Finally, smooth representation is learned by minimizing local smooth discrepancy. Dashed lines indicate that gradients are not applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>A brief demonstration of isotropic and anisotropic methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .Fig. 4 .Fig. 5 .</head><label>345</label><figDesc>The performance of SRDA and SRDA* on digits and VisDA datasets. Blue bars denote SRDA while red ones denote SRDA*. FGSM, VAT and RAN denote SRDA F , SRDA V and SRDA G , respectively. FGSM*, VAT* and RAN* denote SRDA F * , SRDA V * and SRDA G * , respectively. (a)-(c) T-SNE plots of SRDA for VisDA classification experiment. Blue points denote a source domain while red ones denote a target domain. (d)-(f) T-SNE plots of SRDA for MNIST (blue)?USPS (red). (a)-(c) T-SNE plots of SRDA* for VisDA classification experiment. Blue points denote a source domain while red ones denote a target domain. (d)-(f) T-SNE plots of SRDA* for MNIST (blue)?USPS (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>(a)-(c) display the relationship between LSD (red line) and a model's accuracy (blue line). Three SRDA models are evaluated on VisDA. As discrepancy decreases, the accuracy increases. (d) displays the relationship between LSD and a model's accuracy in MCD. The model with higher accuracy gets a lower LSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and L. He are with the Department of Computer Science and Technology, Tongji University, Shanghai 201804, China (email: caiguanyu@tongji.edu.cn; helianghua@tongji.edu.cn). M. Zhou is with the Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ 07102 USA (e-mail: zhou@njit.edu). H. Alhumade is with the Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah 21481, Saudi Arabia (email: halhumade@kau.edu.sa)</figDesc><table /><note>D. Hu is with the Key Laboratory of EMW Information, Fudan University, Shanghai 200433, China (e-mail: hudie@fudan.edu.cn).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>ACCURACY PERCENTAGE OF DIGITS CLASSIFICATION EXPERIMENT AMONG ALL FOUR TASKS. THE FIRST ROW CORRESPONDS TO THE PERFORMANCE IF NO ADAPTION IS IMPLEMENTED. WE EVALUATE THREE SRDA MODELS WITH DIFFERENT METHODS FOR ADDING NOISE. SRDA* DENOTES THE MODELS THAT ARE OPTIMIZED IN AN IMAGE LEVEL. THE RESULTS ARE CITED FROM EACH STUDY.</figDesc><table><row><cell></cell><cell>SVHN</cell><cell>SYNSIG</cell><cell>MNIST</cell><cell>USPS</cell></row><row><cell>Method</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>MNIST</cell><cell>GTSRB</cell><cell>USPS</cell><cell>MNIST</cell></row><row><cell>Source Only</cell><cell>67.1</cell><cell>85.1</cell><cell>76.7</cell><cell>63.4</cell></row><row><cell>DAN [9]</cell><cell>71.1</cell><cell>91.1</cell><cell>-</cell><cell>-</cell></row><row><cell>DANN [16]</cell><cell>71.1</cell><cell>88.7</cell><cell>77.1</cell><cell>73.0</cell></row><row><cell>DSN [54]</cell><cell>82.7</cell><cell>93.1</cell><cell>91.3</cell><cell>-</cell></row><row><cell>ADDA [34]</cell><cell>76.0</cell><cell>-</cell><cell>89.4</cell><cell>90.1</cell></row><row><cell>CoGAN [17]</cell><cell>-</cell><cell>-</cell><cell>91.2</cell><cell>89.1</cell></row><row><cell>ATDA [38]</cell><cell>86.2</cell><cell>96.2</cell><cell>-</cell><cell>-</cell></row><row><cell>ASSC [55]</cell><cell>95.7</cell><cell>82.8</cell><cell>-</cell><cell>-</cell></row><row><cell>DRCN [56]</cell><cell>82.0</cell><cell>-</cell><cell>91.8</cell><cell>73.7</cell></row><row><cell>MCD [39]</cell><cell>96.2</cell><cell>94.4</cell><cell>94.2</cell><cell>94.1</cell></row><row><cell>Source Only</cell><cell>82.4</cell><cell>88.6</cell><cell>-</cell><cell>-</cell></row><row><cell>VMT [23]</cell><cell>99.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VADA [21]</cell><cell>94.5</cell><cell>99.2</cell><cell>-</cell><cell>-</cell></row><row><cell>DIRT [21]</cell><cell>99.4</cell><cell>99.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Source Only</cell><cell>67.1</cell><cell>85.1</cell><cell>76.7</cell><cell>63.4</cell></row><row><cell>SRDA F</cell><cell>95.96</cell><cell>90.87</cell><cell>85.00</cell><cell>95.78</cell></row><row><cell>SRDA F  *</cell><cell>22.70</cell><cell>not converge</cell><cell>32.73</cell><cell>85.37</cell></row><row><cell>SRDA V</cell><cell>98.90</cell><cell>92.44</cell><cell>84.64</cell><cell>95.49</cell></row><row><cell>SRDA V  *</cell><cell>89.47</cell><cell>29.04</cell><cell>88.49</cell><cell>92.17</cell></row><row><cell>SRDA G</cell><cell>99.17</cell><cell>93.61</cell><cell>94.76</cell><cell>95.03</cell></row><row><cell>SRDA G *</cell><cell>89.51</cell><cell>49.86</cell><cell>93.25</cell><cell>95.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>STUDY.</figDesc><table><row><cell>Method</cell><cell cols="2">Plane Bcycl</cell><cell>Bus</cell><cell>Car</cell><cell cols="9">Horse Knife Mcycl Person Plant Sktbrd Train Truck Mean</cell></row><row><cell>Source Only</cell><cell>55.1</cell><cell>53.3</cell><cell>61.9</cell><cell>59.1</cell><cell>80.6</cell><cell>17.9</cell><cell>79.7</cell><cell>31.2</cell><cell>81.0</cell><cell>26.5</cell><cell>73.5</cell><cell>8.5</cell><cell>52.4</cell></row><row><cell>DAN [9]</cell><cell>87.1</cell><cell>63.0</cell><cell>76.5</cell><cell>42.0</cell><cell>90.3</cell><cell>42.9</cell><cell>85.9</cell><cell>53.1</cell><cell>49.7</cell><cell>36.3</cell><cell>85.8</cell><cell>20.7</cell><cell>61.1</cell></row><row><cell>DANN [16]</cell><cell>81.9</cell><cell>77.7</cell><cell>82.8</cell><cell>44.3</cell><cell>81.2</cell><cell>29.5</cell><cell>65.1</cell><cell>28.6</cell><cell>51.9</cell><cell>54.6</cell><cell>82.8</cell><cell>7.8</cell><cell>57.4</cell></row><row><cell>MCD(2) [39]</cell><cell>81.1</cell><cell>55.3</cell><cell>83.6</cell><cell>65.7</cell><cell>87.6</cell><cell>72.7</cell><cell>83.1</cell><cell>73.9</cell><cell>85.3</cell><cell>47.7</cell><cell>73.2</cell><cell>27.1</cell><cell>69.7</cell></row><row><cell>MCD(3) [39]</cell><cell>90.3</cell><cell>49.3</cell><cell>82.1</cell><cell>62.9</cell><cell>91.8</cell><cell>69.4</cell><cell>83.8</cell><cell>72.8</cell><cell>79.8</cell><cell>53.3</cell><cell>81.5</cell><cell>29.7</cell><cell>70.6</cell></row><row><cell>MCD(4) [39]</cell><cell>87.0</cell><cell>60.9</cell><cell>83.7</cell><cell>64.0</cell><cell>88.9</cell><cell>79.6</cell><cell>84.7</cell><cell>76.9</cell><cell>88.6</cell><cell>40.3</cell><cell>83.0</cell><cell>25.8</cell><cell>71.9</cell></row><row><cell>GPDA [41]</cell><cell>83.0</cell><cell>74.3</cell><cell>80.4</cell><cell>66.0</cell><cell>87.6</cell><cell>75.3</cell><cell>83.8</cell><cell>73.1</cell><cell>90.1</cell><cell>57.3</cell><cell>80.2</cell><cell>37.9</cell><cell>73.3</cell></row><row><cell>VMT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>ACCURACY PERCENTAGE OF OFFICE-31 CLASSIFICATION EXPERIMENT AMONG ALL FOUR TASKS. THE FIRST ROW CORRESPONDS TO THE PERFORMANCE IF NO ADAPTION IS IMPLEMENTED. WE EVALUATE TWO SRDA MODELS WITH DIFFERENT METHODS FOR ADDING NOISE. THE RESULTS ARE CITED FROM EACH STUDY.</figDesc><table><row><cell></cell><cell>A</cell><cell>A</cell><cell>D</cell><cell>W</cell><cell></cell></row><row><cell>Method</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>AVG</cell></row><row><cell></cell><cell>D</cell><cell>W</cell><cell>A</cell><cell>A</cell><cell></cell></row><row><cell>Source Only</cell><cell>68.9</cell><cell>68.4</cell><cell>62.5</cell><cell>60.7</cell><cell>65.2</cell></row><row><cell>GFK [58]</cell><cell>74.5</cell><cell>72.8</cell><cell>63.4</cell><cell>61.0</cell><cell>67.9</cell></row><row><cell>TCA [6]</cell><cell>74.1</cell><cell>72.7</cell><cell>61.7</cell><cell>60.9</cell><cell>67.4</cell></row><row><cell>DAN [9]</cell><cell>78.6</cell><cell>80.5</cell><cell>63.6</cell><cell>62.8</cell><cell>71.4</cell></row><row><cell>RTN [13]</cell><cell>77.5</cell><cell>84.5</cell><cell>66.2</cell><cell>64.8</cell><cell>73.3</cell></row><row><cell>DANN [16]</cell><cell>79.7</cell><cell>82.0</cell><cell>68.2</cell><cell>67.4</cell><cell>74.3</cell></row><row><cell>SRDA F</cell><cell>82.5</cell><cell>84.7</cell><cell>62.5</cell><cell>61.0</cell><cell>72.7</cell></row><row><cell>SRDA G</cell><cell>78.8</cell><cell>83.2</cell><cell>67.3</cell><cell>64.8</cell><cell>73.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV CLASSIFICATION</head><label>IV</label><figDesc>ACCURACY PERCENTAGE OF VISDA CLASSIFICATION EXPERIMENT FOR DIFFERENT BATCHSIZE.</figDesc><table><row><cell>Method</cell><cell>Batchsize</cell><cell>Average Accuracy</cell></row><row><cell></cell><cell>32</cell><cell>71.1</cell></row><row><cell>SRDA F</cell><cell>16 8</cell><cell>69.1 64.5</cell></row><row><cell></cell><cell>4</cell><cell>29.5</cell></row><row><cell></cell><cell>32</cell><cell>69.5</cell></row><row><cell>SRDA V</cell><cell>16 8</cell><cell>66.5 62.2</cell></row><row><cell></cell><cell>4</cell><cell>34.3</cell></row><row><cell></cell><cell>32</cell><cell>73.5</cell></row><row><cell>SRDA G</cell><cell>16 8</cell><cell>71.1 64.2</cell></row><row><cell></cell><cell>4</cell><cell>51.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 We test different ? {0.3, 0.4, 0.5, 0.6, 0.7} on the digits datasets. SRDA F , SRDA V and SRDA G are denoted by fgsm, vat and random, respectively</figDesc><table><row><cell>94.2 94.4 94.6 94.8 95.0 95.2 95.4 95.6 95.8 Accuracy</cell><cell>94.47 95.26 95.41</cell><cell>95.21 94.46 95.78</cell><cell>95.03 95.78 95.49</cell><cell>95.12 94.96 94.18</cell><cell>95.12 94.9 94.96 fgsm vat random</cell><cell>82 84 86 88 Accuracy 90 92 94</cell><cell>84.6 82.26 93.42</cell><cell>83.37 86.54 93.41</cell><cell>85.0 84.64 94.76</cell><cell>82.09 88.84 94.57</cell><cell>81.37 83.91 94.61 fgsm vat random</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70</cell></row><row><cell></cell><cell></cell><cell cols="3">(a) USPS?MNIST</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) MNIST?USPS</cell><cell></cell></row><row><cell>93</cell><cell></cell><cell></cell><cell>92.44 93.61</cell><cell></cell><cell>fgsm vat random</cell><cell>98</cell><cell>97.92 98.67</cell><cell>98.81</cell><cell>98.9 99.17</cell><cell>98.91</cell><cell>98.9 fgsm vat random</cell></row><row><cell>88 89 90 91 92 Accuracy</cell><cell>87.71 89.19 89.73</cell><cell>88.33 88.73 89.51</cell><cell>90.87</cell><cell>88.37 89.68 89.91</cell><cell>90.46 89.45 89.56</cell><cell>90 92 94 96 Accuracy</cell><cell>93.15</cell><cell>94.91 95.96</cell><cell>95.96</cell><cell>89.9 94.67</cell><cell>90.83 95.1</cell></row><row><cell></cell><cell cols="5">0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70</cell><cell></cell><cell cols="5">0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70</cell></row><row><cell></cell><cell cols="4">(c) SYNSIG?GTSRB</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(d) SVHN?MNIST</cell><cell></cell></row><row><cell cols="3">Fig. 6. (a)-(d):</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the theorems assume binary classification (y ? 0, 1). However, they can be directly extended to multi-class settings.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding self-training for gradual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/kumar20c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, H. D. III and A. Singh</editor>
		<meeting>the 37th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="5468" to="5479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transfer learning for visual categorization: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1019" to="1034" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prediction reweighting for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1682" to="1695" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/2983-analysis-of-representations-for-domain-adaptation.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>B. Sch?lkopf, J. C. Platt, and T. Hoffman</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR, 07-09</idno>
		<ptr target="http://proceedings.mlr.press/v37/long15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, F. Bach and D. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning, ser. Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Central moment discrepancy (cmd) for domaininvariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Natschl?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saminger-Platz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep kernels for non-parametric two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/liu20m.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, H. D. III and A. Singh</editor>
		<meeting>the 37th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="6316" to="6326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/liang20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, H. D. III and A. Singh</editor>
		<meeting>the 37th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="6028" to="6039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with adversarial residual transform networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>abs/1701.04862</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A DIRT-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1q-TM-AW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virtual mixup training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Neural Information Processing Systems, ser. NIPS&apos;04</title>
		<meeting>the 17th International Conference on Neural Information Processing Systems, ser. NIPS&apos;04<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-class heterogeneous domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v20/13-580.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">57</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Heterogeneous domain adaptation: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5588" to="5602" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heterogeneous domain adaptation through progressive alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Heterogeneous domain adaptation via nonlinear matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="984" to="996" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised optimal transport for heterogeneous domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/412</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/412" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2969" to="2975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-source heterogeneous unsupervised domain adaptation via fuzzy-relation neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain adaptation-can quantity compensate for quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="202" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s10472-013-9371-9</idno>
		<ptr target="https://doi.org/10.1007/s10472-013-9371-9" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularized semipaired kernel cca for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehrkanoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3199" to="3213" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enhanced subspace distribution matching for fast visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abusorrah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Social Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generative adversarial networks: introduction and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="588" to="598" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3403" to="3417" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation: A deep max-margin gaussian process approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="4375" to="4385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalization error estimation under covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Information-Based Induction Sciences</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1xsqj09Fm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Evaluation of traffic sign recognition methods trained on synthetically generated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
		<editor>J. Blanc-Talon, A. Kasinski, W. Philips, D. Popescu, and P. Scheunders</editor>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="576" to="583" />
		</imprint>
	</monogr>
	<note>Advanced Concepts for Intelligent Vision Systems</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: A multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visda: The visual domain adaptation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1710.06924</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>Computer Vision -ECCV 2010, K. Daniilidis, P. Maragos, and N. Paragios</editor>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="213" to="226" />
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6254-domain-separation-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2784" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (4), ser</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ai-based modeling and data-driven evaluation for smart manufacturing processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>O&amp;apos;hagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1026" to="1037" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A novel automatic classification system based on hybrid unsupervised and supervised machine learning for electrospun nanofibers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ieracitano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paviglianiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pasero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Morabito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="76" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Kiss+ for rapid and accurate pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abusorrah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="394" to="403" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Can virtual samples solve small sample size problem of kissme in pedestrian re-identification of smart transportation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3766" to="3776" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
