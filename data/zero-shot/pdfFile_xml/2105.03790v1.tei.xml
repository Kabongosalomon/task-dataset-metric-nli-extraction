<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kollias</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktoriia</forename><surname>Sharmanska</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Multi-Task Learning</term>
					<term>Heterogeneous Tasks</term>
					<term>Weak Supervision</term>
					<term>Negative Transfer</term>
					<term>Distribution Matching</term>
					<term>Co-annotation</term>
					<term>Coupling</term>
					<term>Distillation</term>
					<term>holistic learning</term>
					<term>Zero-shot Learning</term>
					<term>Few-shot Learning</term>
					<term>Affect recognition in-the-wild</term>
					<term>emotion and expression classification</term>
					<term>detection</term>
					<term>valence</term>
					<term>arousal</term>
					<term>action units</term>
					<term>FaceBehaviorNet</term>
					<term>attribute detection</term>
					<term>face identification !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-Task Learning (MTL) has emerged as a methodology in which multiple tasks are jointly learned by a shared learning algorithm, such as a deep neural network. MTL is based on the assumption that the tasks under consideration are related; therefore it exploits shared knowledge for improving performance on each individual task. Tasks are generally considered to be homogeneous, i.e., to refer to the same type of problem, e.g., classification. Moreover, MTL is usually based on ground truth annotations with full, or partial overlap across tasks; i.e., for each input sample, there exist annotations for all or most of the tasks. In this work, we deal with heterogeneous MTL, simultaneously addressing detection, classification and regression problems. We explore task-relatedness as a means for co-training, in a weakly-supervised way, tasks that contain little, or even non-overlapping annotations. Task-relatedness is introduced in MTL, either explicitly through prior expert knowledge, or through data-driven studies. We propose a novel distribution matching approach, in which knowledge exchange is enabled between tasks, via matching of their predictions' distributions. Based on this approach, we build FaceBehaviorNet, the first framework for large-scale face analysis, by jointly learning all facial behavior tasks. We develop case studies for: i) continuous affect estimation, facial action unit detection and basic emotion recognition; ii) facial attribute detection and face identification. We illustrate that co-training via task relatedness alleviates negative transfer, i.e., cases in which MT model's performance is, in some task(s), worse than that of a single-task model. Since FaceBehaviorNet learns features that encapsulate all aspects of facial behavior, we conduct zero-and few-shot learning to perform tasks beyond the ones that it has been trained for, such as compound emotion recognition. By conducting a very large experimental study, utilizing 10 databases, we illustrate that our approach outperforms, by large margins, the state-of-the-art in all tasks and in all databases, even in these which have not been used in its training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>H OLISTIC frameworks, where several learning tasks are interconnected and explicable by the reference to the whole, are common in computer vision. A diverse set of examples includes a scene understanding framework that reasons about 3D object detection, semantic segmentation and depth reconstruction <ref type="bibr" target="#b0">[1]</ref>, a face analysis framework that addresses face detection, landmark localization, gender recognition, age estimation <ref type="bibr" target="#b1">[2]</ref>, a universal network for low-, mid-, high-level vision <ref type="bibr" target="#b2">[3]</ref>, a large-scale framework of visual tasks for indoor scenes <ref type="bibr" target="#b3">[4]</ref>. Most if not all prior works rely on building a multi-task framework where learning is done based on the ground truth annotations with full or partial overlap across tasks. During training, all the tasks are optimised simultaneously aiming for representation learning that supports a holistic view of the framework. Our approach for building such framework falls under heterogeneous multi-task learning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. As opposed to standard multi-task learning with a single type of e.g. classification tasks, heterogeneous learning addresses jointly different types of supervised tasks including classification, detection, and regression problems, which poses new challenges for effective knowledge transfer.</p><p>What makes our work different from the previous holistic approaches is exploring the idea of task-relatedness as means for co-training the heterogeneous tasks. In our work, relatedness between heterogeneous tasks is either provided explicitly in a form of expert knowledge, or is inferred based on empirical studies. Importantly, in co-training, the related tasks exchange their predictions and iteratively teach each other so that predictors of all tasks can excel even if we have limited or no data for some of them. How to perform knowledge exchange and cotraining between heterogeneous tasks that could generalize to different scenarios and across datasets is the key challenge that we address in this work. We propose an effective distribution matching approach based on distillation <ref type="bibr" target="#b6">[7]</ref>, where knowledge exchange between tasks is enabled via distribution matching over their predictions. Based on this approach, we build the first holistic framework for large-scale face analysis, FaceBehaviorNet, with case studies in affective computing and in face recognition. In affective computing, we have heterogeneous tasks such as predicting categorical emotions (e.g. happy, sad, angry, surprised), predicting continuous dimensions of valence and arousal (how positive/negative, active/passive the emotional state is), predicting activations of binary action units <ref type="bibr" target="#b7">[8]</ref> (activation of facial muscles) to explain the human's affective state. In face recognition, we consider the two interconnected tasks of face identification and facial attribute detection in a single holistic framework.</p><p>Up until now training holistic models has been primarily addressed by combining multiple datasets to solve individual tasks <ref type="bibr" target="#b1">[2]</ref>, or by collecting the annotations in terms of all tasks arXiv:2105.03790v1 [cs.CV] 8 May 2021 <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. As an example, let us consider facial behavior analysis which is one of our case studies. A lot of effort has been made towards collecting datasets of naturalistic facial behavior captured in uncontrolled conditions, in-the-wild <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Among the three heterogeneous tasks in affective computing, collecting annotations of action units is particularly costly, as it requires skilled annotators to perform the task. Nevertheless there has been a lot of effort to collect those annotations and develop automatic toolboxes <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The datasets collected so far have annotations for training some of the heterogeneous tasks and despite significant effort <ref type="bibr" target="#b13">[14]</ref>, there is no dataset that for each image or video has complete annotations of all three tasks. Cotraining via task relatedness is an effective way of aggregating knowledge across datasets and transferring it across heterogeneous tasks, especially with little or non-overlapping annotations.</p><p>In this work we discuss two strategies to infer task relatedness: i) via domain knowledge, ii) via dataset annotation, see <ref type="table">Table 1</ref>, <ref type="table" target="#tab_2">Table 2</ref> in our case studies. For example, the three aforementioned tasks of facial behavior analysis are interconnected with known strengths of relatedness from the literature. In <ref type="bibr" target="#b7">[8]</ref>, the facial action coding system (FACS) has been built to indicate for each of the basic expressions its prototypical action units. In <ref type="bibr" target="#b14">[15]</ref>, a dedicated user study has been conducted to study the relationship between AUs activations and emotion expressions for basic types and beyond -see <ref type="table">Table 1</ref>. In <ref type="bibr" target="#b15">[16]</ref>, the authors show that neural networks trained for expression recognition implicitly learn action units. Also, in <ref type="bibr" target="#b16">[17]</ref> the authors have discovered that valence and arousal dimensions could be interpreted by AUs. For example, AU12 (lip corner puller) is related to positive valence. In our second case study on face recognition, we have an example of a dataset such as CelebA <ref type="bibr" target="#b17">[18]</ref>, where annotations for both tasks, identification and attribute prediction, are available for each image. We can infer task relatedness based on the annotations empirically -see examples in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>One of the important challenges in multi-task learning is how to avoid negative transfer, when the performance of the multitask model can be worse than that of a single-task model <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Negative transfer occurs naturally in the multi-task learning scenarios when: i) source data are heterogeneous or less related (as all tasks are diverse to each other, there is no suitable common latent representation and thus multi-task learning produces poor representations); ii) one task dominates the training process (in this scenario, one group of related tasks may dominate the training process and negative transfer may occur simultaneously on tasks outside the dominant group). We demonstrate empirically that the proposed distribution matching approach based on task relatedness can alleviate the problem of negative transfer in FaceBehaviorNet.</p><p>Our main contributions are as follows: ? We propose a flexible holistic framework that can accommodate heterogeneous tasks with encoding prior knowledge of tasks relatedness. In our experiments we evaluate two effective strategies of task relatedness: a) obtained from domain knowledge, e.g. based on a cognitive study <ref type="bibr" target="#b14">[15]</ref>, and b) inferred empirically from dataset annotations. ? We propose an effective weakly-supervised learning approach that couples, via distribution matching and label coannotation, heterogeneous tasks which contain little, or even non-overlapping annotations; we show its effectiveness for face analysis in two case studies: affective computing and face recognition. ? We present the first, to the best of our knowledge, holistic network for facial behavior analysis; this is capable of simultaneously predicting 7 basic expressions, 17 action units and continuous valence-arousal emotion dimensions in-the-wild. For network training and evaluation we utilize publicly available in-the-wild databases. This network will be made publicly available. All available databases are automatically annotated for all tasks by this network; these annotations will also be made publicly available. <ref type="bibr">?</ref> We conduct an extensive experimental study, currently the largest to the best of our knowledge, in which we evaluate FaceBehaviorNet on 10 databases and compare its performance to single-task networks. We demonstrate that FaceBehaviorNet when trained with the proposed coupling losses greatly outperforms single-task networks in all tasks and in all databases, even in ones that have not been used in its training. This validates that the network's capabilities are enhanced when it is jointly trained for all related tasks. It is also shown that the distribution matching approach for knowledge distillation across heterogeneous tasks successfully prevents negative transfer in multi-task learning. We also show that FaceBehaviorNet displays the best performance across all 10 databases outperforming the-stateof-the-art methods in each database. ? We further explore the feature representation learned in the joint training and show its generalization abilities on the task of compound expression recognition when no or little training data is available (zero-shot and few-shot learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Works exist in literature that use emotion labels to complement missing AU annotations or increase generalization of AU classifiers <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Our work deviates from such methods, as we target joint learning of all three facial behavior tasks via a single holistic framework, whilst these works perform only AU detection (and not emotion recognition and valence-arousal estimation). Multi-task learning (MTL) was first studied in <ref type="bibr" target="#b23">[24]</ref>, where the authors proposed to jointly learn parallel tasks sharing a common representation; they used part of the knowledge learned when solving one task, so as to improve learning of the other related tasks. Since then, several approaches have adopted MTL for solving different problems in computer vision and machine learning. In the face analysis domain, the use of MTL is somewhat limited. In <ref type="bibr" target="#b24">[25]</ref>, Face-SSD was proposed for detecting faces and performing various -separate and independent-face-related tasks, including smile recognition, face attribute prediction and valencearousal estimation; of these tasks just one is implemented at a time and there are no MTL experiments. The authors just mention that MTL can be used in Face-SSD and the network's optimization loss function should be the sum of the independent task losses.</p><p>In <ref type="bibr" target="#b25">[26]</ref>, MTL was tackled through a neural network that jointly handled face recognition and facial attribute prediction tasks. At first the authors trained a network for facial attribute detection on CelebA; then they used it to generate attribute labels for another database (CASIA-WebFace) that only contained facial identification (id) labels. Finally, the authors trained a multi-task network for attribute detection and face identification on that database (that contained the id labels and the generated attribute labels). The loss function of the network was the sum of the independent task losses.</p><p>An approach targeting a problem similar to ours is <ref type="bibr" target="#b26">[27]</ref>, presenting a knowledge augmented deep learning framework for joint AU detection and facial expression recognition. The described framework consisted of a knowledge model -represented by a Bayesian Network -and three neural network based submodels. An image-based FER model performed facial expression classification directly from image data. An AU model performed AU detection from the images. The knowledge model was used to weakly supervise the learning of the AU detector. An AUbased FER model performed expression recognition from AU detection results; it was mainly introduced to assist the model integration process. The three neural network models were initially trained independently and they were then refined jointly until convergence. It should be mentioned that this work does not cover valence-arousal estimation. Moreover, it utilizes only highly controlled databases for training and evaluation; there is no proof that it can be applied effectively to real world in-the-wild data cases. In addition, it does not generate a single network, but three distinct ones; having high time and space requirements and high computational complexity.</p><p>Another work with a goal similar to ours is <ref type="bibr" target="#b27">[28]</ref>, where a unified model performing facial action unit detection, expression classification, and valence-arousal estimation was proposed as part of the ABAW Competition at IEEE FG 2020 1 . The authors used the Aff-Wild2 database <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b28">[29]</ref> that contains annotations for all 3 tasks. However not all images were annotated for all tasks. To tackle the incomplete labels' problem, the authors at first trained a teacher multi-task model using only the given complete labels. Then by testing that network on the database, they generated annotations (soft labels) for the missing labels. Finally they trained a student multi-task network on the union of the original and soft labels; that network outperformed the teacher one. They also applied data balancing techniques and developed ensembles for further boosting the performance. The teacher model did not take into account the fact that the three tasks are interconnected -they simply used an overall loss equal to the sum of the independent task losses. Thus, the student model did not learn this relatedness. It should also be added that this work utilized and was evaluated on only one database, i.e., the Aff-Wild2; the soft labels generated by the teacher model were not reliable.</p><p>In <ref type="bibr" target="#b29">[30]</ref> a multi-task and multi-modal network was proposed for valence-arousal estimation, action unit detection and seven basic expression classification. The authors utilized a 3D ResNet for processing the image modality (video frames) and a ResNet for processing the audio modality (Mel-spectrograms). The features from these two networks were concatenated and fed to a fully connected layer that provided the final estimates for the three tasks. However, this work was trained with and was evaluated on the Aff-Wild2 database, utilizing only the overlapping and complete annotations.</p><p>In <ref type="bibr" target="#b30">[31]</ref>, a two-level attention with two stage multi-task learning framework was constructed for emotion recognition and valencearousal estimation. In the first attention level, a CNN extracted position-level features and then in the second an RNN with selfattention was proposed to model the relationship between layerlevel features. This work utilized the AffectNet database <ref type="bibr" target="#b10">[11]</ref>, annotated for both tasks and thus only containing overlapping and complete annotations. In addition, this work did not tackle the action unit detection task. <ref type="bibr" target="#b0">1</ref>. https://ibug.doc.ic.ac.uk/resources/fg-2020-competition-affectivebehavior-analysis/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED APPROACH</head><p>Let us consider a set of m tasks {T i } m i=1 . In task T i , the observations are generated by the underlying distribution D i over inputs X and their labels Y associated with the task. For the i-th task T i , the training set D i consists of n i data points (x i j , y i j ), j = 1, . . . , n i with x i j ? R d and its corresponding output y i j ? R if it is a regression task, or y i j ? {0, 1} if it is a binary classification task, or y i j ? {0, 1} k (one-hot encoding) if it is a (mutually exclusive) k-class classification task.</p><p>Then, the goal of MTL is to find m hypothesis: h 1 , ..., h m over the hypothesis space H to control the average expected error over all tasks: 1 m m i=1 E (x,y)?Di L(h i (x), y) with L being the loss function. We can also define a weight w i ? ? m , {w i } m i=1 &gt; 0 to govern the contribution of each task. Then the overall loss is:</p><formula xml:id="formula_0">L M T = 1 m m i=1 w i ? E (x,y)?Di L(h i (x), y).<label>(1)</label></formula><p>If it is a regression task, the loss L can take a form of MAE, MSE or a correlation-based loss. If it is a binary classification task, the loss L can be binary/sigmoid cross entropy loss. If it is a (mutually exclusive) k-class classification task, the loss L can be softmax cross entropy loss. In the case of a neural network the hypothesis can be expressed as f ({?}, x) where {?} denotes the set of weights of the neural network learned during training.</p><p>In the following, we present the proposed framework via two examined case studies. The framework includes inferring the relationship between the tasks (either via domain knowledge or dataset annotation) and using it for coupling them during MTL. The coupling is achieved via the proposed co-annotation and distribution matching losses. These losses can be incorporated and used in any deep neural network that performs MTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Case Study I: Affective Computing</head><p>We start with the multi-task formulation of the facial behavior model. In this model we have three objectives: (1) learning seven basic emotions, (2) detecting activations of 17 binary facial action units, (3) learning the intensity of the valence and arousal continuous affect dimensions. We train a multi-task neural network model to jointly perform (1)- <ref type="bibr" target="#b2">(3)</ref>. For a given image x ? X , we can have label annotations of either one of seven basic emotions y emo ? {1, 2, . . . , 7}, or 17 2 binary action units activations y au ? {0, 1} 17 , or two continuous affect dimensions, valence and arousal, y va ? [?1, 1] 2 . For simplicity of presentation, we use the same notation x for all images leaving the context to be explained by the label notations. We train the multi-task model by minimizing the following objective:</p><formula xml:id="formula_1">L M T = ? 1 L Emo + ? 2 L AU + ? 3 L V A (2) L Emo = E x,yemo ? log p(y emo |x) L AU = E x,yau ? log p(y au |x) L V A = 1 ? CCC(y va ,? va ),</formula><p>where the first term is the cross entropy loss computed over images with a basic emotion label, the second term is the binary cross entropy loss computed over images with 17 AUs activations, log p(y au |x) :=  1: Relatedness between: i) basic emotions and their prototypical and observational AUs from <ref type="bibr" target="#b14">[15]</ref>: the weights w in brackets correspond to the fraction of annotators that observed the AU activation; ii) basic emotions and AUs, inferred from Aff-Wild2: the weights w in brackets correspond to the percentage of images annotated with the specific expression in which the AU was activated.</p><p>Cognitive-Psychological Study <ref type="bibr" target="#b14">[15]</ref> Empirical , where for i ? {v, a}, y i is the ground truth,? i is the predicted value and</p><formula xml:id="formula_2">? i = 2 ? E (y i ? E yi ) ? (? i ? E? i ) E 2 (y i ? E yi ) 2 + E 2 (? i ? E? i ) 2 + (E yi ? E? i ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Task-Relatedness</head><p>In the seminal work of <ref type="bibr" target="#b14">[15]</ref>, the authors conduct a study on the relationship between emotions (basic and compound) and facial action unit activations. The summary of the study is a Table of the emotions and their prototypical and observational action units, which we include in <ref type="table">Table 1</ref> for completeness. Prototypical action units are ones that are labelled as activated across all annotators' responses, observational are action units that are labelled as activated by a fraction of annotators. For example, in emotion happiness the prototypical are AU12 and AU25, the observational is AU6 with weight 0.51 (observed by 51% of the annotators). <ref type="table">Table 1</ref> provides the relatedness between emotion categories and action units obtained from this cognitive and psychological study with human participants. Alternatively we can infer task relatedness from external dataset annotations. In particular, we use the Aff-Wild2 database, which is the first in-the-wild database that contains annotations for all three behavior tasks to infer task relatedness. The dataset is fully annotated with basic emotions and continual emotions of valence and arousal, and a subset of it is annotated with action units. We first train a network for AU detection on the union of Aff-Wild2 and GFT databases <ref type="bibr" target="#b31">[32]</ref>, and use this network to automatically annotate Aff-Wild2 with AUs. <ref type="table">Table 1</ref> shows the distribution of AUs for each basic expression that we use as task relatedness for distribution matching. In parenthesis next to each AU is the percentage of images annotated with the specific expression in which this AU was activated. In the following, we use the domain knowledge, specifically the cognitive and psychological study <ref type="bibr" target="#b14">[15]</ref>, to encode task relatedness and introduce the proposed approach for coupling the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Coupling of basic emotions and AUs</head><p>Via Co-annotation. We propose a simple strategy of coannotation to couple the training of emotions and action unit predictions. Given an image x with the ground truth basic emotion y emo , we enforce the prototypical and observational AUs of this emotion to be activated. We co-annotate the image (x, y emo ) with y au ; this image contributes to both L Emo and L AU 3 in eq. 2. We re-weight the contributions of the observational AUs with the annotators' agreement score (from <ref type="table">Table 1)</ref>.</p><p>Similarly, for an image x with the ground truth action units y au , we check whether we can co-annotate it with an emotion label. For an emotion to be present, all its prototypical and observational AUs have to be present. In cases when more than one emotion is possible, we assign the label y emo of the emotion with the largest requirement of prototypical and observational AUs. The image (x, y au ) that is co-annotated with the emotion label y emo contributes to both L AU and L Emo in eq. 2. We call this approach the FaceBehaviorNet with co-annotation.</p><p>Via Distribution Matching. The aim here is to align the predictions of the emotions and action units tasks during training. For each sample x we have the predictions of emotions p(y emo |x) as the softmax scores over seven basic emotions and we have the prediction of AUs activations p(y i au |x), i = 1, . . . , 17 as the sigmoid scores over 17 AUs.</p><p>The distribution matching idea is simple: we match the distribution over AU predictions p(y i au |x) with the distribution q(y i au |x), where the AUs are modeled as a mixture over the basic emotion categories:</p><formula xml:id="formula_3">q(y i au |x) = yemo?{1,...,7} p(y emo |x) p(y i au |y emo ),<label>(3)</label></formula><p>where p(y i au |y emo ) is defined deterministically from <ref type="table">Table 1</ref> and is 1 for prototypical/observational action units, or 0 otherwise. For example, AU2 is prototypical for emotion surprise and observational for emotion fear and thus q(y AU2 |x) = p(y surprise |x) + p(y fear |x) <ref type="bibr" target="#b3">4</ref> .</p><p>This matching aims to make the network's predicted AUs consistent with the prototypical and observational AUs of the network's predicted emotions. So if, e.g., the network predicts the emotion happiness with probability 1, i.e., p(y happiness |x) = 1, then the prototypical and observational AUs of happiness -AUs 12, 25 and 6-need to be activated in the distribution q:</p><formula xml:id="formula_4">q(y AU12 |x) = 1; q(y AU25 |x) = 1; q(y AU6 |x) = 1; q(y i au |x) = 0, i ? {1, .., 14}.</formula><p>In spirit of the distillation approach <ref type="bibr" target="#b6">[7]</ref>, we match the distributions p(y i au |x) and q(y i au |x) by minimizing the cross entropy <ref type="bibr" target="#b2">3</ref>. Here we overload slightly our notations; for co-annotated images, yau has variable length and only contains prototypical and observational AUs. 4. We also tried a variant with reweighting for observational AUs, i.e. p(y i au |yemo) = w with the soft targets loss term <ref type="bibr" target="#b4">5</ref> :</p><formula xml:id="formula_5">L DM = E x 17 i=1 [?p(y i au |x)log q(y i au |x)] ,<label>(4)</label></formula><p>where all available training samples are used to match the predictions. We call this approach FaceBehaviorNet with distrmatching.</p><p>Via Mixing the two strategies, co-annotation and distribution matching. A mix of the two strategies, co-annotation and distribution matching, is also possible. Given an image x with the ground truth annotation of the action units y au , we can first co-annotate it with a soft label in form of the distribution over emotions and then match it with the predictions of emotions p(y emo |x).</p><p>More specifically, at first we compute, for each basic emotion, an indicator score, I(y emo |x) over its prototypical and observational AUs being present:</p><formula xml:id="formula_6">I(y emo |x) = yau,wau?{1,...,17} w au ? y au yau,wau?{1,...,17} w au , y emo ? {1, . . . , 7}<label>(5)</label></formula><p>w au = ? ? ? 1, au is prototypical for y emo (from <ref type="table">Table 1</ref>) w, au is observational for y emo (from <ref type="table">Table 1</ref>) 0, otherwise For example, for emotion happiness, the indicator score</p><formula xml:id="formula_7">I(happiness|x) = (y AU12 + y AU25 + 0.51 ? y AU6 )/(1 + 1 + 0.51),</formula><p>or all weights equal 1 if without reweighting. Then, we convert the indicator scores to probability scores over emotion categories; this soft emotion label, q(y emo |x), is computed as following:</p><formula xml:id="formula_8">q(y emo |x) = e I(yemo|x) y emo e I(y emo |x) , {y emo , y emo } ? {1, . . . , 7}<label>(6)</label></formula><p>In this variant, every single image that has ground truth annotation of AUs will have a soft emotion label assigned. Finally we match the predictions p(y emo |x) and the soft emotion label q(y emo |x) by minimizing the cross entropy with the soft targets loss term:</p><formula xml:id="formula_9">L SCA = E x yemo?{1,...,7} [?p(y emo |x)log q(y emo |x)] (7)</formula><p>We call this approach FaceBehaviorNet with soft co-annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Coupling of categorical emotions, AUs with continuous affect</head><p>In our work, continuous affect (valence and arousal) is implicitly coupled with the basic expressions and action units via a joint training procedure. Also one of the datasets we used has annotations for categorical and continuous emotions (AffectNet <ref type="bibr" target="#b10">[11]</ref>). Studying an explicit relationship between them is a novel research direction beyond the scope of this work. <ref type="bibr" target="#b4">5</ref>. This can be seen as minimizing the KL-divergence KL(p||q) across the 17 action units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Compound Expressions: Zero-and Few-shot Learning</head><p>Knowledge Generalization in Zero-shot Learning After Face-BehaviorNet was trained (either without or with any coupling loss or any combination of them), it was able to effectively capture and solve the three behavior tasks of valence-arousal estimation, action unit detection and basic expression classification. Face-BehaviorNet has learned features that encapsulate all aspects of facial behavior. Therefore, exploiting this fact, we describe how FaceBehaviorNet can generalize its knowledge in other emotion recognition contexts, in a zero-shot manner. For this, we use the predictions of FaceBehaviorNet together with the rules from <ref type="bibr" target="#b14">[15]</ref> to generate compound emotion predictions. We compute a candidate score, C s (y c?emo ), for each class y c?emo :</p><formula xml:id="formula_10">C s (y c?emo ) = I au + F emo + D va<label>(8)</label></formula><formula xml:id="formula_11">I au = [ 17 k=1 p(y k au |y emo )] ?1 ? 17 k=1 p(y k au |x) p(y k au |y emo ) F emo = p(y emo1 ) + p(y emo2 ) D va = 1, p(y v |x) &gt; 0 0, otherwise</formula><p>I au expresses FaceBehaviorNet's predictions of only the prototypical (and observational) AUs that are associated with this compound class according to <ref type="bibr" target="#b14">[15]</ref>. In this manner, every AU acts as an indicator for this particular emotion class. This terms describes the confidence (probability) of AUs that this compound emotion is present.</p><p>F emo expresses FaceBehaviorNet's predictions of only the basic expression classes emo1 and emo2 that are mixed and form the compound class (e.g., if the compound class is happily surprised then emo1 is happy and emo2 is surprised).</p><p>D va is added only to the happily surprised and happily disgusted classes and is either 0 or 1 depending on whether FaceBe-haviorNet's valence prediction is negative or positive, respectively. The rationale is that, from all compound classes, only happily surprised and happily disgusted classes have positive valence. All other compound classes are expected to have negative valence as they correspond to negative emotions.</p><p>The final prediction is the class that obtained the maximum candidate score.</p><p>Robust Prior for Few-Shot Learning By having learned complex and emotionally rich features, FaceBehaviorNet can constitute a robust prior for compound emotion recognition, especially for datasets that are quite small in terms of size. Up to now, there exist only a couple of in-the-wild datasets annotated in terms of compound expressions and contain less than 5K images in total. Therefore FaceBehaviorNet can be used as a pre-trained network and can be further fine-tuned (either with freezing some of its parts or not) to perform compound emotion classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Case Study II: Face Recognition</head><p>We start with the multi-task formulation of the facial behavior model. In this model we have two objectives: (1) detecting activations of 40 binary facial attributes, (2) learning to classify 10, 177 identities (ids). We train a multi-task neural network model to jointly perform <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula">(2)</ref>. For a given image x ? X , we can have label annotations of either one of 10, 177 ids y id ? {1, . . . , 10177}, or 40 binary facial attributes y attr ? {0, 1} 40 . For simplicity of presentation, we use the same notation x for all images leaving the context to be explained by the label notations. We train the multi-task model by minimizing the following objective:</p><formula xml:id="formula_12">L M T = ? 1 L Id + ? 2 L Attr (9) L Id = E x,y id ? log e p(y id |x) 10177 y id =1 e p(y id |x) L Attr = E x,yattr ? 40 i=1 y i attr log p(y i attr |x) + E x,yattr ? 40 i=1 (1 ? y i attr )log (1 ? p(y i attr |x))</formula><p>where the first term is the cross entropy loss computed over images with an id label and p(y id |x) is the prediction of the id that corresponds to the positive class; the second term is the binary cross entropy loss computed over images with 40 attribute activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Task-Relatedness</head><p>There has been no relatedness study, like the one of <ref type="table">Table 1</ref>, between facial attributes and ids. Therefore we inferred it empirically from dataset annotations. In particular, we used the CelebA database, which contains overlapping annotations for facial attributes and identities. In other words, each image in the database is annotated both in terms of facial attributes and identities. We calculated the distribution of attributes for each identity; more precisely, for each identity (id) and for each attribute (attr), we divided the number of images in which the attribute attr existed, by the total amount of images that the specific identity id existed; as a result, we created the corresponding percentages. <ref type="table" target="#tab_2">Table 2</ref> illustrates the relationship between some identities and some of the 40 facial attributes. It can be seen that identities 1 and 5000 are male, whereas 2, 1000, 10000 and 10177 are female. All the images in CelebA where these 4 femals are displayed, show them young; some images in CelebA of the two male indentities show them young and some others show them old. These two male did not wear either lipstick, or necklace, in all of their images. The four female were wearing lipstick and necklace in some of their images, but not in others. Neither of the four female had a 5 o' clock shadow, whereas some images of the male had. All four female were highly attractive, whereas the two male were not that much. Mostly the two male had bags under their eyes in the images; one of them was bald in some images (probably when he was old) and he was wearing a necktie (no female was wearing a necktie).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Coupling of ids and attributes via distribution matching</head><p>Since the database that we utilized contained complete and overlapping annotations, we only propose the distribution matching loss for coupling the facial attributes' and ids' predictions. The aim is similar to the one defined and explained in Section 3.2.2; we want to align the predictions of the ids and attributes tasks during training. For each sample x we have the predictions of ids p(y id |x) as the softmax scores over 10,177 identities; we also have the prediction of attribute activations p(y i attr |x), i = 1, . . . , 40, as sigmoid scores, over 40 AUs.</p><p>At first, we model the attributes as a mixture over the identities, creating distribution q(y i attr |x):</p><formula xml:id="formula_13">q(y i attr |x) = y id ?{1,...,10177} p(y id |x) p(y i attr |y id ),<label>(10)</label></formula><p>where p(y i attr |y id ) is defined in <ref type="table" target="#tab_2">Table 2</ref>. For example, for the '5 o' clock' attribute: q(y 1 attr |x) = 0.34483 ? p(y id 1 |x) + 0.8 ? p(y i 5000 |x), assuming that for all other ids: p(y i attr |y id ) = 0. Next, we match the distributions p(y i attr |x) and q(y i attr |x) by minimizing the cross entropy with the soft targets loss term:</p><formula xml:id="formula_14">L DM = E x 17 i=1 [?p(y i au |x)log q(y i au |x)] ,<label>(11)</label></formula><p>where all available training samples are used to match the predictions. We call this approach FaceBehaviorNet with distr-matching. <ref type="figure" target="#fig_1">Fig.1</ref> shows the structure of the holistic (multi-task, multi-domain and multi-label) FaceBehaviorNet, which is based on residual units. 'bn' stands for batch normalization, the convolution layer is in the format: filter height ? filter width 'conv.', number of output feature maps; the stride is equal to 2, everywhere. In the affective computing case study: a (linear) output layer that gives final estimates for valence and arousal; it also gives 7 basic expression logits that are passed through a softmax function to get the final 7 basic expression predictions; lastly, it gives 17 AU logits that are passed through a sigmoid function to get the final 17 AU predictions. One can see that the predictions for all tasks are pooled from the same feature space. <ref type="figure" target="#fig_1">Fig.1</ref> illustrates FaceBe-haviorNet for that case. In the case study of face recognition, the (linear) output layer gives 10, 177 id logits that are passed through a softmax function to get the final id predictions and it also gives 40 attribute logits that are passed through a sigmoid function to get the final attribute predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">FaceBehaviorNet structure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Databases</head><p>Let us first describe the databases that we utilized in all our experiments. In the affective computing case study, we selected to This database has been extented to Aff-Wild2 that consists of 545 videos with around 2.8M frames. The AFEW-VA dataset <ref type="bibr" target="#b32">[33]</ref> consists of 600 video clips (without audio) totalling to around 30K frames that were annotated in terms of valence-arousal; the values are discrete in the range of [-10,10]. The values have been scaled to [-1,1] so as to be consistent with the other datasets.</p><p>The AffectNet database <ref type="bibr" target="#b10">[11]</ref> contains around 1M facial images, 400K of which were manually annotated in terms of 7 discrete expressions (plus contempt) and valence-arousal that ranges in [?1, 1]. The training set of this database consists of around 321K images and the validation of 5K. The validation set is balanced across the different emotion categories. The RAF-DB database <ref type="bibr" target="#b33">[34]</ref> contains 12.2K training and 3K test facial images annotated in terms of the 7 basic and 11 compound emotion categories.</p><p>The EmotioNet database <ref type="bibr" target="#b34">[35]</ref> contains around 1M images and was released for the EmotioNet Challenge in 2017 <ref type="bibr" target="#b35">[36]</ref>. 950K images were automatically annotated and the remaining 50K images were manually annotated with 11 AUs <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26)</ref>; around half of the latter constituted the validation and the other half the test set of the Challenge. Additionally, a subset of about 2.5K images was annotated with the 6 basic and 10 compound emotions. The DISFA database <ref type="bibr" target="#b36">[37]</ref> is a lab controlled database consisting of 27 videos each of which has 4,845 frames, where each frame is coded with the AU intensity on a six-point discrete scale. AU intensities equal or greater than 2 are considered as occurrence, while others are treated as non-occurrence. There are in total 12 AUs <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26)</ref>. The GFT database consists of 96 videos of 96 subjects totalling around 130K frames. It is annotated for the occurrence of 10 AUs <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24)</ref>. The training set consists of 78 subjects of around 108K frames and the test set of 18 subjects of around 24.5K frames.</p><p>The BP4D-Spontaneous database <ref type="bibr" target="#b37">[38]</ref> (in the rest of the paper we refer to it as BP4D) contains 61 subjects with 223K frames and is annotated for the occurrence and intensity of 27 AUs. There are 21 subjects with 75.6K images in the training, 20 subjects with 71.2K images in the development and 20 subjects with 75.7K images in the test partition. This database has been used as a part of the FERA 2015 Challenge <ref type="bibr" target="#b38">[39]</ref>, in which only 11 AUs (1,2,4,6,7,10,12,14,15,17,23) were used. The BP4D+ database <ref type="bibr" target="#b39">[40]</ref> is an extension of BP4D incorporating different modalities as well as more subjects (140). It is annotated for occurrence of 34 AUs and intensity for 5 of them. It has been used as a part of the FERA 2017 Challenge <ref type="bibr" target="#b40">[41]</ref>, in which only 10 AUs (1,4,6,7,10,12,14,15,17,23) were used. There are 2952 videos of 41 subjects with 9 different views in the training set, 1431 of 20 subjects with 9 different views in the validation set and 1080 videos of 30 subjects in the test set.</p><p>Here let us note that for AffectNet, BP4D and BP4D+, no test set is released; thus we use the released validation set to test on and divide the training set into subject independent training and validation subsets.</p><p>In the face recognition case, we utilized the CelebA database. It contains 202,600 images of 10,177K identities (celebrities), each with 40 attribute annotations. It contains around 160K images in the training, 20K in the validation and 20K in the test set. These sets are subject independent. For our experiment, we generated a new split into three subject dependent sets; each set contained images from all 10,177 identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Measures</head><p>We use: i) the CCC for Aff-Wild and Aff-Wild2 (CCC was the evaluation criterion of the respective Challenges), Affectnet and AFEW-VA, ii) the mean diagonal value of the confusion matrix for RAF-DB (this criterion was selected for evaluating the performance on this database by <ref type="bibr" target="#b33">[34]</ref>); the accuracy for AffectNet, iii) the F1 score for DISFA, GFT, BP4D and BP4D+ (this metric was the evaluation criterion of the FERA 2015 and 2017 Challenges); for AU detection in EmotioNet the Challenge's metric was the average between: a) the mean (across all AUs) F1 score and b) the mean (across all AUs) accuracy; for the expression classification, it was the average between: a) the average (across all emotions) F1 score and b) the unweighted average recall (UAR) over all emotion categories, iv) the total accuracy and average F1 score for the attributes and ids in CelebA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pre-and Post-Processing</head><p>Data Cleaning: We performed data cleaning on the AffectNet database that contains overlapping annotations for valence-arousal and 7 basic expressions. We removed images for which there was a mismatch between the values of valence and arousal and the discrete expressions. In more detail: i) images annotated as neutral should have radius of the valence-arousal vector smaller than 0.15; ii) images annotated as sad or disgusted or fearful should have negative valence; iii) images annotated as angry should have negative valence and positive arousal; iv) images annotated as happy should have positive valence.</p><p>Data Subsampling: Aff-Wild is a video database with consecutive frames having the same (or very similar) values for both valence and arousal. However FaceBehaviorNet is a CNN that does not exploit the temporal dependencies between frames; thus in each video, for each frame that we kept, we skipped the following four.</p><p>Face Pre-Processing: We used the SSH detector <ref type="bibr" target="#b41">[42]</ref> based on ResNet and trained on the WiderFace dataset <ref type="bibr" target="#b42">[43]</ref> to extract, from all images, face bounding boxes and 5 facial landmarks; the latter were used for face alignment. In the CelebA case we used the aligned data distributed with the database. All cropped and aligned images were resized to 112 ? 112 ? 3 pixel resolution and their intensity values were normalized to [?1, 1]. Prediction Post-Processing: Because Aff-Wild is a video database and FaceBehaviorNet is a CNN that does not exploit the temporal dependencies between frames, we performed median filtering of the -per frame -predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training Implementation Details</head><p>At this point let us describe the strategy that was used, in the affect computing task, for feeding images from different databases to FaceBehaviorNet during training. At first, the training set was split into three different sets, each of which contained images that were annotated in terms of either valence-arousal, or action units, or seven basic expressions; let us denote these sets as VA-Set, AU-Set and EXPR-Set, respectively. During training, at each iteration, three batches, one from each of these sets (as can be seen in <ref type="figure" target="#fig_1">Fig.1)</ref>, were concatenated and fed to FaceBehaviorNet. This step is important for network training, because: i) the network minimizes the objective function of eq. 2; at each iteration, the network has seen images from all categories and thus all loss terms contribute to the objective function, ii) since the network sees an adequate number of images from all categories, the weight updates (during gradient descent) are not based on noisy gradients; this in turn prevents poor convergence behaviors; otherwise, we would need to tackle these problems, e.g. do asynchronous SGD as proposed in <ref type="bibr" target="#b2">[3]</ref> to make the task parameter updates decoupled, iii) the CCC cost function (defined in Section 3) needs an adequate sequence of predictions.</p><p>Since VA-Set, AU-Set and EXPR-Set had different sizes, they needed to be 'aligned'. To do so, we selected the batches of these sets in such a manner, so that after one epoch we will have sampled all images in the sets. In particular, we chose batches of size 200, 124 and 52 for the AU-Set, VA-Set and EXPR-Set, respectively. The training of FaceBehaviorNet was performed in an end-toend manner, using the Momentum optimizer with 0.9 momentum and a learning rate of 10 ?4 . Training was performed on a Tesla V100 32GB GPU; training time was about 2 days. The TensorFlow platform has been used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study I: Affective Computing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Results: Ablation Study</head><p>First, we compared the performance of FaceBehaviorNet when trained with the losses of eq. 2 and: i) without using the coupling losses described in Section 3, ii) with co-annotation coupling loss, iii) with soft co-annotation coupling loss, iv) with distr-matching coupling loss and v) with soft co-annotation and distr-matching coupling losses. <ref type="table">Table 3</ref> shows the results for all these approaches, when the task relatedness was drawn from the cognitive study, or when it was inferred empirically from Aff-Wild2.</p><p>Many deductions can be made. Firstly, when FaceBehaviorNet was trained with a coupling loss, or with any combination of coupling losses, it displayed a better performance than when trained with no coupling loss. This holds on all databases and in both task relatedness scenarios. This validates the fact that the proposed losses helped to couple the three studied tasks regardless of which relatedness scenario was followed. Secondly, the performance in estimation of valence and arousal was improved, although we did not explicitly designed a coupling loss for this; we only coupled emotion categories and action units. We conjecture that when action unit detection and expression classification accuracy improves (due to coupling), valence and arousal performance also improves, because valence and arousal are implicitly coupled with emotions via joint dataset annotations.</p><p>Finally, in both scenarios, across all databases, best results have been achieved when FaceBehaviorNet was trained with both soft co-annotation and distr-matching losses. In particular, in both settings, an average performance increase of more than 2.5% has been observed when using both coupling losses, compared to the cases when only one of them was used. One can also observe that when task relatedness was inferred from Aff-Wild2, FaceBehaviorNet -trained with both coupling losses-displayed an average performance gain of 0.5% in expression classification and 0.5% in AU detection compared to the case when task relatedness was inferred from the cognitive study of <ref type="bibr" target="#b14">[15]</ref>.</p><p>Next, we utilized three state-of-the-art and broadly used networks, VGG-FACE <ref type="bibr" target="#b43">[44]</ref>, ResNet-50 <ref type="bibr" target="#b44">[45]</ref> and DenseNet-121 <ref type="bibr" target="#b45">[46]</ref>. We trained these networks in a multi-task manner (without using any coupling loss) with all the databases described in Section 4.1 and compared their performance to that of FaceBehaviorNet (trained without any coupling loss). As shown in <ref type="table" target="#tab_5">Table 4</ref>, the Face-BehaviorNet has proven to provide the best results, outperforming the MT-VGG-FACE by 1.6%, MT-ResNet by 1.4% and the MT-DenseNet by 4%, on average (across all databases' metrics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Results: Comparison with State-of-the-Art and Single-Task Methods</head><p>Next, we trained a single-task FaceBehaviorNet (ST-FaceBehaviorNet) on all dimensionally annotated databases, so as to predict valence and arousal; we also trained another similar network on all categorically annotated databases, to perform seven basic expression classification; finally we trained a third similar network on all databases annotated with action units, so as to perform AU detection. All these networks were based on residual units and had the same structure as FaceBehaviorNet; their only difference was the output layer. For brevity, these three single-task networks are denoted as '(3 ?) ST-FaceBehaviorNet' in one row of <ref type="table" target="#tab_7">Table 5</ref>.</p><p>We compared these networks' performance with the performance of FaceBehaviorNet when trained with and without the 3: Performance evaluation of valence-arousal, seven basic expression and action units predictions on all used databases provided by the FaceBehaviorNet when trained with/without the coupling losses, under the two task relatedness scenarios; 'AFA Score' is the average between the F1 Score and the Accuracy  coupling losses. We also compared them with the performance of the state-of-the-art (sota) methodologies for each utilized database: i) the best performing CNN (VGG-FACE) <ref type="bibr" target="#b46">[47]</ref> [9] on Aff-Wild; ii) the best performing network (AffWildNet) <ref type="bibr" target="#b46">[47]</ref> [9] on Aff-Wild; iii) the baseline networks (AlexNet) <ref type="bibr" target="#b10">[11]</ref> on AffectNet (in <ref type="table" target="#tab_7">Table 5</ref> they are denoted as '(2 ?) AlexNet' as they are two different networks: one for VA estimation and another for expression classification); iv) the state-of-the-art VGG-FACE <ref type="bibr" target="#b47">[48]</ref> for VA estimation on AffectNet; v) the state-of-  <ref type="table" target="#tab_7">Table 5</ref> displays the performance of all these networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-coupled MTL vs Sota &amp; ST:</head><p>In <ref type="table" target="#tab_7">Table 5</ref> it can be seen that when no coupling loss is used for training FaceBehaviorNet, the network outperforms the state-of-the-art in BP4D+ by 5%, BP4D by 25% for AU detection and in AffectNet by 2% for arousal estimation. In all other databases it displays either a slightly worse performance (1% in three databases), or worse performance (3%, 5% and 7% in three other databases). <ref type="table" target="#tab_7">Table 5</ref> additionally illustrates a comparison between the three single task ST-FaceBehaviorNet and (multi-task) FaceBehaviorNet trained without any coupling loss. It can be observed that the multi-task network displays a better performance for AU detection and VA estimation, but an inferior one for expression classification. The latter indicates that negative transfer occurs in the case of basic expressions. This negative transfer effect was caused by the fact that some of the related tasks -valence-arousal and action units -dominated the training process. It should be mentioned that these two tasks included more data, i.e., images and corresponding annotations, than the expression recognition task. In particular, there were twice more data in VA and three times more data in AUs compared to basic expressions. Negative transfer largely depends on the size of labeled data per task <ref type="bibr" target="#b18">[19]</ref>. In fact, the amount of labeled data per task has a direct effect on the feasibility and reliability of discovering shared regularities between the joint distributions of the tasks in MTL.</p><p>A way of overcoming negative transfer in expression recognition would be to change (i.e., increase) the lambda value in the expression loss (that controls the relative importance of the task), or decrease the lambda values in the VA and AU losses in the total loss function of eq. 2 that is minimized by the multitask model during training. However, this could severely affect the performance of the other tasks. Furthermore, this lambda hyperparameter tuning is a computationally expensive procedure, which lasts many days for each trial. It should be added that this is an ad-hoc methodology which does not guarantee to work on other tasks, or in other databases. In order to balance the performance on many tasks, <ref type="bibr" target="#b19">[20]</ref> proposed an iterative method which uses the training loss of each task to indicate whether it is well trained or not, and then decreases the relative weights of the well trained tasks. The problem with this approach is that it is based on costly evaluation of performance indicators during each training iteration and that this is performed in a rather task-agnostic way.</p><p>Negative transfer may be induced by conflicting gradients among the different tasks <ref type="bibr" target="#b55">[56]</ref>. Searching for Pareto solutions <ref type="bibr" target="#b56">[57]</ref> could remedy this. <ref type="bibr" target="#b56">[57]</ref> tackled multi-task learning problems through multi-objective optimization, with decomposition of the problem into a set of constrained sub-problems with different trade-off preferences (among different tasks). However, this approach is rather complex, providing a finite set of solutions that do not always satisfy the MTL requirements and finally need to perform trade-offs among tasks.</p><p>Through the proposed coupling loss, knowledge of the task  <ref type="bibr" target="#b8">[9]</ref> 0.57 relationship was infused in network training, thus providing it, in a simple manner, with higher level representation of the relationship between the tasks; it was not based on performance indicators and it did not perform any trade-offs between the different tasks. The fact that the proposed coupling losses tackled the negative transfer is illustrated by the performance shown in <ref type="table" target="#tab_7">Tables 5 and 3,</ref> where FaceBehaviorNet trained with the proposed coupling losses outperformed by a large margin both the independently trained single task networks and the multi-task network trained without any coupling loss.</p><formula xml:id="formula_15">0.43 - - - - - - - - (2 ? ) AlexNet [11] - - 0.60 0.34 0.58 - - - - - VGG-FACE [48] - - 0.61 0.48 - - - - - - RAN-ResNet18 + [49] - - - - 0.60 - - - - - VGG-FACE-mSVM [34] - - - - - 0.58 - - - - DLP-CNN [34] - - - - - 0.74 - - - - AlexNet [36] - - - - - - 0.61 - - - ResNet-34 [50] - - - - - - 0.73 - - - JAA-Net [51] - - - - - - - 0.56 - - LP-Net [52] - - - - - - - 0.57 - - DLE extension [53] - - - - - - - - 0.51 - VGG-FACE [54] - - - - - - - - - 0.48 ARL [55] - - - - - - - - - 0.51 (3 ?) ST-</formula><p>Coupled MTL vs Non-coupled MTL, ST &amp; Sota: In <ref type="table" target="#tab_7">Table 5</ref>, it can be observed that FaceBehaviorNet, when trained with the two coupling losses (in either task relatedness setting), outperformed FaceBehaviorNet trained without any coupling loss by: 9% (average CCC) on Aff-Wild; 8.5% (average CCC) and 6% (accuracy) on AffectNet; 11% on RAF-DB; 5% on EmotioNet; 8% on DISFA; 7% on BP4D and 6% on BP4D+. It further outperformed by a large margin the three ST-FaceBehaviorNet networks (10.3% on average across all databases, with the minimum difference in performance being 5% and the maximum 22%). Finally, in <ref type="table" target="#tab_7">Table 5</ref>, it can be observed that FaceBehaviorNet, when trained with the two coupling losses (in either task relatedness setting), outperformed by a very large margin every state-of-the-art method in all databases in all tasks. More precisely, it outperformed: i. AffWildNet by 8% (average CCC) for VA estimation on Aff-Wild; this is despite the fact that AffWildNet is a CNN-RNN that exploited the fact that Aff-Wild is an audio-visual database and despite the fact that facial landmarks were provided as additional inputs to the network, thus improving its performance; it also outperformed the best performing CNN in Aff-Wild by 16% ii. VGG-FACE by 7.5% (average CCC) for VA estimation on AffectNet, although VGG-FACE was trained with the original images plus thousands of generated images by VA-StarGAN; it also outperformed the AlexNet baseline by 15%</p><p>iii. RAN-ResNet18 + by 5% for expression classification on Af-fectNet, despite the fact that the network was trained with a region based loss that encouraged a high attention weight for the most important regions in the input images; it also outperformed AlexNet (that used a weighted loss that heavily penalized the network for misclassifying examples from under-represented classes and penalized the network less for misclassifying examples from well-represented classes), by 7% iv. DLP-CNN by 4% for expression classification on RAF-DB, although this network was trained using a joint classical softmax loss -which forced different classes to stay apart -and a newly created loss -that pulled the locally neighboring faces of the same class together; it also outperformed VGG-FACE-mSVM, that used the standard cross entropy loss, by 20% v. ResNet-34 by 4% for AU detection on EmotioNet; it also outperformed AlexNet by 16% vi. both JAA-Net and LP-Net by 6% and 5%, respectively, for AU detection on DISFA, despite the fact that these networks have additionally used facial landmarks as additional inputs, thus improving their performance vii. DLE extension by 34% for AU detection on BP4D viii. ARL by 11% for AU detection on BP4D+; it also outperformed VGG-FACE by 14%</p><p>At this point let us mention that in our approach the loss functions that we utilized (for expression classification and for AU detection) were standard losses (binary and softmax cross entropy, respectively). As was shown above, in most state-ofthe-art approaches, more elaborate and advanced loss functions have been used. We could utilize more elaborate and advanced losses as well, but the focus of this work has been on the coupling between the tasks exploiting their relationship -inferred either empirically from external dataset annotations, or from a cognitive study; therefore we chose not to use such losses whose specific selection could affect the presented analysis and the obtained results. As a future work, we will incorporate more advanced losses in the multi-task learning setting, which will further enhance the achieved results.</p><p>It might be argued that the more data used for network training (i.e., the additional data coming from the multiple tasks that are being solved, even if they contain partial or non-overlapping annotations), the better network performance will be in all tasks. However, as was shown, the three studied tasks are heterogeneous and negative transfer can occur, or sub-optimal models can be produced for some, or even all tasks <ref type="bibr" target="#b57">[58]</ref>. As shown in <ref type="table" target="#tab_7">Table 5</ref>, FaceBehaviorNet, when trained with both proposed coupling losses, achieved a better performance on all databases than the independently trained single-task models. This illustrates that simultaneously training this end-to-end architecture with the heterogeneous databases and coupling the corresponding tasks and annotations, led to improved performance. The fact that the network additionally outperformed the state-of-the-art methods, in both task relatedness settings, verified the generality of the proposed losses; network performance was boosted, independently of the type of task relatedness that was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Results: Generalisation to Unseen Databases</head><p>At this point let us mention that each task and corresponding database contains ambiguous cases: i) there is generally discrepancy in the perception of the disgust, fear, sadness and (negative) surprise emotions across different people (of different ethnicity, race and age) and across databases; emotional displays and their perception are not universal, i.e., facial expressions are displayed and interpreted differently depending on the cultural background of subjects and annotators <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref> ; ii) the exact valence and arousal value for a particular affect is also not consistent among databases; iii) the AU annotation process is hard to do and error prone, creating incosistency among databases (e.g., regarding the dependencies among AUs, such as AU12 -lip corner puller-and AU15 -lip corner depressor-that cannot co-occur as their corresponding muscle groups -Zgomaticus major and Depressor anguli oris, respectively-are unlikely to be simultaneously activated).</p><p>Furthermore each database contains its own bias. The bias is either in terms of the race, ethnicity and ages of the subjects displayed in the database or the race, ethnicity, age and culture of the experts that performed the annotations (having their own bias in judging the depicted affect). When an affect recognition system is trained on one database, then the system inherently learns the bias towards facial displays present in the training data. Therefore, when the system is tested on another, new and unseen database (with other demographics and statistics), its performance is not as good. An example is the AffectNet database that contains images of European Americans and has been annotated mainly by European Americans. Another example is GFT database that mainly contains videos of white and black people.</p><p>The excellent generalization performance of FaceBehaviorNet -across the test sets-of the 7 databases whose training sets have been used for its training, is an indicator that the network and the proposed coupling losses tackled the aforementioned issues. By jointly training on all databases and by coupling the tasks, we overcame these limitations as shown in the extensive experimental study across the 7 databases. In order to further illustrate and validate that FaceBehaviorNet learned good and robust features, we show that it is capable of generalizing its knowledge and capabilities in other new and unseen affect recognition databases that have not been utilized during its training and contain different statistics and contexts. <ref type="table" target="#tab_9">Table 6</ref> compares the performance of FaceBehaviorNet when trained with both coupling losses with the performance of state-ofthe-art networks on two new databases. It is worth mentioning that these two databases, AFEW-VA and GFT, have not been utilized during FaceBehaviorNet's training and no fine-tuning was performed; FaceBehaviorNet was just tested on the new and unseen databases. It can be observed that FaceBehaviorNet outperformed both AffWildNet (that was further trained on AFEW-VA) and JAA-Net (that was trained on GFT) by 4% (on average) and 5%, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Results: Zero-Shot and Few-Shot Learning</head><p>In order to further show and validate that FaceBehaviorNet learned good features encapsulating all aspects of facial behavior, we conducted zero-and few-shot learning experiments for classifying compound expressions. Given that there exist only 2 datasets (EmotioNet and RAF-DB) annotated with compound expressions and that they do not contain a lot of samples (less than 5,000 each), at first, we used the predictions of FaceBehaviorNet together with the rules from <ref type="bibr" target="#b14">[15]</ref> to generate compound emotion predictions in a zero-shot learning manner, as was described in Section 3.1.4. Additionally, to demonstrate the superiority of FaceBehaviorNet, we used it as a pre-trained network in a few-shot learning experiment. We took advantage of the fact that our network has learned good features and used them as priors for fine-tuning the network to perform compound emotion classification.</p><p>RAF-DB database At first, we performed zero-shot experiments on the 11 compound categories of RAF-DB. We computed a candidate score, C s (y emo ), for each class y emo as shown in Section 3.1.4. <ref type="table" target="#tab_10">Table 7</ref> shows the results of this approach when we used the predictions of FaceBehaviorNet trained with and without the soft co-annotation and distr-matching losses. Best results have been obtained when the network was trained with the coupling losses. One can observe, that this approach outperformed by 15.1% the baseline VGG-FACE-mSVM <ref type="bibr" target="#b33">[34]</ref> that been trained on RAF-DB for compound emotion classification. It also outperformed by 2.1% the state-of-the-art and best performing network, DLP-CNN that has also been trained on RAF-DB for compound emotion classification; DLP-CNN used a loss function designed for this specific task. Next, we targeted few-shot learning. In particular, we finetuned FaceBehaviorNet (trained with and without the soft coannotation and distr-matching losses) on the small training set of RAF-DB. In <ref type="table" target="#tab_10">Table 7</ref>, it can be seen that the fine-tuned FaceBehaviorNet, trained with and without the coupling losses, outperformed by large margins, 23.7% and 10.7%, respectively, the baseline VGG-FACE-mSVM and the state-of-the-art and best performing DLP-CNN.</p><p>EmotioNet database Next, we performed zero-shot experiments on the EmotioNet basic and compound set that was released for the related Challenge. This set includes 6 basic plus 10 compound categories, as described at the beginning of this Section. Our zeroshot methodology was similar to the one described above for the RAF-DB database.</p><p>The results of this experiment are shown in <ref type="table" target="#tab_10">Table 7</ref>. Best results were also obtained when the network was trained with the two coupling losses. It can be observed that this approach outperformed by 5.7% and 8.6% in F1 score and Unweighted Average Recall (UAR), respectively, the state-of-the-art and winner of EmotioNet Challenge, NTechLab's <ref type="bibr" target="#b35">[36]</ref> approach, which used the Emotionet's images with compound annotation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study II: Face Recognition</head><p>At first, we trained a single-task FaceBehaviorNet (ST-FaceBehaviorNet) on CelebA to detect the 40 facial attributes; we also trained another similar network on CelebA to perform classification into the 10,177 different identities. The networks were based on residual units and had the same structure as FaceBehaviorNet; their only difference was the output layer. For brevity, these two single-task networks are denoted as '(2 ?) ST-FaceBehaviorNet' in one row of <ref type="table" target="#tab_11">Table 8</ref>. We compared these networks' performance with the performance of FaceBehaviorNet when trained with the distribution matching coupling loss and when trained without the coupling loss. <ref type="table" target="#tab_11">Table 8</ref> shows that the multi-task FaceBehaviorNet, when trained without the coupling loss, outperformed the 2 single-task networks ST-FaceBehaviorNet in both studied tasks and in both metrics (Total Accuracy and average F1 Score). In more detail, it displayed an improved performance by 2.28% and 2.1% in the Accuracy and F1 Score metrics, respectively, for identity classification; it also displayed an improved performance by 1.91% and 0.89% in the Accuracy and F1 Score metrics, respectively, for attribute detection. This shows that the two studied facial heterogeneous tasks were coherently correlated to each other; training the end-to-end multi-task architecture therefore, led to improved performance and no negative transfer occurred. <ref type="table" target="#tab_11">Table 8</ref> further shows that the multi-task FaceBehaviorNet, when trained with the distribution matching coupling loss, greatly outperformed its counterpart that was trained without that loss, in both studied tasks and in both metrics. More precisely, when training with the coupling loss, performance increased by 4.57% and 5.9% in the Accuracy and F1 Score metrics, respectively, for identity classification; performance also increased by 1.24% and 2.3% in the Accuracy and F1 Score metrics, respectively, for attribute detection. This proves the effectiveness of the proposed distribution matching loss.</p><p>Next, we utilized, as an ablation study, three state-of-theart and widely used networks, VGG-FACE, ResNet-50 and DenseNet-121. We trained these networks in a multi-task manner (without using the coupling loss) on CelebA and compared their performance to that of FaceBehaviorNet that was also trained without coupling loss (for a fair comparison). As shown in <ref type="table" target="#tab_11">Table  8</ref>, FaceBehaviorNet has proven to be the best architecture as it provided the best results, outperforming on all tasks and metrics the MT-VGG-FACE by at least 2%, the MT-ResNet by at least 1.8% and the MT-DenseNet by at least 4%.</p><p>Finally, let us mention that the achieved performance of FaceBehaviorNet trained with the distribution matching loss for attribute detection was higher than the performance of various state-of-the-art methodologies on the same database. FaceBehav-iorNet reached an accuracy of 93.22%, whereas DMTL <ref type="bibr" target="#b60">[61]</ref> achieved 92.6% accuracy, MCNN-AUX <ref type="bibr" target="#b61">[62]</ref> achieved an accuracy of 91.29% and Face-SSD <ref type="bibr" target="#b24">[25]</ref> achieved an accuracy of 90.29%. These comparisons are provided just as an indication; they are not direct comparisons. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we target heterogeneous MTL, i.e., simultaneously addressing detection, classification and regression problems. We propose co-training task in a weakly-supervised way, by exploring their relatedness. Relatedness between the heterogeneous tasks is either provided explicitly in a form of expert knowledge, or is inferred based on empirical studies. In co-training, the related tasks exchange their predictions and iteratively teach each other so that predictors of all tasks can excel even if we have limited or no data for some of them. We propose an effective distribution matching approach based on distillation, where knowledge exchange between tasks is enabled via distribution matching over their predictions. Based on this approach, we build the first holistic framework for large-scale face analysis, FaceBehaviorNet, with case studies in affective computing and in face recognition. In the first case study, FaceBehaviorNet is trained for joint basic expression recognition, action unit detection and valencearousal estimation. All publicly available databases that study facial behavior tasks in-the-wild, have been utilized. In the latter case study, FaceBehaviorNet is trained for joint facial attribute detection and face identification. An extensive experimental study -across 10 databases-is performed that compares the performance of the holistic (multi-task, multi-domain, multi-label) FaceBehav-iorNet (trained with and without taking into account the taskrelatedness) to the performance of the single-task networks, as well as to the performance of the state-of-the-art. FaceBehaviorNet consistently outperformed, by a large margin, all of them ,in all databases, in both case studies, even mitigating bias and tackling negative transfer. Finally, we explored the feature representation learned by FaceBehaviorNet in the joint training and showed its generalization abilities on the task of compound expressions, under zero-shot and few-shot learning settings. Viktoriia Sharmanska Viktoriia Sharmanska is currently a Lecturer in AI at the Department of Informatics, University of Sussex, and an honorary lecturer at Imperial College London, UK. During 2017-2020, she was a recipient of a prestigious Imperial College Research Fellowship at the Department of Informatics, working on deep learning methods for human behavior analysis. Dr Sharmanska has co-authored numerous papers published at CVPR, ICCV/ECCV, NeurIPS, on novel statistical machine learning methodologies applied to computer vision problems, such as attribute-based object recognition, learning using privileged information, cross-modal learning, and recently on human facial behavior analysis and algorithmic fairness methods. She has built an international reputation such as being among the youngest Area Chair for top-tier international conferences in computer vision and deep learning such as ICLR since 2019, and CVPR 2021. Dr Sharmanska has received a number of prestigious awards, such as the Imperial College Research Fellowship 2017, Outstanding Reviewer Award at CVPR 2019. Her current research interests include deep learning methods for human behavior understanding from facial and bodily cues, video data synthesis, and algorithmic fairness methods to mitigate machine bias in visual data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>17 i=1 17 k=1 ? k , 2 .</head><label>17172</label><figDesc>?i?[y i au log p(y i au |x)+(1?y i au )log (1?p(y i au |x))] In fact,<ref type="bibr" target="#b16">17</ref> is an aggregate of action units in all datasets; typically each dataset has from 10 to 12 AUs labelled by purposely trained annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>The holistic (multi-task, multi-domain, multi-label) FaceBehaviorNet in the affective computing case study; 'VA/AU/EXPR-BATCH' refers to batches annotated in terms of VA/AU/7 basic expressions work with these databases because they provide a large number of samples with accurate annotations of valence-arousal, basic expressions and AUs. Training with these datasets allows our networks to learn to recognize affective states under a large number of image conditions (e.g., each database includes images at different resolutions, poses, orientations and lighting conditions). These datasets also include a variety of samples in both genders, ethnicity, races and ages.The Aff-Wild database [9]<ref type="bibr" target="#b9">[10]</ref> has been the first large scale captured in-the-wild database, containing 298 videos (200 subjects) of around 1.25M frames, annotated in terms of valencearousal that range in [?1, 1]. It served as benchmark for the Aff-Wild Challenge organized in CVPR 2017. The training set consists of around 1M frames and the test set of around 216K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Dimitrios Kollias ,</head><label>Kollias</label><figDesc>Fellow of the Higher Education Academy, holder of a Post-Graduate Certificate and member of the IEEE, is currently a Senior Lecturer in Computer Science with the School of Computing and Mathematical Sciences, University of Greenwich. He has been the recipient of the prestigious Teaching Fellowship of Imperial College London. He has obtained the Ph.D. from the Department of Computing, Imperial College London, where he was a member of the iBUG group. Prior to this, he received the Diploma/M.Sc. in Electrical and Computer Engineering from the ECE School of the National Technical University of Athens, Greece, and the M.Sc. in Advanced Computing from the Department of Computing of Imperial College London. He has published his research in the top journals and conferences on machine learning, perception and computer vision such as IJCV, CVPR, ECCV, BMVC, IJCNN, ECAI and SSCI. He is a reviewer in top journals and conferences, such as CVPR, ECCV, ICCV, AAAI, TIP, TNNL, TAC, Neurocomputing, Pattern Recognition and Neural Networks. He is the Chair in two the Competitions and Workshops in ICCV 2021. He has been Competition Chair and Workshop Chair in IEEE FG 2020. He has won many grants and awards, such as from the City and Guilds College Association, the Imperial College Trust and the Complex &amp; Intelligent Systems Journal. He has h-index 18 and i10-index 19. His research interests span the areas of machine and deep learning, deep neural networks, computer vision, affective computing and medical imaging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1} indicates whether the image contains annotation for AU i . The third term measures the concordance correlation coefficient between the ground truth valence and arousal y va and the predicted? va , CCC(y va ,? va ) = ?a+?v 2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Evidences, Aff-Wild2</cell></row><row><cell>Emotion</cell><cell>Prototypical AUs</cell><cell>Observational AUs (with weights w)</cell><cell>AUs (with weights w)</cell></row><row><cell>happiness</cell><cell>12, 25</cell><cell>6 (0.51)</cell><cell>12 (0.82), 25 (0.7), 6 (0.57), 7 (0.83), 10 (0.63)</cell></row><row><cell>sadness</cell><cell>4, 15</cell><cell>1 (0.6), 6 (0.5), 11 (0.26), 17 (0.67)</cell><cell>4 (0.53), 15 (0.42), 1 (0.31), 7 (0.13), 17 (0.1)</cell></row><row><cell>fear</cell><cell>1, 4, 20, 25</cell><cell>2 (0.57), 5 (0.63), 26 (0.33)</cell><cell>1 (0.52), 4 (0.4), 25 (0.85), 5 (0.38), 7 (0.57), 10 (0.57)</cell></row><row><cell>anger</cell><cell>4, 7, 24</cell><cell>10 (0.26), 17 (0.52), 23 (0.29)</cell><cell>4 (0.65), 7 (0.45), 25 (0.4), 10 (0.33), 9 (0.15)</cell></row><row><cell>surprise</cell><cell>1, 2, 25, 26</cell><cell>5 (0.66)</cell><cell>1 (0.38), 2 (0.37), 25 (0.85), 26 (0.3), 5 (0.5), 7 (0.2)</cell></row><row><cell>disgust</cell><cell>9, 10, 17</cell><cell>4 (0.31), 24 (0.26)</cell><cell>9 (0.21), 10 (0.85), 17 (0.23), 4 (0.6), 7 (0.75), 25 (0.8)</cell></row><row><cell>where ? i ? {0,</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Examples of task relatedness between identities and facial attributes inferred from CelebA</figDesc><table><row><cell>Identities</cell><cell>5 o' Clock Shadow</cell><cell>Arched Eyebrows</cell><cell>Attractive</cell><cell>Bags Under Eyes</cell><cell>Bald</cell><cell>Bangs</cell><cell>Male</cell><cell>Wearing Lipstick</cell><cell>Wearing Necklace</cell><cell>Wearing Necktie</cell><cell>Young</cell></row><row><cell># 1</cell><cell>0.34483</cell><cell>0.03449</cell><cell>0.27586</cell><cell>0.4138</cell><cell>0.</cell><cell>0.</cell><cell>1.</cell><cell>0.</cell><cell>0.</cell><cell>0.</cell><cell>0.68966</cell></row><row><cell># 2</cell><cell>0.</cell><cell>0.125</cell><cell>0.875</cell><cell>0.125</cell><cell>0.</cell><cell>0.25</cell><cell>0.</cell><cell>0.5</cell><cell>0.125</cell><cell>0.</cell><cell>1.</cell></row><row><cell># 1000</cell><cell>0.</cell><cell>0.22727</cell><cell>1.</cell><cell>0.</cell><cell>0.</cell><cell>0.04546</cell><cell>0.</cell><cell>1.</cell><cell>0.</cell><cell>0.</cell><cell>1.</cell></row><row><cell># 5000</cell><cell>0.8</cell><cell>0.1</cell><cell>0.4</cell><cell>0.6</cell><cell>0.03333</cell><cell>0.06667</cell><cell>1.</cell><cell>0.</cell><cell>0.</cell><cell>0.1</cell><cell>0.5</cell></row><row><cell># 10000</cell><cell>0.</cell><cell>0.26667</cell><cell>1.</cell><cell>0.</cell><cell>0.</cell><cell>0.</cell><cell>0.</cell><cell>0.76667</cell><cell>0.1</cell><cell>0.</cell><cell>1.</cell></row><row><cell># 10177</cell><cell>0.</cell><cell>0.07692</cell><cell>0.69231</cell><cell>0.</cell><cell>0.</cell><cell>0.07692</cell><cell>0.</cell><cell>0.61539</cell><cell>0.15385</cell><cell>0.</cell><cell>1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">Performance comparison of different widely used</cell></row><row><cell cols="6">networks and FaceBehaviorNet, when used in the multi-task</cell></row><row><cell cols="3">framework (without coupling loss)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Databases</cell><cell></cell><cell>Face-BehaviorNet</cell><cell>MT-VGG-FACE</cell><cell>MT-ResNet</cell><cell>MT-DenseNet</cell></row><row><cell>Aff-Wild</cell><cell>CCC-V CCC-A</cell><cell>0.52 0.38</cell><cell>0.55 0.36</cell><cell>0.54 0.38</cell><cell>0.52 0.35</cell></row><row><cell></cell><cell>CCC-V</cell><cell>0.56</cell><cell>0.56</cell><cell>0.53</cell><cell>0.53</cell></row><row><cell>AffectNet</cell><cell>CCC-A</cell><cell>0.50</cell><cell>0.46</cell><cell>0.48</cell><cell>0.44</cell></row><row><cell></cell><cell>F1 Score</cell><cell>0.59</cell><cell>0.53</cell><cell>0.54</cell><cell>0.52</cell></row><row><cell></cell><cell>Mean</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RAF-DB</cell><cell>diag. of</cell><cell>0.67</cell><cell>0.67</cell><cell>0.67</cell><cell>0.64</cell></row><row><cell></cell><cell>conf. matrix</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Emotionet</cell><cell>AFA Score</cell><cell>0.72</cell><cell>0.72</cell><cell>0.71</cell><cell>0.69</cell></row><row><cell>DISFA</cell><cell>F1 score</cell><cell>0.54</cell><cell>0.52</cell><cell>0.52</cell><cell>0.49</cell></row><row><cell>BP4D</cell><cell>F1 score</cell><cell>0.76</cell><cell>0.70</cell><cell>0.73</cell><cell>0.68</cell></row><row><cell>BP4D+</cell><cell>F1 score</cell><cell>0.56</cell><cell>0.57</cell><cell>0.56</cell><cell>0.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Performance evaluation of valence-arousal, seven basic expression and action units predictions on all utilized databases provided by the FaceBehaviorNet and state-of-the-art methods; 'AFA Score' is the average between the F1 Score and the Accuracy; in parenthesis is the result in Aff-Wild without post-processing</figDesc><table><row><cell>Databases</cell><cell cols="2">Aff-Wild</cell><cell></cell><cell>AffectNet</cell><cell></cell><cell>RAF-DB</cell><cell>EmotioNet</cell><cell>DISFA</cell><cell>BP4D</cell><cell>BP4D+</cell></row><row><cell></cell><cell>CCC-V</cell><cell>CCC-A</cell><cell>CCC-V</cell><cell>CCC-A</cell><cell>Accuracy</cell><cell>Mean diagonal of conf. matrix</cell><cell>AFA Score</cell><cell>F1 Score</cell><cell>F1 Score</cell><cell>F1 Score</cell></row><row><cell>best performing CNN [47] [9]</cell><cell>0.51</cell><cell>0.33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AffWildNet [47]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>Performance comparison of FaceBehaviorNet and state-of-the-art methods on two databases that were not utilized for its training; in parenthesis is the result without post-processing</figDesc><table><row><cell>Databases</cell><cell cols="2">AFEW-VA</cell><cell>GFT</cell></row><row><cell>Methods</cell><cell>CCC-V</cell><cell>CCC-A</cell><cell>F1 Score</cell></row><row><cell>JAA-Net [51]</cell><cell>-</cell><cell>-</cell><cell>0.55</cell></row><row><cell>fine-tuned AffWildNet [47] [9]</cell><cell>0.52</cell><cell>0.56</cell><cell>-</cell></row><row><cell>FaceBehaviorNet both coupling losses</cell><cell>0.57 (0.55)</cell><cell>0.59 (0.57)</cell><cell>0.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 :</head><label>7</label><figDesc>Performance evaluation of generated compound emotion predictions; 'UAR' denotes Unweighted Average Recall</figDesc><table><row><cell>Databases</cell><cell cols="2">EmotioNet</cell><cell>RAF-DB</cell></row><row><cell>Methods</cell><cell>F1 Score</cell><cell>UAR</cell><cell>Mean diag. of conf. matrix</cell></row><row><cell>zero-shot, FaceBehaviorNet, no coupling loss</cell><cell>0.243</cell><cell>0.260</cell><cell>0.382</cell></row><row><cell>zero-shot, FaceBehaviorNet, both coupling losses</cell><cell>0.312</cell><cell>0.329</cell><cell>0.467</cell></row><row><cell>NTechLab [36]</cell><cell>0.255</cell><cell>0.243</cell><cell>-</cell></row><row><cell>VGG-FACE-mSVM [34]</cell><cell>-</cell><cell>-</cell><cell>0.316</cell></row><row><cell>DLP-CNN [34]</cell><cell>-</cell><cell>-</cell><cell>0.446</cell></row><row><cell>fine-tuned FaceBehaviorNet, no coupling loss</cell><cell>-</cell><cell>-</cell><cell>0.468</cell></row><row><cell>fine-tuned FaceBehaviorNet, both coupling losses</cell><cell>-</cell><cell>-</cell><cell>0.553</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8 :</head><label>8</label><figDesc>Performance evaluation on CelebA; 'Acc' denotes the total accuracy</figDesc><table><row><cell>CelebA</cell><cell>Ids</cell><cell></cell><cell cols="2">Attributes</cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>MT-VGG-FACE</cell><cell>80.07</cell><cell>72.2</cell><cell>89.87</cell><cell>70.24</cell></row><row><cell>MT-ResNet</cell><cell>80.83</cell><cell>72.9</cell><cell>90.11</cell><cell>71.38</cell></row><row><cell>MT-DenseNet</cell><cell>78.11</cell><cell>70.02</cell><cell>87.57</cell><cell>67.88</cell></row><row><cell>(2 ?) ST-FaceBehaviorNet</cell><cell>80.43</cell><cell>73.5</cell><cell>90.07</cell><cell>71.78</cell></row><row><cell>FaceBehaviorNet, no coupling loss</cell><cell>82.71</cell><cell>75.6</cell><cell>91.98</cell><cell>72.67</cell></row><row><cell>FaceBehaviorNet, distr-matching</cell><cell>87.28</cell><cell>81.5</cell><cell>93.22</cell><cell>74.97</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Holistic 3d scene understanding from a single geo-tagged image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3964" to="3972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6129" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Oxford University Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep affect prediction in-thewild: Aff-wild database and challenge, deep architectures, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aff-wild: Valence and arousal &apos;in-the-wild&apos;challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03985</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR&apos;16)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR&apos;16)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-dataset learning and person-specific normalisation for automatic action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04855</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Do deep neural networks learn facial action units when doing expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emotion categories and dimensions in the facial communication of affect: An integrated approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">798</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Characterizing and avoiding negative transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Loss-balanced task weighting to reduce negative transfer in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From emotions to action units with hidden and semi-hidden-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Binefa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3703" to="3711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple facial action unit recognition enhanced by facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4089" to="4094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Expression-assisted facial action unit recognition under incomplete au annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="91" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Registration-free face-ssd: Single shot analysis of smiles, facial attributes, and affect in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multitask deep neural network for joint face recognition and facial attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2017 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge augmented deep neural networks for joint facial expression and action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multitask emotion recognition with incomplete labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="828" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analysing affective behavior in the first abaw 2020 competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schulc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hajiyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="794" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream aural-visual affect analysis in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kuhnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rumberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Two-level attention with two-stage multi-task learning for facial emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12139</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sayette group formation task (gft) spontaneous facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="581" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Afew-va database for valence and arousal estimation in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Emotionet challenge: Recognition of facial expressions of emotion in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01210</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Affective Computing</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fera 2015-second facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fera 2017-addressing head pose in the third facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?nchez-Lozano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SSH: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recognition of affect in the wild using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Va-stargan: Continuous affect generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Region attention networks for pose and occlusion robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Facial action recognition using very deep networks for highly imbalanced class distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep adaptive attention for joint facial action unit detection and face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Local relationship learning with person-specific shape regularization for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Discriminant multi-label manifold embedding for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Y?ce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">View-independent facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facial action unit detection using attention and relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on affective computing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pareto multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-L</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-third Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding and improving information transfer in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Universality reconsidered: Diversity in making meaning of facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gendron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Crivelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current directions in psychological science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A comparative analysis of emotion-detecting ai systems with respect to algorithm performance and dataset diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI</title>
		<meeting>the 2019 AAAI/ACM Conference on AI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="377" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Heterogeneous face attribute estimation: A deep multi-task learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Attributes for improved attributes: A multi-task network for attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07360</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
