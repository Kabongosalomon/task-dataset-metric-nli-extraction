<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ODFNet: Using orientation distribution functions to characterize 3D point clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022">2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><forename type="middle">H</forename><surname>Sahin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Istanbul Technical University</orgName>
								<address>
									<addrLine>Computer Engineering</addrLine>
									<postCode>34469</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alican</forename><surname>Mertan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Istanbul Technical University</orgName>
								<address>
									<addrLine>Computer Engineering</addrLine>
									<postCode>34469</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gozde</forename><surname>Unal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Istanbul Technical University</orgName>
								<address>
									<addrLine>Computer Engineering</addrLine>
									<postCode>34469</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ODFNet: Using orientation distribution functions to characterize 3D point clouds</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Computers &amp; Graphics</title>
						<imprint>
							<date type="published" when="2022">2022</date>
						</imprint>
					</monogr>
					<note>Contents lists available at ScienceDirect Computers &amp; Graphics journal homepage: www.elsevier.com/locate/cag</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Learning new representations of 3D point clouds is an active research area in 3D vision, as the order-invariant point cloud structure still presents challenges for the design of neural network architectures. Recent work explored learning global, local, or multiscale features for point clouds. However, none of the earlier methods focused on capturing contextual shape information by analyzing local orientation distributions of points. In this paper, we use point orientation distributions around a point in order to obtain an expressive local neighborhood representation for point clouds. We achieve this by dividing the spherical neighborhood of a given point into predefined cone volumes, and statistics inside each volume are used as point features. In this way, a local patch can be represented not only by the selected point's nearest neighbors, but also by considering a point density distribution defined along multiple orientations around the point. We are then able to construct an orientation distribution function (ODF) neural network that makes use of an ODFBlock which relies on MLP (multi-layer perceptron) layers. The new ODFNet model achieves state-of-the-art accuracy for object classification on ModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet and S3DIS datasets. <ref type="table">(Gozde Unal)</ref> constructing a global feature transformation on all the points in the point cloud while respecting their order invariance. While PointNet did not use any neighborhood information, it is argued and shown that using local features improves the performance in recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Recent approaches to extracting local features from point clouds include creating spherical volumes <ref type="bibr" target="#b2">[3]</ref> or choosing knearest neighbors [7] and collecting local information of each point in those defined neighborhoods. The latest studies Dense-Point [8] and ShellNet <ref type="bibr" target="#b8">[9]</ref>, that obtained state-of-the-art results for classification on the ModelNet40 classification benchmark <ref type="bibr" target="#b5">[6]</ref>, create spherical regions around each point. DensePoint employs spheres of different sizes in each layer of the neural network to obtain features from multiple scales; whereas in Shell-Net, coordinates of points in each shell are transformed via an MLP (multi-layer perceptron), and a max-pooling operation aggregates the features across shells. In both methods, the points in the sphere are handled in such a way as to ignore their orientations with respect to the selected point. In [10], a spherical arXiv:2012.04708v2 [cs.CV] 15 Jul 2022</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) are widely used in vision and pattern recognition problems like object classification, object recognition, and segmentation <ref type="bibr" target="#b0">[1]</ref>. However, CNNs have not been applicable to point clouds until recent years. The main obstacle to this was the problem of how to interpret a point in a point cloud representation, which has a permutation-invariant structure, a property not possessed by pixels or voxels in 2D or 3D images. On a 2D or 3D image grid, the convolution operation is defined as a weighted sum in a local neighborhood, which is defined by the kernel. However, in a point cloud, a similar neighborhood structure among the points does not exist. PointNet <ref type="bibr" target="#b1">[2]</ref> pioneered the way to utilizing neural network models for the point cloud classification problem by aiming at <ref type="figure">Fig. 1</ref>. A plane object from the ModelNet40 <ref type="bibr" target="#b5">[6]</ref> dataset. For each point in a point cloud, ODFNet calculates the distribution of nearby points inside a spherical region. To do this, it utilizes cones with predefined orientations that spanning the spherical sectors. These cones can be of different scales and radii.</p><p>convolution is presented alongside octree partitioning where the sphere is divided into bins, and bin features are obtained by averaging the features of the points inside the bin, without using any point density information.</p><p>In order to increase the representative power of local features, we employ the distribution of orientations of points in a neighborhood with respect to a reference point. This leads to a new representation named point Orientation Distribution Functions (ODFs) for point clouds. ODFs can be computed by dividing each sphere around a point into a set of cones along predefined orientations, and calculating the density of points in each cone, as depicted in <ref type="figure">Figure 1</ref>. To increase the representative power of the feature, overlapping cones are also used. Some example ODFs are given in <ref type="figure" target="#fig_0">Figure 2</ref>. It can be observed that the ODF at the tip of the gun object, the ODFs on the corners of the plane or the table, and on the surface of the car, clearly capture the relative orientation of points with respect to the given center point. As such, we utilize the ODFs with their enhanced capability to compactly summarize the local neighborhood structure of a point cloud to our advantage in our point cloud analysis network model design.</p><p>The main contributions of our work can be summarized as such:</p><p>? The point ODFs, which incorporate the directional information inside a spherical neighborhood, are defined.</p><p>? A dedicated neural network architecture, the ODFNet, for classification and segmentation of point clouds, is presented.</p><p>? For different rotation-invariance scenarios, two different ways of utilizing ODFs for a point are presented. Both representations, which are either fully rotation-invariant or only rotation-invariant in the x-y plane, can be used depending on the alignment conditions of the environment.</p><p>? The ODFNet architecture is tested on popular benchmarks, and state-of-the-art (SoTA) accuracy scores on Model- Net40, Shapenet <ref type="bibr" target="#b10">[11]</ref>, S3DIS, <ref type="bibr" target="#b11">[12]</ref> and ScanObjectNN <ref type="bibr" target="#b12">[13]</ref> are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Earlier studies like <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref> focusing on the classification of 3D objects prefer to voxelize the objects and use the voxelized occupancy map as an input to a neural network. However, this approach is not efficient for two reasons: First, the voxelization quality is highly related to the selected grid spacing and, as grid spacing dimensions get lower, distortion of a voxelized 3D object increases. Although high-resolution voxel grids are desirable, they are impractical due to computational constraints. Considering that the input to a voxel grid network is a 3D matrix, the network will consume significantly more storage and computation power when compared to 2D networks. A second drawback is that this is a very sparse representation, hence a superfluous amount of data is unnecessarily processed. <ref type="bibr" target="#b16">[17]</ref> constructed a new representation to decrease the data amount by processing voxels, but the deformation problem remains.</p><p>Another approach is obtaining 2D views or depth maps and using them as inputs to a neural network <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Feng et al. <ref type="bibr" target="#b21">[22]</ref> construct hypergraphs extracted from view-based networks. However, view-based approaches are not deemed favorable as well since complete object or scene information is not utilized.</p><p>In order to make use of the standard grid convolution operation from the Euclidean CNN domain, Hua et al. project a grid onto every point where filter kernels are placed, and features are calculated over those grids via the convolution operation <ref type="bibr" target="#b22">[23]</ref>. Li et al. <ref type="bibr" target="#b23">[24]</ref> presented an architecture that learns a transformation matrix that weighs and permutes the points to be used in grid convolution.</p><p>PointNet <ref type="bibr" target="#b1">[2]</ref> is considered as the first attempt to use point clouds as raw inputs to a neural network. As it might be expected, the main difficulty of using directly the points instead of view renders or voxelized 3D maps comes from the set representation since all permutations on a point set describes the same entry. Hence in PointNet, to classify a point cloud, all points are processed in multi-layer perceptrons in parallel (i.e. as shared weights for all points) to obtain point features and a symmetric function (e.g. a max-pooling operation) is used over these features to obtain an aggregated global feature. Although studies like <ref type="bibr" target="#b22">[23]</ref> showed that ordered points can be used without a symmetric function, the symmetric function notion is widely used <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>. Other studies using PointNet as a backbone network or in the middle steps include <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>PointNet++ <ref type="bibr" target="#b2">[3]</ref> focuses on the fact that the original PointNet loses local features since all points are treated independently until the max-pool step. Thus, they hierarchically sample and group the point cloud and implement mini PointNets for each group. In <ref type="bibr" target="#b6">[7]</ref>, DGCNN, which handles the point cloud as a graph and uses a k-nearest neighbor approach to construct the connections, is introduced. Then, an edge convolution operation to perform a convolution centered on the selected point according to its nearest neighbors is defined. In SpiderCNN <ref type="bibr" target="#b4">[5]</ref>, a new convolution operation that benefits from Taylor series expansion is presented. In KPConv <ref type="bibr" target="#b26">[27]</ref>, kernel points with learnable weights are defined inside a local neighborhood and a linear correlation between a kernel point position and a neighbor point position is calculated and multiplied by these weights. In DensePoint <ref type="bibr" target="#b7">[8]</ref> and ShellNet <ref type="bibr" target="#b8">[9]</ref>, a dedicated convolution is defined relying on statistics inside local neighborhoods, particularly spherical regions. Lei et al. <ref type="bibr" target="#b9">[10]</ref> design a procedure where spherical regions are divided into bins and point features for each bin are collected according to bin weights. Then mean of the point features of each bin is used as bin features. In Con-vPoint <ref type="bibr" target="#b27">[28]</ref>, convolution operation differentiates for spatial and feature operations where the spatial operations are done on randomly selected parts. For further reading on point clouds, a detailed survey on this topic can be investigated <ref type="bibr" target="#b28">[29]</ref>.</p><p>The works we examine so far are not invariant to rotation changes. Recently, developing rotation-invariant point features started to attract more attention in the point cloud community. In <ref type="bibr" target="#b29">[30]</ref>, the points are mapped on an icosahedral lattice and a new convolution operation to perform on this structure is defined. In <ref type="bibr" target="#b30">[31]</ref>, a rotation-invariant convolution, the RIConv operator is presented to obtain features from distances and angles in a rotation-invariant manner. Kim et al. presented RI-GCN which uses graph convolutions hierarchically <ref type="bibr" target="#b31">[32]</ref>. In CG-Conv <ref type="bibr" target="#b32">[33]</ref>, local reference frames are created for each point neighborhood to obtain the local features and the global features are calculated via anchors. In the point cloud processing literature, the lack of any orientation-specific local feature representation motivated us to propose the ODFNet in this work. The inspiration for ODFs comes from the orientational probability distribution functions in the Diffusion MRI field <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> that characterize the water diffusion in the brain. Those ODFs model the heterogeneous local tissue micro-structure in order to extract underlying multiple axonal fiber populations. For point clouds, the indirect analogy relies on constructing orientation distributions of the local point cloud mass that can reveal and help resolve the local geometry of the 3D shape along several directions. This is the main motivation in proposing ODFs for characterizing local structure in point clouds. Theoretically, for ODF estimation, one could use mathematical techniques such as spherical harmonics decomposition <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, or for instance fitting von Mises-Fisher distributions <ref type="bibr" target="#b37">[38]</ref>, in which the latter involves constructing a multivariate normal distribution on the sphere. Alternatively, one could take a purely discrete approach to estimate an ODF through a histogram computation, by binning the local sphere around a given point into conic volumetric sections, as we perform in this work. We present numerical evidence that shows adopting ODFs in point clouds provides mostly competitive and in some cases superior performance among existing point cloud representations in problems of classification and segmentation of point clouds. This supports our conjecture that the inclusion of directional statistics of the local point cloud density leads to an improved localized structural representation and hence provides a performance upgrade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our approach relies upon Orientation Distribution Functions due to their capability to express directional properties of local point cloud structure. In this section, we first describe the ODFs, the dedicated ODFBlock which is depicted in <ref type="figure" target="#fig_1">Figure 3</ref>, and then we present the ODFNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ODF Representation for Point Clouds</head><p>ODFs that we propose in this work rely on the number of points in a local conic neighborhood at multiple orientations. Furthermore, to capture the local point density information in a hierarchy of scales, we utilize multi-scale cones with different apex angles and heights. There are three important components in defining ODFs; namely defining cones, their alignment given a point, and calculation of ODF values.</p><p>Defining cones: We divide the local sphere around a given point into conical volumes. The motivation for the usage of cones to parcellate the sphere comes from ODF-based methods for the medical imaging field <ref type="bibr" target="#b38">[39]</ref>. Although there are some studies dividing the sphere using cylinders <ref type="bibr" target="#b34">[35]</ref>, a more efficient parcellation in terms of conical volumes is preferred in this work. Each of these cones can be characterized by their direction v l , apex angle ? k , and length d n as illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. We define 42 conic neighborhood directions v l for each point, where directions are obtained after the first tessellation of an icosahedron. This is selected empirically by observing that further increasing the amount of tessellation causes a high rate of intersection between the cones and reducing it decreases the representation power. For each neighborhood direction v l , we utilize multiple cones to capture features from a hierarchy of neighborhoods. To this end, we use 2 different apex angles ? k , 31.71 degrees, which is the smallest angle that covers the whole sphere, and 60 degrees creating some intersection between the cones; and 4 different distances d n , where d n is the distance of the n th neighbor of the given point, and n is selected from the collection of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>. Thus, for each point, 336 different cones are obtained (42 cone directions v l , 4 distances d n , and 2 apex angles ? k ).</p><p>Aligning neighborhoods for a given point: In order to make the proposed ODF representation more adaptive to orientation changes of the objects, in the calculation of the ODFs, pivot directions need to be selected. Aligning cone directions v l according to pivot directions calculated for each point as in <ref type="figure" target="#fig_3">Figure 5</ref> effectively makes the ODF representation robust to orientation changes of the objects.</p><p>Particularly, two orthogonal directions have to be specified to achieve rotation-invariant ODF representations. We devise two methods for selecting these directions, namely RI-XY and RI-XYZ. RI-XY achieves rotation-invariance in the x-y plane. Thus, it can be used when the object is aligned according to the z-axis. For RI-XY, for every point, projections of the selected point's 32 nearest neighbors on the x-y plane are calculated and the densest direction is selected as the pivot. Using the pivot direction and the z-axis, the ODF directions are aligned. In <ref type="figure" target="#fig_4">Figure  7</ref>, some pivot directions are shown for the RI-XY method. In addition to this alignment's contribution to obtain a rotationinvariant representation for rotations in the x-y plane ( <ref type="figure" target="#fig_4">Figure  7</ref>.a), it also leads to symmetric representations for symmetric points <ref type="figure" target="#fig_4">(Figure 7</ref>.b). Unless otherwise stated, the RI-XY method is used for the experiments in this paper.</p><p>RI-XYZ allows us to define fully rotation-invariant representations. For RI-XYZ, the direction from the selected point x to the object center c ob ject is chosen as the first pivot direction. The second pivot direction is the cross product of the first pivot and the vector from point x to the center of the 32 nearest neighbors c local as depicted in <ref type="figure">Fig 6.</ref> This pivot selection technique can be applied when no information about the object's alignment is available. Thus, RI-XYZ could be used for scenarios with totally unaligned objects. However a decrease in performance is expected since the relative coordinates of the points are vastly changed.</p><p>Calculation of ODF values: To compute the ODF value at a point x i ? S , where S denotes the set of all points, and for a specific cone with an apex angle 2? k , height d n , and center direction vector v l ,</p><formula xml:id="formula_0">ODF(x i , ? k , d n , v l ) = x j ?S ,i j 1(||x i ? x j || 2 &lt; d n ) ? 1(acos (x i ? x j )? v l ||x i ? x j ||||v l || &lt; ? k ) (1)</formula><p>which gives us the point count inside the selected cone. Here, 1(?) refers to the indicator function. Since the cone heights d n are selected according to the n th -neighbor distance, this value is then normalized by n. In our experiments on a single NVIDIA Titan RTX graphics card, calculation of the cone values takes under ? 76 msecs for a point cloud of 1024 points despite its complex information. It is also advantageous that the representation is calculated only once in the network.</p><p>x c local c object <ref type="figure">Fig. 6</ref>. An example of rotation-invariant pivot calculation in RI-XYZ. The blue arrow shows the direction to the object center which is the first pivot, the red arrow shows the direction to the center of the neighbors, and the green arrow is the second pivot direction which is the cross product of the other two vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ODFBlock</head><p>In a point cloud, those differently sized and oriented cones that we construct help capture local variations in terms of point density, and encode them into the ODF features, which is provided into the dedicated neural network block in <ref type="figure" target="#fig_1">Figure 3</ref>. After calculating the ODFs at each point, we define the ODFBlock which operates on the ODFs as follows:</p><formula xml:id="formula_1">ODFBlock(x i , ? d , ? g ) = ODFGlob ? d , ODFDir ? g , ODF(x i )</formula><p>(2) where ODF(x i ) in short denotes a tensor of point density values ODF(x i , ? k , d n , v l ) for different cones along a collection of direction vectors in S 2 . Parameters ? d = [? 1 d , ? 2 d , ..., ? m d ], and ? g = [? 1 g , ? 2 g , ..., ? p g ] are learnable parameters of the ODFBlock. ODFDir is an mlp that embeds the ODF tensor through aggregating features by collecting features of each direction, and ODFGlob is another mlp which aggregates the output embedding over all directions to obtain the aggregate ODFBlock output features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ODFNet Network Models 3.3.1. ODFNet</head><p>To exploit our point ODF's representation capability over point clouds, we design different ODFNets, which benefit from ODFs as well as point locations to capture both local and global features of point clouds and, which can be used for classification and segmentation tasks. As depicted in <ref type="figure" target="#fig_5">Figure 8</ref>, ODFNet first calculates the ODFs for each point as given by <ref type="bibr">Equation 1</ref> for N c cones rotated around N d direction vectors. Then for each point, it benefits from two different mlps inside ODFBlock: ODFDir and ODFGlob. For differently scaled cones placed along the same direction, ODFDir calculates features capturing the point density along that direction. ODFGlob then operates on the features captured by ODFDir in order to fuse those features and the result is later concatenated with the point coordinates.</p><p>In the blocks shown as ODFBlock in <ref type="figure" target="#fig_5">Figure 8</ref>, in a similar manner with DGCNNs <ref type="bibr" target="#b6">[7]</ref>, each point's features are combined with its nearest neighbors' features and the difference vector features, where the latter represents x ? x i for x indicating a point location and x i indicating one of its 32 neighbors.</p><p>For the classification task, the last layers of the architecture include a max-pooling operation to produce a global feature vector describing the shape. After three fully connected layers, class scores are obtained at the output. For part segmentation in ShapeNet, the ODFNet architecture makes use of the categorical vector, which indicates the one-hot-coded object class. It is fed to the segmentation part of the ODFNet, as in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b1">[2]</ref>. The output size of the final output layer in both tasks depends on the number of object classes and object parts. For semantic segmentation in S3DIS, nearly the same architecture with part classification is used. However, since the dataset also includes RGB color information, colors and color differences are also used to obtain difference features and location features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">ODFNet-xyz</head><p>To ensure the global invariance, we used RI-XYZ pivot selection method and slightly changed the architecture as shown in <ref type="figure" target="#fig_5">Figure 8</ref>.e to obtain ODFNet-xyz, where instead of difference vectors we used a vector set of magnitude of difference vectors |x ? x i |, point's distances to object center |x ? c ob ject |, point neighbors' distances to object center |x i ?c ob ject |, angles between points and their neighbors ?(x, x i ), and angles between points and object center ?(x, c ob ject ). Also ?(x, c ob ject ) and |x ? c ob ject | are concatenated to the features from the last block of the network.  </p><formula xml:id="formula_2">(a) (b) (c) (d) (e) Mlp-block{ 0 ? } (<label>, 64) ODF</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we present the details of the experiments and performance evaluation results for ODFNet on classification and segmentation tasks 1 . We select widely used benchmark datasets and evaluate the ODFNet model over those, in order to provide a comparison with the existing methods. Particularly, we experiment and report results on ModelNet40 and ScanOb-jectNN for classification, ShapeNet for part segmentation, and S3DIS for scene segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Shape Classification</head><p>For shape classification, we evaluated our model on Mod-elNet40 <ref type="bibr" target="#b5">[6]</ref> which consists of mesh models for 40 different categories, and ScanObjectNN <ref type="bibr" target="#b12">[13]</ref> which contains 3D reallife scans for 13 different object categories which also contains noise and missing parts.</p><p>ModelNet: To have a fair comparison, we use the preprocessed data from <ref type="bibr" target="#b1">[2]</ref> and use 1024 points for each object. Following the previous studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2]</ref>, the objects are fit into a unit sphere. Recent works prefer to do scaling and translation <ref type="bibr" target="#b7">[8]</ref>, scaling and perturbing <ref type="bibr" target="#b6">[7]</ref>, and only perturbing <ref type="bibr" target="#b8">[9]</ref> for data augmentation during training. Instead, nonuniform scaling, flip- <ref type="bibr" target="#b0">1</ref> The source code for our ODFNet model will be provided at the time of publication.</p><p>ping in x and y directions, and rotation by multiples of 90 degrees are applied. Also, random sampling is used as in <ref type="bibr" target="#b23">[24]</ref> by deleting half of the points before the last classification block before max-pooling. Since the classification features are obtained by a max-pooling layer, the deletion operation does not affect the network structure. The ODFs are calculated regardless of this operation to avoid shape inconsistencies. To make a fair comparison with the previous work, we compared our study with the methods that use only 1024 points for each object. The results in <ref type="table" target="#tab_1">Table 1</ref> show that our implementation outperforms other methods by an overall accuracy score of 93.4% via a single prediction. Also, by applying a voting mechanism with random scaling and averaging the predictions, we obtained stateof-the-art results among the studies that follow a similar voting procedure.</p><p>Using the RI-XYZ pivot to calculate the ODFs, and ODFNetxyz network architecture, we also evaluated our ODF features in three different scenarios focusing on rotation invariance: train and test with z rotations (z/z), train and test with SO3 rotations (SO3/SO3), train with z rotations and test with SO3 rotations (z/SO3).</p><p>In <ref type="table">Table 2</ref>, we examined the results for these experiments in three groups of works (separated by horizontal lines in the table). The first group of networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44]</ref> which indicates the rotation-variant networks, for the z/z scenario, generally achieves scores that are comparable to their scores for the default setup. However, they lack robustness and their performance drops drastically for SO3/SO3 and z/SO3 scenarios. For the second group of approximately rotationinvariant networks, small standard deviations of accuracy are obtained for different scenarios. The third group, which also includes the ODFNet-xyz, is fully rotation-invariant as verified by the zero standard deviation values. The performances of rotation-invariant methods are consistent across the three scenarios. According to the scores, the ODFNet achieves the second-best results for the more challenging SO3/SO3 and z/SO3 scenarios among totally rotation-invariant methods. Despite ODFNet's success for the z/z scenario and the original scenario <ref type="table" target="#tab_1">(Table 1)</ref>, its accuracy is drastically decreased for the z/SO3 scenario. It is a natural result of depending on the object's alignment to the XY-plane for both the training procedure and the architecture.</p><p>ScanObjectNN: We use the original dataset that has 2048 points for each object to have a fair comparison. To evaluate the ODFNet on ScanObjectNN, an augmentation procedure similar to the ModelNet experiments is used. However, because the object point counts are larger here, 1024 points are randomly selected at train time. There are five different tasks: OBJ ONLY, OBJ BG, PB T25, PB T25R, PB T50R, and PB T50RS. In OBJ BG, objects with background noise are classified. OBJ ONLY consists of objects having no background noise. The other sets are augmented versions of OBJ BG. Comparing our results with the scores given in <ref type="bibr" target="#b12">[13]</ref>, the ODFNet model produces SoTA accuracy scores as can be observed in <ref type="table">Table 3</ref>.</p><p>Further Experiments: To further investigate our network's capacity for point cloud object abstraction, for each test subject, we extract the output of the last classification layer for the ODFNet, as well as for DensePoint and ShellNet, where the latter two are the previous SoTA point cloud architectures. Then, using two widely utilized dimensionality reduction tech- <ref type="table">Table 2</ref>. Comparisons of the classification accuracy under different rotation settings. Best results are bolded and second bests are underlined. std. represents the standard deviation of accuracy between different settings. The ODFNet-xyz architecture is only used for these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>z/z SO3/SO3 z/SO3 std. VoxNet <ref type="bibr" target="#b15">[16]</ref> 83.0 87.3 -3.0 SubVolSup <ref type="bibr" target="#b46">[47]</ref> 88 , we project those vectors onto 2D space and assess this mapping by the Silhouette score <ref type="bibr" target="#b54">[55]</ref>, which evaluates whether each object is well matched to its own cluster. The scores are given in <ref type="table" target="#tab_5">Table 5</ref>, and the projections are visualized in <ref type="figure">Figure 9</ref>. As can be seen from the results, although quantitative scores on the classification of these methods are close to each other, ODFNet's features appear more distinctive and produce a relatively more separated layout than the other two methods, which is also observed in the Silhouette scores. Furthermore, to understand which points generate global features at the classification step, features before the last maxpooling of the network are examined. Heat maps according to the contribution of each point to the final result are obtained for ODFNet and DGCNN as given in <ref type="figure" target="#fig_6">Figure 10</ref>. It can be observed that the ODFNet selects more diverse and seemingly important feature points from the point clouds when compared to the DGCNN 2 . According to the visual results, we can conclude that for objects with relatively more complex geometric shapes, visually representative points, which are mostly corners and endpoints, are selected by the ODFNet. The ODFs for those points that are around corners and edges have anisotropic distributions while the ODFs on the flat areas have isotropic distributions with similar strengths over many directions.</p><p>We also further investigate our method to analyze the effects of two important decisions we make when calculating ODF values: number of different conic neighborhood directions and pivot selection. <ref type="table" target="#tab_6">Table 6</ref> shows the accuracies for different settings. The results for Experiment A, B, and C indicate that 42 directions which are obtained by the second tessellation of the icosahedron give the best performance compared to the first (12 <ref type="table">Table 3</ref>. Classification Accuracies for different tasks in ScanObjectNN <ref type="bibr" target="#b12">[13]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>OBJ BG PB T25 PB T25R PB T50R PB T50RS OBJ ONLY 3DmFV <ref type="bibr" target="#b50">[51]</ref> 68.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Part Segmentation</head><p>We evaluated our segmentation network on the ShapeNet part segmentation benchmark <ref type="bibr" target="#b10">[11]</ref>, which contains 14007 training and 1874 test samples, 16 object categories each of them partitioned into {2 ? 6} parts, making a total of 50 parts. Following the practice of the previous state-of-the-art <ref type="bibr" target="#b7">[8]</ref>, which used ensembling, during the testing phase, for every test object, we also obtain scaled versions of test objects by +0.3%,?0.3% in each direction and averaged their scores. We compare our performance with those studies using a point cloud structure. Our experimental results for ShapeNet part segmentation are reported for ODFNet along with previous methods in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Scene Segmentation</head><p>For scene segmentation, the commonly used S3DIS dataset <ref type="bibr" target="#b11">[12]</ref> is utilized. The dataset contains point clouds sampled from six different challenging scenes. General practice in experimentation with this dataset involves training with a leave-one-out cross-validation (6-fold) strategy. State-of-the-art results are obtained for S3DIS for the overall accuracy measure as shown in <ref type="table" target="#tab_7">Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>We perform ablation studies on the ModelNet40 dataset for the classification task to further analyze our decisions for ODFNet and ODF-xyz.</p><p>To quantify the functionality of different modules of the ODFNet, we experimented with removing ODF-Dir and ODF-Glob blocks. For these experiments, we use 42 conic neighborhood directions and the RI-XY pivot selection method. The performance results in <ref type="table" target="#tab_8">Table 8</ref> show that both ODF-Dir and ODF-Glob blocks contribute to the performance.</p><p>For the ODFNet-xyz, another ablation study is carried out in order to analyze the two factors contributing to the rotation invariance: (i) using rotation invariant feature vectors instead of difference vectors; and (ii) using RI-XYZ, that is selection of pivot directions that are also rotation invariant, instead of RI-XY. The results given in <ref type="table">Table 9</ref> show that by using both factors, the best accuracy is obtained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions and Conclusion</head><p>Our experimental results demonstrate that ODFNet achieves the SoTA performance in both the classification task and particularly for the more challenging segmentation task in ShapeNet and S3DIS. This provides evidence to our hypothesis that the ODF representation, which exploits the idea of incorporation of local point orientation distribution characteristics into the point cloud neural network models, is highly beneficial.</p><p>The point orientation distribution features help the neural network models capture further informative characteristics of the point cloud. This is revealed by the attention that the ODFNet model pays to the unique identifying points on an object such as corners, tips, and borders between planar regions. Features generated by the ODFNet correlate with an increased representation power for point clouds.</p><p>Moreover, we investigate the rotation invariance properties of ODF representations through a variant of the proposed ODFNet: ODFNet-xyz, which is fully rotation-invariant. Our experimental results show that ODF features can be effectively calculated in a rotation-invariant manner, and ODFNet-xyz performs comparably against state-of-the-art rotation-invariant point cloud analysis models.</p><p>From the results in <ref type="table" target="#tab_1">Table 1</ref> and 2, we can conclude that the original point locations and other location-based properties are highly discriminative. However, there is a trade-off between exploiting rotation-invariant features versus locationbased rotation-variant point features, as location-based features are naturally not robust to rotation changes. Indeed, when the objects are rotated, the performances of rotation-variant networks drastically decrease. Moreover, the established benchmarks and procedures for evaluations on those benchmarks do not reward the rotation-invariant representations, as indicated by the inferior performance of rotation-invariant methods on those benchmarks.</p><p>As for future work, in geometric representation learning of point clouds, capturing essential defining characteristics of an object, whether through better augmented definitions of local patches around points as the ODFNet does or in other similarly effective ways of local and global encoding schemes, could provide further improvements in supervised and unsupervised learning tasks on point clouds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>ODFs for some selected points on example point clouds from the ShapeNet<ref type="bibr" target="#b10">[11]</ref> dataset. Line length indicates strength.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The ODFBlock and its usage to obtain ODF features. Here N c and N d represent the number of different cones and directions, respectively. For every point, ODF values are computed along different cones. Then, ODF values are processed in ODFBlocks. An ODFBlock consists of two mlps: ODFDir and ODFGlob.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>An example ODF cone is placed on x i along direction v l . Cones at multiple scales (of heights and apex angles) are used to capture point density features at a hierarchy of neighborhoods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>An example alignment of the neighborhoods for a given point. The same rotation that aligns the two neighborhood directions, directions of the blue and the green cone, with the two pivot directions, the blue and green arrows, is applied to all neighborhood directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>For the RI-XY method, ODF directions according to nearest neighbors are depicted. Green arrows indicate pivot and blue arrows indicate the z-axis. (a) By rotating the objects in the x-y plane, ODF values do not change since the directions are aligned with respect to rotation-invariant pivots. (b) For symmetric points of the same object, nearly symmetric pivot directions are obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>(a) Mlp-block common structure in the ODFNet that is employed for tasks of (b) classification, (c) part segmentation (d) Scene segmentation. (e) represents the ODFNet-XYZ which is fully rotation-invariant. c represents class count.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Different heat maps for DGCNN and ODFNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Overall Accuracies (OA) for point cloud classification results on ModelNet40 dataset. (p: points, n: normals)</figDesc><table><row><cell>Method</cell><cell cols="2">input voting OA</cell></row><row><cell>PointNet [2]</cell><cell>p</cell><cell>89.2</cell></row><row><cell>PointNet++ [3]</cell><cell>p+n</cell><cell>90.7</cell></row><row><cell>SpiderCNN [5]</cell><cell>p+n</cell><cell>92.4</cell></row><row><cell>Point2Seq [40]</cell><cell>p</cell><cell>92.6</cell></row><row><cell>InterpCNN [41]</cell><cell>p</cell><cell>93.0</cell></row><row><cell>PointwiseCNN [23]</cell><cell>p</cell><cell>86.1</cell></row><row><cell cols="2">ShapeContextNet [42] p</cell><cell>90.0</cell></row><row><cell>KCNet [43]</cell><cell>p</cell><cell>91.0</cell></row><row><cell>PointCNN [24]</cell><cell>p</cell><cell>92.2</cell></row><row><cell>RS-CNN [44]</cell><cell>p</cell><cell>92.4</cell></row><row><cell>ShellNet [9]</cell><cell>p</cell><cell>93.1</cell></row><row><cell>DGCNN [7]</cell><cell>p</cell><cell>92.9</cell></row><row><cell>ODFNet</cell><cell>p</cell><cell>93.4</cell></row><row><cell>Kd-network [45]</cell><cell>p</cell><cell>91.8</cell></row><row><cell>GDANet [46]</cell><cell>p</cell><cell>93.8</cell></row><row><cell>DensePoint [8]</cell><cell>p</cell><cell>93.2</cell></row><row><cell>RS-CNN [44]</cell><cell>p</cell><cell>93.6</cell></row><row><cell>ODFNet</cell><cell>p</cell><cell>94.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>ShapeNet Part segmentation results for different architectures. Input column indicates whether points (p) and normals (n) are used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">Silhouette scores for last layers of ODFNet, DGCNN and ShellNet</cell></row><row><cell cols="3">after UMAP [53] and t-SNE [54] projection. {worst:best}:{-1:1}</cell></row><row><cell>Method</cell><cell cols="2">tSNE S. Score UMAP S. Score</cell></row><row><cell>ODFNet</cell><cell>0.623</cell><cell>0.660</cell></row><row><cell>DensePoint</cell><cell>0.453</cell><cell>0.474</cell></row><row><cell>ShellNet</cell><cell>0.472</cell><cell>0.466</cell></row><row><cell>UMAP</cell><cell></cell><cell>T-SNE</cell></row><row><cell>DensePoint ODFNet ShellNet</cell><cell></cell><cell></cell></row></table><note>Fig. 9. The projections from the last layer outputs of ODFNet, DensePoint and ShellNet architectures for ModelNet40 dataset onto a 2D space using the UMAP [53] and t-SNE [54] methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>An experiment on selection of hyperparameters: direction count and pivot selection. OA refers to overall accuracy.</figDesc><table><row><cell cols="4">Experiment Dir. Count Pivot Sel. OA</cell></row><row><cell>A</cell><cell>12</cell><cell>RI-XY</cell><cell>92.9</cell></row><row><cell>B</cell><cell>42</cell><cell>RI-XY</cell><cell>93.4</cell></row><row><cell>C</cell><cell>162</cell><cell>RI-XY</cell><cell>93.1</cell></row><row><cell>D</cell><cell>12</cell><cell cols="2">RI-XYZ 91.6</cell></row><row><cell>E</cell><cell>42</cell><cell cols="2">RI-XYZ 91.8</cell></row><row><cell>F</cell><cell>162</cell><cell cols="2">RI-XYZ 91.4</cell></row><row><cell cols="4">directions) and third (162 directions) tessellations. The results</cell></row><row><cell cols="4">for the remaining experiments show that choosing pivots that</cell></row><row><cell cols="4">are rotation-invariant in the x-y plane works better compared</cell></row><row><cell cols="4">to the pivots that are fully rotation-invariant. Because all the</cell></row><row><cell cols="4">training data is aligned according to the z-direction, using this</cell></row><row><cell cols="3">direction as a pivot improves the performance.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Overall accuracies and mIOU values for point cloud scene segmentation results on S3DIS dataset.</figDesc><table><row><cell>Method</cell><cell>OA</cell><cell>mIoU</cell></row><row><cell>PointNet [2]</cell><cell cols="2">78.6 47.6</cell></row><row><cell>PointNet++ [3]</cell><cell cols="2">81.0 54.5</cell></row><row><cell>PointSIFT [56]</cell><cell cols="2">88.7 70.2</cell></row><row><cell>Engelmann [57]</cell><cell cols="2">84.0 58.3</cell></row><row><cell cols="3">3DContextNet [58] 84.9 55.6</cell></row><row><cell>PointWeb [59]</cell><cell cols="2">87.3 66.7</cell></row><row><cell>ShellNet [9]</cell><cell cols="2">87.1 66.8</cell></row><row><cell>PointCNN [24]</cell><cell cols="2">88.1 65.4</cell></row><row><cell>InterpCNN [41]</cell><cell cols="2">88.7 66.7</cell></row><row><cell>DGCNN [7]</cell><cell cols="2">84.1 56.1</cell></row><row><cell>Liu et al. [60]</cell><cell cols="2">88.5 64.1</cell></row><row><cell>RandLA-Net [61]</cell><cell cols="2">88.0 70.0</cell></row><row><cell>HEPIN [62]</cell><cell cols="2">88.2 67.8</cell></row><row><cell>PointWeb [59]</cell><cell cols="2">87.3 66.7</cell></row><row><cell>CF-SIS [63]</cell><cell cols="2">88.0 74.0</cell></row><row><cell>ODFNet</cell><cell cols="2">90.8 72.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Ablation study results on ODFNet. OA refers to overall accuracy.Table 9. Ablation study results on ODFNet-xyz. OA refers to overall accuracy.</figDesc><table><row><cell></cell><cell cols="5">Experiment ODF-Dir ODF-Glob OA</cell></row><row><cell></cell><cell>A</cell><cell></cell><cell></cell><cell></cell><cell>93.4</cell></row><row><cell></cell><cell>B</cell><cell></cell><cell></cell><cell></cell><cell>92.3</cell></row><row><cell></cell><cell>C</cell><cell></cell><cell></cell><cell></cell><cell>91.9</cell></row><row><cell></cell><cell>D</cell><cell></cell><cell></cell><cell></cell><cell>91.2</cell></row><row><cell>Exp.</cell><cell>Rot.Inv. Features</cell><cell>RI-XYZ</cell><cell>z/z</cell><cell cols="2">SO3/SO3 z/SO3 std.</cell></row><row><cell>E</cell><cell></cell><cell></cell><cell cols="2">90.4 85.0</cell><cell>42.9</cell><cell>26.0</cell></row><row><cell>F</cell><cell></cell><cell></cell><cell cols="2">91.6 89.5</cell><cell>24.9</cell><cell>37.9</cell></row><row><cell>G</cell><cell></cell><cell></cell><cell cols="2">90.2 90.2</cell><cell>90.2</cell><cell>0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Pointwise heat maps cannot be created for DensePoint and ShellNet since they do not use directly the points but their distribution.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet++</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global context aware local features for robust 3d point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ppfnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spidercnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densepoint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Octree guided cnn with spherical kernels for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9631" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1588" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Orientation-boosted voxel nets for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>arXiv:160403351</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fpnn: Field probing neural networks for 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voxnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ensemble of panorama-based convolutional neural networks for 3d model classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sfikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="208" to="218" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning for 3d shape classification from multiple depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Minto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing</title>
		<meeting>IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-view 3d object retrieval with deep embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5526" to="5537" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hypergraph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
	<note>Convolution on x-transformed points</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Point cloud autoencoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Foldingnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders for compact representations of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zamorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zieba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Klukowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stokowiec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page">102921</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Je</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kpconv</surname></persName>
		</author>
		<idno>arXiv:190408889</idno>
		<title level="m">Flexible and deformable convolution for point clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convpoint: Continuous convolutions for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="24" to="34" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<idno>arXiv:191212033</idno>
		<title level="m">Deep learning for 3d point clouds: A survey</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spherical fractal convolutional neural networks for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="452" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rotation invariant convolutions for 3d point clouds deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rotation-invariant local-to-global representation learning for 3d point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<title level="m">Global context aware convolutions for 3d point cloud understanding. arXiv preprint arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Tuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1358" to="1372" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A higher-order tensor vessel tractography for segmentation of vascular structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2172" to="2185" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On computing the underlying fiber directions from the diffusion orientation distribution function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularized, fast, and robust analytical q-ball imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Angelino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fitzgibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="497" to="510" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Von mises-fisher mixture model of the diffusion odf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vemuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yezierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mareci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regularization of bending and crossing white matter fibers in mri q-ball fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Ehricke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Klose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="916" to="926" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Point2sequence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8778" to="8785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1578" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning geometrydisentangled representation for complementary understanding of 3d object point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>arXiv:201210921 2020</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning so (3) equivariant representations with spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Effective rotation-invariant point cnn with spherical harmonics kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Poulenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rakotosaona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ponty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep hierarchical cluster network with rigorously rotation-invariant representation for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clusternet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4994" to="5002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno>arXiv:180310091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Umap</surname></persName>
		</author>
		<idno>arXiv:180203426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lvd</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Pointsift: A sift-like network module for 3d point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno>arXiv:180700652</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Know what your neighbors do: 3d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Kd tree guided hierarchical learning of point clouds using local and global contextual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointweb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-prediction for joint instance and semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="187" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Randlanet: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hierarchical point-edge interaction network for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10433" to="10441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cf-sis: Semantic-instance segmentation of 3d point clouds by context fusion with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Youk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1661" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
