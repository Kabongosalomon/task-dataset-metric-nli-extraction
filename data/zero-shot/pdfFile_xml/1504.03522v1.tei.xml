<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Scene Text Localization and Recognition with Local Character Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk??</forename><surname>Neumann</surname></persName>
							<email>neumalu1@cmp.felk.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Cybernetics</orgName>
								<orgName type="laboratory">Centre for Machine Perception</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
							<email>matas@cmp.felk.cvut.cz</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Centre for Machine Perception</orgName>
								<orgName type="department" key="dep2">Department of Cybernetics</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Scene Text Localization and Recognition with Local Character Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An unconstrained end-to-end text localization and recognition method is presented. The method detects initial text hypothesis in a single pass by an efficient region-based method and subsequently refines the text hypothesis using a more robust local text model, which deviates from the common assumption of region-based methods that all characters are detected as connected components.</p><p>Additionally, a novel feature based on character stroke area estimation is introduced. The feature is efficiently computed from a region distance map, it is invariant to scaling and rotations and allows to efficiently detect text regions regardless of what portion of text they capture.</p><p>The method runs in real time and achieves state-of-the-art text localization and recognition results on the ICDAR 2013 Robust Reading dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Scene text localization and recognition, also known as textin-the-wild or PhotoOCR, is an interesting problem with many application areas such as translation, assistance to the visually impaired or searching large image databases (e.g. Flickr or Google Images) by their textual content. But unlike traditional document OCR, none of the scene text recognition methods has yet achieved sufficient accuracy and speed for practical applications.</p><p>Text localization can be computationally very expensive because in an image of N pixels in general up to any of the 2 N subsets can correspond to text. Methods based on the sliding-window localize individual characters <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref> or whole words <ref type="bibr" target="#b4">[5]</ref> by shifting a classification window of multiple sizes across the image, drawing inspiration from other object detection problems where this approach has been has been successfully applied <ref type="bibr" target="#b20">[21]</ref>. Such methods are robust to noise and blur, since features aggregated over a larger area, but the crucial disadvantage is that the number of windows that needs to be classified grows rapidly when text with different scale, aspect, rotation and other distortions has to be found.</p><p>Methods based on connected components <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref> find individual characters as connected components of a certain local property (color, intensity, stroke-width, etc.), so that the complexity is unaffected by the text parameters as characters of all scales and orientations can be detected in one pass. Moreover, the connected component representation provides a segmentation which can be exploited in a standard OCR stage. The main drawback is the assumption that a character is a single connected component, which is brittle -a change in a single image pixel introduced by noise can cause an unproportional change in the connected component size, <ref type="bibr">(</ref> shape or other properties. The methods are also incapable of detecting characters which consist of several connected components or where text is present as characters joint together.</p><p>In the proposed method, we generalize the region-based approach by detecting arbitrary fragments and groups of characters alongside characters themselves in a single stage. As previously suggested <ref type="bibr" target="#b2">[3]</ref>, we exploit the observation that text consists of strokes and we propose a unified approach to effectively detect and further segment regions which are formed of strokes, regardless whether they represent a part of a character, a whole character or a group of characters joint together, thus dropping the common assumption of a one to one correspondence between a character and its connected component representation.</p><p>In the initial stage, candidate regions are effectively detected as MSERs <ref type="bibr" target="#b9">[10]</ref> with the "strokeness" property and grouped into initial text line hypotheses, where each text line hypothesis is then individually segmented or rejected using an iterative and more robust segmentation approach, which is capable of segmenting characters that cannot be obtained by thresholding (and therefore neither as MSERs). In order to estimate the "strokeness" of a region we introduce a novel feature based on Stroke Support Pixels (SSPs) which exploits the observation that one can draw any character by taking a brush with a diameter of the stroke width and drawing through certain points of the character (see <ref type="figure" target="#fig_0">Figure 3</ref>) -we refer to such points as stroke support pixels (SSPs). The SSPs have the property that they are in the middle of a character stroke, which we refer to as the stroke axis, the distance to the region boundary is half of the stroke width, but unlike skeletons they do not necessary form a connected graph.</p><p>Since the area (i.e. the number of pixels) of an ideal stroke is the product of the stroke width and the length of the stroke, the "strokeness" is estimated by the stroke area ratio feature ? which compares the actual area of a region with the ideal stroke area calculated from the SSPs. The feature estimates the proportion of region pixels which are part of a character stroke and therefore it allows to efficiently differentiate between text regions (regardless of how many characters they represent) and the background clutter. The feature is efficiently computed from a region distance map, it is invariant to scaling and rotation and it is more robust to noise than methods which aim to estimate a single stroke width value <ref type="bibr" target="#b2">[3]</ref> as small pixel changes do not cause unproportional changes to the estimate. At last but not least, the SSPs are also exploited in the subsequent supervised segmentation stage to build a more accurate text color model, as by definition the SSPs are placed inside the character where the character color varies the least.</p><p>The rest of the paper is structured as follows: In Section II, an overview of previously published methods is presented, in the Section III the proposed method is introduced and in Section IV, the experimental evaluation is given. The paper is concluded in the Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORK</head><p>Numerous methods which focus solely on text localization in real-world images have been published. The "slidingwindow" based methods <ref type="bibr" target="#b8">[9]</ref> use a window which is moved over the image and the presence of text is estimated on the basis of local image features. The majority of recently published methods for text localization however uses the connected component approach <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The methods differ in their approach to individual character detection, which could be based on edge detection, character energy calculation or extremal region detection, but they all share the assumption that a single character is detected as a single connected component. The winning method in text localization of Yin et al. <ref type="bibr" target="#b24">[25]</ref> at the latest ICDAR 2013 Robust Reading competition <ref type="bibr" target="#b6">[7]</ref> also falls into this category as individual characters are detected as Maximally Stable Extremal Regions (MSERs) <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSER Detection</head><p>Character/Multi-character/ Background Classification Text Line Hypotheses Text Recognition Local Iterative Segmentation <ref type="figure">Fig. 2</ref>. Overview of the method. Initial text hypotheses efficiently generated by a MSER detector are further refined using a local text model, unique to each text line Other methods focus only on text recognition, where the text is manually localized by a human annotator. The text is recognized on various levels, ranging from characters <ref type="bibr" target="#b3">[4]</ref> to the whole word level <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The winning method <ref type="bibr" target="#b0">[1]</ref> was able to correctly recognize 82.8% of the manually croppedwords in the latest ICDAR Robust Reading competition <ref type="bibr" target="#b6">[7]</ref>. Although the methods for cropped-word recognition give an upper-bound on currently achievable text recognition performance, they in fact assume there exists a text localization method with a 100% accuracy, which currently is far from being true. Moreover, since the text was localized by a human, it is not clear that such text localization is even possible without the recognition, because the human annotator could have used the actual content of the text to create the annotation for localization.</p><p>For an exhaustive survey of text localization and recognition methods refer to the ICDAR Robust Reading competition results <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Initial Candidates Detection</head><p>In the initial stage, candidate regions are detected as MSERs <ref type="bibr" target="#b9">[10]</ref>. The MSER detector is often exploited in the literature <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b24">[25]</ref> to effectively detect individual characters, however this assumption may not always hold -there are many instances where individual characters cannot be detected as MSERs because only a portion of a character is a MSER (see <ref type="figure" target="#fig_2">Figure 1b</ref>) or a single MSER corresponds to multiple characters or even whole words (see <ref type="figure" target="#fig_1">Figure 5</ref>, middle row).</p><p>In the proposed method, we significantly relax the assumption of individual characters being detected as MSERs (or even Extremal Regions <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>) by considering the MSER detector as an efficient first stage in order to generate initial text hypothesis, with no assumptions what level of text (i.e. part of characters, characters or words) individual MSERs represent. In other words, the proposed method assumes that at least a small portion of the text in the image triggers the MSER detector to generate an initial hypothesis, but it does require that all characters are detected as MSERs, as the individual characters are detected at a later stage using a local text model.</p><p>In order to build initial text hypotheses, all MSERs in an image (detected in the intensity and hue channels) are first classified into 3 distinct classes: The character class represents a single character (or a significant portion of it), the multicharacters class represents an arbitrary group of characters joint together as a single component (e.g. a portion of a word, a whole word or even several words) and the background class represents all non-textual content (e.g. background textures). The MSERs classified as characters and multi-characters are used to initialize a local text model (see Section III-C), whilst the MSERs classified as background are discarded.  For each region, the following features are calculated: stroke area ratio ? = As A , aspect ratio w h , compactness ? A P , convex hull area ratio A Ac and holes area ratio A h Ac , where w and h denote width and height of the region's bounding box, A denotes the region area (i.e. number of pixels), P denotes the length of the perimeter, A c denotes the convex hull area, A h denotes the total area of region holes and A s denotes the character strokes area.</p><p>In order to estimate the character strokes area, a distance transform map is calculated for the region binary mask and only pixels corresponding to local distance maxima are considered (see <ref type="figure" target="#fig_1">Figure 5</ref>) -we refer to these pixels as Stroke Support Pixels (SSPs), because the pixels determine the position of a latent character stroke axis.</p><p>In order to estimate the area of the character strokes? s , one could simply sum the distances associated with the SSPs</p><formula xml:id="formula_0">A s = 2 i?S d i<label>(1)</label></formula><p>where S are the SSPs and d i is the distance of the pixel i to Such an estimate is correct for an straight stroke of an odd width, however it becomes inaccurate for strokes of an even width (because there are two support pixels for a unitary stroke length) or when the support pixels are not connected to each other as a result of noise at the region boundary or changing stroke width (see <ref type="figure">Figure 4</ref>). We therefore propose to compensate the estimate by introducing weights w i , which ensure normalization to a unitary stroke length by counting the number of pixels in a 3 ? 3 neighborhood of each SSP</p><formula xml:id="formula_1">A s = 2 i?S w i d i w i = 3 |N i | (2)</formula><p>where |N i | denotes the number of SSPs within the 3 ? 3 neighborhood of the pixel of i (including the pixel i itself). The numerator value is given by the observation that for a straight stroke, there are 3 support pixels in the 3 ? 3 neighborhood (see <ref type="figure">Figure 4a</ref>).</p><p>To generate the training data, all MSERs from the ICDAR 2013 Training Set <ref type="bibr" target="#b6">[7]</ref> dataset images were labeled using the ground truth segmentation masks -if the MSER overlaps sufficiently (more than 70% of pixels) with a ground truth character segmentation it is labeled as a character, if it overlaps with multiple ground truth character segmentations it is labeled as a multi-character and if it does not overlap with any segmentation it is labeled as background. MSERs which do not fall into any of the above categories were not used in the training. Using the aforementioned procedure, a dataset of 121,000 background MSERs, 14,000 character MSERs and 1,200 multi-character MSERs was obtained. A random subset of 20,000 samples was then used to train an SVM classifier <ref type="bibr" target="#b1">[2]</ref> with a RBF <ref type="bibr" target="#b11">[12]</ref> kernel using a one-against-all strategy, where each class was assigned a weight inversely proportional to its ratio in the training dataset in order to deal with the unbalanced number of samples for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Text Line Hypotheses</head><p>Given the initial set of text hypothesis in the form of detected character and multi-character regions, the proposed method proceeds to build a local text model. The model is inferred for each text line individually, where we consider a text line as a sequence of characters which can be fitted by a line and which has the same typographic and appearance properties.</p><p>The character and multi-character regions are first clustered into initial text line hypotheses using an efficient exhaustive search strategy adapted from <ref type="bibr" target="#b12">[13]</ref>, where each neighboring character triplet and each multi-character region is assigned a bottom line estimate (see <ref type="figure" target="#fig_2">Figure 1c</ref>), which serves as a distance measure for a standard agglomerative clustering approach. In order to enforce that one region is present only in one text line, initial text lines are simply grouped into clusters based on presence of identical regions (two text lines are a member of the same cluster if they have at least one region in common) and then in each cluster only the longest text line is kept; this can be viewed as a voting process, where in each cluster text lines vote for the text direction and the longest text line wins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Local Iterative Segmentation</head><p>Each text line hypothesis is further refined using a local text model, individual for each text line. We formulate the problem of finding the local text model as a energy minimization task in a standard graph cut framework by adapting the iterative segmentation approach of GrabCut <ref type="bibr" target="#b16">[17]</ref> by dynamically changing the processed image area based on current segementation in each iteration.</p><p>Let us recall that in the graph cuts framework the objective is a minimization of a the Gibbs energy</p><formula xml:id="formula_2">E(?, ?, z) = U (?, ?, z) + V (?, z)<label>(3)</label></formula><p>where U (?, ?, z) is the data term, V (?, z) is the smoothness term, ? is the vector of labels for each pixel, ? represents the image color distributions for background and foreground and z is the image.</p><p>Following <ref type="bibr" target="#b16">[17]</ref>, the data term is a Gaussian Mixture Model (one GMM for foreground and one for background) and the smoothness term is the based on the Euclidean distance in the RGB color space. Each pixel within the text line boundingbox is then labeled as definitive foreground DF, probable foreground PF or background B in the following iterative process (see <ref type="figure" target="#fig_2">Figure 1d</ref>  The final segmentation of the text line is a obtained by taking the connected components of the PF pixels. If all pixels in the text line bounding-box converged to the same label (e.g. all are labeled as PF), the whole text line is discarded as it most likely represents a false positive. Pixels with the PF label which do not fit the bottom line estimate (see <ref type="figure" target="#fig_2">Figure 1e</ref>) or which are located at the boundary of the text line bounding-box are ignored in the GMM estimation as they typically represent interpunction or fragments of characters in neighbouring text lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Text Recognition</head><p>Given the segmentations obtained in the previous stage, each connected component is assigned a Unicode label(s) by an OCR module, which is trained on synthetic data <ref type="bibr" target="#b15">[16]</ref>. Following the standard approach <ref type="bibr" target="#b19">[20]</ref>, the connected components with the aspect ratio above a predefined threshold are chopped to generate more region hypotheses in order to cater for joint characters. Each connected component with a label then represents a node in a direct acyclic graph, where the edges represent a "is-a-predecessor" relation. The final sequence of labels is then found as an optimal path in such a graph <ref type="bibr" target="#b14">[15]</ref>.</p><p>Because the graph is relatively small (when compared to <ref type="bibr" target="#b14">[15]</ref>, where there are several segmentations for each character), second order language model features were added in order to improve recognition accuracy without any significant impact on memory complexity.</p><p>The whole pipeline runs independently over multiple scales for each image and in the final stage the detected words are aggregated into a single output, while eliminating overlapping words (which typically represent the same word detected in  <ref type="bibr" target="#b6">[7]</ref> 66.4 88.5 75.9 2013 our previous method <ref type="bibr" target="#b14">[15]</ref> 64.8 87.5 74.5 2013 Kim (ICDAR'11 winner) <ref type="bibr" target="#b17">[18]</ref> 62.5 83.0 71.3 N/A multiple scales) by keeping only the word whose corresponding path in the graph has the lowest cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The proposed method was evaluated using the ICDAR 2013 Robust Reading competition dataset <ref type="bibr" target="#b6">[7]</ref>, which contains 1189 words and 6393 letters in 255 images.</p><p>Using the ICDAR 2013 competition evaluation scheme <ref type="bibr" target="#b6">[7]</ref>, the method achieves recall 72.4%, precision 81.8% and fmeasure 77.4% in text localization (see <ref type="figure" target="#fig_3">Figure 6</ref> for sample outputs). The method achieves significantly better recall than the winner of ICDAR 2013 Robust Reading competition (66%) and the method of Yin et al. <ref type="bibr" target="#b24">[25]</ref> (68%) and outperforms all the previous methods in the f-measure -see <ref type="table" target="#tab_2">Table I</ref>.</p><p>In end-to-end text recognition, the method correctly localized and recognized 543 words (45%), where a word is considered correctly recognized if all its characters match the ground truth (using case-sensitive comparison). On the other hand, the method "hallucinated" 79 words in total which do not have any overlap with the ground truth.</p><p>The main reasons for method failure are character-like objects near the text (e.g. pictographs, arrows, etc.) and lowcontrast characters which are not picked up in the initial stage. The average run time on a standard 2.7GHz PC is 800ms per image.</p><p>V. CONCLUSION An end-to-end real-time text localization and recognition method was presented. The method detects initial text hypothesis in a single pass by an efficient region-based method and subsequently refines the text hypothesis using a more robust local text model, which deviates from the common assumption of region-based methods that all characters are detected as connected components.</p><p>Additionally, a novel feature based on Stroke Support Pixels (SSPs) is introduced. The feature is based on an observation, that one can draw any character by taking a brush with a diameter of the stroke width and drawing through certain points of the character. The feature is efficiently computed from a region distance map, it is invariant to scaling and rotations and allows to efficiently detect text regions regardless of what portion of text they capture.</p><p>On the standard ICDAR 2013 dataset <ref type="bibr" target="#b6">[7]</ref>, the method achieves state-of-the-art results in text localization (f-measure 77.6%) and improves previously published results for end-toend text recognition, with the average run time of 800ms per image.</p><p>Future work includes dealing with current limitations of the method, such as the inability to detect single-or twoletter words if they are not part of a longer text line and the assumption of a straight base-line in the text line hypothesis stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Area A of an ideal stroke is a product of the stroke width sw and the length of the stroke s l . This is approximated by summing double the distances d i of Stroke Support Pixels (SSPs) along the stroke axis S SW</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of stroke area ratio ? values for character (top row), multicharacter (middle row) and background (bottom row) connected components. Distance map denoted by pixel intensity, Stroke Support Pixels (SSPs) denoted red the boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>-g): 1 )</head><label>1</label><figDesc>Initialize all pixels belonging to a character or a multi-character region as PF, others as B 2) Calculate a new text line bounding-box by encapsulating all PF pixels and expand it by ? h and ? v pixels in the horizontal resp. vertical direction 3) Find SSPs amongst PF pixels and mark them asDF 4) Learn GMM parameters, using the DF pixels to train the foreground model and B pixels to train the background model 5) Create edges from the source to the DF and PF pixels, and from the B pixels to the sink 6) Estimate the segmentation by finding the minimal cut -mark pixels in the source subgraph as PF, pixels in the sink subgraph as B 7) Repeat from Step 2, until convergence The value ? h is set to the average region width in the text line and the ? v is one third of the text line bounding-box height.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Text localization and recognition examples on the ICDAR 2013 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I .</head><label>I</label><figDesc>COMPARISON WITH MOST RECENT RESULTS ON THE ICDAR 2013 DATASET.</figDesc><table><row><cell>method</cell><cell>recall</cell><cell>precision</cell><cell>f</cell><cell>published</cell></row><row><cell>proposed method</cell><cell>72.4</cell><cell>81.8</cell><cell>77.1</cell><cell></cell></row><row><cell>Yin et al. [25]</cell><cell>68.3</cell><cell>86.3</cell><cell>76.2</cell><cell>2014</cell></row><row><cell>TexStar (ICDAR'13 winner)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PhotoOCR: reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An introduction to Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-03" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2010</title>
		<imprint>
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic labeling for scene text database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Document Analysis and Recognition (ICDAR 2013)</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1397" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaboost for text detection in natural scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jung-Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="429" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Orientation robust text line detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4034" to="4041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
		<title level="m">ICDAR 2013 robust reading competition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
	<note>ICDAR 2013</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Region-based discriminative feature pooling for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4050" to="4057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaboost for text detection in natural scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2011 International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="429" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust wide-baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Top-down and bottom-up cues for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2012</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2687" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An introduction to kernel-based learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="181" to="201" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text localization in real-world images using efficiently pruned exhaustive search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR 2011, sept. 2011</title>
		<imprint>
			<biblScope unit="page" from="687" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On combining multiple segmentations in scene text recognition</title>
	</analytic>
	<monogr>
		<title level="m">ICDAR 2013</title>
		<meeting><address><addrLine>California, US</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="523" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
	</analytic>
	<monogr>
		<title level="m">ACCV 2010, ser</title>
		<imprint>
			<date type="published" when="2010-11" />
			<biblScope unit="volume">6495</biblScope>
			<biblScope unit="page" from="2067" to="2078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">ICDAR 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scene text detection using graph model built upon maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
	<note type="report_type">PR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An overview of the tesseract ocr engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="629" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2011</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2012</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4042" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="970" to="983" />
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
