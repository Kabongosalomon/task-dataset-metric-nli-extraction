<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LM-Critic: Language Models for Unsupervised Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LM-Critic: Language Models for Unsupervised Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training a model for grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs, but manually annotating such pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if the LM assigns it a higher probability than its local perturbations. We apply this LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical/grammatical pairs for training a corrector. We evaluate our approach on GEC datasets across multiple domains (CoNLL-2014, BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the unsupervised setting (+7.7 F 0.5 ) and the supervised setting (+0.5 F 0.5 ).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammatical error correction (GEC) is the task of fixing grammatical errors in text, such as typos, tense and article mistakes. Recent works cast GEC as a translation problem, using encoder-decoder models to map bad (ungrammatical) sentences into good (grammatical) sentences <ref type="bibr" target="#b58">(Yuan and Briscoe, 2016;</ref><ref type="bibr" target="#b53">Xie et al., 2016;</ref><ref type="bibr" target="#b15">Ji et al., 2017;</ref><ref type="bibr" target="#b6">Chollampatt and Ng, 2018;</ref><ref type="bibr" target="#b17">Junczys-Dowmunt et al., 2018)</ref>. These methods rely on a combination of human-labeled data (i.e., bad, good pairs) <ref type="bibr" target="#b37">(Nicholls, 2003;</ref><ref type="bibr" target="#b55">Yannakoudakis et al., 2011;</ref> and synthetic data, which are generated by corrupting good sentences into synthetic bad, good pairs <ref type="bibr" target="#b0">(Awasthi et al., 2019;</ref><ref type="bibr" target="#b24">Kiyono et al., 2019)</ref>. Human-labeled pairs are representative of real human errors but are expensive to obtain, while synthetic pairs are cheap but are unrealistic, deviating from the distribution of grammatical In this work, we train a fixer for grammatical error correction (GEC) by leveraging LM-Critic that assesses the grammaticality. (b) LM-Critic deems a sentence to be grammatical if a pretrained language model (e.g., GPT2) assigns it a higher probability than candidates in its local neighborhood (e.g., edit distance 1). errors humans make <ref type="bibr" target="#b13">(Grundkiewicz et al., 2019)</ref>.</p><p>How to obtain inexpensive yet realistic paired data to improve GEC remains a key challenge, especially in domains or languages with no labeled GEC data <ref type="bibr" target="#b32">(Napoles et al., 2019;</ref><ref type="bibr" target="#b31">N?plava and Straka, 2019)</ref>. Break-It-Fix-It (BIFI; <ref type="bibr" target="#b57">Yasunaga and Liang (2021)</ref>) is a recent method to obtain realistic paired data from unlabeled data, which has shown promise in the task of source code repair. The idea of BIFI is that using an initial fixer (e.g., trained on synthetic data) and a critic that tells if an input is bad or good (e.g., compiler, which checks if code has an error), BIFI iteratively trains the fixer and a breaker to generate better paired data. Specifically, BIFI (1) applies the fixer to bad examples and keeps outputs accepted by the critic, (2) trains a breaker on the re-sulting paired data and uses it to generate more pairs, and (3) trains the fixer on the pairs generated in Step (1) and (2). This way, BIFI adapts the fixer to more realistic distributions of bad, good pairs, only using unlabeled data. However, BIFI is not directly applicable to GEC because it requires an oracle critic (e.g., compiler), which does not exist for GEC.</p><p>In this work, we propose LM-Critic, a simple approximate critic for assessing grammaticality ( ?3), and apply it with BIFI to learn GEC from unlabeled data ( ?4). Specifically, motivated by recent progress in large language models (LMs) (e.g., GPT2, GPT3; <ref type="bibr" target="#b42">Radford et al. (2019)</ref>; <ref type="bibr" target="#b1">Brown et al. (2020)</ref>) and an intuition that a good LM assigns a higher probability to grammatical sentences than ungrammatical counterparts, we use an LM's probability to define a critic for grammaticality. A naive approach is to deem a sentence as grammatical if its probability exceeds an absolute threshold, but this does not work in practice, e.g., LMs may assign a high probability just because the sentence has more common words. We hence compare probabilities in local neighborhood of sentences. Concretely, LM-Critic is defined by two components, an LM (e.g., GPT2) and a neighborhood function (e.g., edit distance 1), and deems a sentence to be grammatical if the LM assigns it the highest probability in its local neighborhood ( <ref type="figure" target="#fig_0">Figure 1</ref>; local optimum criterion). Using this LM-Critic, we apply BIFI to the GEC task. Notably, our approach, both the LM-Critic and GEC learning, does not require labeled data.</p><p>We evaluate our proposed approach on GEC benchmarks across multiple domains, <ref type="bibr">CoNLL-2014</ref><ref type="bibr" target="#b36">(Ng et al., 2014</ref>, <ref type="bibr">BEA-2019</ref>, GMEG-yahoo, and GMEG-wiki <ref type="bibr" target="#b32">(Napoles et al., 2019)</ref>. We achieve strong performance in the unsupervised setting (i.e., no labeled data), outperforming the baseline fixer trained on synthetic data by 7.7 F 0.5 on average. We also evaluate in the supervised setting, where we take the stateof-the-art model GECToR <ref type="bibr" target="#b39">(Omelianchuk et al., 2020)</ref> as the baseline fixer, and further fine-tune it by applying our approach using unlabeled data. We achieve 65.8 / 72.9 F 0.5 on CoNLL-2014 / BEA-2019, outperforming GECToR by 0.5 F 0.5 . Our results also suggest that while existing BIFI assumed access to an oracle critic (i.e., compiler), an approximate critic (i.e., LM-Critic) can also help to improve model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem setup</head><p>The task of grammatical error correction (GEC) is to map an ungrammatical sentence x bad into a grammatical version of it, x good (one that has the same intended meaning). A GEC model (fixer) f aims to learn this mapping, typically using a paired dataset D pair = {(x bad (i) ,x good (i) )}. In particular, we call it labeled if the pairs are human-annotated. In contrast, we call unlabeled data a set of raw sentences D unlabel = {x (i) }. For simplicity, we use "good"/"bad" to mean grammatical/ungrammatical interchangeably. Unlike a fixer, which maps x bad to x good , a critic c merely assesses whether an input is good or bad: for a sentence x,</p><formula xml:id="formula_0">c(x) = 1 if x is good 0 if x is bad.<label>(1)</label></formula><p>Given unlabeled data x's (some of which are good, some of which are bad), and a language model (LM), which returns a probability distribution p(x) over sentences x, we aim to define the critic ( ?3; LM-Critic) and use that to obtain the fixer ( ?4; BIFI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LM-Critic</head><p>The core of our approach to GEC is a critic, which returns whether a sentence is good (grammatical) or bad <ref type="bibr">(ungrammatical)</ref>. Motivated by recent progress in large-scale pre-trained LMs (e.g., GPT2, GPT3; <ref type="bibr" target="#b42">Radford et al. (2019);</ref><ref type="bibr" target="#b1">Brown et al. (2020)</ref>), we aim to use an LM's probability score to define a critic for grammaticality. Specifically, we propose a criterion that deems a sentence to be good if it has the highest probability within its local neighborhood (local optimum criterion; ?3.1). We implement this criterion using a pretrained LM and a sentence perturbation function (LM-Critic; ?3.2). We then do an intrinsic study on how well LM-Critic works in practice ( ?3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local optimum criterion of grammaticality</head><p>Our starting point is the idea that a good LM assigns a higher probability to grammatical sentences than ungrammatical ones. With this idea, a naive way to judge grammaticality might be to find a threshold (?) for the absolute probability, and let the critic be:</p><formula xml:id="formula_1">AbsThr-Critic(x) = 1 if p(x) &gt; ? 0 otherwise.<label>(2)</label></formula><p>However, this does not work in practice. In <ref type="figure" target="#fig_0">Figure  1</ref>, for instance, "Alice likes cats" (4th sentence) is grammatical but has a lower probability (according to GPT2) than "Better that it" (2nd sentence), which is ungrammatical. This is because the two sentences have different meanings and are not directly comparable. We also empirically find that this critic based on absolute threshold does not work well ( ?3.3.3). This observation motivates us to compare sentences with the same intended meaning, and leads to the following two refined intuitions.</p><p>Intuition 1 (Correlation of grammaticality and probability). For a grammatical sentence, x good , and an ungrammatical version of it (with the same intended meaning), x bad , we have p(x bad ) &lt; p(x good ).</p><p>(3)</p><p>Intuition 2 (Local neighborhood of sentences). Assume for simplicity that every sentence has exactly one grammatical version of it (i.e., if the sentence is grammatical, itself; if not, its corrected version). 1 For each sentence x, there is a set of sentences, B(x) (local neighborhood), that consists of the grammatical version and all other ungrammatical versions of x.</p><p>Assuming the above two intuitions, we obtain the following criterion for judging grammaticality, where the idea is to compare sentences within the meaning-preserving local neighborhood.</p><p>Local optimum criterion of grammaticality.</p><p>For each sentence x, we let B(x) be its local neighborhood as defined in Intuition 2. We then have</p><formula xml:id="formula_2">x is grammatical iff x = argmax x ?B(x) p(x ).<label>(4)</label></formula><p>The justification is as follows. If x is grammatical, then by Intuition 1, x has a higher probability than any other sentences in B(x), as they are ungrammatical; hence, we have the RHS of iff. On the other hand, if x is ungrammatical, then by Intuition 1, the grammatical version of x has a higher probability than x, which contradicts with the RHS of iff. The idea is to deem a sentence to be grammatical if it has the highest probability within its meaningpreserving local neighborhood ( <ref type="figure" target="#fig_0">Figure 1</ref>). We will next describe how to implement this criterion in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation of LM-Critic</head><p>We implement LM-Critic by approximating the local optimum criterion. First, for the sentence probability p(x), we use a pretrained LM's probability score. As obtaining the ground-truth local neighborhood B(x) is difficult, we aim to get an approximate, B(x): we implement a sentence perturbation function b, and letB(x) be samples from b(x). To check the grammaticality of a sentence, we apply the local optimum criterion (Eq 4) usingB(x):</p><formula xml:id="formula_3">LM-Critic(x) = ? ? ? 1 if x = argmax x ?B(x) p(x ) 0 otherwise.<label>(5)</label></formula><p>There are three decisions for implementing LM-Critic: choice of a pretrained LM, perturbation function b, and sampling method of perturbations.</p><p>Pretrained LM. We experiment with various sizes of GPT2 models <ref type="bibr" target="#b42">(Radford et al., 2019</ref>)-GPT2 (117M parameters), GPT2-medium (345M), GPT2-large (774M), GPT2-xl (1.6B). These LMs were trained on a large set of web text (40GB).</p><p>Perturbation function. We study three variants:</p><p>? ED1. Given a sentence, we generate edit-distance one (ED1) perturbations in the character space. Following prior works in typo generation <ref type="bibr" target="#b41">(Pruthi et al., 2019;</ref><ref type="bibr" target="#b16">Jones et al., 2020)</ref>, we randomly insert a lowercase letter, delete a character, replace a character, or swap two adjacent characters. ? ED1 + Word-level heuristics (all). ED1 can cover most of the character-level typos but may not cover word-level grammatical errors, such as missing an article. Besides ED1, here we include heuristics for word-level perturbations used in <ref type="bibr" target="#b0">Awasthi et al. (2019)</ref>, which randomly inserts, deletes, or replaces a word based on its dictionary. Please refer to Awasthi et al. for more details. ? ED1 + Word-level heuristics.</p><p>We noticed that the above word-level heuristics include perturbations that may alter the meaning of the original sentence (e.g., deleting/inserting "not"). Therefore, we remove such heuristics here.</p><p>Sampling perturbations. As the output space of the perturbation function b is large, we obtain samples from b(x) to beB(x). We experiment with random sampling with sizes of 100, 200 and 400, motivated by the finding that with the GPT2 models, a batch size of 100 sentences can fit into a single GPU of 11GB memory. Other (potentially more efficient) sampling methods include gradient-based sampling which picks perturbation sentences in a direction that increases the sentence probability (analogous to adversarial perturbations; <ref type="bibr" target="#b45">Szegedy et al. (2013)</ref>; <ref type="bibr" target="#b50">Wallace et al. (2019)</ref>), but we focus on random sampling in this work.</p><p>The advantage of LM-Critic is that as LMs can be trained on a wide range of unlabeled corpora, it is unsupervised and usable in various domains of text. Pretrained LM How often p(x bad ) &lt; p(x good )?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT2</head><p>94.7% GPT2-medium 95.0% GPT2-large 95.9% GPT2-xl 96.0% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Empirical analysis</head><p>We study how well our LM-Critic works in practice. We prepare an evaluation data for judging grammaticality in ?3.3.1. We first perform a simple check to make sure that LMs' probability score correlates with grammaticality ( ?3.3.2). We then study the performance of LM-Critic judging grammaticality ( ?3.3.3). The analysis we conduct in this section is just an intrinsic evaluation of LM-Critic. Our main goal is to use LM-Critic with BIFI for learning GEC, which we describe and evaluate in ?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Evaluation data</head><p>To gain insights into how well LM-Critic judges grammaticality, we prepare a simple evaluation data consisting of (x bad ,x good ) sentence pairs. As experimenting with multiple datasets is desired in GEC , we construct a combined evaluation set from the dev sets of multiple GEC benchmarks, GMEG-wiki <ref type="bibr" target="#b32">(Napoles et al., 2019)</ref>, GMEG-yahoo, and BEA-2019 , which span the domains of Wikipedia, Yahoo!Answers, and essay/learner English. Specifically, we sampled ?600 labeled pairs of (x bad ,x good ) in total from the three benchmarks. We filter out examples where x bad = x good in this process. We acknowledge that while we use annotated (x bad ,x good ) pairs for the evaluation here, this does not fully match the way LM-Critic will be used in BIFI ( ?4), where the critic is run on unlabeled sentences; our study here is just to gain intrinsic insights into LM-Critic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Analysis of LM probability</head><p>Using the evaluation data, we first make sure that pretrained LMs' probability correlates with grammaticality. <ref type="figure" target="#fig_1">Figure 2</ref> shows a histogram for the probability log p(x) of grammatical (green) and ungrammatical (red) sentences computed by  </p><formula xml:id="formula_4">Examples of p(x bad ) &gt; p(x good ) (Comma)</formula><p>x bad : The video was filmed on January 22 and is set to premiere on February 22.</p><p>x good : The video was filmed on January 22, and is set to premiere on February 22.</p><p>(Quotation) x bad : The blast could be heard across the whole city centre.</p><p>x good : The blast could be heard across the whole city center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of</head><formula xml:id="formula_5">p(x ) &gt; p(x good ), x ?B(x good ) (Singular/plural)</formula><p>x :</p><p>They are affiliated to either the state boards or to national education boards.</p><p>x good : They are affiliated to either the state board or to national education boards.</p><p>(Tense)</p><p>x :</p><p>As well as touring Europe, they tour with such acts as Green Day.</p><p>x good : As well as touring Europe, they toured with such acts as Green Day. GPT2. In <ref type="table" target="#tab_0">Table 1</ref>, we study how often pretrained LMs actually assign a higher probability to x good than x bad on the evaluation pairs (x bad ,x good ). We find that the LMs satisfy p(x bad ) &lt; p(x good ) about 94% of the time, with a slight increase when using a larger model (from GPT2 to GPT2-xl). We find that the remaining pairs with p(x bad ) &gt; p(x good ) consist mostly of cases where x good adds commas or quotations to x bad (see <ref type="table" target="#tab_4">Table 3</ref> top for examples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Performance of LM-Critic</head><p>In ?3.3.2 we simply made sure that pretrained LMs' probability correlates with grammaticality. Here we study LM-Critic's performance of judging bad/good sentences, on the evaluation set {(x bad (i) ,x good (i) )}. We treat the label of x bad 's and x good 's to be "bad" and "good", respectively, and measure the precision (P), recall (R), F 0.5 of LM-Critic recognizing "bad" and "good". Denoting the critic as c, precision and recall for "bad" are defined as</p><formula xml:id="formula_6">P (bad) = |{x : c(x) = 0}| ? |{x bad }| |{x : c(x) = 0}| ,<label>(6)</label></formula><formula xml:id="formula_7">R (bad) = |{x : c(x) = 0}| ? |{x bad }| |{x bad }| .<label>(7)</label></formula><p>P (good) and R (good) are defined similarly. F 0.5 score is a combined metric of P and R that is commonly used in grammatical error detection/correction literature.</p><p>Baseline critic. First, as a baseline, we evaluate the critic based on absolute threshold, described in Eq 2. We set the threshold ? as the average probability of all good and bad sentences in the evaluation data. This method achieves 54.3 F 0.5 <ref type="bibr">(bad)</ref> and 56.0 F 0.5 (good) , using GPT2.</p><p>Proposed LM-Critic. <ref type="table" target="#tab_2">Table 2</ref> shows the results of our proposed LM-Critic, using different choices of a perturbation function, sample size, and pretrained LM. Recall that LM-Critic predicts "bad" correctly if it finds a perturbed sentence with higher probability, and predicts "good" correctly if the input has the highest probability among the sampled perturbations.</p><p>? Perturbation function b <ref type="table">(top table)</ref>. We set the pretrained LM to be GPT2 and the perturbation sample size to be 100, and vary the perturbation function. We find that when the perturbation space is small ("ED1"), LM-Critic may make false predictions of "good", leading to low P <ref type="bibr">(good)</ref> and low R <ref type="bibr">(bad)</ref> . When the perturbation space is large ("ED1 + word(all)"), LM-Critic may make false predictions of "bad", leading to low R (good) and low P <ref type="bibr">(bad)</ref> . "ED1 + word" is the most balanced and achieves the best F 0.5 ; henceforth, we use this perturbation method for all our experiments. Overall, our LM-Critic outperforms the baseline critic by substantial margins. ? Sample size of perturbations <ref type="table">(middle table)</ref>.</p><p>We set the LM to be GPT2 and vary the perturbation sample size. Increasing the sample size tends to improve P (good) and R <ref type="bibr">(bad)</ref> , and improve the overall F 0.5 performance slightly. ? Pretrained LM <ref type="table">(bottom table)</ref>. We vary the LM. Increasing the LM size makes slight or no improvement in F 0.5 on the dataset we used.</p><p>We also analyze when LM-Critic fails. When LM-Critic predicts a false "good" (labeled "bad" but predicted "good"), it is commonly because of p(x bad ) &gt; p(x good ) (as described in ?3.3.2; <ref type="table" target="#tab_4">Table  3</ref> top), or perturbation sampling not hitting a better version of the input x bad . When LM-Critic predicts a false "bad" (labeled "good" but predicted "bad"), it is because some perturbation x ?B(x good ) yields p(x ) &gt; p(x good ). Common examples are the change of tense or singular / plural (see <ref type="table" target="#tab_4">Table  3</ref> bottom for examples). This indicates that even if we use a conservative edit-distance like ED1, there may be unnecessary perturbations (tense, singular/plural) that pretrained LMs prefer, which is a limitation of our current LM-Critic.</p><p>The analysis done in this section is an intrinsic evaluation of LM-Critic. Our main goal is to use LM-Critic with BIFI for learning GEC, which we describe in ?4. While LM-Critic is not perfect in itself as we have seen in this section (it is an approximate critic), we will show that it is helpful for obtaining realistic paired data to improve the downstream GEC performance. Henceforth, we use the "ED1 + word" perturbation, a sample size of 100, and GPT2 for our LM-Critic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning GEC with LM-Critic</head><p>Break-It-Fix-It (BIFI; Yasunaga and Liang <ref type="formula" target="#formula_0">(2021))</ref> is an existing method that uses a critic to obtain realistic paired data from unlabeled data. BIFI was originally studied in the source code repair task where an oracle critic (e.g., compiler) exists, but there is no oracle critic in GEC. Here, we propose to apply BIFI to the GEC task by using LM-Critic as the critic ( ?4.1), and evaluate this approach on GEC benchmarks ( ?4.2). The difference from the original BIFI is that our task is GEC rather than code repair, and we use an approximate critic (i.e., LM-Critic) instead of an oracle critic (i.e., compiler).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Approach</head><p>Our goal is to learn a fixer f that maps an ungrammatical sentence x bad into the grammatical version x good . A common method to obtain paired data for GEC from unlabeled text is to heuristically corrupt good sentences (synthetic data) <ref type="bibr" target="#b0">(Awasthi et al., 2019;</ref><ref type="bibr" target="#b24">Kiyono et al., 2019)</ref>. However, such synthetic errors do not match the distributions of real grammatical errors humans make, which may result in accuracy drops <ref type="bibr" target="#b9">(Daume III and Marcu, 2006)</ref>. To mitigate this mismatch, BIFI aims to obtain more realistic paired data and train the fixer on it.</p><p>Specifically, BIFI takes as inputs: ? Critic c, for which we use LM-Critic ? Unlabeled data D unlabel . Using the critic c, examples in D unlabel can be split into bad ones D bad = {x | x ? D unlabel , c(x) = 0} and good ones D good = {y | y ? D unlabel , c(y) = 1} ? Initial fixer f 0 , which could be trained on synthetic data (unsupervised setting; ?4.2.2) or labeled data (supervised setting; ?4.2.3) and improves the fixer by performing a cycle of data generation and training: (1) we apply the fixer f to the bad examples D bad , which consists of real grammatical errors made by humans, and use the critic to assess if the fixer's output is good-if good, we keep the pair; (2) we train a breaker b on the resulting paired data-consequently, the breaker can generate more realistic errors than the initial synthetic data; (3) we apply the breaker to the good examples D good ; (4) we finally train the fixer on the newly-generated paired data in <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula">(3)</ref>. This cycle can be iterated to improve the fixer and the breaker simultaneously. Formally, BIFI does the following in each round k (= 1,2,...,K):</p><formula xml:id="formula_8">P (f ) k = {(x, f k?1 (x)) | x ? D bad , c(f k?1 (x)) = 1} (8) b k = TRAIN good?bad (P (f ) k ) (9) P (b) k = {(b k (y), y) | y ? D good , c(b k (y)) = 0}</formula><p>(10)</p><formula xml:id="formula_9">f k = TRAIN bad?good (P (f ) k ? P (b) k ),<label>(11)</label></formula><p>where each equation corresponds to the steps (1)-(4) in the description above. TRAIN good?bad (P) trains an encoder-decoder model that maps "good"-side examples to "bad"-side examples in paired data P, and TRAIN bad?good (P) does the reverse. Red font indicates the use of critic. The key intuition of BIFI is that thanks to the critic, (i) we can extract D bad from the unlabeled data D unlabel and incorporate realistic grammatical errors into our data (as opposed to the synthetic data), and (ii) we can verify if the "bad"-side and "good"-side of the generated pairs are actually "bad" and "good" (Eq 8, 10; red font), which improves the correctness of generated training data compared to vanilla backtranslation <ref type="bibr" target="#b43">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b25">Lample et al., 2018)</ref>. We refer readers to <ref type="bibr" target="#b57">Yasunaga and Liang (2021)</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>We study our proposed approach (BIFI with LM-Critic) on GEC benchmarks, in both unsupervised and supervised settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluation data</head><p>We evaluate on four GEC benchmarks, <ref type="bibr">CoNLL-2014</ref><ref type="bibr">test (Ng et al., 2014</ref>, BEA-2019 dev / test , GMEG-yahoo and GMEGwiki tests <ref type="bibr" target="#b32">(Napoles et al., 2019)</ref>, which span domains of essay/learner English, Wikipedia, and Ya-hoo!Answers. For CoNLL-2014, we use the official M 2 scorer <ref type="bibr" target="#b7">(Dahlmeier and Ng, 2012)</ref>, and for others we use the ERRANT metric <ref type="bibr" target="#b3">(Bryant et al., 2017)</ref>. We describe the training data separately for unsupervised ( ?4.2.2) and supervised ( ?4.2.3) settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Unsupervised setting</head><p>Setup and data. We consider the setup with no labeled training data. Existing GEC works (e.g., <ref type="bibr" target="#b0">Awasthi et al. (2019)</ref>; <ref type="bibr" target="#b39">Omelianchuk et al. (2020)</ref>) prepare synthetic paired data by heuristically corrupting sentences from the One-billion-word corpus <ref type="bibr" target="#b5">(Chelba et al., 2013)</ref>. We follow the same procedure, and train an encoder-decoder Transformer <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref> on this synthetic data to be our baseline fixer. The size of the synthetic data is 9M pairs. We then apply the BIFI training on top of the baseline fixer. As our unlabeled data to be used for BIFI, we want text that is likely to contain both ungrammatical and grammatical sentences. Hence, we take 10M sentences in total from the Yahoo!Answers corpus <ref type="bibr" target="#b60">(Zhang et al., 2015)</ref> and the Wikipedia histories data <ref type="bibr" target="#b12">(Grundkiewicz and Junczys-Dowmunt, 2014)</ref> for which we take sentences prior to revisions. 2 This unlabeled data is in the domains of two of our benchmarks (GMEG-wiki and GMEGyahoo) but not of CoNLL-2014 and BEA-2019.</p><p>Implementation details. The encoder-decoder Transformer architecture has 12 layers, 16 attention heads and hidden state size of 768. The model parameters are initialized with the BART-base release <ref type="bibr" target="#b26">(Lewis et al., 2020)</ref>, and then optimized by Adam <ref type="bibr" target="#b23">(Kingma and Ba, 2015)</ref>, with batch size of 512 sequences, learning rate 0.0001, and gradient clipping 1.0 <ref type="bibr" target="#b40">(Pascanu et al., 2013)</ref>, on a single GTX Titan X GPU. For generation, we use beam search with beam size 10. We run the BIFI algorithm for K = 1 round. The total training time takes 2 days.</p><p>Results. <ref type="table" target="#tab_6">Table 4</ref> shows the results on the four GEC benchmarks. "Transformers" is our baseline fixer, trained on the synthetic paired data. Our proposed approach ("+BIFI") outperforms the baseline by substantial margins across the benchmarks, e.g., +8 F 0.5 on GMEG-wiki and yahoo.</p><p>Since our method ("+BIFI") uses more (unlabeled) data than the baseline ("Transformer"), to be fully fair, we also conduct an experiment that controls the amount of training data seen by the model: Specifically, we apply BIFI to the baseline fixer without the critic, i.e., the model sees the same amount of newly-generated paired data as    "+BIFI" but they are not verified by LM-Critic. This system ("+BIFI with no critic") did not improve on the baseline much. These results indicate that the paired data generated by BIFI with LM-Critic is indeed more realistic and helpful than the initial synthetic data or pairs generated without LM-Critic. The improved results in this unsupervised setting suggest that our approach is especially useful in domains with no labeled GEC data for training (e.g., GMEG-wiki and yahoo; CoNLL-2014 and BEA-2019 have labeled data, which we use in ?4.2.3).</p><p>Our results also suggest that while existing BIFI assumed access to an oracle critic (i.e., compiler), an approximate critic (i.e., LM-Critic) can also help to improve model learning. Our conjecture is that as long as the LM-Critic is better than random guessing (e.g., 70 F 0.5 as shown in ?3.3.3), it is useful for improving the quality of GEC training data generated in BIFI (Eq 8, 10), which in turns improves GEC performance. An interesting future direction is to use the breaker learned in BIFI (Eq 9 for the perturbation function in LM-Critic ( ?3.2) to further improve the critic, which may in turn help BIFI as well as GEC performance, creating a positive loop of learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Supervised setting</head><p>Setup and data. We also consider the common leaderboard setup that uses labeled training data and evaluates on CoNLL-2014 and BEA-2019. We take the state-of-the-art model, GECToR <ref type="bibr" target="#b39">(Omelianchuk et al., 2020)</ref>, as our baseline fixer. Following <ref type="bibr" target="#b39">Omelianchuk et al. (2020)</ref>, GECToR is first trained on the synthetic paired data described in ?4.2.2, and is then trained on the labeled data available for the BEA-2019 task, which is the combination of:</p><p>? NUS Corpus of Learner English (NUCLE) <ref type="bibr" target="#b8">(Dahlmeier et al., 2013)</ref> ? Lang-8 Corpus of Learner English (Lang-8) <ref type="bibr" target="#b29">(Mizumoto et al., 2011;</ref><ref type="bibr" target="#b46">Tajiri et al., 2012)</ref> ? FCE dataset <ref type="bibr" target="#b55">(Yannakoudakis et al., 2011)</ref> ? Write &amp; Improve + LOCNESS Corpus (W&amp;I + LOCNESS)  They are all in the domain of <ref type="bibr">CoNLL-2014 and</ref><ref type="bibr">BEA-2019 (learner/essay English)</ref>. The total size of the labeled data is 1M pairs.</p><p>We then apply the BIFI training on top of GECToR. As our unlabeled data to be used for BIFI, we use 10M sentences taken from Yahoo! Answers and Wikipedia histories (same as ?4.2.2). Implementation details. We use the same hyperparameters and training procedures for GECToR as in <ref type="bibr" target="#b39">Omelianchuk et al. (2020)</ref>. We run the BIFI algorithm for K = 1 round. The total training time takes 4 days, on a single GTX Titan X GPU.</p><p>Results. <ref type="table" target="#tab_7">Table 5</ref> shows our results on CoNLL-2014 test and BEA-2019 test, along with existing systems on the leaderboard. Our approach ("+BIFI") provides an additional boost over our base model ("GECToR"). This suggests that BIFI with LM-Critic is helpful not only in the unsupervised setting but also when a substantial amount of labeled data (1M pairs) is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Analysis</head><p>Varying the amount of labeled data. We have studied GEC results when we have no labeled data ( ?4.2.2) and when we use all the labeled data (1M (a) Pairs generated by synthetic corruption</p><p>x bad : We look forward the to better treatments in the future.</p><p>x good : We look forward to better treatments in the future.</p><p>x bad : The president-elect stayed away so as not to foregin matters until Bush.</p><p>x good : The president-elect stayed away so as not to complicate matters for Bush.</p><p>(b) Pairs generated by BIFI without LM-Critic</p><p>x bad : If anyone is interested, here's the kink.</p><p>x good : If anyone is interested, here's the kinks.</p><p>x bad : If you can't find a match yourself, horse trader will helps. x good : If you can't find a match yourself, horse traders will help.</p><p>(c) Pairs generated by BIFI with LM-Critic (Ours)</p><p>x bad : First Light is a award-winning novel by Sunil Gangopadhyay.</p><p>x good : First Light is an award-winning novel by Sunil Gangopadhyay.</p><p>x bad : Except latter, the rivers are in underground tubes and not visible.</p><p>x good : Except for the latter, the rivers are in underground tubes and not visible. <ref type="table">Table 6</ref>: Examples of paired data generated by (a) synthetic corruption, (b) BIFI without critic, and (c) BIFI with LM-Critic. (a) tends to deviate from the type of grammatical errors humans make. (b) tends to have pairs where xgood is broken (e.g., the first pair) or xbad is already grammatical, as pairs are not verified by a critic. (c) is the most realistic.</p><p>(Input) The system is designed to use amplitude comparision for height finding.</p><p>(Baseline) The system is designed to use amplitude comparison for height find.</p><p>(BIFI) The system is designed to use amplitude comparison for height finding.</p><p>(Input) Lugu Lake, set in the subalpine zone in Hengduan is a landscape of pine-covered ecoregion. (Baseline) Lugu Lake, set in the subalpine zone in Hengduan, is their landscape of pine-covered ecoregion. (BIFI) Lugu Lake, set in the subalpine zone in Hengduan, is a landscape of pine-covered ecoregion. 3). Here we analyze the interpolation.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we show the GEC performance (F 0.5 ) on the BEA-2019 dev set, when varying the amount of labeled data available for training from 0 to 1M. The blue line indicates a Transformer model first trained on the synthetic data and then trained on the available labeled data, which is our baseline. The orange line indicates that this baseline model is further trained with BIFI. We observe that BIFI outperforms the baseline consistently and is particularly helpful in low-resource regimes.</p><p>Pairs generated by BIFI. We quantitatively saw in ?4.2.2 that the paired data generated by BIFI is helpful for learning GEC. Here we provide qualitative examples to compare the paired data generated by (a) synthetic corruption, (b) BIFI without critic, and (c) BIFI with LM-Critic <ref type="table">(Table 6</ref>). We observe that (a) tends to deviate from the type of grammatical errors humans make (e.g., inserting /replacing words arbitrarily); (b) tends to have pairs where x good is broken (e.g., the first pair in Table 6(b)) or x bad is actually grammatical, as pairs are not verified by a critic; and (c) is the most realistic.</p><p>GEC model outputs. In <ref type="table" target="#tab_8">Table 7</ref>, we analyze examples where the baseline fixer trained on synthetic data ("Transformer") fails but our model ("+BIFI") succeeds. We find that the baseline tends to make unnecessary edits (e.g., changing verb inflection or articles), due to the heuristics used when generating synthetic data. In contrast, BIFI achieves higher precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work and discussion</head><p>Grammatical error correction (GEC). GEC models are commonly trained from human-labeled data <ref type="bibr" target="#b37">(Nicholls, 2003;</ref><ref type="bibr" target="#b8">Dahlmeier et al., 2013;</ref><ref type="bibr" target="#b55">Yannakoudakis et al., 2011;</ref>, or synthetic data generated by heuristically corrupting unlabeled sentences <ref type="bibr" target="#b0">(Awasthi et al., 2019;</ref><ref type="bibr" target="#b61">Zhao et al., 2019;</ref><ref type="bibr" target="#b13">Grundkiewicz et al., 2019;</ref><ref type="bibr">Katsumata and Komachi, 2019;</ref><ref type="bibr" target="#b39">Omelianchuk et al., 2020)</ref>. Several works aim to improve the methods for generating paired data, such as learning a breaker from existing labeled data <ref type="bibr" target="#b27">(Lichtarge et al., 2019)</ref>, applying backtranslation <ref type="bibr" target="#b43">(Sennrich et al., 2016)</ref> to GEC <ref type="bibr" target="#b54">(Xie et al., 2018;</ref><ref type="bibr" target="#b24">Kiyono et al., 2019)</ref>, and synthesizing extra paired data by comparing model predictions and references <ref type="bibr" target="#b11">(Ge et al., 2018)</ref>. Different from the above works, our method (i) does not require labeled data (works for both unsupervised and supervised settings), and (ii) uses LM-Critic to filter the "bad"-side and "good"-side of generated pairs.</p><p>Automatic text evaluation. Popular metrics used to assess the quality of text in GEC include GLEU <ref type="bibr" target="#b33">(Napoles et al., 2015</ref><ref type="bibr" target="#b35">(Napoles et al., , 2017</ref>, M 2 <ref type="bibr" target="#b7">(Dahlmeier and Ng, 2012)</ref>, ERRANT <ref type="bibr" target="#b3">(Bryant et al., 2017)</ref> and I-measure <ref type="bibr" target="#b10">(Felice and Briscoe, 2015)</ref>. While these methods require reference text to compare to, LM-Critic does not. Several prior works also study reference-less methods to assess grammaticality of text: In contrast, LM-Critic only requires an LM, which is unsupervised and can be pretrained on various domains of unlabeled corpora.</p><p>Pretrained LM for text evaluation. Several works use pretrained LMs for text evaluation. For reference-based metrics, <ref type="bibr" target="#b59">Zhang et al. (2020)</ref> use an LM's embeddings to measure the similarity between input text and reference text. For reference-less metrics, several works <ref type="bibr" target="#b18">(Kann et al., 2018;</ref><ref type="bibr" target="#b44">Stahlberg et al., 2019)</ref> use an LM's probability as a fluency score of text. While this provides a continuous score for fluency, it in itself cannot classify grammatical / ungrammatical sentences. Our LM-Critic goes a step further to consider the local optimum criterion for classifying grammaticality. The reason we want a classifier (critic) is that we work on unsupervised learning of GEC. In the unsupervised setting, there is a distributional shift problem-the synthetically-generated paired data does not match the distribution of grammatical errors humans make. BIFI is a solution for obtaining realistic paired data in an unsupervised way, but it requires a critic. This led us to design a critic for GEC in this work. We note that LM-Critic is not meant to replace existing evaluation metrics for GEC, but rather is an approximate critic to assess grammaticality and help the learning of GEC. Separately, several works <ref type="bibr" target="#b47">(Tenney et al., 2019;</ref><ref type="bibr" target="#b14">Hewitt and Manning, 2019;</ref><ref type="bibr" target="#b56">Yasunaga and Lafferty, 2019;</ref><ref type="bibr" target="#b4">Cao et al., 2020)</ref> induce grammar or syntactic structures from LMs, suggesting that LMs can learn about grammaticality in an unsupervised way. As this capacity is likely to grow with the size of LMs <ref type="bibr" target="#b42">(Radford et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al., 2020;</ref><ref type="bibr" target="#b1">Kaplan et al., 2020)</ref>, we think that how to leverage pretrained LMs for GEC will become an increasingly important research problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented LM-Critic, a method that uses a pretrained language model (LM) as a critic for assessing sentence grammaticality. Using LM-Critic and the BIFI algorithm, we learn grammatical error correction (GEC) by generating realistic training data from unlabeled text. Notably, our approach does not require labeled data, and can also be viewed as an unsupervised method to turn a (GPT2scale) pretrained LM into an actual GEC system. Using multiple GEC datasets, we showed that our approach achieves strong performance on unsupervised GEC, suggesting the promise of our method for domains and languages with no labeled GEC data. We hope this work opens up research avenues in LM-based critics and unsupervised GEC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Idea behind LM-Critic: Local optimum criterion Illustration of LM-Critic. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Probability of grammatical (green) and ungrammatical (red) sentences, computed by a pretrained LM (GPT2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>GEC results (y-axis) when varying the amount of labeled data available for training (x-axis). BIFI is particularly helpful in low-resource regimes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b51">Wan et al. (2005)</ref>;<ref type="bibr" target="#b30">Mutton et al. (2007)</ref>;<ref type="bibr" target="#b48">Vadlapudi and Katragadda (2010)</ref> use part-ofspeech (POS) tagger or parser predictions to score grammaticality; Napoles et al. (2016); Warstadt et al. (2018); Katinskaia et al. (2019); Niu and Penn (2020) train grammatical error detection (GED) or acceptability judgement systems. However, these works require POS taggers, parsers or GED systems trained on labeled data, which may not scale or generalize well beyond the domain of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>How well sentence probability returned by pretrained LMs correlates with grammaticality empirically.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ED1 58.7 90.1 63.1 78.8 36.8 64.2 ED1 + word(all) 69.7 10.2 32.2 51.5 95.5 56.7 ED1 + word 68.4 75.5 69.7 72.7 65.1 71.1</figDesc><table><row><cell>Perturbation</cell><cell cols="2">Recognize "Good" Recognize "Bad" P R F 0.5 P R F 0.5</cell></row><row><cell>Sample size</cell><cell cols="2">Recognize "Good" Recognize "Bad" P R F 0.5 P R F 0.5</cell></row><row><cell>100</cell><cell cols="2">68.4 75.5 69.7 72.7 65.1 71.1</cell></row><row><cell>200</cell><cell cols="2">71.3 71.5 71.4 71.4 71.3 71.4</cell></row><row><cell>400</cell><cell cols="2">72.6 68.7 71.8 70.3 74.0 71.0</cell></row><row><cell>Pretrained LM</cell><cell cols="2">Recognize "Good" Recognize "Bad" F 0.5 F 0.5</cell></row><row><cell>GPT2</cell><cell>69.7</cell><cell>71.1</cell></row><row><cell>GPT2-medium</cell><cell>69.9</cell><cell>71.0</cell></row><row><cell>GPT2-large</cell><cell>70.3</cell><cell>71.3</cell></row><row><cell>GPT2-xl</cell><cell>69.9</cell><cell>71.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of LM-Critic, when using different choices of a perturbation function, sample size, and pretrained LM described in ?3.2. (Top) We set the LM to be GPT2 and the perturbation sample size to be 100, and vary the perturbation function b. "ED1 + word" achieves the best F0.5. Henceforth, we use this perturbation function. (Middle) We set the LM to be GPT2 and vary the perturbation sample size. Increasing the sampling size improves the performance slightly. (Bottom) We vary the LM. Increasing the LM size makes slight or no improvement in F0.5 on the dataset we used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>x bad : Uprising is a 1980 roots reggae album by Bob Marley &amp; The Wailers. x good : "Uprising" is a 1980 roots reggae album by Bob Marley &amp; The Wailers.</figDesc><table><row><cell>(British spelling)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Failure cases of LM-Critic. (Top) GPT2 assigns a higher probability to bad sentences. (Bottom) our neighborhood function ("ED1 + word") includes sentences with a higher LM probability than the original good sentence.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>GEC results in the unsupervised setting ( ?4.2.2). "Transformers" is trained on synthetic paired data as in<ref type="bibr" target="#b0">Awasthi et al. (2019)</ref>. If we train it on more realistic paired data generated by BIFI (bottom row), it achieves improved results.</figDesc><table><row><cell>GEC system</cell><cell>Ens.</cell><cell cols="4">CoNLL-2014 (test) BEA-2019 (test) P R F 0.5 P R F 0.5</cell></row><row><cell>GPT3 (175B) with prompting</cell><cell></cell><cell cols="4">62.4 25.0 48.0 50.8 38.2 47.6</cell></row><row><cell>Zhao et al. (2019)</cell><cell></cell><cell cols="3">67.7 40.6 59.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Awasthi et al. (2019)</cell><cell></cell><cell cols="3">66.1 43.0 59.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Kiyono et al. (2019)</cell><cell></cell><cell cols="4">67.9 44.1 61.3 65.5 59.4 64.2</cell></row><row><cell>Zhao et al. (2019)</cell><cell></cell><cell cols="3">74.1 36.3 61.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Awasthi et al. (2019)</cell><cell></cell><cell cols="3">68.3 43.2 61.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Grundkiewicz et al. (2019)</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">64.2 72.3 60.1 69.5</cell></row><row><cell>Kiyono et al. (2019)</cell><cell></cell><cell cols="4">72.4 46.1 65.0 74.7 56.7 70.2</cell></row><row><cell>Kantor et al. (2019)</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.3 58.0 73.2</cell></row><row><cell>GECToR (Omelianchuk et al., 2020)</cell><cell></cell><cell cols="4">77.5 40.1 65.3 79.2 53.9 72.4</cell></row><row><cell>GECToR (our base)</cell><cell></cell><cell cols="4">77.5 40.1 65.3 79.2 53.9 72.4</cell></row><row><cell>+ BIFI (ours)</cell><cell></cell><cell cols="4">78.0 40.6 65.8 79.4 55.0 72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>GEC results in the supervised setting with labeled data available</figDesc><table><row><cell>0.5</cell><cell>50</cell><cell></cell></row><row><cell>GEC result F</cell><cell>40</cell><cell>no BIFI BIFI</cell></row><row><cell></cell><cell>0</cell><cell>1k Labeled training data 10k 100k 1,000k</cell></row><row><cell>( ?4.2.3). "Ens." indicates an ensemble system.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Examples where the baseline fixer trained with synthetic data fails but BIFI succeeds. The baseline tends to make unnecessary edits (e.g., changing verb inflection or articles, due to heuristics used when generating synthetic data).</figDesc><table /><note>pairs) ( ?4.2.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We acknowledge that this assumption may not hold in some cases, e.g., an ungrammatical sentence may have no correction ("asdfghgfdsa"-just a random typo?) or multiple corrections ("The cat sleep."-change "sleep" to the present tense or past?). We accept this assumption considering that it is often sufficient in common GEC datasets, and leave the relaxation of the assumption for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is not paired data, as we only take sentences pre revision, not post revision.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Pang Wei Koh, Tianyi Zhang, Rodrigo Castellon, members of the Stanford P-Lambda, SNAP and NLP groups, as well as our anonymous reviewers for valuable feedback. This work was supported in part by a Funai Foundation Scholarship and NSF CAREER Award IIS-1552635.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility</head><p>Code and data are available at https://github.com/michiyasunaga/ LM-Critic. Experiments are available at https://worksheets. codalab.org/worksheets/ 0x94456a63e1ee4ccfaabdc7f6a356cc82.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parallel iterative edit models for local sequence transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasna</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabyasachi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are fewshot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The bea-2019 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>?istein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic annotation and evaluation of error types for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised parsing via constituency tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multilayer convolutional encoder-decoder neural network for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner english: The nus corpus of learner english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth workshop on innovative use of NLP for building educational applications</title>
		<meeting>the eighth workshop on innovative use of NLP for building educational applications</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial Intelligence research</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards a standard evaluation method for grammatical error detection and correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fluency boost learning and inference for neural grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The wiked error corpus: A corpus of corrective wikipedia edits and its application to grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural grammatical error correction systems with unsupervised pre-training on synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A nested attention neural hybrid model for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust encodings: A framework for combating adversarial typos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Approaching neural grammatical error correction as a low-resource machine translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubha</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sentence-level fluency evaluation: References help, but can be spared!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to combine grammatical error corrections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edo</forename><surname>Cohen-Karlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Menczel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">and Dario Amodei. 2020. Scaling laws for neural language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple admissibility in language learning: Judging grammaticality using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anisia</forename><surname>Katinskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sardana</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Yangarber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 7th Workshop on Balto-Slavic Natural Language Processing Proceedings of the Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">2019. (almost) unsupervised grammatical error correction using synthetic comparable corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoru</forename><surname>Katsumata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An empirical study of incorporating pseudo data into grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Corpora generation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Lichtarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-corpora evaluation and analysis of grammatical error correction models-is single-corpus evaluation enough?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mining revision log of language learning sns for automated japanese error correction of second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gleu: Automatic evaluation of sentencelevel fluency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grammatical error correction in low-resource scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>N?plava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Noisy User-generated Text</title>
		<meeting>the 5th Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Enabling robust grammatical error correction in new domains: Data sets, metrics, and analyses. Transactions of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>N?dejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ground truth for grammatical error correction metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">There&apos;s no comparison: Referenceless evaluation metrics in grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Jfleg: A fluency corpus and benchmark for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Chapter of the Association for Computational Linguistics (EACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The conll-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The cambridge learner corpus: Error coding and analysis for lexicography and elt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Corpus Linguistics</title>
		<meeting>the Corpus Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grammaticality and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingcheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</title>
		<meeting>the First Workshop on Evaluation and Comparison of NLP Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gector-grammatical error correction: Tag, not rewrite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostiantyn</forename><surname>Omelianchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Atrasevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Chernodub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Skurzhanskyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 15th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combating adversarial misspellings with robust word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>OpenAI blog</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural grammatical error correction with finite state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tense and aspect error correction for esl learners using global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshikazu</forename><surname>Tajiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bert rediscovers the classical nlp pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On automated evaluation of readability of summaries: Capturing grammaticality, focus, structure and coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravikiran</forename><surname>Vadlapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Katragadda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 student research workshop</title>
		<meeting>the NAACL HLT 2010 student research workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Universal adversarial triggers for attacking and analyzing nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Searching for grammaticality: Propagating dependencies in the viterbi algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth European Workshop on Natural Language Generation (ENLG-05)</title>
		<meeting>the Tenth European Workshop on Natural Language Generation (ENLG-05)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09727</idno>
		<title level="m">Neural language correction with character-based attention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Noising and denoising natural language: Diverse backtranslation for grammar correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Genthial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading esol texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Topiceq: A joint topic and mathematical equation model for scientific texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Break-It-Fix-It: Unsupervised Learning for Program Repair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Grammatical error correction using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
