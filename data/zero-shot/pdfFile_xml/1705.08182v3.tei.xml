<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unmasking the abnormal events in video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>24 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Smeureanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>24 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>24 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Popescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SecurifAI</orgName>
								<address>
									<addrLine>24 Mircea Vod?</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unmasking the abnormal events in video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel framework for abnormal event detection in video that requires no training sequences. Our framework is based on unmasking, a technique previously used for authorship verification in text documents, which we adapt to our task. We iteratively train a binary classifier to distinguish between two consecutive video sequences while removing at each step the most discriminant features. Higher training accuracy rates of the intermediately obtained classifiers represent abnormal events. To the best of our knowledge, this is the first work to apply unmasking for a computer vision task. We compare our method with several state-of-the-art supervised and unsupervised methods on four benchmark data sets. The empirical results indicate that our abnormal event detection framework can achieve state-of-the-art results, while running in real-time at 20 frames per second.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Abnormal event detection in video is a challenging task in computer vision, as the definition of what an abnormal event looks like depends very much on the context. For instance, a car driving by on the street is regarded as a normal event, but if the car enters a pedestrian area, this is regarded as an abnormal event. A person running on a sports court (normal event) versus running outside from a bank (abnormal event) is another example. Although what is considered abnormal depends on the context, we can generally agree that abnormal events should be unexpected events <ref type="bibr" target="#b9">[10]</ref> that occur less often than familiar (normal) events. As it is generally impossible to find a sufficiently representative set of anomalies, the use of traditional supervised learning methods is usually ruled out. Hence, most abnormal event detection approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref> learn a model of familiarity from a given training video and label events as abnormal if they deviate from the model. In this paper, we consider an even more challenging setting, in which no additional training sequences are available <ref type="bibr" target="#b5">[6]</ref>. As in this setting we cannot build a model in advance and find deviations from it, our approach is completely unsupervised, as we briefly explain next. Our method labels <ref type="bibr">Figure 1</ref>. Our anomaly detection framework based on unmasking <ref type="bibr" target="#b11">[12]</ref>. The steps are processed in sequential order from (A) to (H). Best viewed in color. a short-lasting event as abnormal if the amount of change from the immediately preceding event is substantially large. We quantify the change as the training accuracy of a linear classifier applied on a sliding window that comprises both the preceding and the currently examined event, as illustrated in <ref type="figure">Figure 1</ref>. We consider that the first half of the window frames are labeled as normal and take them as reference. We suppose the second half are labeled as abnormal, but we seek to find if this hypothesis is indeed true. We extract both motion and appearance features from the frames, and train a binary classifier with high regulariza-tion to distinguish between the labeled frames. We retain the training accuracy of the classifier and repeat the training process by eliminating some of the best features. This process is known as unmasking <ref type="bibr" target="#b11">[12]</ref> and it was previously used for authorship verification of text documents. To the best of our knowledge, we are the first to apply unmasking for a computer vision task. After a certain number of iterations with unmasking, we can build a profile (plot) with the collected accuracy rates in order to assess if the current event, represented by the second half of the frames, does contain enough changes to consider it abnormal. Intuitively, if the change is significant, the classification accuracy should stay high even after eliminating a certain amount of discriminating features. Otherwise, the accuracy should drop much faster as the discriminating features get eliminated, since the classifier will have a hard time separating two consecutive normal events. We estimate the accuracy profile obtained by unmasking with the mean of the accuracy rates, and consider the mean value to represent the anomaly score of the frames belonging to the current event.</p><p>We perform abnormal event detection experiments on the Avenue <ref type="bibr" target="#b14">[15]</ref>, the Subway <ref type="bibr" target="#b0">[1]</ref>, the UCSD <ref type="bibr" target="#b15">[16]</ref> and the UMN <ref type="bibr" target="#b16">[17]</ref> data sets in order to compare our unsupervised approach with a state-of-the-art unsupervised method <ref type="bibr" target="#b5">[6]</ref> as well as several supervised methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. The empirical results indicate that we obtain better results than the unsupervised approach <ref type="bibr" target="#b5">[6]</ref> and, on individual data sets, we reach or even surpass the accuracy levels of some supervised methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. Unlike the approach of <ref type="bibr" target="#b5">[6]</ref>, our method can process the video in real-time at 20 frames per second.</p><p>We organize the paper as follows. We present related work on abnormal event detection in Section 2. We describe our unsupervised learning framework in Section 3. We present the abnormal event detection experiments in Section 4. Finally, we draw our conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Abnormal event detection is usually formalized as an outlier detection task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, in which the general approach is to learn a model of normality from training data and consider the detected outliers as abnormal events. Some abnormal event detection approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref> are based on learning a dictionary of normal events, and label the events not represented by the dictionary as abnormal. Other approaches have employed deep features <ref type="bibr" target="#b25">[26]</ref> or locality sensitive hashing filters <ref type="bibr" target="#b26">[27]</ref> to achieve better results.</p><p>There have been some approaches that employ unsupervised steps for abnormal event detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>, but these approaches are not fully unsupervised. The approach presented in <ref type="bibr" target="#b6">[7]</ref> is to build a model of familiar events from training data and incrementally update the model in an un-supervised manner as new patterns are observed in the test data. In a similar fashion, Sun et al. <ref type="bibr" target="#b21">[22]</ref> train a Growing Neural Gas model starting from training videos and continue the training process as they analyze the test videos for anomaly detection. Ren et al. <ref type="bibr" target="#b17">[18]</ref> use an unsupervised approach, spectral clustering, to build a dictionary of atoms, each representing one type of normal behavior. Their approach requires training videos of normal events to construct the dictionary. Xu et al. <ref type="bibr" target="#b25">[26]</ref> use Stacked Denoising Auto-Encoders to learn deep feature representations in a unsupervised way. However, they still employ multiple one-class SVM models to predict the anomaly scores.</p><p>To the best of our knowledge, the only work that does not require any kind of training data for abnormal event detection is <ref type="bibr" target="#b5">[6]</ref>. The approach proposed in <ref type="bibr" target="#b5">[6]</ref> is to detect changes on a sequence of data from the video to see which frames are distinguishable from all the previous frames. As the authors want to build an approach independent of temporal ordering, they create shuffles of the data by permuting the frames before running each instance of the change detection. Our framework is most closely related to <ref type="bibr" target="#b5">[6]</ref>, but there are several key differences that put a significant gap between the two approaches. An important difference is that our framework is designed to process the video online, as expected for practical real-world applications. Since the approach of Del Giorno et al. <ref type="bibr" target="#b5">[6]</ref> needs to permute the test video frames before making a decision, the test video can only be processed offline. As they discriminate between the frames in a short window and all the frames that precede the window, their classifier will require increasingly longer training times as the considered window reaches the end of the test video. In our case, the linear classifier requires about the same training time in every location of the video, as it only needs to discriminate between the first half of the frames and the second half of the frames within the current window. Moreover, we train our classifier in several loops by employing the unmasking technique. Del Giorno et al. <ref type="bibr" target="#b5">[6]</ref> use the same motion features as <ref type="bibr" target="#b14">[15]</ref>. We also use spatio-temporal cubes <ref type="bibr" target="#b14">[15]</ref> to represent motion, but we remove the Principal Component Analysis (PCA) step for two reasons. First of all, we need as many features as we can get for the unmasking technique which requires more features to begin with. Second of all, training data is required to learn the PCA projection. Different from <ref type="bibr" target="#b5">[6]</ref>, we additionally use appearance features from pre-trained convolutional neural networks <ref type="bibr" target="#b2">[3]</ref>. With all these distinct characteristics, our framework is able to obtain better performance in terms of accuracy and time, as shown in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We propose an abnormal event detection framework based on unmasking, that requires no training data. Our anomaly detection framework is comprised of eight major steps, which are indexed from A to H in <ref type="figure">Figure 1</ref>. We next provide an overview of our approach, leaving the additional details about the non-trivial steps for later. We first apply a sliding window algorithm (step A) and, for each window of 2 ? w frames (step B), we suppose that the first w frames are normal and the last w frames are abnormal (step C). After extracting motion or appearance features (step D), we apply unmasking (steps E to G) by training a classifier and removing the highly weighted features for a number of k loops. We take the accuracy rates after each loop (step F) and build the accuracy profile of the current window (step G). Abnormal events correspond to high (almost constant) accuracy profiles (depicted in red), while normal events correspond to dropping accuracy profiles (depicted in blue). We compute the anomaly score for the last w frames as the mean of the retained accuracy rates (step H).</p><p>For the sake of simplicity, there are several important aspects that are purposely left out in <ref type="figure">Figure 1</ref>. First of all, we divide the frames into 2 ? 2 spatial bins, thus obtaining four sub-videos, which we process individually through our detection framework until step G. Hence, for each video frame, we produce four anomaly scores, having one score per bin. Before step H, we assign the score of each frame as the maximum of the four anomaly scores corresponding to the 2 ? 2 bins. Second of all, we apply the framework independently using motion features on one hand and appearance features on the other. For each kind of features, we divide the video into 2 ? 2 bins and obtain a single anomaly score per frame as detailed above. To combine the anomaly scores from motion and appearance features, we employ a late fusion strategy by averaging the scores for each frame, in step H. Third of all, we take windows at a predefined interval s (stride), where the choice of s can generate overlapping windows (e.g. s = 1 and w = 10). In this situation, the score of a frame is obtained by averaging the anomaly scores obtained after processing every separate window that includes the respective frame in its second half. We apply a Gaussian filter to temporally smooth the final anomaly scores. We present additional details about the motion and appearance features (step D) in Section 3.1, and about the unmasking approach (steps E to G) in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Features</head><p>Unlike other approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, we apply the same steps in order to extract motion and appearance features from video, irrespective of the data set. Motion features. Given the input video, we resize all frames to 160 ? 120 pixels and uniformly partition each frame to a set of non-overlapping 10 ? 10 patches. Corresponding patches in 5 consecutive frames are stacked together to form a spatio-temporal cube, each with resolution 10 ? 10 ? 5. We then compute 3D gradient features on each spatio-temporal cube and normalize the resulted fea-ture vectors using the L 2 -norm. To represent motion, we essentially employ the same approach as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, but without reducing the feature vector dimension from 500 to 100 components via PCA. This enables us to keep more features for unmasking. Since unmasking is about gradually eliminating the discriminant features, it requires more features to begin with. As <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, we eliminate the cubes that have no motion gradients (the video is static in the respective location). We divide the frames into 2?2 spatial bins of 80?60 pixels each, obtaining at most 48 cubes per bin. Bins are individually processed through our detection framework. It is important to mention that each spatio-temporal cube is treated as an example in step E ( <ref type="figure">Figure 1</ref>) of our framework. Although we classify spatio-temporal cubes as <ref type="bibr" target="#b5">[6]</ref>, we assign the anomaly score to the frames, not the cubes.</p><p>Appearance features. In many computer vision tasks, for instance predicting image difficulty <ref type="bibr" target="#b8">[9]</ref>, higher level features, such as the ones learned with convolutional neural networks (CNN) <ref type="bibr" target="#b12">[13]</ref> are the most effective. To build our appearance features, we consider a pre-trained CNN architecture able to process the frames as fast as possible, namely VGG-f <ref type="bibr" target="#b2">[3]</ref>. Considering that we want our detection framework to work in real-time on a standard desktop computer, not equipped with expensive GPU, the VGG-f <ref type="bibr" target="#b2">[3]</ref> is an excellent choice as it can process about 20 frames per second on CPU. We hereby note that better anomaly detection performance can probably be achieved by employing deeper CNN architectures, such as VGG-verydeep <ref type="bibr" target="#b20">[21]</ref>, GoogLeNet <ref type="bibr" target="#b22">[23]</ref> or ResNet <ref type="bibr" target="#b7">[8]</ref>.</p><p>The VGG-f model is trained on the ILSVRC benchmark <ref type="bibr" target="#b18">[19]</ref>. It is important to note that fine-tuning the CNN for our task is not possible, as we are not allowed to use training data in our unsupervised setting. Hence, we simply use the pre-trained CNN to extract deep features as follows. Given the input video, we resize the frames to 224 ? 224 pixels. We then subtract the mean imagine from each frame and provide it as input to the VGG-f model. We remove the fully-connected layers (identified as fc6, fc7 and softmax) and consider the activation maps of the last convolutional layer (conv5) as appearance features. While the fullyconnected layers are adapted for object recognition, the last convolutional layer contains valuable appearance and pose information which is more useful for our anomaly detection task. Ideally, we would like to have at least slightly different representations for a person walking versus a person running. From the conv5 layer, we obtain 256 activation maps, each of 13?13 pixels. As for the motion features, we divide the activation maps into 2 ? 2 spatial bins of 7 ? 7 pixels each, such that the bins have a one-pixel overlap towards the center of the activation map. For each bin, we reshape the bins into 49 dimensional vectors and concatenate the vectors corresponding to the 256 filters of the conv5 layer into a single feature vector of 12544 (7 ? 7 ? 256) components.</p><p>The final feature vectors are normalized using the L 2 -norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Change Detection by Unmasking</head><p>The unmasking technique <ref type="bibr" target="#b11">[12]</ref> is based on testing the degradation rate of the cross-validation accuracy of learned models, as the best features are iteratively dropped from the learning process. Koppel et al. <ref type="bibr" target="#b11">[12]</ref> offer evidence that this unsupervised technique can solve the authorship verification problem with very high accuracy. We modify the original unmasking technique by considering the training accuracy instead of the cross-validation accuracy, in order to use this approach for online abnormal event detection in video. We apply unmasking for each window of 2?w frames at a stride s, where w and s are some input parameters of our framework. Our aim is to examine if the last w frames in a given window represent an abnormal event or not. To achieve this purpose, we compare them with the first w (reference) frames in the window. We assume that the first w frames are labeled as normal and the last w frames are labeled as abnormal, and train a linear classifier to distinguish between them. By training a classifier without unmasking, we would only be able to determine if the first half of the window is distinguishable from the second half. Judging by the classifier's accuracy rate, we may consider that the last w frames are abnormal if the accuracy is high and normal if the accuracy is low. This is essentially the underlying hypothesis of <ref type="bibr" target="#b5">[6]</ref>, with the difference that they assume all preceding frames (from the entire test video) as normal. Nevetheless, we consider only the immediately preceding w frames as reference for our algorithm to run in real-time. As the number of normal (reference) samples is much lower, the classifier might often distinguish two normal events with high accuracy, which is not desired. Our main hypothesis is that if two consecutive events are normal, then whatever differences there are between them will be reflected in only a relatively small number of features, despite possible differences in motion and appearance. Therefore, we need to apply unmasking in order to determine how large is the depth of difference between the two events. Similar to <ref type="bibr" target="#b5">[6]</ref>, we train a Logistic Regression classifier with high regularization. Different from <ref type="bibr" target="#b5">[6]</ref>, we eliminate the m best features and repeat the training for a number of k loops, where m and k are some input parameters of our framework. As the most discriminant features are gradually eliminated, it will be increasingly more difficult for the linear classifier to distinguish the reference examples, that belong to the first half of the window, from the examined examples, that belong to the second half. However, if the training accuracy rate over the k loops drops suddenly, we consider that the last w frames are normal according to our hypothesis. On the other hand, if the accuracy trend is to drop slowly, we consider that the analyzed frames are abnormal. Both kinds of accuracy profiles are shown in step G of <ref type="figure">Figure 1</ref> for illus-trative purposes, but, in practice, we actually obtain a single accuracy profile for a given window. In the end, we average the training accuracy rates over the k loops and consider the average value to represent the degree of anomaly of the last w frames in the window. We thus assign the same anomaly score to all the examined frames. It is interesting to note that Del Giorno et al. <ref type="bibr" target="#b5">[6]</ref> consider the probability that an example belongs to the abnormal class, hence assigning a different score to each example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Sets</head><p>We show abnormal event detection results on four data sets. It is important to note that we use only the test videos from each data set, and perform anomaly detection without using the training videos to build a model of normality. Avenue. We first consider the Avenue data set <ref type="bibr" target="#b14">[15]</ref>, which contains 16 training and 21 test videos. In total, there are 15328 frames in the training set and 15324 frames in the test set. Each frame is 640 ? 360 pixels. Locations of anomalies are annotated in ground truth pixel-level masks for each frame in the testing videos. Subway. One of the largest data sets for anomaly detection in video is the Subway surveillance data set <ref type="bibr" target="#b0">[1]</ref>. It contains two videos, one of 96 minutes (Entrance gate) and another one of 43 minutes (Exit gate). The Entrance gate video contains 144251 frames and the Exit gate video contains 64903 frames, each with 512 ? 384 resolution. Abnormal events are labeled at the frame level. In some previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>, the first 15 minutes (22500 frames) in both videos are kept for training, although others <ref type="bibr" target="#b3">[4]</ref> have used more than half of the video for training. UCSD. The UCSD Pedestrian data set <ref type="bibr" target="#b15">[16]</ref> is perhaps one of the most challenging anomaly detection data sets. It includes two subsets, namely Ped1 and Ped2. Ped1 contains 34 training and 36 test videos with a frame resolution of 238 ? 158 pixels. There are 6800 frames for training and 7200 for testing. Pixel-level anomaly labels are provided for only 10 test videos in Ped1. All the 36 test videos are annotated at the frame-level. Ped2 contains 16 training and 12 test videos, and the frame resolution is 360 ? 240 pixels. There are 2550 frames for training and 2010 for testing. Although Ped2 contains pixel-level as well as frame-level annotations for all the test videos, most previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> have reported the pixellevel performance only for Ped1. The videos illustrate various crowded scenes, and anomalies are bicycles, vehicles, skateboarders and wheelchairs crossing pedestrian areas. UMN. The UMN Unusual Crowd Activity data set <ref type="bibr" target="#b16">[17]</ref> consists of three different crowded scenes, each with 1453, 4144, and 2144 frames, respectively. The resolution of each frame is 320 ? 240 pixels. In the normal settings people walk around in the scene, and the abnormal behavior is de- fined as people running in all directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>We employ ROC curves and the corresponding area under the curve (AUC) as the evaluation metric, computed with respect to ground truth frame-level annotations, and, when available (Avenue and UCSD), pixel-level annotations. We define the frame-level and pixel-level AUC as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> and others. At the frame-level, a frame is considered a correct detection if it contains at least one abnormal pixel. At the pixel-level, the corresponding frame is considered as being correctly detected if more than 40% of truly anomalous pixels are detected. We use the same approach as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> to compute the pixel-level AUC. The frame-level scores produced by our framework are assigned to the corresponding spatio-temporal cubes. The results are smoothed with the same filter used by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> in order to obtain our final pixel-level detections. Although many works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> include the Equal Error Rate (EER) as evaluation metric, we agree with <ref type="bibr" target="#b5">[6]</ref> that metrics such as the EER can be misleading in a realistic anomaly detection setting, in which abnormal events are expected to be very rare. Thus, we do not use the EER in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>We extract motion and appearance features from the test video sequences. We use the code available online at https://alliedel.github.io/anomalydetection/ to compute the 3D motion gradients. For the appearance features, we consider the pre-trained VGG-f <ref type="bibr" target="#b2">[3]</ref> model provided in MatCon-vNet <ref type="bibr" target="#b24">[25]</ref>. To detect changes, we employ the Logistic Regression implementation from VLFeat <ref type="bibr" target="#b23">[24]</ref>. In all the experiments, we set the regularization parameter of Logistic Regression to 0.1, and we use the same window size as <ref type="bibr" target="#b5">[6]</ref>, namely w = 10. We use the same parameters for both motion and appearance features.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we present preliminary results on five test videos from the Avenue data set to motivate our parameter and implementation choices. Regarding the CNN features, we show that slightly better results can be obtained with the conv5 features rather than the fc6 or fc7 features. An improvement of 2.5% is obtained when we include unmasking. In the unmasking procedure, we use k = 10 loops and eliminate the best m = 50 features (top 25 weighted as positive and top 25 weighted as negative). A performance gain of 1.4% can also be achieved when we divide the frames into 2 ? 2 bins instead of processing the entire frames. As for the stride, we present results with choices for s ? {1, 2, 5, 10}. The time increases as we apply unmasking and spatial bins, but we can compensate by increasing the stride. We can observe that strides up to 10 frames do not imply a considerable decrease in terms of frame-level or pixel-level AUC. Thus, we can set the stride to 5 for an optimal trade-off between accuracy and speed. We show that very good results can also be obtained with motion features.</p><p>In the end, we combine the two kinds of features and reach our best frame-level AUC (82.6%). For the speed evaluation, we independently measure the time required to extract features and the time required to predict the anomaly scores on a computer with Intel Core i7 2.3 GHz processor and 8 GB of RAM using a single core. We present the number of frames per second (FPS) in <ref type="table" target="#tab_0">Table 1</ref>. Using two cores, one for feature extraction and one for change detection by unmasking, our final model is able to process the videos at nearly 20 FPS. For the rest of the experiments, we show results with both kinds of features using a stride of 5 and bins of 2 ? 2, and perform change detection by unmasking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on the Avenue Data Set</head><p>We first compare our unmasking framework based on several types of features with an unsupervised approach <ref type="bibr" target="#b5">[6]</ref> as well as a supervised one <ref type="bibr" target="#b14">[15]</ref>. The frame-level and pixellevel AUC metrics computed on the Avenue data set are presented in <ref type="table">Table 2</ref>. Compared to the state-of-the-art unsuper-Method Frame AUC Pixel AUC Lu et al. <ref type="bibr" target="#b14">[15]</ref> 80.9% 92.9% Del Giorno et al. <ref type="bibr" target="#b5">[6]</ref> 78.3% 91.0% Ours (conv5) 80.5% 92.9% Ours (3D gradients) 80.1% 93.0% Ours (late fusion) 80.6% 93.0% <ref type="table">Table 2</ref>. Abnormal event detection results in terms of frame-level and pixel-level AUC on the Avenue data set. Our unmasking framework is compared with a state-of-the-art unsupervised approach <ref type="bibr" target="#b5">[6]</ref> as well as a supervised one <ref type="bibr" target="#b14">[15]</ref>.  vised method <ref type="bibr" target="#b5">[6]</ref>, our framework brings an improvement of 2.3%, in terms of frame-level AUC, and an improvement of 2.0%, in terms of pixel-level AUC. The results are even more impressive, considering that our framework processes the video online, while the approach proposed in <ref type="bibr" target="#b5">[6]</ref> works only in offline mode. Moreover, our frame-level and pixellevel AUC scores reach about the same level as the supervised method <ref type="bibr" target="#b14">[15]</ref>. Overall, our results on the Avenue data set are noteworthy. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the frame-level anomaly scores, for test video 4 in the Avenue data set, produced by our unmasking framework based on combining motion and appearance features using a late fusion strategy. According to the ground-truth anomaly labels, there are two abnormal events in this video. In <ref type="figure" target="#fig_0">Figure 2</ref>, we notice that our scores correlate well to the ground-truth labels, and we can easily identify both abnormal events by setting a threshold of around 0.5, without including any false positive detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Frame AUC Entrance gate Exit gate Cong et al. <ref type="bibr" target="#b4">[5]</ref> 80.0% 83.0% Del Giorno et al. <ref type="bibr" target="#b5">[6]</ref> 69.1% 82.4% Ours (conv5) 69.5% 84.7% Ours (3D gradients) 71.3% 86.3% Ours (late fusion) 70.6% 85.7% <ref type="table">Table 3</ref>. Abnormal event detection results in terms of frame-level AUC on the Subway data set. Our unmasking framework is compared with a state-of-the-art unsupervised approach <ref type="bibr" target="#b5">[6]</ref> as well as a supervised one <ref type="bibr" target="#b4">[5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on the Subway Data Set</head><p>On the Subway data set, we compare our unmasking framework with two approaches, an unsupervised one <ref type="bibr" target="#b5">[6]</ref> and a supervised one <ref type="bibr" target="#b4">[5]</ref>. The comparative results are presented in <ref type="table">Table 3</ref>. On this data set, we generally obtain better results by using motion features rather than appearance features. Our late fusion strategy is not able to bring any improvements. Nevertheless, for each and every type of features, we obtain better results than the state-of-the-art unsupervised approach <ref type="bibr" target="#b5">[6]</ref>. When we combine the features, our improvements are 1.5% on the Entrance gate video and 3.3% on the Exit gate video. Remarkably, we even obtain better results than the supervised method <ref type="bibr" target="#b4">[5]</ref> on the Exit gate video. On the other hand, our unsupervised approach, as well as the approach of Del Giorno et al. <ref type="bibr" target="#b5">[6]</ref>, obtains much lower results on the Entrance gate video.</p><p>Although there are many works that used the Subway data set in the experiments <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>, most of these works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref> did not use the frame-level AUC as evaluation metric. Therefore, we excluded these works from Method Ped1</p><p>Ped2 Frame Pixel Frame AUC AUC AUC Kim et al. <ref type="bibr" target="#b10">[11]</ref> 59.0% 20.5% 69.3% Mehran et al. <ref type="bibr" target="#b16">[17]</ref> 67.5% 19.7% 55.6% Mahadevan et al. <ref type="bibr" target="#b15">[16]</ref> 81.8% 44.1% 82.9% Cong et al. <ref type="bibr" target="#b4">[5]</ref> -46.1% -Saligrama et al. <ref type="bibr" target="#b19">[20]</ref> 92.7% --Lu et al. <ref type="bibr" target="#b14">[15]</ref> 91.8% 63.8% -Ren et al. <ref type="bibr" target="#b17">[18]</ref> 70.7% 56.2% -Xu et al. <ref type="bibr" target="#b25">[26]</ref> 92.1% 67.2% 90.8% Zhang et al. <ref type="bibr" target="#b26">[27]</ref> 87.0% 77.0% 91.0% Sun et al. <ref type="bibr" target="#b21">[22]</ref> 93  <ref type="table">Table 4</ref>. Abnormal event detection results in terms of frame-level and pixel-level AUC on the UCSD data set. Our unmasking framework is compared with several state-of-the-art supervised methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>our comparison presented in <ref type="table">Table 3</ref>. However, there is a recent work <ref type="bibr" target="#b3">[4]</ref> that provides the frame-level AUC, but it uses only 47% of the Entrance gate video for testing. For a fair comparison, we evaluated our unmasking framework based on the late fusion strategy in their setting, and obtained a frame-level AUC of 78.1%. Our score is nearly 14.6% lower than the score of 92.7% reported in <ref type="bibr" target="#b3">[4]</ref>, confirming that there is indeed a significant performance gap between supervised and unsupervised methods on the Entrance gate video. Nevertheless, in <ref type="figure" target="#fig_2">Figure 4</ref>, we can observe some interesting qualitative results obtained by our framework on the Entrance gate video. The true positive abnormal events are a person sidestepping the gate and a person jumping over the gate, while false positive detections are two persons walking synchronously and a person running to catch the train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results on the UCSD Data Set</head><p>Del Giorno et al. <ref type="bibr" target="#b5">[6]</ref> have excluded the UCSD data set from their experiments because nearly half of the frames in each test video contain anomalies, and, they expect abnormal events to be rare in their setting. Although we believe that our approach would perform optimally in a similar setting, we still compare our unsupervised approach with several state-of-the-art methods that require training data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. In <ref type="table">Table 4</ref>, we present the frame-level and pixel-level AUC for Ped1, and the frame-level AUC for Ped2. In terms of frame-level AUC, we obtain better results than two supervised methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. In terms of pixel-level AUC, we obtain better results than four methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. On Ped1, our results are only 3 or 4% lower than to those reported by Ren et al. <ref type="bibr" target="#b17">[18]</ref>, while more recent supervised approaches achieve much better results <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>. As most of the previ- <ref type="figure">Figure 5</ref>. Frame-level ROC curves of our framework versus <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> on UCSD Ped1. Best viewed in color. <ref type="figure">Figure 6</ref>. Pixel-level ROC curves of our framework versus <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> on UCSD Ped1. Best viewed in color. ous works, we have included the frame-level and pixel-level ROC curves for Ped1, to give the means for a thorough comparison with other approaches. <ref type="figure">Figure 5</ref> shows the framelevel ROC corresponding to the frame-level AUC of 68.4% reached by our unmasking framework based on late fusion, while <ref type="figure">Figure 6</ref> shows the pixel-level ROC corresponding to the pixel-level AUC of 52.4% obtained with the same con-  <ref type="table">Table 5</ref>. Abnormal event detection results in terms of frame-level AUC on the UMN data set. Our unmasking framework is compared with several state-of-the-art supervised methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref> as well as an unsupervised approach <ref type="bibr" target="#b5">[6]</ref>.  figuration for our approach. Some qualitative results of our unsupervised framework based on late fusion are illustrated in <ref type="figure" target="#fig_3">Figure 7</ref>. The true positive abnormal events are a car intruding a pedestrian area and a bicycle rider intruding a pedestrian area, while false positive detections are a bicycle rider and two persons walking synchronously and two persons walking in opposite directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Results on the UMN Data Set</head><p>On the UMN data set, we compare our unmasking framework with a state-of-the-art unsupervised method <ref type="bibr" target="#b5">[6]</ref> and several supervised ones <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. In <ref type="table">Table 5</ref>, we present the frame-level AUC score for each individual scene, as well as the average score for all the three scenes. Compared to the unsupervised approach of Del Giorno et al. <ref type="bibr" target="#b5">[6]</ref>, we obtain an improvement of 4.1%. On the first scene, our performance is on par with the supervised approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. As illustrated in <ref type="figure" target="#fig_4">Figure 8</ref>, our approach is able to correctly identify the two abnormal events in the first scene without any false positives, by applying a threshold of around 0.5. On the last scene, the performance of our unmasking framework based on late fusion is less than 2% lower than the best supervised approach <ref type="bibr" target="#b21">[22]</ref>. Furthermore, we are able to surpass the performance reported in <ref type="bibr" target="#b4">[5]</ref> for the third scene, by 1.8%. Our results are much worse on the second scene. We believe that the changes in illumination when people enter the room have a negative impact on our approach. The impact becomes more noticeable when we employ motion features alone, as the frame-level AUC is only 84.9%. Since the CNN features are more robust to illumination variations, we obtain a framelevel AUC of 86.5%. These observations are also applicable when we analyze the false positive detections presented in <ref type="figure" target="#fig_5">Figure 9</ref>. Indeed, the example in the bottom left corner of <ref type="figure" target="#fig_5">Figure 9</ref> illustrates that our method triggers a false detection when a significant amount of light enters the room as the door opens. The true positive examples in <ref type="figure" target="#fig_5">Figure 9</ref> represent people running around in all directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this work, we proposed a novel framework for abnormal event detection in video that requires no training sequences. Our framework is based on unmasking <ref type="bibr" target="#b11">[12]</ref>, a technique that has never been used in computer vision, as far as we know. We have conducted abnormal event detection experiments on four data sets in order to compare our approach with a state-of-the-art unsupervised approach <ref type="bibr" target="#b5">[6]</ref> and several supervised methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. The empirical results indicate that our approach gives better performance than the unsupervised method <ref type="bibr" target="#b5">[6]</ref> and some of the supervised ones <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>. Unlike Del Giorno et al. <ref type="bibr" target="#b5">[6]</ref>, we can process the video online, without any accuracy degradation.</p><p>We have adopted a late fusion strategy to combine motion and appearance features, but we did not observe any considerable improvements when using this strategy. In future work, we aim at finding a better way of fusing motion and appearance features. Alternatively, we could develop an approach to train (unsupervised) deep features on a related task, e.g. action recognition, and use these features to represent both motion and appearance information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Frame-level anomaly detection scores (between 0 and 1) provided by our unmasking framework based on the late fusion strategy, for test video 4 in the Avenue data set. The video has 947 frames. Ground-truth abnormal events are represented in cyan, and our scores are illustrated in red. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>True positive (top row) versus false positive (bottom row) detections of our unmasking framework based on the late fusion strategy. Examples are selected from the Avenue data set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>True positive (top row) versus false positive (bottom row) detections of our unmasking framework based on the late fusion strategy. Examples are selected from the Subway Entrance gate. Best viewed in color. However, using this threshold there are some false positive detections on other test videos from the Avenue data set. We show some examples of true positive and false positive detections in Figure 3. The true positive abnormal events are a person running and a person throwing an object, while false positive detections are a person holding a large object and a person sitting on the ground.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>True positive (top row) versus false positive (bottom row) detections of our unmasking framework based on the late fusion strategy. Examples are selected from the UCSD data set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Frame-level anomaly detection scores (between 0 and 1) provided by our unmasking framework based on the late fusion strategy, for the first scene in the UMN data set. The video has 1453 frames. Ground-truth abnormal events are represented in cyan, and our scores are illustrated in red. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>True positive (top row) versus false positive (bottom row) detections of our unmasking framework based on the late fusion strategy. Examples are selected from the UMN data set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Abnormal event detection results using various features, bins and window strides in our framework. The frame-level and the pixel-level AUC measures are computed on five test videos randomly chosen from the Avenue data set. For all models, the window size is 10 and the regularization parameter is 0.1. The number of frames per second (FPS) is computed by running the models on a computer with Intel Core i7 2.3 GHz processor and 8 GB of RAM using a single core.</figDesc><table><row><cell>Features</cell><cell>Bins</cell><cell cols="5">Unmasking Stride Frame AUC Pixel AUC Feature Extraction</cell><cell>Prediction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time (FPS)</cell><cell>Time (FPS)</cell></row><row><cell>VGG-f fc7</cell><cell>1 ? 1</cell><cell>no</cell><cell>1</cell><cell>78.3%</cell><cell>95.0%</cell><cell>21.4</cell><cell>376.2</cell></row><row><cell>VGG-f fc6</cell><cell>1 ? 1</cell><cell>no</cell><cell>1</cell><cell>78.4%</cell><cell>95.0%</cell><cell>20.7</cell><cell>376.3</cell></row><row><cell>VGG-f conv5</cell><cell>1 ? 1</cell><cell>no</cell><cell>1</cell><cell>78.6%</cell><cell>95.0%</cell><cell>20.1</cell><cell>59.8</cell></row><row><cell>VGG-f conv5</cell><cell>1 ? 1</cell><cell>yes</cell><cell>1</cell><cell>81.1%</cell><cell>95.3%</cell><cell>20.1</cell><cell>9.8</cell></row><row><cell>VGG-f conv5</cell><cell>2 ? 2</cell><cell>yes</cell><cell>1</cell><cell>82.5%</cell><cell>95.4%</cell><cell>20.1</cell><cell>9.4</cell></row><row><cell>VGG-f conv5</cell><cell>2 ? 2</cell><cell>yes</cell><cell>2</cell><cell>82.5%</cell><cell>95.4%</cell><cell>20.1</cell><cell>18.3</cell></row><row><cell>VGG-f conv5</cell><cell>2 ? 2</cell><cell>yes</cell><cell>5</cell><cell>82.4%</cell><cell>95.4%</cell><cell>20.1</cell><cell>42.2</cell></row><row><cell>VGG-f conv5</cell><cell>2 ? 2</cell><cell>yes</cell><cell>10</cell><cell>82.0%</cell><cell>95.3%</cell><cell>20.1</cell><cell>78.1</cell></row><row><cell>3D gradients</cell><cell>2 ? 2</cell><cell>yes</cell><cell>5</cell><cell>79.8%</cell><cell>95.1%</cell><cell>726.3</cell><cell>34.9</cell></row><row><cell cols="2">3D gradients + conv5 (late fusion) 2 ? 2</cell><cell>yes</cell><cell>5</cell><cell>82.6%</cell><cell>95.4%</cell><cell>19.6</cell><cell>19.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2415" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Discriminative Framework for Anomaly Detection in Large Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online Detection of Abnormal Events Using Incremental Coding Length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How hard can it be? Estimating the difficulty of visual search in an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2157" to="2166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Principled Approach to Detecting Surprising Events in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="631" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bonchek-Dokow</surname></persName>
		</author>
		<title level="m">Measuring Differentiability: Unmasking Pseudonymous Authors. Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2007-12-01" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1261" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection at 150 FPS in MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anomaly Detection in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised Behavior-Specific Dictionary Learning for Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="28" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online growing neural gas for anomaly detection in changing surveillance scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going Deeper With Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">VLFeat: An Open and Portable Library of Computer Vision Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MatConvNet -Convolutional Neural Networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACMMM</title>
		<meeting>eeding of ACMMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Deep Representations of Appearance and Motion for Anomalous Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno>8.1-8.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="302" to="311" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
