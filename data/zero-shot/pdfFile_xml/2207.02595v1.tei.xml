<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoning</forename><surname>Wu</surname></persName>
							<email>haoning001@e.ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Sensetime Research and Tetras AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Hou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Sensetime Research and Tetras AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Sensetime Research and Tetras AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisi</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Quality Assessment</term>
					<term>fragments</term>
					<term>Quality-retained Sampling</term>
					<term>End-to-End Learning</term>
					<term>State-of-the-Art</term>
					<term>High Efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current deep video quality assessment (VQA) methods are usually with high computational costs when evaluating high-resolution videos. This cost hinders them from learning better video-quality-related representations via end-to-end training. Existing approaches usually consider naive sampling to reduce the computational cost, such as resizing and cropping. However, they obviously corrupt quality-related information in videos and are thus not optimal to learn good representations for VQA. Therefore, there is an eager need to design a new quality-retained sampling scheme for VQA. In this paper, we propose Grid Mini-patch Sampling (GMS), which allows consideration of local quality by sampling patches at their raw resolution and covers global quality with contextual relations via mini-patches sampled in uniform grids. These mini-patches are spliced and aligned temporally, named as fragments. We further build the Fragment Attention Network (FANet) specially designed to accommodate fragments as inputs. Consisting of fragments and FANet, the proposed FrAgment Sample Transformer for VQA (FAST-VQA) enables efficient end-to-end deep VQA and learns effective video-qualityrelated representations. It improves state-of-the-art accuracy by around 10% while reducing 99.5% FLOPs on 1080P high-resolution videos. The newly learned video-quality-related representations can also be transferred into smaller VQA datasets and boost the performance on these scenarios. Extensive experiments show that FAST-VQA has good performance on inputs of various resolutions while retaining high efficiency. We publish our code at https://github.com/timothyhtimothy/FAST-VQA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>More and more videos with a variety of contents are collected in-the-wild and uploaded to the Internet every day. With the growth of high-definition video recording devices, a growing proportion of these videos are in high resolution (e.g. ? 1080P ). Classical video quality assessment (VQA) algorithms based on  <ref type="figure">Fig. 1</ref>: Motivation for fragments: (a) The computational cost (FLOPs&amp;Memory at Batch Size 4) for existing VQA methods is high especially on high-resolution videos. (b) Sampling approaches. Naive approaches such as resizing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref> and cropping <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> cannot preserve video quality well. Zoom in for clearer view.</p><p>handcrafted features are difficult to handle these videos with diverse content and degradation. In recent years, deep-learning-based VQA methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b20">21]</ref> have shown better performance on in-the-wild VQA benchmarks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. However, the computational cost of deep VQA methods increases quadratically when applied to high resolution videos, and a video of size 1080 ? 1920 would require 42.5? floating point operations (FLOPs) than normal 224 ? 224 inputs (as <ref type="figure">Fig. 1(a)</ref> shows), limiting these methods from practical applications. It is urgent to develop new VQA methods that are both effective and efficient.</p><p>Meanwhile, with high memory cost noted in <ref type="figure">Fig. 1(a)</ref>, existing methods usually regress quality scores with fixed features extracted from pre-trained networks for classification tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b9">10]</ref> to alleviate memory shortage problem on GPUs instead of end-to-end training, preventing them from learning video-quality-related representations that better represent quality information and limiting their accuracy. Existing approaches apply naive sampling on images or videos by resizing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref> or cropping <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> (as <ref type="figure">Fig. 1(b)</ref> shows) to reduce this cost and enable end-to-end training. However, they both cause artificial quality corruptions or changes during sampling, e.g., resizing corrupts local textures that are significant for predicting video quality, while cropping causes mismatched global quality with local regions. Moreover, the severity of these problems increases with the raw resolution of the video, making them unsuitable for VQA tasks.</p><p>To improve the practical efficiency and the training effectiveness of deep VQA methods, we propose a new sampling scheme, Grid Mini-patch Sampling (GMS), to retain the sensitivity to original video quality. GMS cuts videos into spatially uniform non-overlapping grids, randomly sample a mini-patch from each grid, and then splice mini-patches together. In temporal view, we constrain the position of mini-patches to align across frames, in order to ensure the sensitivity  on temporal variations. We name these temporally aligned and spatially spliced mini-patches as fragments. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, The proposed fragments can well preserve the sensitivity on both spatial and temporal quality. First, it preserves the local texture-related quality information (e.g., spot blurs happened in video 1/2 ) by retaining the original resolution in patches. Second, benefiting from the globally uniformly sampled grids, it covers the global quality even though different regions have different qualities (e.g., video 3 ). Third, by splicing the mini-patches, fragments retains contextual relations of patches so that the model can learn global scene information of the original frames. At last, with temporal alignment, fragments preserve temporal quality sensitivity by retaining the inter-frame variations in mini-patches from raw resolution, so they can be used to spot temporal distortions in videos and distinguish between severely shaking videos (e.g., video 5 ) from relatively stable shots (e.g., video 6 ). However, it is non-trivial to build a network using the proposed fragments as inputs. The network should follow two principles: 1) It should better extract the quality-related information preserved in fragments, including the retained local textures inside the raw resolution patches and the contextual relations between the spliced mini-patches; 2) It should distinguish the artificial discontinuity between mini-patches in fragments from the authentic quality degradation in the original videos. Based on these two principles, we propose a Fragment Attention Network (FANet) with Video Swin Transformer Tiny (Swin-T) <ref type="bibr" target="#b26">[27]</ref> as the backbone. Swin-T has a hierarchical structure and processes inputs with patch-wise operations, which is naturally suitable for proceeding with proposed fragments.  Furthermore, to avoid the negative impact of discontinuity between minipatches on quality prediction, we propose two novel modules, i.e., Gated Relative Position Biases (GRPB) and Intra-Patch Non-Linear Regression (IP-NLR), to correct for the self-attention computation and the final score regression in the FANet respectively. Specifically, considering that some pairs in the same attention window might have the same relative position (e.g., <ref type="figure" target="#fig_3">Fig. 3</ref>(a) A-C, D-E, A-B), but the cross-patch attention pairs (A-C, D-E) are in far actual distances while intra-patch attention pairs (A-B) are in much nearer actual distances in the original video, we propose GRPB to explicitly distinguish these two kinds of attention pairs to avoid confusion of discontinuity between patches and authentic video artifacts. In addition, due to the discontinuity, different mini-patches contain diverse quality information ( <ref type="figure" target="#fig_3">Fig. 3(b)</ref>), thus pooling operation before score regression applied in existing methods may confuse the information. To address this issue, we design IP-NLR as a quality-sensitive head, which first regresses the quality scores of mini-patches independently with non-linear layers and pools them after the regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Patch Attention Pair</head><p>In summary, we propose the FrAgment Sample Transformer for VQA (FAST-VQA), with the following contributions:</p><p>1. We propose fragments, a new sampling strategy for VQA that preserves both local quality and unbiased global quality with contextual relations via uniform Grid Mini-patch Sampling (GMS). The fragments can reduce the complexity of assessing 1080P videos by 97.6% and enables effective end-toend training of VQA with quality-retained video samples. 2. We propose the Fragment Attention Network (FANet) to learn the local and contextual quality information from fragments, in which the Gated Relative Position Biases (GRPB) module is proposed to distinguish the intrapatch and cross-patch self-attention and the Intra-Patch Non-Linear Regression (IP-NLR) is proposed for better quality regression from fragments. 3. The proposed FAST-VQA can learn video-quality-related representations efficiently through end-to-end training. These quality features help FAST-VQA to be 10% more accurate than the existing state-of-the-art approaches and 8% better than full-resolution Swin-T baseline with fixed recognition features. Through transfer learning, these quality features also significantly improve the best benchmark performance for small VQA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Classical VQA Methods Classical VQA methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25]</ref> handcrafted features to evaluate video quality. Among recent works, TLVQM <ref type="bibr" target="#b19">[20]</ref> uses a combination of spatial high-complexity and temporal low-complexity handcraft features and VIDEVAL <ref type="bibr" target="#b35">[36]</ref> ensembles different handcraft features to model the diverse authentic distortions. However, the reasons affecting the video quality are quite complicated and cannot be well captured with these handcrafted features.</p><p>Fixed-feature-based Deep VQA Methods Due to the extremely high computational cost of deep networks on high resolution videos, existing deep VQA methods train only a feature regression network with fixed deep features. Among them, VSFA <ref type="bibr" target="#b21">[22]</ref> uses the features extracted by pre-trained ResNet-50 <ref type="bibr" target="#b10">[11]</ref> from ImageNet-1k <ref type="bibr" target="#b4">[5]</ref> and GRU <ref type="bibr" target="#b3">[4]</ref> for temporal regression. MLSP-FF <ref type="bibr" target="#b7">[8]</ref> also uses heavier Inception-ResNet-V2 <ref type="bibr" target="#b32">[33]</ref> for feature extraction. Some methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> use the features extractor pre-trained with IQA datasets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>. PVQ <ref type="bibr" target="#b39">[40]</ref> also extracts features pretrained on action recognition dataset <ref type="bibr" target="#b15">[16]</ref> for better perception on inter-frame distortion. These methods are limited by their high computational cost on high resolution videos. Additionally, without end-to-end training, fixed features pretrained by other tasks are not optimal for extracting qualityrelated information, which also limits the accuracy of quality assessment.</p><p>VQA Datasets Tab. 1 shows common VQA datasets, other video datasets and their sizes. The early VQA datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b6">7]</ref> are synthesized with specialized distortion and have a very small volume. Some recent in-the-wild VQA datasets like KoNViD-1k <ref type="bibr" target="#b11">[12]</ref>, YouTube-UGC <ref type="bibr" target="#b37">[38]</ref> and LIVE-VQC <ref type="bibr" target="#b31">[32]</ref> are still small compared to datasets for other video tasks such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref>. Recently, LSVQ <ref type="bibr" target="#b39">[40]</ref>, a large-scale VQA dataset with 39,076 videos is publicly available. With end-toend deep learning of the proposed FAST-VQA, the video-quality-related features learnt on large-scale LSVQ dataset can be transferred into smaller VQA datasets to reach better performance.  <ref type="bibr" target="#b15">[16]</ref> Video Recognition NA 306,245 ActivityNet <ref type="bibr" target="#b1">[2]</ref> Video Action Localization NA 27,801 AVA <ref type="bibr" target="#b8">[9]</ref> Atomic Action Detection NA 386,000 CVD2014 <ref type="bibr" target="#b29">[30]</ref> Video Quality Assessment Synthetic In-capture 234 KoNViD-1k <ref type="bibr" target="#b11">[12]</ref> Video Quality Assessment In-the-wild 1,200 LIVE-VQC <ref type="bibr" target="#b31">[32]</ref> Video Quality Assessment In-the-wild 585 Youtube-UGC <ref type="bibr" target="#b37">[38]</ref> Video Quality Assessment</p><p>In-the-wild 1,147 LSVQ <ref type="bibr" target="#b39">[40]</ref> Video Quality Assessment In-the-wild 39,076</p><p>Vision Transformers Vision transformers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref> have shown effective on computer vision tasks. They cut images or videos into non-overlapping patches as input and perform self-attention operations between them. The patch-wise operations in vision transformers naturally distinguish the edges of mini-patches and are suitable for handling with the proposed fragments.</p><p>Grid Partition: grids</p><formula xml:id="formula_0">G f ? G f Patch Splicing Patch Sampling : Patch Size S S f ? S f Frame-t Frame-(t+1)</formula><p>Temporal Alignment</p><formula xml:id="formula_1">G f ? S f G f ? S f</formula><p>Mini-patches fragments <ref type="figure">Fig. 4</ref>: The pipeline for sampling fragments with Grid Mini-patch Sampling (GMS), including grid partition, patch sampling, patch splicing, and temporal alignment. After GMS, the fragments are fed into the FANet <ref type="figure" target="#fig_5">(Fig. 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we introduce the full pipeline of the proposed FAST-VQA method. An input video is first sampled into fragments via Grid Mini-patch Sampling (GMS, Sec. 3.1). After sampling, the resultant fragments are fed into the Fragment Attention Network (FANet, Sec. 3.2) to get the final prediction of the video's quality. We introduce both parts in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Grid Mini-patch Sampling (GMS)</head><p>To well preserve the original video quality after sampling, we follow several important principles when designing the sampling process for fragments. We will illustrate the process along with these principles below.</p><p>Preserving global quality: uniform grid partition. To include each region for quality assessment and uniformly assess quality in different areas, we design the grid partition to cut video frames into uniform grids with each grid having the same size (as shown in <ref type="figure">Fig. 4</ref>). We cut the t-th video frame</p><formula xml:id="formula_2">V t into G f ? G f uniform grids with the same sizes, denoted as G t = {g 0,0 t , ..g i,j t , ..g G f ?1,G f ?1 t }, where g i,j</formula><p>t denotes the grid in the i-th row and j-th column. The uniform grid partition process is formalized as follows.</p><formula xml:id="formula_3">g i,j t = V t [ i ? H G f : (i + 1) ? H G f , j ? W G f : (j + 1) ? W G f ]<label>(1)</label></formula><p>where H and W denote the height and width of the video frame.</p><p>Preserving local quality: raw patch sampling. To preserve the local textures (e.g. blurs, noises, artifacts) that are vital in VQA, we select raw resolution patches without any resizing operations to represent local textural quality in grids. We employ random patch sampling to select one mini-patch MP i,j t of size of S f ?S f from each grid g i,j t . The patch sampling process is as follows.</p><formula xml:id="formula_4">MP i,j t = S i,j t (g i,j t )<label>(2)</label></formula><p>where S i,j t is the patch sampling operation for frame t and grid i, j.</p><p>Preserving temporal quality: temporal alignment. It is widely recognized by early works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40</ref>] that inter-frame temporal variations are influential to video qualities. To retain the raw temporal variations in videos (with T frames), we strictly align the sample areas during patch sampling operations S in different frames, as the following constraint shows.</p><formula xml:id="formula_5">S i,j t = S i,? t ? 0 ? t,t &lt; T, 0 ? i, j &lt; G f<label>(3)</label></formula><p>Preserving contextual relations: patch splicing. Existing works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8]</ref> have shown that the global scene information and contextual information affects quality predictions. To keep the global scene information of the original videos, we keep the contextual relations of mini-patches by splicing them into their original positions, as the following equation shows:</p><formula xml:id="formula_6">F i,j t = F t [i ? S f : (i + 1) ? S f , j ? S f : (j + 1) ? S f ] = MP i,j t , 0 ? i, j &lt; G f<label>(4)</label></formula><p>where F denote the spliced and temporally aligned mini-patches after the Grid Mini-patch Sampling (GMS) pipeline, named as fragments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fragment Attention Network (FANet)</head><p>The Overall Framework. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the overall framework of FANet. It uses a Swin-T with four hierarchical self-attention layers as backbone. We also design the following modules to adapt it to fragments well.</p><p>Gated Relative Position Biases. Swin-T adds relative position bias (RPB) that uses learnable Relative Bias <ref type="table">Table (</ref>T) to represent the relative positions of pixels in attention pairs (QK T ). For fragments, however, as discussed in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>, the cross-patch pairs have much large actual distances than intra-patch pairs and should not be modeled with the same bias table. Therefore, we propose the gated relative position biases (GRPB, <ref type="figure" target="#fig_5">Fig. 5(b)</ref>) that uses learnable real position bias table (T real ) and pseudo position bias table (T pseudo ) to replace T.</p><p>The mechanisms of them are the same as T but they are learnt separately and used for intra-patch and cross-patch attention pairs respectively. Denote G as the intra-patch gate (G i,j = 1 if i, j are in the same mini-patch else G i,j = 0), the self-attention matrix (M A ) with GRPB is calculated as:   <ref type="figure">(Fig. 4)</ref>.</p><formula xml:id="formula_7">B In,(i,j) = T real FRP(i,j) ; B Cr,(i,j) = T pseudo FRP(i,j)<label>(5)</label></formula><formula xml:id="formula_8">M A = QK T + G ? B In + (1 ? G) ? B Cr<label>(6)</label></formula><p>where FRP(i, j) is the relative position of pair (i, j) in fragments.</p><p>Intra-Patch Non-Linear Regression. As illustrated in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>, different minipatches have diverse qualities due to discontinuity between them. If we pool features from different patches before regression, the quality representations of mini-patches will be confused with each other. To avoid this problem, we design the Intra-Patch Non-Linear Regression (IP-NLR, <ref type="figure" target="#fig_5">Fig. 5(c)</ref>) to regress the features via non-linear layers (R NL ) first, and perform pooling following the regression. Denote features as f , output score as s pred , pooling operation as P ool(?), the IP-NLR can be expressed as follows:</p><formula xml:id="formula_9">s pred = P ool(R NL (f ))<label>(7)</label></formula><p>4 Experiments</p><p>In the experiment part, we conduct several experiments to evaluate and analyze the performance of the proposed FAST-VQA model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Setup</head><p>Implementation Details We use the Swin-T <ref type="bibr" target="#b26">[27]</ref> pretrained on Kinetics-400 <ref type="bibr" target="#b15">[16]</ref> dataset to initialize the backbone in FANet. As Tab. 2 shows, we implement two sampling densities for fragments: FAST-VQA (normal density) and FAST-VQA-M (lower density &amp; higher efficiency), and accomodate window sizes in FANet to the input sizes. Without special notes, all ablation studies are on variants of FAST-VQA. We use PLCC (Pearson linear correlation coef.) and SRCC (Spearman rank correlation coef.) as metrics and use differentiable PLCC loss l =</p><p>(1?PLCC(s pred ,sgt)) 2 as loss function. We set the training batch size as 16.  <ref type="bibr" target="#b11">[12]</ref> and LIVE-VQC <ref type="bibr" target="#b31">[32]</ref>, two widely-recognized in-the-wild VQA benchmark datasets. In Tab. 3, we compare with existing classical and deep VQA methods and our baseline, the full-resolution Swin-T with feature regression instead of endto-end training (denoted as 'Full-res Swin-T features'). With its video-qualityrelated representations, FAST-VQA achieves at most 10% improvement to PVQ, the existing state-of-the-art on LSVQ 1080p . Even the efficient version FAST-VQA-M can outperform existing state-of-the-art. FAST-VQA also shows significant improvement to its fixed-feature-based baseline with the same backbone, demonstrating that the proposed new quality-retained sampling with end-toend training scheme for VQA is not only much more efficient (with only 2.36% FLOPs required on 1080P videos) but also notably more accurate (with 8.10% improvement on PLCC metric for LSVQ 1080p ) than the existing fixed-featurebased paradigm.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency of FAST-VQA</head><p>To demonstrate the efficiency of FAST-VQA, we compare the FLOPs and running times on CPU/GPU (average of ten runs per sample) of the proposed FAST-VQA with existing deep VQA approaches on different resolutions, see Tab. 4. We also draw the performance-FLOPs curve on LSVQ 1080p and LIVE-VQC in <ref type="figure" target="#fig_7">Fig. 6</ref>. As we can see, FAST-VQA reduces up to 210? FLOPs and 247? running time than PVQ while obtaining notably better performance. The more efficient version, FAST-VQA-M, only requires 1/1273 FLOPs of PVQ and 1/258 FLOPs of our full-resolution baseline while still achieving slightly better performance. Moreover, FAST-VQA (especially FAST-VQA-M) also runs very fast even on CPU, which reduces the hardware requirements for the applications of deep VQA methods. All these comparisons show the unprecedented efficiency of proposed FAST-VQA. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transfer Learning with Video-quality-related Representations</head><p>FAST-VQA also makes the pretrain-finetune scheme on VQA possible with affordable computation resources. With FAST-VQA, we can pretrain with large VQA datasets in end-to-end manner to learn quality related features, and then transfer to specific VQA scenarios where only small datasets are available. Note that this manner is not applicable to current methods due to their high computational load (as discussed in Sec. 4.3). We use LSVQ as the large dataset and choose four small datasets representing diverse scenarios, including LIVE-VQC (real-world mobile photography, 240P-1080P), KoNViD-1k (various contents collected online, all 540P), CVD2014 (synthetic in-capture distortions, 480P-720P), LIVE-Qualcomm (selected types of distortions, all 1080P) and YouTube-UGC (user-generated contents, including computer graphic contents, 360P-2160P 5 ). We divide each dataset into random splits for 10 times and report the average result on the test splits. As Tab. 5 shows, with video-quality-related representations, the proposed FAST-VQA outperforms the existing state-of-the-arts on all these scenarios while obtaining much higher efficiency. Note that YouTube-UGC contains 4K(2160P) videos but FAST-VQA still performs well. Even without video-quality-related representations, FAST-VQA also still achieves competitive performance, while these features steadily improve the performance. It implies that the pretrained FAST-VQA can be able to serve as a strong backbone that boost further downstream tasks related to video quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies on fragments</head><p>For the first part of ablation studies, we prove the effectiveness of fragments by comparing with other common sampling approaches and different variants of fragments (Tab. 6). We keep the FANet structure fixed during this part.</p><p>Comparing with resizing/cropping In Group 1 of Tab. 6, we compare the proposed fragments with two common sampling approaches: bilinear resizing and random cropping. The proposed fragments are notably better than bilinear resizing on high-resolution (LSVQ 1080p ) (+4%) and cross-resolution (LIVE-VQC) scenarios (+4%). Fragments still lead to non-trivial 2% improvements than resizing on lower-resolution scenarios where the problems of resizing is not that severe. This proves that keeping local textures is vital for VQA. Fragments also largely outperform single random crop as well as ensemble of multiple crops, suggesting that retaining the uniform global quality is also critical to VQA.</p><p>Comparing with variants of fragments We also compare with three variants of fragments in Tab. 6, Group 2. We prove the effectiveness of uniform grid partition by comparing with random mini-patches (ignore grids while sampling), and the importance of retaining contextual relations by comparing with shuffled mini-patches. Fragments show notable improvements than both variants. Moreover, the proposed fragments show much better performance than the variant without temporal alignment especially on high resolution videos, suggesting that preserving the inter-frame temporal variations is necessary for fragments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Studies on FANet</head><p>Effects of GRPB and IP-NLR In the second part of ablation studies, we analyze the effects of two important designs in FANet: the proposed Gated Relative Position Biases (GRPB) and Intra-Patch Non-Linear Regression (IP-NLR) VQA Head as in Tab. 7. We compare the IP-NLR with two variants: the linear regression layer and the non-linear regression layers with pooling before regression (PrePool ). Both modules lead to non-negligible improvements especially on high-resolution (LSVQ 1080p ) or cross-resolution (LIVE-VQC) scenarios. As the discontinuity between mini-patches is more obvious in high-resolution videos, this result suggests that the corrected position biases and regression head are helpful on solving the problems caused by such discontinuity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Reliability and Robustness Analyses</head><p>As FAST-VQA is based on samples rather than original videos while a single sample for fragments only keeps 2.4% spatial information in 1080P videos, it is important to analyze the reliability and robustness of FAST-VQA predictions.</p><p>Reliability of Single Sampling. We measure the reliability of single sampling in FAST-VQA by two metrics: 1) the assessment stability of different single samplings on the same video; 2) the relative accuracy of single sampling compared with multiple sample ensemble. As shown in Tab. 8, the normalized std. dev. of different sampling on a same video is only around 0.01, which means the sampled fragments are enough to make very stable predictions. Compared with 6-sample ensemble, sampling only once can already be 99.40% as accurate even on the pure high-resolution test set (LSVQ 1080P ). They prove that a single sample of fragments is enough stable and reliable for quality assessment even though only a small proportion of information is kept during sampling. Robustness on Different Resolutions To analyze the robustness of FAST-VQA on different resolutions, we divide the cross-resolution VQA benchmark set LIVE-VQC into three resolution groups: (A) 1080P (110 videos); (B) 720P (316 videos); (C) ?540P (159 videos) to see the performance of FAST-VQA on different resolutions, compared with several variants. As the results shown in Tab. 9, the proposed FAST-VQA shows good performance (? 0.80 SRCC&amp;PLCC) on all resolution groups and most superior improvement than other variants on Group (A) with 1080P high-resolution videos, proving that FAST-VQA is robust and reliable on different resolutions of videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Qualitative Results: Local Quality Maps</head><p>The proposed IP-NLR head with patch-wise independent quality regression enables FAST-VQA to generate patch-wise local quality maps, which helps us to  qualitatively evaluate what quality information can be learned in FAST-VQA. We show the patch-wise local quality maps and the re-projected frame quality maps for a 1080P video (from LIVE-VQC <ref type="bibr" target="#b31">[32]</ref> dataset) in <ref type="figure" target="#fig_8">Fig. 7</ref>. As the patch-wise quality maps and re-projected quality maps in <ref type="figure" target="#fig_1">Fig. 7 (column 2&amp;4)</ref> shows, FAST-VQA is sensitive to textural quality information and distinguishes between clear (Frame 0) and blurry textures (Frame 12/24). It demonstrates that FAST-VQA with fragments (column 3) as input is sensitive to local texture quality. Furthermore, the qualities of the action-related areas are notably different from the background areas, showing that FAST-VQA effectively learns the global scene information and contextual relations in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our paper has shown that proposed fragments are effective samples for video quality assessment (VQA) that better retain quality information in videos than naive sampling approaches, to tackle the difficulties as results of high computing and memory requirements when high-resolution videos are to be evaluated. Based on fragments, the proposed end-to-end FAST-VQA achieves higher efficiency (?99.5% FLOPs) and accuracy (+10% PLCC) simultaneously than existing state-of-the-art method PVQ on 1080P videos. We hope that the FAST-VQA can bring deep VQA methods into practical use for videos in any resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This study is supported under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, relative to FAST-VQA) (a) FLOPs and Memory Cost at different resolutions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fragments, in spatial view (a) and temporal view (b). Zoom-in views of minipatches show that fragments can retain spatial local quality information (a), and spot temporal variations such as shaking across frames (b). In (a), spliced mini-patches also keep the global scene information of original frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Motivation for GRPB: Distinguishing Cross-Patch &amp; Intra-Patch attention pairsSelf-attention window (b) Motivation for IP-NLR Head: Patches have diverse qualities colors denote they come from different mini-patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Motivation for the two proposed modules in FANet: (a) Gated Relative Position Biases (GRPB); (b) Intra-Patch Non-Linear Regression (IP-NLR) head. The structures for the two modules are illustrated in Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Gated Relative Position Biases (GRPB) (a) Hierarchical Swin-T Backbone with GRPB (c) Intra-Patch Non-Linear Regression (IP-NLR) +Intra-Patch Bias B in +Cross-Patch Bias B Cr Intra-Patch Gate non-linear layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>The overall framework for FANet, including the Gated Relative Position Biases (GRPB) and Intra-Patch Non-Linear Regression (IP-NLR) modules. The input fragments come from Grid Mini-patch Sampling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>On High-resolution Scenario (LSVQ-1080P) (b) On Cross-resolution Scenario (LIVE-VQC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>The Performance-FLOPs curve of proposed FAST-VQA and baseline methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Spatial-temporal patch-wise local quality maps, where red areas refer to low predicted quality and green areas refer to high predicted quality. This sample video is a 1080P video selected from LIVE-VQC<ref type="bibr" target="#b31">[32]</ref> dataset. Zoom in for clearer view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell>Distortion Type</cell><cell>Size</cell></row><row><cell>Kinetics-400</cell><cell></cell><cell></cell><cell></cell></row></table><note>Common datasets in VQA and other video tasks. Most common VQA datasets are to small (noted in red) to learn sufficient quality representations independently.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of FAST-VQA and FAST-VQA-M with lower sampling density. Benchmark Sets We use the large-scale LSVQ train [40] dataset with 28,056 videos for training FAST-VQA. For evaluation, we choose 4 testing sets to test the model trained on LSVQ. The first two sets, LSVQ test and LSVQ 1080p are official intra-dataset test subsets for LSVQ, while the LSVQ test consists of 7,400 various resolution videos from 240P to 720P, and LSVQ 1080p consists of 3,600 1080P high resolution videos. We also evaluate the generalization ability of FAST-VQA on cross-dataset evaluations on KoNViD-1k</figDesc><table><row><cell>Methods</cell><cell>Number of Frames (T )</cell><cell>Patch Size (Sf )</cell><cell>Number of Grids (Gf )</cell><cell>Window Size in FANet</cell><cell cols="2">FLOPs Parameters</cell></row><row><cell>FAST-VQA</cell><cell>32</cell><cell>32</cell><cell>7</cell><cell>(8,7,7)</cell><cell>279G</cell><cell>27.7M</cell></row><row><cell>FAST-VQA-M</cell><cell>16</cell><cell>32</cell><cell>4</cell><cell>(4,4,4)</cell><cell>46G</cell><cell>27.5M</cell></row><row><cell>Training &amp;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Type/</cell><cell></cell><cell cols="4">Intra-dataset Test Sets</cell><cell cols="4">Cross-dataset Test Sets</cell></row><row><cell cols="2">Testing Set/</cell><cell cols="2">LSVQtest</cell><cell cols="2">LSVQ1080p</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">LIVE-VQC</cell></row><row><cell>Groups</cell><cell>Methods</cell><cell cols="8">SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC</cell></row><row><cell>Existing Classical</cell><cell>BRISQUE[28] TLVQM[20]</cell><cell>0.569 0.772</cell><cell>0.576 0.774</cell><cell>0.497 0.589</cell><cell>0.531 0.616</cell><cell>0.646 0.732</cell><cell>0.647 0.724</cell><cell>0.524 0.670</cell><cell>0.536 0.691</cell></row><row><cell></cell><cell>VIDEVAL[36]</cell><cell>0.794</cell><cell>0.783</cell><cell>0.545</cell><cell>0.554</cell><cell>0.751</cell><cell>0.741</cell><cell>0.630</cell><cell>0.640</cell></row><row><cell>Existing Deep</cell><cell cols="2">VSFA[22] PVQ wo/ patch [40] 0.814 0.801</cell><cell>0.796 0.816</cell><cell>0.675 0.686</cell><cell>0.704 0.708</cell><cell>0.784 0.781</cell><cell>0.794 0.781</cell><cell>0.734 0.747</cell><cell>0.772 0.776</cell></row><row><cell></cell><cell>PVQ w/ patch [40]</cell><cell>0.827</cell><cell>0.828</cell><cell>0.711</cell><cell>0.739</cell><cell>0.791</cell><cell>0.795</cell><cell>0.770</cell><cell>0.807</cell></row><row><cell cols="2">Full-res Swin-T[27] features</cell><cell>0.835</cell><cell>0.833</cell><cell>0.739</cell><cell>0.753</cell><cell>0.825</cell><cell>0.828</cell><cell cols="2">0.794 0.809</cell></row><row><cell cols="2">FAST-VQA-M (Ours)</cell><cell cols="6">0.852 0.854 0.739 0.773 0.841 0.832</cell><cell cols="2">0.788 0.810</cell></row><row><cell cols="2">FAST-VQA (Ours)</cell><cell cols="8">0.876 0.877 0.779 0.814 0.859 0.855 0.823 0.844</cell></row><row><cell cols="2">Improvement to PVQ w/ patch</cell><cell>+6%</cell><cell>+6%</cell><cell cols="2">+10% +10%</cell><cell>+9%</cell><cell>+8%</cell><cell>+7%</cell><cell>+5%</cell></row></table><note>Comparison with existing methods (classical and deep) and our baseline (Full- res Swin-T features). The 1st/2nd best scores are colored in red and blue, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>FLOPs and running time (on GPU/CPU, average of ten runs) comparison of FAST-VQA, state-of-the-art methods and our baseline on different resolutions. We boldface FLOPs ? 500G and running time ? 1s.</figDesc><table><row><cell></cell><cell></cell><cell>540P</cell><cell></cell><cell>720P</cell><cell cols="2">1080P</cell></row><row><cell>Method</cell><cell>FLOPs(G)</cell><cell>Time(s)</cell><cell>FLOPs(G)</cell><cell>Time(s)</cell><cell>FLOPs(G)</cell><cell>Time(s)</cell></row><row><cell>VSFA[22]</cell><cell>1024936.7?</cell><cell>2.603/92.761</cell><cell>1818465.2?</cell><cell>3.571/134.9</cell><cell>40919147?</cell><cell>11.14/465.6</cell></row><row><cell>PVQ[40]</cell><cell>1464652.5?</cell><cell>3.091/97.85</cell><cell>2202979.0?</cell><cell>4.143/144.6</cell><cell>58501210?</cell><cell>13.79/538.4</cell></row><row><cell>Full-res Swin-T[27] feat.</cell><cell>303210.9?</cell><cell>3.226/102.0</cell><cell>535719.2?</cell><cell>5.049/166.2</cell><cell>1185242.5?</cell><cell>8.753/234.9</cell></row><row><cell>FAST-VQA (Ours)</cell><cell>2791?</cell><cell>0.044/9.019</cell><cell>2791?</cell><cell>0.043/9.530</cell><cell>2791?</cell><cell>0.045/9.142</cell></row><row><cell>FAST-VQA-M (Ours)</cell><cell>460.165?</cell><cell>0.019/0.729</cell><cell>460.165?</cell><cell>0.019/0.613</cell><cell>460.165?</cell><cell>0.019/0.714</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The finetune results on LIVE-VQC, KoNViD, CVD2014 and YouTube-UGC datasets, compared with existing classical and fixed-backbone deep VQA methods, and ensemble approaches of classical (C) and deep (D) branches.</figDesc><table><row><cell cols="2">Finetune Dataset/</cell><cell cols="2">LIVE-VQC</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">CVD2014</cell><cell cols="4">LIVE-Qualcomm YouTube-UGC</cell></row><row><cell>Groups</cell><cell>Methods</cell><cell>SRCC</cell><cell>PLCC</cell><cell cols="5">SRCC PLCC SRCC PLCC SRCC</cell><cell>PLCC</cell><cell cols="2">SRCC PLCC</cell></row><row><cell>Existing Classical</cell><cell>TLVQM[20] VIDEVAL[36]</cell><cell>0.799 0.752</cell><cell>0.803 0.751</cell><cell>0.773 0.783</cell><cell>0.768 0.780</cell><cell>0.83 NA</cell><cell>0.85 NA</cell><cell>0.77 NA</cell><cell>0.81 NA</cell><cell>0.669 0.779</cell><cell>0.659 0.773</cell></row><row><cell></cell><cell>RAPIQUE[35]</cell><cell>0.755</cell><cell>0.786</cell><cell>0.803</cell><cell>0.817</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>0.759</cell><cell>0.768</cell></row><row><cell></cell><cell>VSFA[22]</cell><cell>0.773</cell><cell>0.795</cell><cell>0.773</cell><cell>0.775</cell><cell>0.870</cell><cell>0.868</cell><cell>0.737</cell><cell>0.732</cell><cell>0.724</cell><cell>0.743</cell></row><row><cell>Existing Fixed Deep</cell><cell>PVQ[40] GST-VQA[3]</cell><cell>0.827 NA</cell><cell>0.837 NA</cell><cell>0.791 0.814</cell><cell>0.786 0.825</cell><cell>NA 0.831</cell><cell>NA 0.844</cell><cell>NA 0.801</cell><cell>NA 0.825</cell><cell>NA NA</cell><cell>NA NA</cell></row><row><cell></cell><cell>CoINVQ[37]</cell><cell>NA</cell><cell>NA</cell><cell>0.767</cell><cell>0.764</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>0.816</cell><cell>0.802</cell></row><row><cell>Ensemble</cell><cell>CNN+TLVQM[21]</cell><cell>0.825</cell><cell>0.834</cell><cell>0.816</cell><cell>0.818</cell><cell>0.863</cell><cell>0.880</cell><cell>0.810</cell><cell>0.833</cell><cell>NA</cell><cell>NA</cell></row><row><cell>C+D</cell><cell>CNN+VIDEVAL[36]</cell><cell>0.785</cell><cell>0.810</cell><cell>0.815</cell><cell>0.817</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>0.808</cell><cell>0.803</cell></row><row><cell cols="2">Full-res Swin-T[27] features</cell><cell>0.799</cell><cell>0.808</cell><cell>0.841</cell><cell>0.838</cell><cell>0.868</cell><cell>0.870</cell><cell>0.788</cell><cell>0.803</cell><cell>0.798</cell><cell>0.796</cell></row><row><cell cols="2">FAST-VQA-M (Ours)</cell><cell>0.803</cell><cell>0.828</cell><cell cols="2">0.873 0.872</cell><cell cols="2">0.877 0.892</cell><cell>0.804</cell><cell>0.838</cell><cell>0.768</cell><cell>0.765</cell></row><row><cell cols="2">FAST-VQA w/o VQ-representations (Ours)</cell><cell>0.765</cell><cell>0.782</cell><cell>0.842</cell><cell>0.844</cell><cell>0.871</cell><cell>0.888</cell><cell>0.756</cell><cell>0.778</cell><cell>0.794</cell><cell>0.784</cell></row><row><cell cols="2">FAST-VQA (ours)</cell><cell>0.849</cell><cell>0.865</cell><cell cols="2">0.891 0.892</cell><cell cols="2">0.891 0.903</cell><cell>0.819</cell><cell>0.851</cell><cell cols="2">0.855 0.852</cell></row><row><cell cols="2">Improvements led by VQ-representations</cell><cell cols="8">+11.0% +10.6% +5.8% +5.7% +2.3% +1.7% +8.3% +9.4%</cell><cell cols="2">+7.7% +8.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on fragments: comparison with resizing, cropping (Group 1) and different variants for fragments (Group 2).</figDesc><table><row><cell>Testing Set/</cell><cell cols="2">LSVQtest</cell><cell cols="2">LSVQ1080p</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">LIVE-VQC</cell></row><row><cell>Methods/Metric</cell><cell cols="8">SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC</cell></row><row><cell cols="3">Group 1: Naive Sampling Approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bilinear resizing</cell><cell>0.857</cell><cell>0.859</cell><cell>0.752</cell><cell>0.786</cell><cell>0.841</cell><cell>0.840</cell><cell>0.772</cell><cell>0.814</cell></row><row><cell>random cropping</cell><cell>0.807</cell><cell>0.812</cell><cell>0.643</cell><cell>0.677</cell><cell>0.734</cell><cell>0.776</cell><cell>0.740</cell><cell>0.773</cell></row><row><cell>-test with 3 crops</cell><cell>0.838</cell><cell>0.835</cell><cell>0.727</cell><cell>0.754</cell><cell>0.841</cell><cell>0.827</cell><cell>0.785</cell><cell>0.809</cell></row><row><cell>-test with 6 crops</cell><cell>0.843</cell><cell>0.844</cell><cell>0.734</cell><cell>0.761</cell><cell>0.845</cell><cell>0.834</cell><cell>0.796</cell><cell>0.817</cell></row><row><cell cols="2">Group 2: Variants of fragments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>random mini-patches</cell><cell>0.857</cell><cell>0.861</cell><cell>0.754</cell><cell>0.790</cell><cell>0.844</cell><cell>0.845</cell><cell>0.792</cell><cell>0.818</cell></row><row><cell>shuffled mini-patches</cell><cell>0.858</cell><cell>0.863</cell><cell>0.761</cell><cell>0.799</cell><cell>0.849</cell><cell>0.847</cell><cell>0.796</cell><cell>0.821</cell></row><row><cell cols="2">w/o temporal alignment 0.850</cell><cell>0.853</cell><cell>0.736</cell><cell>0.779</cell><cell>0.823</cell><cell>0.816</cell><cell>0.764</cell><cell>0.802</cell></row><row><cell>fragments (ours)</cell><cell cols="8">0.876 0.877 0.779 0.814 0.859 0.855 0.823 0.844</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on FANet design: the effects for GRPB and IP-NLR modules.</figDesc><table><row><cell>Testing Set/</cell><cell cols="2">LSVQtest</cell><cell cols="2">LSVQ1080p</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">LIVE-VQC</cell></row><row><cell>Variants/Metric</cell><cell cols="8">SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC</cell></row><row><cell>w/o GRPB</cell><cell>0.873</cell><cell>0.872</cell><cell>0.769</cell><cell>0.805</cell><cell>0.854</cell><cell>0.853</cell><cell>0.808</cell><cell>0.832</cell></row><row><cell>semi-GRPB on Layer 1/2</cell><cell>0.873</cell><cell>0.875</cell><cell>0.772</cell><cell>0.809</cell><cell>0.856</cell><cell>0.851</cell><cell>0.812</cell><cell>0.838</cell></row><row><cell>linear Regression</cell><cell>0.872</cell><cell>0.873</cell><cell>0.768</cell><cell>0.803</cell><cell>0.847</cell><cell>0.849</cell><cell>0.810</cell><cell>0.835</cell></row><row><cell cols="2">PrePool non-linear Regression 0.873</cell><cell>0.874</cell><cell>0.771</cell><cell>0.805</cell><cell>0.851</cell><cell>0.850</cell><cell>0.813</cell><cell>0.834</cell></row><row><cell>FANet (ours)</cell><cell cols="8">0.876 0.877 0.779 0.814 0.859 0.855 0.823 0.844</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Assessment stability and relative accuracy of single sampling of fragments.</figDesc><table><row><cell>Testing Set/</cell><cell cols="4">LSVQtest LSVQ1080p KoNViD-1k LIVE-VQC</cell></row><row><cell>Score Range</cell><cell>0-100</cell><cell>0-100</cell><cell>1-5</cell><cell>0-100</cell></row><row><cell>std. dev. of Single Samplings</cell><cell>0.65</cell><cell>0.79</cell><cell>0.046</cell><cell>1.07</cell></row><row><cell>Normalized std. dev.</cell><cell>0.0065</cell><cell>0.0079</cell><cell>0.0115</cell><cell>0.0107</cell></row><row><cell>Relative Pair Accuracy compared with 6-samples</cell><cell>99.59%</cell><cell>99.40%</cell><cell>99.45%</cell><cell>99.52%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Performance comparison on different resolution groups of LIVE-VQC dataset.</figDesc><table><row><cell>Resolution</cell><cell></cell><cell>(A): 1080P</cell><cell></cell><cell></cell><cell>(B): 720P</cell><cell></cell><cell></cell><cell>(C): ?540P</cell><cell></cell></row><row><cell>Variants</cell><cell cols="9">SRCC PLCC KRCC SRCC PLCC KRCC SRCC PLCC KRCC</cell></row><row><cell>Full-res Swin features (Baseline)</cell><cell>0.771</cell><cell>0.774</cell><cell>0.584</cell><cell>0.796</cell><cell>0.811</cell><cell>0.602</cell><cell>0.810</cell><cell>0.853</cell><cell>0.625</cell></row><row><cell>bilinear resizing (Sampling Variant)</cell><cell>0.758</cell><cell>0.773</cell><cell>0.573</cell><cell>0.790</cell><cell>0.822</cell><cell>0.599</cell><cell>0.835</cell><cell>0.878</cell><cell>0.650</cell></row><row><cell cols="2">random cropping (Sampling Variant) 0.765</cell><cell>0.768</cell><cell>0.565</cell><cell>0.774</cell><cell>0.787</cell><cell>0.581</cell><cell>0.730</cell><cell>0.809</cell><cell>0.535</cell></row><row><cell>w/o GRPB (FANet Variant)</cell><cell>0.796</cell><cell>0.785</cell><cell>0.598</cell><cell>0.802</cell><cell>0.820</cell><cell>0.608</cell><cell>0.834</cell><cell>0.883</cell><cell>0.649</cell></row><row><cell>FAST-VQA (Ours)</cell><cell cols="9">0.807 0.806 0.610 0.803 0.825 0.610 0.840 0.885 0.654</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Also, RAPIQUE<ref type="bibr" target="#b34">[35]</ref> can also infer rapidly on CPU that requires 17.3s for 1080P videos. Yet, it is not compatible with GPU Inference due to its handcrafted branch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Due to privacy reasons, the current public version of YouTube-UGC is incomplete and only with 1147 videos. The peer comparison is only for reference.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning generalized spatialtemporal deep feature representation for no-reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">In-capture mobile video distortions: A study of subjective behavior and objective algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2061" to="2077" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Konvid-150k: A dataset for noreference video quality assessment of videos in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?tz-Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Access 9</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="72139" to="72160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The konstanz natural video database (konvid-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sziranyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4041" to="4056" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for noreference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous estimation of image quality and distortion via multi-task convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on image processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ArXiv abs/1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Musiq: Multi-scale image quality transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="5148" to="5157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blind natural video quality prediction via statistical temporal features and deep spatial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3311" to="3319" />
		</imprint>
	</monogr>
	<note>MM &apos;20</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quality assessment of in-the-wild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2351" to="2359" />
		</imprint>
	</monogr>
	<note>MM &apos;19</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unified quality assessment of in-the-wild videos with mixed datasets training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1238" to="1257" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Which has better visual quality: The clear blue sky or a blurry animal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1221" to="1234" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the effectiveness of video perceptual representation in blind video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cvd2014-a database for evaluating no-reference video quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oittinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>H?kkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3073" to="3086" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the dct domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J&amp;apos;egou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient user-generated video quality prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 Picture Coding Symposium (PCS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ugc-vqa: Benchmarking blind video quality assessment for user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4449" to="4464" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rich features for perceptual quality assessment of ugc videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="13435" to="13444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Subjective quality assessment for youtube ugc dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">A</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10088</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Patch-vq: &apos;patching up&apos; the video quality problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)). pp</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)). pp</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="14019" to="14029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Long short-term convolutional transformer for no-reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia. p. 2112-2120. MM &apos;21</title>
		<meeting>the 29th ACM International Conference on Multimedia. p. 2112-2120. MM &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep neural networks for no-reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2349" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Blind image quality assessment using a deep bilinear convolutional neural network. IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="36" to="47" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
