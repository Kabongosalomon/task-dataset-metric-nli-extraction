<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Complementary Feature Enhanced Network with Vision Transformer for Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vol</forename><forename type="middle">X</forename><surname>Xxxxx</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">X</forename><surname>No</surname></persName>
						</author>
						<title level="a" type="main">Complementary Feature Enhanced Network with Vision Transformer for Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image dehazing</term>
					<term>Complementary feature enhanced framework</term>
					<term>Vision transformer</term>
					<term>Intrinsic image decom- position</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional CNNs-based dehazing models suffer from two essential issues: the dehazing framework (limited in interpretability) and the convolution layers (content-independent and ineffective to learn long-range dependency information). In this paper, firstly, we propose a new complementary feature enhanced framework, in which the complementary features are learned by several complementary subtasks and then together serve to boost the performance of the primary task. One of the prominent advantages of the new framework is that the purposively chosen complementary tasks can focus on learning weakly dependent complementary features, avoiding repetitive and ineffective learning of the networks. We design a new dehazing network based on such a framework. Specifically, we select the intrinsic image decomposition as the complementary tasks, where the reflectance and shading prediction subtasks are used to extract the color-wise and texture-wise complementary features. To effectively aggregate these complementary features, we propose a complementary features selection module (CFSM) to select the more useful features for image dehazing. Furthermore, we introduce a new version of vision transformer block, named Hybrid Local-Global Vision Transformer (HyLoG-ViT), and incorporate it within our dehazing networks. The HyLoG-ViT block consists of the local and the global vision transformer paths used to capture local and global dependencies. As a result, the HyLoG-ViT introduces locality in the networks and captures the global and long-range dependencies. Extensive experiments on homogeneous, non-homogeneous, and nighttime dehazing tasks reveal that the proposed dehazing network can achieve comparable or even better performance than CNNs-based dehazing models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N bad weather conditions (such as haze, mist, and fog), the captured outdoor images are usually degraded by the small particles or water droplets suspended in the atmosphere <ref type="bibr" target="#b0">[1]</ref>. Due to the atmospheric scattering, emission and absorption, the degraded images suffer from color distortion and texture blurring <ref type="bibr" target="#b1">[2]</ref>. Mathematically, the Atmospheric Scattering Model (ASM) popularly used to describe the hazy images is:</p><formula xml:id="formula_0">I D (p) = (I H (p) ? A)/t(p) + A,<label>(1)</label></formula><p>. where I H is the hazy image, I D is the scene radiance (hazefree image), A is the atmospheric light, t is the transmission map, and p is the pixel position.</p><p>To solve the ill-posed problem of the ASM (1), early priorbased single image dehazing methods use handcrafted priors that are observed from the color information, such as dark channel prior (DCP) <ref type="bibr" target="#b2">[3]</ref>, color attenuation prior (CAP) <ref type="bibr" target="#b3">[4]</ref> and haze-line prior (HLP) <ref type="bibr" target="#b4">[5]</ref>, and the texture information, such as change of detail prior (CoDP) <ref type="bibr" target="#b5">[6]</ref> and gradient channel prior (GCP) <ref type="bibr" target="#b6">[7]</ref>. However, when the priors are invalid in some instances, these methods may generate unnatural artifacts.</p><p>Recently, it has witnessed the flourishing and rapid development of Convolutional Neural Networks (CNNs), and many CNNs-based image dehazing methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> have been proposed to estimate the haze effects. These methods commonly outperform the prior-based method since the deep networks can implicitly learn the rich haze-relevant features and overcome the limitations of a single specific prior <ref type="bibr" target="#b10">[11]</ref>. However, existing CNNs-based dehazing models suffer from essential issues that stem from two aspects: the dehazing framework and the convolution layers.</p><p>To the first aspect, existing CNNs-based dehazing methods can be divided into two categories. The first is the physical-based framework (as illustrated in <ref type="figure" target="#fig_0">Fig.1 (a)</ref>), such as DehazeNet <ref type="bibr" target="#b7">[8]</ref>, multi-scale CNN model (MSCNN) <ref type="bibr" target="#b8">[9]</ref> and aggregated transmission propagation networks <ref type="bibr" target="#b11">[12]</ref>, which try to predict the atmospheric light A and/or transmission map t at the first step, and then use them to calculate the haze-free image I D according to the ASM <ref type="bibr" target="#b0">(1)</ref>. However, the ASM cannot completely represent the complex hazy imaging process since it ignores the emission and absorption of atmospheric particles, leading to ineffective dehazing results <ref type="bibr" target="#b12">[13]</ref>. Another category is the fully learning-based <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> (as illustrated in <ref type="figure" target="#fig_0">Fig.1 (b)</ref>), including enhanced Pix2pixHD dehazing network (EPDN) <ref type="bibr" target="#b14">[15]</ref>, GridDehazeNet (GDNet) <ref type="bibr" target="#b15">[16]</ref>, multi-scale boosted dehazing network (MSBDN) <ref type="bibr" target="#b16">[17]</ref> and feature fusion attention network (FFA) <ref type="bibr" target="#b17">[18]</ref>. Trained in an end-to-end fashion, these models directly recover the hazefree image. However, they have limitations in interpretability <ref type="bibr" target="#b10">[11]</ref> and usually appear ineffective in dense haze removal. As to the second aspect, on the one hand, convolution is contentindependent as the same convolution kernel is shared to the entire image, ignoring the distinct nature between different image regions <ref type="bibr" target="#b18">[19]</ref>. On the other hand, due to inductive biases such as locality, the convolutions are ineffective to learn longrange dependency information <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. The above discussions inspire us to provide a new framework and a more powerful mechanism to replace some of the convolution layers.  To these ends, we first introduce a novel complementary feature enhanced framework, as illustrated in <ref type="figure" target="#fig_0">Fig.1 (c)</ref>. Unlike previous frameworks that learn the numbers of various features at one time implicitly and inefficiently, the core idea behind the new framework is: different complementary subtasks focus on learning different specific features, i.e. the task-relevant complementary features, respectively; then, these complementary features are aggregated together and served to the primary task, i.e. the dehazing. We design a new dehazing network based on such framework, where specific complementary features are explored by corresponding subnetworks and then collaborated within the dehazing network. Motivated by the observations that color and texture perceptions are the critical visual cues toward identifying objects and understanding scenes <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, we select the intrinsic image decomposition as the 'complementary tasks', in which the intermediate features learned from reflectance prediction are served as the 'color-wise' complementary features, and the 'texture-wise' ones are learned from the shading prediction. That is, our method jointly learns intrinsic image decomposition and haze removal from a single image. Notice that directly aggregating the redundant, complementary features is inefficient, as the complementary subnetworks may output some haze-irrelevant features. Therefore, we further propose a Complementary Features Selection Module (CFSM) to automatically select the 'right' features that are helpful to the dehazing task and weaken the irrelevant ones.</p><p>Very recently, vision transformer <ref type="bibr" target="#b23">[24]</ref> has been known as an alternative to CNNs to learn visual representations due to its content-dependent interactions <ref type="bibr" target="#b24">[25]</ref> and flexibility in modelling long-range dependencies <ref type="bibr" target="#b25">[26]</ref>. However, the quadratic increase in complexity with image size hinders its application on dehazing tasks, which requires high-resolution feature maps. Additionally, it is undeniable that global and local information aggregation are useful for low-level vision tasks, while the vision transformer does not possess the locality <ref type="bibr" target="#b23">[24]</ref>. Inspired by these researches, we propose a Hybrid Local-Global Vision Transformer (HyLoG-ViT), which consists of the local and global vision transformer paths. In the local vision transformer path, the standard transformer blocks are operated in a grid of non-overlapped regions, enabling the model to capture the fine-grained and short-distance information within the local regions. In the global vision transformer path, one transformer block is operated on the downscaled feature maps to capture the global and long-range dependencies. Then, the features from the two paths are hybridized by a convolution layer to improve the local continuity. Compared with the vanilla vision transformer architecture, the HyLoG-ViT has lower computational complexity and brings locality mechanisms to the networks.</p><p>Incorporating the HyLoG-ViT within the complementary feature enhanced framework, we build our dehazing network. Previous works designed specific approaches for different hazy scenes, such as the two-branch neural network (TBNN) <ref type="bibr" target="#b26">[27]</ref>, discrete wavelet transform GAN (DW-GAN) <ref type="bibr" target="#b27">[28]</ref> and ensemble dehazing networks (EDN) <ref type="bibr" target="#b28">[29]</ref> for non-homogeneous dehazing, and maximum reflectance prior (MRP) <ref type="bibr" target="#b29">[30]</ref> and optimal-scale fusion-based dehazing (OSFD) <ref type="bibr" target="#b30">[31]</ref> methods for nighttime dehazing. Interestingly, extensive experiments on homogeneous, non-homogeneous, and nighttime dehazing tasks reveal that our network exhibits good generalization performances on different hazy scenes without any changes in the network architecture during the training. The main contributions of this work are summarized as follows:</p><p>? We propose a new framework and built a complementary feature enhanced network for image dehazing by jointly learning the intrinsic image decomposition and image dehazing. The reflectance and shading prediction tasks provide rich complementary features for the dehazing task, enabling the network to generate high-quality hazefree images with natural color and fine details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single Image Dehazing</head><p>Single image dehazing is an ill-posed problem. Without any auxiliary information, previous dehazing algorithms require the introduction of specific prior knowledge. Through research in the last decade, many priors have been proposed based on the cues of color (such as the DCP <ref type="bibr" target="#b2">[3]</ref>, CAP <ref type="bibr" target="#b3">[4]</ref>, and HLP <ref type="bibr" target="#b4">[5]</ref>) and texture (such as the CoDP <ref type="bibr" target="#b5">[6]</ref> and GCP <ref type="bibr" target="#b6">[7]</ref>). For example, Berman et al. <ref type="bibr" target="#b4">[5]</ref> find that for a haze-free image, its pixels that belong to the same color cluster form to a point while forming a line in a hazy image, namely the haze-line. The GCP uses the image gradient to estimate the depth information and the atmospheric light, preserving texture details more efficiently.   Recently, CNNs-based dehazing networks have been extensively studied. One kind of CNNs-based network restores the clear image by estimating the intermediate variables in the atmospheric scattering model <ref type="bibr" target="#b31">[32]</ref> and then calculates the clear image. For example, the multi-scale CNNs dehazing model (MSCNN) <ref type="bibr" target="#b8">[9]</ref> utilizes a coarse-scale network to estimate the complete transmission map and use a fine-scale network to refine the results. More recent CNNs-based networks tend to learn hazy-to-clear image translation directly <ref type="bibr" target="#b32">[33]</ref>. For example, the MSBDN <ref type="bibr" target="#b16">[17]</ref> removes haze in an end-to-end manner by incorporating boosting and error feedback principles into a U-Net <ref type="bibr" target="#b33">[34]</ref> architecture with a dense feature fusion module.</p><p>Unlike the previous dehazing frameworks, our method relies neither on the atmospheric scattering model nor on a completely black box system. The subnetworks of intrinsic image decomposition provide color-and texture-wise complementary features, enabling the dehazing subnetwork to yield haze-free images with natural color and fine details.</p><p>Non-homogeneous and nighttime dehazing. For different complicated haze patterns, the dehazing method should be specifically designed, such as the TBNN <ref type="bibr" target="#b26">[27]</ref>, DW-GAN <ref type="bibr" target="#b27">[28]</ref> and EDN <ref type="bibr" target="#b28">[29]</ref> for non-homogeneous dehazing, and MRP <ref type="bibr" target="#b29">[30]</ref> and OSFD <ref type="bibr" target="#b30">[31]</ref> methods for nighttime dehazing. Unlike these models, training on different datasets, our network exhibits good generalization performances on homogeneous, non-homogeneous, and nighttime dehazing tasks without any changes in the network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Vision Transformer</head><p>Very recently, vision transformers <ref type="bibr" target="#b23">[24]</ref> have received increasing research interest in image and video vision tasks, including object detection <ref type="bibr" target="#b34">[35]</ref>, image classification <ref type="bibr" target="#b23">[24]</ref> and semantic segmentation <ref type="bibr" target="#b19">[20]</ref>. Many new versions of vision transformers <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> have been proposed to relieve the high computational cost problems. For example, the Swin Transformer <ref type="bibr" target="#b36">[37]</ref> uses a locally-grouped self-attention <ref type="bibr" target="#b25">[26]</ref>, where the input features are separated into a grid of nonoverlapped windows and the vision transformer is operated only within each window. Many other methods have been proposed to bring inductive biases into the vision transformer <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, such as the LocalViT <ref type="bibr" target="#b40">[41]</ref>, which brings a locality mechanism to ViT by employing the depth-wise convolution into the feed-forward network. CvT <ref type="bibr" target="#b39">[40]</ref> proposes a convolutional token embedding to model local spatial contexts and a convolutional projection layer to provide efficiency benefits.</p><p>New versions of vision transformers are also developed and used in low-level vision tasks. For example, the Uformer <ref type="bibr" target="#b42">[43]</ref> designs a general U-shaped Transformer-based structure for image restoration. It also proposes a locally-enhanced window Transformer block to reduce the computation cost. The SwinIR <ref type="bibr" target="#b20">[21]</ref> utilizes residual Swin Transformer blocks to extract deep features for image restoration.</p><p>In our HyLoG-ViT, the local vision transformer path is similar to the LeWin transformer proposed in Uformer. However, the local version used in Uformer alone fails to effectively model global dependencies and preserve the local continuity around those regions. By contrast, our HyLoG-ViT block can capture both local and global dependencies simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Complementary Features Enhanced Framework</head><p>To achieve high performance of haze removal, we propose a novel framework from a new perspective, namely the complementary feature enhanced framework. In this framework, each group of complementary features are learned by corresponding complementary tasks, and together they serve the primary task (i.e. the dehazing task in this paper  Before answering these problems, we here analyse the essential criterion the complementary features should follow. First, each kind of complementary feature represents one interference factor closely related to the prime task. Second, the different complementary features should be weakly correlated with each other so that each complement subtask can focus on doing one thing more effectively. Otherwise, multiple subnetworks learn similar representations, making the network very inefficient. That is, 'Each performs one's own functions; together serve their common mission'.</p><formula xml:id="formula_1">dR 1 dS 1 (b) Encoder (E) e Z e 0</formula><p>Complementary Features: Intuitively, the images captured in hazy or foggy scenes suffer from some interference factors, and the most conspicuous ones are the variation of color and blurriness of edges. Many previous works have witnessed that the information of color <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> and texture <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> are the crucial priors for estimating the haze distribution. Inspired by these observations, we endeavour to build the corresponding subnetworks to learn the color-wise and texturewise complementary features.</p><p>Complementary Tasks: In further investigations, we fund that the intrinsic image decomposition (which decomposes an image into reflectance and shading, as shown in <ref type="figure" target="#fig_3">Fig.2  (a)</ref>) is a good candidate as the complementary task; as the reflectance component contains the actual color of the scenes, and the shading component contains the structure and texture information <ref type="bibr" target="#b44">[45]</ref>. To demonstrate these, we analyse the characters of the reflectance and shading of the hazy and clear images on the homogeneous, non-homogeneous and nighttime haze datasets. <ref type="figure" target="#fig_3">Fig.2</ref>   . We find that the r nve of I CS is very close to the one of I C on the tree datasets, indicating that the shading I CS preserves rich edge and texture information. Finally, we choose the Unsupervised Learning for Intrinsic Image Decomposition (USI3D) <ref type="bibr" target="#b43">[44]</ref> method to generate clear reflectance and shading samples in our training datasets. Most importantly, the USI3D assumes reflectanceshading independence, coinciding with the second criterion of complementary features as aforementioned. The reflectance predilection subnetwork in our model can focus on color-wise complementary features learning, avoiding the interferences of shading components; and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dehazing Network</head><p>In our dehazing model, the intrinsic image decomposition and dehazing are considered a joint model by exploring the former's complementary features that the latter requires. Specifically, our network is a multi-task learning model with a decoder-focused architecture <ref type="bibr" target="#b47">[48]</ref>.</p><p>1) Overall Architecture: The overview of our dehazing network consists of a shared encoder and three decoders, as illustrated in <ref type="figure" target="#fig_4">Fig.3</ref>. Denote the hazy image is I H , the shared encoder E is used to extract the shallow and deep features:</p><formula xml:id="formula_2">e 0 = F 0 E (I H ), e z = F z E (e z?1 ),<label>(2)</label></formula><p>where F 0 E denotes the feature extraction layer used to extract shallow feature e 0 ; F z E denotes z-th stage of encoder E, e z refers to the deep feature at stage z. z ? [1, ? ? ? , Z], and Z is the total number of stages. Three parallel decoders follow the encoder. Decoders D R and D S are used to predict the reflectance and shading of the haze-free image, respectively, and their intermediate features are served as complementary features to the decoder D D to generate the high-quality hazefree image. The decoders are described as:</p><formula xml:id="formula_3">d Z R = F Z D R (d Z E ), d z R = F z D R (d z?1 R , e z ),<label>(3)</label></formula><formula xml:id="formula_4">d Z S = F Z D S (d Z E ), d z S = F z D S (d z?1 S , e z ),<label>(4)</label></formula><formula xml:id="formula_5">d Z D = F Z D D (e Z , d Z R , d Z S ), d z D = F z D D (d z?1 D , d z R , d z S ),<label>(5)</label></formula><p>where F z D R , F z D S and F z D D denote the z-th stage of decoder D R , D S and D D , respectively, z ? [1, ? ? ? , Z]. d z R , d z R and d z R are the intermediate features of the decoder D R , D S and D D at stage z, respectively. The final reflectance I R , shading I S , and haze-free image I D are generated through three image reconstruction layers, i.e., F 0 D R , F 0 D S and F 0 D D , respectively:</p><formula xml:id="formula_6">I R =F 0 D R (d 1 R , e 0 ), I S = F 0 D S (d 1 S , e 0 ), I D =F 0 D D (d 1 D , d 1 R , d 1 S , e 0 ).<label>(6)</label></formula><p>2) Details of the Encoder and Decoder: The encoder E comprises of a feature extraction layer and a series of HyLoG-ViT blocks (see section III-D). The extraction layer is built by a 5 ? 5 convolution and a basic ResNet block <ref type="bibr" target="#b48">[49]</ref> to extract shallow features. Each HyLoG-ViT block is followed by a 4?4 convolution to downscale the spatial resolution with stride 2 and double the channel number. The convolutions used here can also bring the inductive bias into this 'transformerbased' encoder. In the decoder D R and D S , except the bottom stage Z, feature d z?1 R/S from the previous decoder block is first upscaled by a 4?4 deconvolution to expand the spatial resolution with stride 2 and halve the channel number. Then, the output is concatenated with the feature e z from the same stage of encoder E. Therefore, the subnetworks E-D R and E-D S are formed into two U-shaped structures, which alleviates the issue of spatial information loss caused by downscaling. Different from decoder D R and D S , for z-th stage in D D , there have three inputs: the feature d z?1 D from the previous stage; the complementary features d z R and d z S from the same stage of D R and D S , respectively. These input features are fed into a complementary features selection module to select the most useful channels from d z R and d z S dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Complementary Features Selection Module (CFSM)</head><p>One can simply aggregate the complementary features d z R and d z S via element-wise summation or concatenation operation, which, however, is inefficient. Therefore, we propose the CFSM to fuse the complementary features in a nonlinear fashion. The architecture of the CFSM is illustrated in <ref type="figure">Fig.4</ref>.</p><p>Given the complementary features d z R and d z S and the feature d z?1 D (? R h?w?c ), they are first combined via elementwise summation. Then, the outputs are transformed to two channel-wise statistics s ave and s max (? R 1?1?c ) by a global average pooling and a global max pooling, respectively. Each statistic is separated into two streams: one stream for d z R feature selection and another for d z S . Take d z R stream as an example, the s ave and s max are followed by channeldownscaling 1 ? 1 convolution layers to calculate the compact feature vectors, v ave R and v max R (? R 1?1?c/r , where r = 4 in our model), respectively. The feature vectors are fed into two parallel channel-upscaling 1 ? 1 convolution layers and provide feature descriptors t ave R and t max R (? R 1?1?c ). The final feature descriptor is defined as t R = t ave R +t max R . The t R passes through the Sigmoid function to generate attention score a R (? R 1?1?c ) for d z R . Similar, we can get the attention score a S for d z S . The overall process of feature recalibration and aggregation is defined as:  <ref type="figure">Fig. 4</ref>. Schematic for the complementary features selection module (CFSM). ? denotes the element-wise summation. ? denotes the element-wise production.</p><formula xml:id="formula_7">d z?1 D = d z?1 D + a R ? d z R + a S ? d z S .<label>(7)</label></formula><p>Note that our CFSM utilizes the global average-and maxpooling to gather important clues about complementary features, increasing the representation power and can inferring finer channel-wise attention <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hybrid Local-Global Vision Transformer (HyLoG-ViT)</head><p>We propose a HyLoG-ViT model to address the challenges of high computation cost and lack of locality. As shown in <ref type="figure" target="#fig_6">Fig.5</ref>, a HyLoG-ViT block consists of two paths. In the local vision transformer path, the input feature map is grouped into grids of non-overlapped regions, and the transformer block is applied only within each region. Given the feature maps X ? R H?W ?C where the H, W and C are the height, width and channel number of the maps, the computation of the local vision transformer path can be expressed as:</p><formula xml:id="formula_8">X = {X 1 , X 2 , ? ? ? , X N l ?N l }; X i l = T ransf ormer(X i ), i ? [1, 2, ? ? ? , N l ? N l ]; X l = {X 1 l , X 2 l , ? ? ? , X N l ?N l },<label>(8)</label></formula><p>where X i l is the output of the i-th region, X l is the combined output of the local vision transformer path; T ransf ormer(?) represents the standard transformer layer, as illustrated in <ref type="figure" target="#fig_6">Fig.5  (b)</ref>. N l is the region number per column/row. This design enables the vision transformer to focus on capturing regionlevel attention and exploring local context information. However, the local vision transformer path cannot model global dependencies and lose local continuity around those regions. Therefore, we also introduce the global vision transformer path, where the input feature is down-sampled by the average pooling operation to reduce the spatial resolution. The output is fed into the standard transformer layer. The computation of the global vision transformer path is: X g =U psamp ?Ng (T ransf ormer(Avepool ?Ng (X)));</p><p>(9) where X g is the output of the global vision transformer path. Avepool ?Ng (?) is the average pooling with reduction ratio N g ; U psamp ?Ng (?) represents the upsampling operation with upscale ratio N g . The global vision transformer path improves efficiency while still maintaining the capability of aggregating global information. The hybrid outputs of the two paths are concatenated and transformed to the original dimension by a 3 ? 3 convolution layer:</p><formula xml:id="formula_9">X h = Conv 3?3 (Concat(X l , X g ));</formula><p>(10) where X h is the final output of HyLoG-ViT block; Concat(?) is the channel-wise concatenation; Conv 3?3 (?) is a 3 ? 3 convolution layer. This fuse operation maintains the merits of the local and global vision transformer and introduces locality into the network.</p><p>Positional Encoding: The positional encoding used in transformers aims to retain positional information. However, for low-level vision tasks, the input images with varying sizes are commonly different from the training ones. As a result, it may decrease performance <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> and break the translation invariance <ref type="bibr" target="#b25">[26]</ref>. Note that our model can achieve good results even without additional positional encoding modules. We argue that the 3 ? 3 convolution used in <ref type="formula" target="#formula_0">(10)</ref> is sufficient to provide positional information for Transformers.</p><p>Complexity Analysis: For the local vision transformer path, denote the input feature X ? R H?W ?C is grouped into N l ? N l regions and the spatial-resolution of each region is</p><formula xml:id="formula_10">H N l ? W N l ? C.</formula><p>Then, the complexity of the self-attention in the local vision transformer path is reduced from O((HW ) 2 C) to O( (HW ) 2 N 2 l C) compared with the standard self-attention. For the global vision transformer path, denote the spatial dimension of the input feature is downscaled by a reduction ratio N g , and the complexity is O( (HW ) 2 N 2 g C). Therefore, the total self-attention computational complexity of HyLoG-ViT block is O(( 1</p><formula xml:id="formula_11">N 2 l + 1 N 2 g )(HW ) 2 C).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Function</head><p>In our training, the ground truths of reflectance I CR and shading I CS are generated by operating the pre-trained intrinsic image decomposition model USI3D <ref type="bibr" target="#b43">[44]</ref> on the ground truth of clear image I C . For reflectance and haze-free image estimations, we train the networks using the L2 reconstruction loss, conditional adversarial loss <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>, and SSIM loss (as used in <ref type="bibr" target="#b54">[55]</ref>). For the shading estimation, we use the L2 reconstruction loss and an edge preserve loss which is defined as:</p><formula xml:id="formula_12">L e =E I S [|| x I S (p) ? x I CS (p)|| 2 + || y I S (p)) ? y I CS (p)|| 2 ],<label>(11)</label></formula><p>where E is the mean operation on a batch of samples; x and y are the spatial derivatives at x and y directions, respectively. p refers to the spatial location. Overall, the hybrid loss function contains three parts, that is:</p><formula xml:id="formula_13">L = ? R L R + ? S L S + ? D L D ,<label>(12)</label></formula><p>where L R , L S , and L D are the loss functions for the three vision tasks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>A. Experiment Setup 1) Dataset: Our dehazing model is trained on outdoor datasets, including RESIDE dataset <ref type="bibr" target="#b55">[56]</ref>, NH-HAZE dataset <ref type="bibr" target="#b56">[57]</ref> and NHR dataset <ref type="bibr" target="#b30">[31]</ref> for homogeneous haze, nonhomogeneous haze and nighttime haze removal, respectively. For RESIDE dataset, we randomly select 41240 samples from OTS <ref type="bibr" target="#b55">[56]</ref> (outdoor training subset in RESIDE) for training and 500 samples from SOTS <ref type="bibr" target="#b55">[56]</ref> (synthetic objective testing subset in RESIDE) for testing. For the NH-HAZE dataset, we synthetic 9800 samples augmented from 50 original highresolution samples by randomly cropping, flipping, and rotating; and the 41?45-th samples are used for qualitative evaluations. The NHR dataset contains 17900 samples; we select the last 475 for testing and others for training. We also collect 372 and 132 real-world daytime and nighttime hazy images to evaluate the performance of our model.</p><p>2) Implemental Details: Our dehazing model is implemented with PyTorch library and trained on one NVIDIA GeForce RTX 3090 GPU. Our model is trained for 32 epochs on the RESIDE dataset, 20 epochs on the NH-HAZE dataset and 5 epochs on the NHR dataset. We adopt Adam <ref type="bibr" target="#b57">[58]</ref> optimizer with the initial learning rate is 10 ?4 for both CNN and vision transformer backbones. We use the layer normalization <ref type="bibr" target="#b58">[59]</ref> in the vision transformer blocks and Activation Normalization <ref type="bibr" target="#b59">[60]</ref> in CNN layers. The parameters of the loss functions are set as (? R ,? S , ? D ) = (1, 1, 1.5). For the HyLoG-ViT block, we set N l = 8 and the patch size as 2 ? 2 in the local vision transformer path; and in the global vision transformer path, we set N g = 4 and the patch size as 4 ? 4. The code of our model is available at https://github.com/phoenixtreesky7/CFEN-ViT-Dehazing.</p><p>3) Metrics: We use the peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) <ref type="bibr" target="#b60">[61]</ref>, learned perceptual image patch similarity (LPIPS) <ref type="bibr" target="#b61">[62]</ref> and CIEDE2000 <ref type="bibr" target="#b45">[46]</ref> to evaluate the dehazing methods on the synthetic datasets. LPIPS measures the 'perceptual distance' of the two compared images using deep features, which coincides well with human perceptual similarity judgments <ref type="bibr" target="#b62">[63]</ref>. We use the CIEDE2000 to measure the color difference between the dehazed image and its corresponding clear image. We set the parameters [K L , K 1 , K 2 ] in CIEDE2000 are [2, 0.045, 0.015] in the experiments. Another metric we used is the bind contrast enhancement assessment <ref type="bibr" target="#b46">[47]</ref> which contains three indicators, that are: 1) the rate of newly visible edges (r nve ) to evaluate the ability of edge restoration; 2) the geometric mean of the normalized gradient (m ng ) to measure the quality of the image contrast; 3) the rate of saturated pixels (r sp ) to measure the degree of over-saturation <ref type="bibr" target="#b46">[47]</ref>. These three indicators are used to evaluate dehazing models on real-world datasets.  <ref type="bibr" target="#b2">[3]</ref>, HLP <ref type="bibr" target="#b4">[5]</ref>, MSCNN <ref type="bibr" target="#b8">[9]</ref>, GDNet <ref type="bibr" target="#b15">[16]</ref>, EPDN <ref type="bibr" target="#b14">[15]</ref>, MSBDN <ref type="bibr" target="#b16">[17]</ref>, FFA <ref type="bibr" target="#b17">[18]</ref> and Ours, respectively. (l): the ground truth.  <ref type="bibr" target="#b2">[3]</ref>, HLP <ref type="bibr" target="#b4">[5]</ref>, MSCNN <ref type="bibr" target="#b8">[9]</ref>, GDNet <ref type="bibr" target="#b15">[16]</ref>, EPDN <ref type="bibr" target="#b14">[15]</ref>, MSBDN <ref type="bibr" target="#b16">[17]</ref> and FFA <ref type="bibr" target="#b17">[18]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons with State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Homogeneous Dehazing:</head><p>To demonstrate the effectiveness of our dehazing model on homogeneous image dehazing, we firstly evaluate it on the synthetic datasets, including SOTS (outdoor) <ref type="bibr" target="#b55">[56]</ref>, O-HAZE <ref type="bibr" target="#b63">[64]</ref>, and HazeRD <ref type="bibr" target="#b64">[65]</ref>. We compare our method with the prior-based and learning-based models, including DCP <ref type="bibr" target="#b2">[3]</ref>, HLP <ref type="bibr" target="#b4">[5]</ref>, MSCNN <ref type="bibr" target="#b8">[9]</ref>, GDNet <ref type="bibr" target="#b15">[16]</ref>, EPDN <ref type="bibr" target="#b14">[15]</ref>, MSBDN <ref type="bibr" target="#b16">[17]</ref>, and FFA <ref type="bibr" target="#b17">[18]</ref>.</p><p>It can be found from <ref type="figure" target="#fig_7">Fig.6 (i)</ref>, over-saturation phenomenon emerges at the sky regions in the prior-based methods of DCP and HLP, indicating that those priors may be invalid under such conditions. The deep learning-based approaches can provide visually pleasant dehazed results on the SOTS dataset <ref type="bibr" target="#b55">[56]</ref>. The metrics of PSNR, SSIM and LPIPS also demonstrate that GDNet and MSBDN obtain the top two scores on the SOTS dataset. However, those methods exhibit relatively poor performances on the other two challenging datasets, either leading to color distortion results (such as EPDN) or leaving haze at distant scenes (such as GDNet, MSBDN and FFA), as illustrated in <ref type="figure" target="#fig_7">Fig.6</ref> (ii) and (iii). Our model obtains the best PSNR, SSIM, LPIPS and CIEDE2000 on the O-HAZE and HazeRD datasets.</p><p>We further evaluate our method on real-world datasets. The visual comparisons are shown in <ref type="figure" target="#fig_8">Fig.7</ref>. The prior-based methods, i.e. DCP and HLP, tend to yield over-enhanced visual artifacts, especially in the sky regions. Although the CNNs-based methods mitigate the over-estimation artifacts successfully, some of them would produce insufficient dehazing in distant scenes. For example, the dehazed images of MSBDN and FFA appeared almost indistinguishable from the original hazy ones. The reason might lie in the overfitting on the training datasets. As obviously shown in <ref type="figure" target="#fig_8">Fig.7 (b)</ref>, our dehazing model produces plausible visual dehazing results with vivid color and fine details. The clear superiority of our model on the real-world dataset is attributed to the complementary feature enhanced framework. The experimental results also demonstrate that our model learns the color-and texture-wise complementary features effectively.</p><p>2) Non-Homogeneous Dehazing: We then evaluate our method on the non-homogeneous dehazing dataset NH-  HAZE. Our model is compared with several latest nonhomogeneous dehazing models, including EDN-AT <ref type="bibr" target="#b28">[29]</ref>, EDN-3J <ref type="bibr" target="#b28">[29]</ref>, TWNN <ref type="bibr" target="#b26">[27]</ref> and DWGAN <ref type="bibr" target="#b27">[28]</ref>. <ref type="figure" target="#fig_9">Fig.8</ref> reveals the visual comparisons. As we can see, all of the methods remove the non-homogeneous haze successfully. However, the EDN-3J fails to remove haze from the stone steps; the other compared methods seem to fail to restore accurate color at the bushes and lawns (see the second row in <ref type="figure" target="#fig_9">Fig.8</ref>). While our model successfully avoids these issues due to the reflectance subnetwork learning accurate color information, achieving more accurate dehazing results on the NH-HAZE dataset than other methods. The quantitative results are shown in <ref type="table" target="#tab_6">Table II</ref> also reveal that our model obtains the best results, surpassing the second best methods by a considerable margin, surpassing the second best methods 2.13 dB on PSNR, 0.005 on SSIM, 0.025 on LPIPS, and 0.48 on CIEDE2000, respectively. The reason may lie in the proposed HyLoG-ViT, which has a stronger ability in modelling long-range context information than the vanilla CNN.</p><p>3) Nighttime Dehazing: We further evaluate our dehzaing model on the nighttime dehazing, comparing with a lowlight image enhancement and several latest nighttime dehazing models, i.e. the LIME <ref type="bibr" target="#b65">[66]</ref>, LCD <ref type="bibr" target="#b66">[67]</ref>, MRP <ref type="bibr" target="#b29">[30]</ref> and OSFD <ref type="bibr" target="#b30">[31]</ref>. The visual results tested on the synthetic NHR dataset <ref type="bibr" target="#b30">[31]</ref> and real-world dataset are illustrated in <ref type="figure" target="#fig_10">Fig.9</ref> and <ref type="figure" target="#fig_0">Fig.10</ref>,  <ref type="figure" target="#fig_10">Fig.9</ref> and bright regions around the street lamps in <ref type="figure" target="#fig_0">Fig.10</ref> with over-saturation artifacts. While our method produces pleasingly smooth results in such regions, which looks more natural. The qualitative results as shown in <ref type="table" target="#tab_6">Table  III</ref> also demonstrate that our model can perform surprisingly well on nighttime dehazing, outperforming the state-of-the-art MRP and OSFD methods with respect to PSNR, SSIM, LPIPS and CIEDE2000. Especially, our CIEDE2000 is significantly lower than the MRP with 14.54, indicating that the colorwise complementary features learned from the reflectance subnetwork play a beneficial role in preserving accurate color.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>Due to the stronger ability in modelling long-range context information, the network structure with transformer blocks is naturally good at learning the spatially variant features <ref type="bibr" target="#b42">[43]</ref>. Our experiments verify this conclusion. The samples in the SOTS are synthetic by setting the atmospheric light and scattering coefficient as global constants. While in the O-HAZE and NH-HAZE datasets, hazy images are captured in natural scenes where the haze is generated by professional haze machines <ref type="bibr" target="#b63">[64]</ref>. Under foggy and hazy conditions, the atmospheric light and scattering coefficient will no longer be globally constant in natural scenes. Therefore, as shown in <ref type="table" target="#tab_6">Table I</ref> and II, our method can surpass the SOTAs by a considerable margin on the O-HAZE and NH-HAZE, performing relatively better than on the homogeneous haze SOTS dataset.  <ref type="bibr" target="#b65">[66]</ref>, LCD <ref type="bibr" target="#b66">[67]</ref>, MRP <ref type="bibr" target="#b29">[30]</ref> and OSFD <ref type="bibr" target="#b30">[31]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>For quick experiments, all of the models are trained for 10 epochs and evaluated on 300 samples randomly selected from the SOTS <ref type="bibr" target="#b55">[56]</ref> dataset in our ablation studies.</p><p>1) Evaluations on the Framework: We conduct the ablation studies to demonstrate the contribution of the joint model. 1) w/o-RS: our dehazing network without any complementary features, removing the decoders D R and D S ; 2) w/o-S: our dehazing model removing the decoder D S ; 3) w/o-R: our dehazing model removing the decoder D R . Comparing models of w/o-RS, w/o-S, and w/o-R, the latter two models obtain higher PSNR and SSIM than the former, indicating that the joint learning mechanism with complementary features of reflectance or shading indeed boosts the network's dehazing performance. Furthermore, when both complementary features are leveraged in our model, it achieves the best performance, demonstrating the effectiveness of the proposed framework.</p><p>2) Evaluations on the CFSM: For checking the contribution of the CFSM, we conduct the experiment where the model w/o-CFSM refers to the dehazing model that with-out CFSM yet merge the complementary features d z R , d z S and features d z D by summation. The dehazing results are shown in the second row in Tabel V. As we can find, without the CFSM, the dehazing results reduce 0.671 dB on PSNR and 0.0029 on SSIM, indicating the importance and benefits of using the CFSM in the dehazing network.</p><p>3) Evaluations on the HyLoG-ViT: We further evaluate the effectiveness of the proposed HyLoG-ViT model for image dehazing by involving the following different configurations, where the HyLoG-ViT block is replaced by: 1) two basic ResNet <ref type="bibr" target="#b48">[49]</ref> blocks (CNN); 2) the basic ViT block (ViT); 3) the local ViT path (L-ViT); 4) the global ViT path (G-ViT); 5) the sequential stacked local and global ViT paths (LoG-ViT). The model CNN achieves better dehazing performance than the models of ViT and G-ViT. One of the reasons is that Transformers-based models lack some of the inductive biases inherent to model CNN, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient data. Models of LoG-ViT and HyLoG-ViT obtain the top two PSNR and SSIM, indicating that combining local and global interactions are more effective than other models. Comparing them, we can find that our parallel hybrid scheme is better than the sequentially stacked one. 4) Position Encoding: In this experiment, we will show that, without the position encoding/embedding, our model can also achieve high performance on image dehazing. We also verify that the 3?3 convolution layer that fuses the hybrid features from local and global ViT paths is useful. Hence, we compared the following models: 1) PE: our HyLoG adds the learnable position encodings as used in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b67">68]</ref>; 2) ADD: the hybrid features are fused by element-wise summation. The quantitative results are shown in <ref type="table" target="#tab_6">Table VI</ref>. As we can find, there are no significant performance gains to the SSIM when adding the position encoding. The PE model gets even lower the PSNR than Our model. However, the ADD model performs worse than the other two models with respect to the PSNR and SSIM, demonstrating that the 3?3 convolution layer for hybrid features' fusion is vital in the HyLoG-ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUTION</head><p>This paper aims to mitigate the issues of CNNs-based dehazing networks by introducing a new complementary feature enhanced framework and a hybrid local and global vision transformer. To these ends, we first propose a new dehazing framework, which jointly learns the intrinsic image decomposition and dehazing. The reflectance and shading prediction tasks encourage the networks to learn more useful complementary features for image dehazing task. We propose a complementary features selection module to effectively aggregate those complementary features to enhance the useful complementary features while weakening the irrelevant ones. Then, we introduce vision transformers into the dehazing task. We design a new hybrid local-global vision transformer (HyLoG-ViT) which is more computationally efficient than the standard ViT, as it can capture both local and global dependencies. We conduct extensive experiments on homogeneous, non-homogeneous, and nighttime dehazing tasks to evaluate our method. Qualitative and quantitative results reveal that our method achieves comparable or even better performance than CNNs-based dehazing methods, demonstrating the effectiveness of the proposed framework and the HyLoG-ViT. We strongly believe that the complementary feature enhanced framework can be deeply explored and achieve outstanding performances on other image restoration tasks, such as image deraining, low-light image enhancement and old photo restoration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Comparisons with the different CNNs-based dehazing frameworks: (a) physical-based framework, (b) fully learning-based framework, and (c) the proposed complementary feature enhanced framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc>To effectively fuse the complementary features, we propose a Complementary Features Selection Module (CFSM). The CFSM considerably improves the effectiveness of feature aggregation by adaptively enhancing the proper complementary feature channels while weakening the irrelevant ones. ? We propose a new variant of vision transformer, namely Hybrid Local-Global Vision Transformer (HyLoG-ViT), which can model both local and global dependencies with lower computational cost than the vanilla vision transformer. With the HyLoG-ViT, we propose a transformer-based dehazing network for the first time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Shading of IC (ICS) Reflectance of IC (ICR) Clear image (IC) Hazy image (IH) Shading of IH (IHS) Reflectance of IH (IHR) Visible gradients of IH</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Investigations on the intrinsic image decomposition. (a) The reflectance and shading maps of clear and hazy images. (b) Investigations on the reflectance, including ?E maps (left) and CIEDE2000 (right) scores of hazy image I H , hazy reflectance I HR and clear reflectance I CR compared with the clear image I C . The I CR gets the lowest scores on both of the three datasets, indicating that I CR preserves similar color information as the I C . (c) Investigations on the shading, including the visible gradients maps (left) and the rate of newly visible edges indicators (right) of the hazy shading I HS , clear shading I CS and clear image I C . As we can find, I CS and I C contains rich edges information with higher scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Architectures of our dehazing network. (a): the overall dehazing network, which consists of an encoder and three different decoders. (b): the architecture of the shared encoder E. (c): the architecture of decoder D R /D S . (d): the architecture of decoder D D . USI3D: a pre-trained unsupervised intrinsic image decomposition model proposed by<ref type="bibr" target="#b43">[44]</ref>. CFSM: the complementary features selection module. L R , L S and L D : loss functions for the three tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 (</head><label>2</label><figDesc>c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Diagram of the hybrid local-global vision transformer (HyLoG-ViT). f represents the fuse operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Dehazing results of the Synthetic Homogeneous hazy images (on the RESIDE, O-HAZE and HazeRD datasets). (a): the hazy images. (b)-(k): the dehazing results of DCP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Dehazing results of the Real-World Homogeneous hazy images. (a): the hazy images. (b): Our dehazing results. (c)-(k): the dehazing results of Ours, DCP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Dehazing results of the Non-Homogeneous hazy images (on the NH-HAZE dataset). (a): the hazy images. (b)-(f): the dehazing results of EDN-AT<ref type="bibr" target="#b28">[29]</ref>, EDN-3J<ref type="bibr" target="#b28">[29]</ref>, TWNN<ref type="bibr" target="#b26">[27]</ref>, DWGAN<ref type="bibr" target="#b27">[28]</ref> and Ours, respectively. (g): the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Dehazing results of the Synthetic Nighttime hazy images (on NHR dataset). (a): the hazy images. (b)-(f): the dehazing results of LIME<ref type="bibr" target="#b65">[66]</ref>, LCD<ref type="bibr" target="#b66">[67]</ref>, MRP<ref type="bibr" target="#b29">[30]</ref>, OSFD<ref type="bibr" target="#b30">[31]</ref> and Ours, respectively. (g): the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>OSFD (b) Ours (e) MRP (d) LCD Dehazing results of the Real-World Nighttime hazy images. (a): the hazy images. (b): Our dehazing results. (c)-(f): the dehazing results of LIME</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>D. Zhao, J. Li, and H. Li are with the State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University. . J. Li and L. Xu are with the Key Laboratory of Solar Activity, National Astronomical Observatories, Chinese Academy of Sciences. . J. Li and L. Xu are also with the Peng Cheng Laboratory, Shenzhen 518000, China (e-mail: jiali@buaa.edu.cn) . J. Li is the corresponding author. URL: http://cvteam.net</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>?E map of IH?E map of IHR ?E map of ICR</figDesc><table><row><cell></cell><cell cols="3">Homogeneous haze</cell><cell cols="3">Non-Homogeneous haze</cell><cell>Nighttime haze</cell></row><row><cell></cell><cell cols="3">( O-HAZE)</cell><cell cols="3">(NH-HAZE)</cell><cell>(NHR)</cell></row><row><cell></cell><cell>9.56</cell><cell>10.41</cell><cell>7.34</cell><cell cols="2">10.17 9.92</cell><cell>6.00</cell><cell>14.26</cell><cell>17.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.62</cell></row><row><cell></cell><cell>IH</cell><cell cols="2">IHR ICR</cell><cell>IH</cell><cell cols="2">IHR ICR</cell><cell>IH</cell><cell>IHR ICR</cell></row><row><cell></cell><cell cols="7">CIEDE2000 ( ) with [KL, K1, K2]=[100, 0.045, 0.015]</cell></row><row><cell></cell><cell cols="6">(b) Investigations on the reflectance components (compared with the clear image I C )</cell></row><row><cell></cell><cell cols="3">Homogeneous haze</cell><cell cols="3">Non-Homogeneous haze</cell><cell>Nighttime haze</cell></row><row><cell></cell><cell cols="3">(O-HAZE)</cell><cell cols="3">(NH-HAZE)</cell><cell>(NHR)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.72</cell><cell></cell><cell>1.60</cell><cell>2.07</cell><cell>0.44</cell></row><row><cell></cell><cell></cell><cell>0.52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.28</cell></row><row><cell></cell><cell>-0.21</cell><cell></cell><cell></cell><cell>-0.19</cell><cell></cell><cell></cell><cell>-0.08</cell></row><row><cell></cell><cell>IHS</cell><cell cols="2">ICS IC</cell><cell>IHS</cell><cell cols="2">ICS IC</cell><cell>IHS</cell><cell>ICS IC</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Rate of Newly Visible Edges ( )</cell></row><row><cell>(a) Intrinsic image decomposition</cell><cell cols="6">(c) Investigations on the shading components (compared with the hazy image I H )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>(b) shows the ?E maps and CIEDE2000 [46] 1 metrics of the hazy image I H , hazy reflectance I HR and clear reflectance I CR compared with the clear image I C . The investigations demonstrate that the clear reflectance I CR maintain the color information of clear image I C with very low ?E and CIEDE2000. Similarly, we visualise the visible gradients maps and calculate the Rate of Newly Visible Edges r nve<ref type="bibr" target="#b46">[47]</ref> of hazy shading I HS , clear shading I CS and clear image I C compared with the hazy image I C , as illustrated in 1. We set the Luminance coefficient K L , weighting factors K 1 and K</figDesc><table /><note>2 here are 100, 0.045 and 0.015, respectively, to offset the impact of the luminance component.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE I HOMOGENEOUS</head><label>I</label><figDesc>IMAGE DEHAZING RESULTS ON THE RESIDE, O-HAZE, HAZERD, AND REAL-WORLD DATASETS. [a] The bold results indicate the best performances, and the second-best are underlined. ?: The larger the better. ?: The smaller the better.</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>DCP</cell><cell cols="3">HLP MSCNN GDNet</cell><cell cols="2">EPDN MSBDN</cell><cell>FFA</cell><cell>Ours</cell></row><row><cell></cell><cell>PSNR?</cell><cell>14.57</cell><cell>16.44</cell><cell>19.15</cell><cell>29.78</cell><cell>27.42</cell><cell>33.79</cell><cell>29.92</cell><cell>32.17</cell></row><row><cell>SOTS</cell><cell>SSIM? LPIPS?</cell><cell>0.749 0.157</cell><cell>0.784 0.140</cell><cell>0.855 0.112</cell><cell>0.978 0.036</cell><cell>0.946 0.056</cell><cell>0.985 0.038</cell><cell>0.975 0.043</cell><cell>0.970 0.041</cell></row><row><cell></cell><cell>CIEDE200 ?</cell><cell>33.01</cell><cell>28.65</cell><cell>21.65</cell><cell>10.86</cell><cell>25.55</cell><cell>10.36</cell><cell>11.24</cell><cell>14.32</cell></row><row><cell></cell><cell>PSNR?</cell><cell>15.09</cell><cell>15.28</cell><cell>17.22</cell><cell>16.67</cell><cell>17.17</cell><cell>16.76</cell><cell>16.09</cell><cell>29.87</cell></row><row><cell>O-HAZE</cell><cell>SSIM? LPIPS?</cell><cell>0.449 0.318</cell><cell>0.466 0.306</cell><cell>0.521 0.285</cell><cell>0.480 0.296</cell><cell>0.600 0.292</cell><cell>0.460 0.318</cell><cell>0.465 0.323</cell><cell>0.758 0.155</cell></row><row><cell></cell><cell>CIEDE2000 ?</cell><cell>45.72</cell><cell>46.81</cell><cell>37.11</cell><cell>36.39</cell><cell>33.79</cell><cell>36.67</cell><cell>35.15</cell><cell>28.80</cell></row><row><cell></cell><cell>PSNR?</cell><cell>15.92</cell><cell>15.03</cell><cell>15.64</cell><cell>15.32</cell><cell>15.76</cell><cell>15.67</cell><cell>16.09</cell><cell>17.39</cell></row><row><cell>HazeRD</cell><cell>SSIM? LPIPS?</cell><cell>0.643 0.228</cell><cell>0.601 0.284</cell><cell>0.624 0.235</cell><cell>0.673 0.231</cell><cell>0.608 0.243</cell><cell>0.679 0.222</cell><cell>0.667 0.232</cell><cell>0.706 0.224</cell></row><row><cell></cell><cell>CIEDE2000 ?</cell><cell>31.21</cell><cell>27.83</cell><cell>26.88</cell><cell>26.08</cell><cell>30.61</cell><cell>26.25</cell><cell>26.46</cell><cell>25.75</cell></row><row><cell></cell><cell>rnve?</cell><cell>1.393</cell><cell>1.418</cell><cell>0.822</cell><cell>0.704</cell><cell>1.664</cell><cell>1.429</cell><cell>0.600</cell><cell>1.767</cell></row><row><cell>Real World</cell><cell>mng ?</cell><cell>1.570</cell><cell>1.699</cell><cell>1.437</cell><cell>1.116</cell><cell>1.587</cell><cell>1.303</cell><cell>1.123</cell><cell>2.165</cell></row><row><cell></cell><cell>rsp ?</cell><cell cols="2">0.0003 0.0195</cell><cell>0.0267</cell><cell cols="2">0.0010 0.0001</cell><cell cols="2">0.0023 0.0010</cell><cell>0.0000</cell></row><row><cell>[a]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE II NON</head><label>II</label><figDesc>-HOMOGENEOUS DEHAZING RESULTS ON THE NH-HAZE DATASET. The low-light image enhancement method LIME only improves the luminance of the nighttime image while failing to remove the color shift and haze. Our method and the other nighttime dehazing models can improve the luminance, correct color shift and remove haze simultaneously. However, all of LCD, MRP and OSFD yield unnatural results at the sky regions in</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell cols="3">EDN-AT EDN-3J TBNN</cell><cell>DWGAN</cell><cell>Our</cell></row><row><cell></cell><cell>PSNR?</cell><cell>18.92</cell><cell>17.78</cell><cell>17.78</cell><cell>22.13</cell><cell>24.26</cell></row><row><cell>NH-</cell><cell>SSIM?</cell><cell>0.778</cell><cell>0.735</cell><cell>0.706</cell><cell>0.800</cell><cell>0.805</cell></row><row><cell>HAZE</cell><cell>LPIPS?</cell><cell>0.235</cell><cell>0.271</cell><cell>0.272</cell><cell>0.224</cell><cell>0.199</cell></row><row><cell cols="2">CIEDE2000 ?</cell><cell>23.80</cell><cell>24.29</cell><cell>26.50</cell><cell>23.62</cell><cell>23.14</cell></row><row><cell>respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE III NIGHTTIME</head><label>III</label><figDesc>DEHAZING RESULTS ON THE NHR AND REAL-WORLD</figDesc><table><row><cell></cell><cell></cell><cell cols="2">DATASETS.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Metric</cell><cell>LIME</cell><cell>LCD</cell><cell cols="2">MRP OSFD</cell><cell>Ours</cell></row><row><cell></cell><cell>PSNR?</cell><cell>13.93</cell><cell cols="2">13.31 15.067</cell><cell>19.88</cell><cell>22.94</cell></row><row><cell>NHR</cell><cell>SSIM? LPIPS?</cell><cell>0.767 0.470</cell><cell>0.608 0.439</cell><cell>0.694 0.387</cell><cell>0.700 0.371</cell><cell>0.814 0.207</cell></row><row><cell></cell><cell>CIEDE2000 ?</cell><cell>43.36</cell><cell>35.02</cell><cell>32.79</cell><cell>33.35</cell><cell>18.25</cell></row><row><cell>Real-World</cell><cell>rnve ? mng ? rsp ?</cell><cell cols="5">0. 144 5.049 0.0104 0.0136 0.0190 0.0163 0.0000 0.190 0.169 0.149 0.219 5.589 5.406 4.906 5.846</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDIES ON THE FRAMEWORK AND CFSM.</figDesc><table><row><cell>Metric</cell><cell>w/o-RS</cell><cell cols="2">w/o-S</cell><cell cols="2">w/o-R</cell><cell>w/o-CFSM</cell><cell>Our</cell></row><row><cell>PSNR?</cell><cell cols="5">27.248 28.740 27.895</cell><cell>28.911</cell><cell>29.582</cell></row><row><cell>SSIM?</cell><cell cols="5">0.9419 0.9507 0.9599</cell><cell>0.9586</cell><cell>0.9617</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell></row><row><cell></cell><cell cols="6">ABLATION STUDY ON THE HYLOG-VIT.</cell></row><row><cell>Metric</cell><cell>CNN</cell><cell>ViT</cell><cell cols="2">L-ViT</cell><cell cols="2">G-ViT LoG-ViT</cell><cell>Our</cell></row><row><cell>PSNR?</cell><cell cols="6">28.020 27.727 28.375 27.365</cell><cell>28.913</cell><cell>29.582</cell></row><row><cell>SSIM?</cell><cell cols="6">0.9529 0.9404 0.9582 0.9467</cell><cell>0.9591</cell><cell>0.9617</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>STUDY ON THE POSITION ENCODING. Metric PE ADD Our PSNR? 29.49 28.34 29.58 SSIM? 0.963 0.955 0.962</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>. This work is supported by grants from National Natural Science Foundation of China (No.61922006, No.62132002) and CAAI-Huawei MindSpore Open Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision in bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="820" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Physics-based feature dehazing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single image dehazing using haze-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="720" to="734" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single image dehazing using the change of detail prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Color image dehazing using gradient channel prior and guided l 0 filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="326" to="342" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4770" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep retinex network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1100" to="1115" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning aggregated transmission propagation networks for haze removal and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2973" to="2986" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dehazeflow: Multi-scale conditional flow network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Multimedia</title>
		<meeting>the ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2577" to="2585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Haze removal using radial basis function networks for visibility restoration applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3828" to="3838" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhanced pix2pix dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8160" to="8168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Griddehazenet: Attentionbased multi-scale network for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7314" to="7323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-scale boosted dehazing network with dense feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2157" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ffa-net: Feature fusion attention network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Intriguing properties of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10497</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshop</title>
		<meeting>the IEEE International Conference on Computer Vision Workshop</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Human texture perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://www.cs.auckland.ac.nz/?georgy/research/texture/thesis-html/node6.html" />
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2006" to="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual pattern discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A two-branch neural network for non-homogeneous dehazing via ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dw-gan: A discrete wavelet transform gan for nonhomogeneous dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2021</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ensemble dehazing networks for non-homogeneous haze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cherukuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast haze removal for nighttime image using maximum reflectance prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7418" to="7426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nighttime dehazing with a synthetic benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Multimedia</title>
		<meeting>the ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2355" to="2363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contrastive learning for compact single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">560</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02034</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visformer: The vision-friendly transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Localvit: Bringing locality to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03106</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning for intrinsic image decomposition from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint learning of intrinsic images and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Baslamisli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Groenestege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-A</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Color Research &amp; Application: Endorsed by Inter-Society Color Council, The Colour Group (Great Britain), Canadian Society for Color, Color Science Association of Japan, Dutch Society for the Study of Color, The Swedish Colour Centre Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Centre Fran?ais de la Couleur</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2005" />
			<publisher>Colour Society of Australia</publisher>
		</imprint>
	</monogr>
	<note>The ciede2000 colordifference formula: Implementation notes, supplementary test data, and mathematical observations</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Blind contrast enhancement assessment by gradient ratioing at visible edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dumont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Analysis &amp; Stereology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Conditional positional encodings for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems</title>
		<meeting>the Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning a deep single image contrast enhancer from multi-exposure images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2049" to="2062" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Benchmarking single-image dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Nh-haze: An image dehazing benchmark with non-homogeneous hazy and haze-free images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="444" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Imageto-image translation for cross-domain disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems</title>
		<meeting>the Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">O-haze: a dehazing benchmark with real hazy and haze-free outdoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hazerd: an outdoor scene dataset and benchmark for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3205" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Nighttime haze removal based on a new imaging model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4557" to="4561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">310</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
