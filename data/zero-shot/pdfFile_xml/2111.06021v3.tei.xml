<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Contrastive Learning for Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
							<email>zlwang@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Contrastive Learning for Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent feature contrastive learning (FCL) has shown promising performance in self-supervised representation learning. For domain adaptation, however, FCL cannot show overwhelming gains since the class weights are not involved during optimization, which does not guarantee the produced features to be clustered around the class weights learned from source data. To tackle this issue, we propose a novel probabilistic contrastive learning (PCL) in this paper, which not only produces compact features but also enforces them to be distributed around the class weights. Specifically, we propose to use the output probabilities after softmax to perform contrastive learning instead of the extracted features and remove the 2 normalization in the traditional FCL. In this way, the probability will approximate the onehot form, thereby narrowing the distance between the features and the class weights. Our proposed PCL is simple and effective. We conduct extensive experiments on two domain adaptation tasks, i.e., unsupervised domain adaptation and semi-supervised domain adaptation. The results on multiple datasets demonstrate that our PCL can consistently get considerable gains and achieves the state-of-theart performance. In addition, our method also obtains considerable gains on semi-supervised tasks when labeled data is scarce.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks based on fully-supervised learning strategies have made great progress in image classification. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. However, such fully-supervised learning algorithms require the training data and test data to be collected from the same distribution. Directly applying the model trained in the old domain (source) to the new domain (target) often encounters performance degradation. In machine learning community, domain adaptation methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40]</ref> have been proposed to address this issue, which aims to transfer knowledge from source domain to target domain. <ref type="bibr">*</ref>  For the domain adaptation task, it is difficult for the features in target domain to form semantically compact clusters as the target data lacks the supervision signals. Consequently, various categories of the target domain are hard to be distinguished, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a). As we all know, the instance contrastive learning methods based on InfoNCE loss <ref type="bibr" target="#b40">[41]</ref> can learn semantically compact feature representations on unlabeled data, which means that contrastive learning tends to cluster together semantically similar features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref>. Many works have shown that the model trained in this way has good transferability <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b77">78]</ref>. Therefore, a straightforward idea for domain adaptation is that we perform supervised learning in source domain while performing contrastive learning in target domain, where it is expected that each category in the target domain is easy to be distinguished with semantically compact representations, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). However, our study experiments found that the gain from this way is very limited, and even no obvious gain is got for the model equipped with a strong consistency constraint, as shown in <ref type="figure">Figure 2</ref>.</p><p>In fact, a good classification model should meet two key points: 1) The features with the same semantics are as compact as possible. 2) The class weights are located at the class Weak Strong Baseline FCL PCL ECACL-P <ref type="figure">Figure 2</ref>. Comparison of different methods on DomainNet under the SSDA setting of 3-shot with Resnet34. Baseline means MME <ref type="bibr" target="#b45">[46]</ref> and ECACL-P <ref type="bibr" target="#b34">[35]</ref> is the SOTA method. Weak and Strong indicate whether FixMatch <ref type="bibr" target="#b48">[49]</ref> is added to MME.</p><p>feature center in feature space. Actually, contrastive learning only achieves the first one, and does not take into consideration the second one. This is because the current contrastive learning methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b77">78]</ref> usually use the features before the classifier to calculate the contrastive loss, while the class weight information is not involved in the optimization process. Therefore, feature contrastive learning cannot enforce the features to cluster around the class weight. For the domain adaptation tasks, however, there is a significant domain shift problem between the source domain and target domain. For such a situation, it is quite difficult for the class weights learned from the source domain to locate at the class center of the target domain data. Consequently, the features of the target domain are hard to be classified correctly even if they are semantically compact, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). Based on the above analysis, we need address an important thing for contrastive learning in domain adaption: how to take class information during optimization so that the features are enforced to be around the class weight. A naive idea is to directly use the logits after the classifier to calculate the contrastive loss, which introduces the information of class weights. However, it can only constrain the logits of positive samples to be pulled closer togather and does not effectively constrain the distance between the class weights and the features, as discussed in Sec. 5.2. Therefore, in this paper, we focus on how to design the way of use of class weights to clearly constrain the distance between features and class weights.</p><p>We know that the closer a feature is to the class weight, more similar its corresponding probability is to the one-hot form. Inspired by this, we encourage the output probability of the features to be as close to the one-hot form as possible when minimizing the contrastive loss. In this paper, we found that as long as the feature is replaced by the probability and the 2 normalization is removed, the contrastive loss will automatically force the probability of features to approximate the one-hot form. <ref type="figure" target="#fig_0">Figure 1</ref>(c) illustrates the results and we will elaborate on why the use of probability and removing the 2 normalization can make the probability approach one-hot form in Sec. 3.2. In order to distinguish from traditional feature contrastive learning (FCL), we name it probability contrast learning (PCL).</p><p>Our main contributions are three-folds. 1) To the best of our knowledge, we are the first to clearly point out that traditional feature contrastive learning cannot work well for domain adaptation because it does not consider class information to narrow the distance between features and weights. 2) We designed a simple yet effective probabilistic contrastive loss, which only needs to replace the features in FCL with probability and remove the 2 normalization. As a result, our method can enforce the learned features to be distributed around the class weights. 3) We conduct extensive experiments on two domain adaptation tasks to verify the effectiveness of our proposed PCL, i.e., UDA and SSDA. The results well show the superiority of our method to previous state-of-art methods in terms of simplicity and performance. In addition, our method also works well on semi-supervised tasks when the annotated data is scarce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Contrastive Representation Learning</head><p>Contrastive learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b61">62</ref>] is a framework that learns similar/dissimilar representations from data that are organized into similar/dissimilar pairs. Since there is no label information, an instance discrimination pretext task <ref type="bibr" target="#b64">[65]</ref> is used, where a query and a key form a positive pair if they are data-augmented versions of the same image. They form a negative pair otherwise. In these works, an effective contrastive loss function, called InfoNCE <ref type="bibr" target="#b40">[41]</ref> is widely adopted.</p><p>SimCLR <ref type="bibr" target="#b4">[5]</ref> uses self-supervised contrastive learning to first achieve the performance of a supervised ResNet-50 with only a linear classifier trained on self-supervised representations on full ImageNet. <ref type="bibr" target="#b19">[20]</ref> propose MoCo and <ref type="bibr" target="#b6">[7]</ref> extends MoCo to MoCo v2, where a small batch size can also achieve competitive results on full ImageNet <ref type="bibr" target="#b44">[45]</ref>. In addition, many other methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref> are also proposed to further boost performance. <ref type="bibr" target="#b26">[27]</ref> introduce supervised contrastive learning to encourage more compact representation. <ref type="bibr" target="#b11">[12]</ref> introduces a set of parametric class-wise learnable centers to tackle long-tailed recognition.</p><p>All the above works conduct contrastive learning in the feature space and use the 2 normalization. Although these methods can learn semantically more compact feature representations, they do not consider narrowing the distance between features and class weights. However, in domain adaptation tasks, the class weights often deviate from the class center of the target domain features, so contrastive learning does not work well. In this work, we propose to use probabilistic contrastive learning for domain adaptation. It replaces the features in FCL with probability to calculate the contrastive loss. In addition, in order to maintain the characteristic that the 1 -norm of probability equals one, we need to remove the standard 2 -norm normalization in FCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Unsupervised Domain Adaptation</head><p>Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge from a labeled source domain to an unlabeled target domain. The mainstream approaches tend to address UDA by learning domain-invariant representations. These approaches can be categorized into two categories. One category explicitly reduces the domain discrepancy measured by some distribution discrepancy metrics. <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b68">69]</ref> measure the domain similarity in terms of Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b2">[3]</ref>, while <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b41">42]</ref> introduce the metrics based on second-or higher-order statistics. Another popular line is to learn domain-invariant representation using adversarial training. It has been widely studied in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Different from the seminal UDA framework, where unlabeled target data are utilized to explicitly minimize the domain divergence, recent UDA methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b52">53]</ref> have been proposed to explore the data structure of unlabeled data. Our proposed method in this paper belongs to this type of approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Semi-Supervised Domain Adaptation</head><p>Semi-Supervised Domain Adaptation (SSDA) aims to reduce the discrepancy between the source and target distribution in the presence of limited labeled target samples. <ref type="bibr" target="#b45">[46]</ref> first proposes to align the distributions using adversarial training on entropy. <ref type="bibr" target="#b27">[28]</ref> shows the presence of intradomain discrepancy in the target distribution and introduced a framework to mitigate it. <ref type="bibr" target="#b22">[23]</ref> uses consistency alongside multiple adversarial strategies on top of MME. <ref type="bibr" target="#b31">[32]</ref> presents a meta-learning framework for SSDA. <ref type="bibr" target="#b69">[70]</ref> breaks down the SSDA problem into two subproblems, namely, SSL in the target domain and UDA problem across the source and target domains, and then proposes to learn the optimal weights of the network using co-training. <ref type="bibr" target="#b32">[33]</ref> proposes an adversarial adaptive clustering loss to group the features of unlabeled target data into clusters and perform cluster-wise feature alignment across the source and target domains. It uses FixMatch to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Semi-Supervised Learning</head><p>Semi-Supervised Learning (SSL) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref> aims to leverage the vast amount of unlabeled data with limited labeled data to improve performance. <ref type="bibr" target="#b70">[71]</ref> classifies the SSL methods into five categories, i.e., generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Recently, the consistency-based approach has attracted the attention of many reseachers. Mean teacher <ref type="bibr" target="#b54">[55]</ref> uses two different models to ensure consistency across similar images. Mix-Match <ref type="bibr" target="#b1">[2]</ref> and ReMixMatch <ref type="bibr" target="#b0">[1]</ref> use interpolation between labeled and unlabeled data to generate perturbed features. FixMatch <ref type="bibr" target="#b48">[49]</ref> achieves impressive performance by generating the confident pseudo labels of the unlabeled samples and treating them as labels for the perturbed samples. Due to the effectiveness and simplicity of FixMatch, it is also widely applied in other semi-supervised tasks, such as semantic segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b79">80]</ref> and object detection <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b53">54]</ref>. In this work, we consider semi-supervised image classification task and also use FixMatch as a strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first review feature contrastive learning widely used in unsupervised learning, and then elaborate on our probabilistic contrastive learning. Formally, let</p><formula xml:id="formula_0">B = {(x i ,x i )} N i=1</formula><p>be a batch of data pairs, where N is the batch size, and x i andx i are two random transformations of a sample. We define the model M = E ? F with the feature extractor E and the classifier F . Here F has the parameters W = (w 1 , ..., w C ), where C is the number of classes, and w k is the class weights of the k-th class (also called class prototype). We use the E to extract the features from B, and</p><formula xml:id="formula_1">get F = {(f i ,f i )} N i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Contrastive Learning</head><p>For a query feature f i , the featuref i is the positive and all other samples are the negative. Then the InfoNCE loss <ref type="bibr" target="#b40">[41]</ref> has the following form</p><formula xml:id="formula_2">fi = ? log exp(sg(f i ) g(f i )) j =i exp(sg(f i ) g(f j )) + k exp(sg(f i ) g(f k )) ,<label>(1)</label></formula><p>where g(f ) = f ||f ||2 is a standard 2 -normalization operation widely used in feature contrastive learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b4">5]</ref>, and s is the scaling factor.</p><p>From Equation <ref type="formula" target="#formula_2">(1)</ref>, we can observe that there is no class weight information involved in fi . Therefore, in the optimization process of contrastive loss, it is impossible to constrain the features to gather around the class weight. In order to solve this problem, a direct idea is to use logit after classifier for contrastive learning. However, although this way introduces the class weights, it can only constrain the distance between logits rather than the distance between the class weights and the features. Therefore, simply using logits to calculate contrastive loss cannot get satisfactory results and we will analyze it in Sec. 5.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Probabilistic Contrastive Learning</head><p>Contrastive learning based on InfoNCE loss <ref type="bibr" target="#b40">[41]</ref> shows better representation ability than other forms of contrastive loss <ref type="bibr" target="#b59">[60]</ref>. Therefore, instead of changing the form of In-foNCE loss, we hope to make the feature f i close to class weights by employing a new input f i in calculating the contrastive loss. The loss about the new feature f i is defined by</p><formula xml:id="formula_3">f i = ? log exp(sf i f i ) j =i exp(sf i f j ) + k exp(sf i f k ) . (2)</formula><p>For the design of f i , it is expected that the smaller the f i , the closer the feature f i is to the class weights. To minimize Equation <ref type="formula">(2)</ref>, one feasible solution is to maximize f i f i . On the other hand, we know that the closer f i is to the class weight, the closer its output probability p i = softmax(W f i ) is to the one-hot form, i.e., satisfying the following form</p><formula xml:id="formula_4">p i = (0, .., 1, .., 0).<label>(3)</label></formula><p>That is, we can narrow the distance between the feature and the class weights by forcing the output probability of the feature to approximate the one-hot form. Based on the above analysis, the new input f i needs to satisfy a characteristic: when f i f i is maximized, the output probability p i will approximate the one-hot form as in Equation <ref type="formula" target="#formula_4">(3)</ref>. Actually, the probability p i itself satisfies this property. Here we explain the detail below.</p><p>Note that p i = (p i,1 , ..., p i,C ) andp i = (p i,1 , ...,p i,C ) are both the probability distributions. Then we have</p><formula xml:id="formula_5">0 ? p i,c ? 1, 0 ?p i,c ? 1, ?c ? {1, ..., c, ..., C}<label>(4)</label></formula><p>In addition, the 1 -norm of p i andp i equals one, i.e.,</p><formula xml:id="formula_6">||p i || 1 = c p i,c = 1 and ||p i || 1 = cp i,c = 1.</formula><p>Obviously, we have:</p><formula xml:id="formula_7">p i p i = c p i,cpi,c ? 1.<label>(5)</label></formula><p>The equality holds if and only if p i =p i and both of them have a one-hot form as in Equation <ref type="formula" target="#formula_4">(3)</ref>. In other words, in order to maximize p ip i , the p i andp i need satisfy the onehot form at the same time. Therefore, p i can server as the new input f i in Equation <ref type="formula">(2)</ref>. It should be noted that from the derivation process, it can be seen that the characteristic that the 1 -norm of probability equals one guarantees that the maximum value of p ip i can only be reached when p i andp i satisfy the one-hot form at the same time, so we cannot perform 2 -norm normalization operation on probability like the traditional FCL. Finally, our new contrastive loss is defined by</p><formula xml:id="formula_8">pi = ? log exp(sp i p i ) j =i exp(sp i p j ) + k exp(sp i p k ) .<label>(6)</label></formula><p>Comparing Equation <ref type="formula" target="#formula_2">(1)</ref> and Equation <ref type="formula" target="#formula_8">(6)</ref>, we can see two main differences. First, Equation <ref type="formula" target="#formula_8">(6)</ref> uses the probability p i in contrastive learning instead of the extracted features f i . Second, Equation <ref type="formula" target="#formula_8">(6)</ref> removes the 2 -norm normalization g. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our proposed PCL is quite concise. It needs be emphasized that Equation <ref type="formula" target="#formula_8">(6)</ref> is not a simple extension of contrastive learning with projection head <ref type="bibr" target="#b4">[5]</ref>. Because, analogous to the projection head, the most direct way is to use the classifier as the projection head, and then use logits instead of probability for contrastive learning. However, comparing with FCL, it does not improve the performance of the model in domain adaptation tasks, as discussing in Sec. 5.2. In addition, even if we directly extend to probability, we do not have enough reason to discard the 2 -norm normalization g that has proven effective in many contrastive learning works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4]</ref>. However, under our motivation, in order to ensure that p i is close to the one-hot form, we must ensure that the the 1 -norm of p i equals one, that is, we need to discard the 2 -norm normalization g , which will be discussed in Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>Our loss function is defined as:</p><formula xml:id="formula_9">L = L ori + ?L P CL .<label>(7)</label></formula><p>Here L ori represents the loss function used by the baseline model and</p><formula xml:id="formula_10">L P CL = i ( p W i + p W i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this paper, we mainly focus on the problem of domain adaptation classification. We conduct the experiments on unsupervised domain adaptation in Sec. 4.1 and semi-supervised domain adaptation in Sec. 4.2 on multiple benchmarks, in which the state-of-the-art methods are used for comparison. In addition, we also verify the effectiveness of our method on semi-supervised learning in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on Unsupervised Domain Adaptation</head><p>Datasets. We evaluated our method in the following two standard benchmarks for UDA. Office-Home <ref type="bibr" target="#b58">[59]</ref> consists of images of everyday objects organized into four domains: Artistic (Ar), Clipart (Cl), Product (Pr), and Real-world (Rw). It contains 15,500 images of 65 classes. VisDA-2017 <ref type="bibr" target="#b42">[43]</ref> is a large-scale dataset for synthetic-to-real domain adaptation. It contains 152,397 synthetic images for the source domain and 55,388 real-world images for the target domain. Following the standard transductive setting <ref type="bibr" target="#b76">[77]</ref> for UDA, we use all labeled source data and all unlabeled target data, and test on the same unlabeled target data. Experimental Details. We adopt the GVB-GD <ref type="bibr" target="#b13">[14]</ref> architecture as our baseline. That is, GVB is adopted for both the generator and discriminator, and we follow the original experimental settings in GVB-GD. We use the ResNet-50 <ref type="bibr" target="#b20">[21]</ref> model pre-trained on ImageNet as the backbone network for both Office-Home and VisDA-2017. For network training, we use mini-batch stochastic gradient descent (SGD) with a momentum of 0.9 and a weight decay of 0.001. The initial learning rate is set to 0.001 for Office-Home, and 0.0003 for VisDA-2017. The max iteration number is set to 20k. For the hyper-parameters in PCL, we set s = 7.0 and ? = 0.05 and fix them. Comparison with SOTA. Here we compare different representative UDA methods, including "DAN" <ref type="bibr" target="#b37">[38]</ref>, "DANN" <ref type="bibr" target="#b17">[18]</ref>, "GTA" <ref type="bibr" target="#b47">[48]</ref>, "TAT" <ref type="bibr" target="#b36">[37]</ref>, "SymNet" <ref type="bibr" target="#b74">[75]</ref>, "MDD" <ref type="bibr" target="#b73">[74]</ref>, "BNM" <ref type="bibr" target="#b12">[13]</ref>, "MetaAlign" <ref type="bibr" target="#b62">[63]</ref>, "FixBi" <ref type="bibr" target="#b39">[40]</ref>, "ToAlign" <ref type="bibr" target="#b63">[64]</ref>, "SCDA" <ref type="bibr" target="#b35">[36]</ref>, "TCM" <ref type="bibr" target="#b71">[72]</ref>, and "GVB" <ref type="bibr" target="#b13">[14]</ref>. <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref> give the results on Office-Home and VisDA-2017. Inspired by some previous semi-supervised domain adaptation methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>, we also add FixMatch to GVB, and it can achieve remarkable performance improvement. Such results are consistent with <ref type="bibr" target="#b76">[77]</ref> which reveals that the semi-supervised models are strong unsupervised domain adaptation learners. We evaluate our proposed method by applying PCL it to GVB and GVB with Fix-Match. From the results, it can be seen that our method can bring consistent improvements and achieves state-of-the-art performance combined with FixMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Semi-Supervised Domain Adaptation</head><p>Datasets. We evaluate the effectiveness of our proposed approach on two SSDA image classification benchmarks, i.e., DomainNet <ref type="bibr" target="#b41">[42]</ref> and Office-Home. DomainNet is initially a multi-source domain adaptation benchmark. Similar to MME <ref type="bibr" target="#b45">[46]</ref>, we only select 4 domains Real, Clipart, Painting, and Sketch (abbr. R, C, P, and S), each of which contains images of 126 categories. Office-Home is a widely used UDA benchmark and consists of Real, Clipart, Art, and Product (abbr. R, C, A, and P) domains with 65 classes. For fair comparison, the settings of our benchmark datasets refer to the existing SSDA approaches <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b27">28]</ref>, including adaptation scenarios of each dataset, the number of labeled target data (typically 1-shot or 3-shot per class), sample selection strategies, etc. In particular, we choose MME <ref type="bibr" target="#b45">[46]</ref> as our baseline and report the results on both ResNet34 <ref type="bibr" target="#b20">[21]</ref> and AlexNet <ref type="bibr" target="#b29">[30]</ref>. Experimental Details. Following MME <ref type="bibr" target="#b45">[46]</ref>, we remove the last linear layer of AlexNet and ResNet34, while adding a new classifier F . We also use the model pre-trained on ImageNet to initialize all layers except F . We adopt SGD with a momentum of 0.9 and set the initial learning rate is 0.01 for fully-connected layers whereas it is set 0.001 for other layers. The max iteration number is set to 50k. For the hyper-parameters in PCL, we set s = 7 in all experiments. We set ? = 0.1 under the setting of 3-shot for AlexNet and set ? = 0.2 under the setting of 3-shot for ResNet34 in DomainNet. For all other experiments, we set ? = 0.05. Comparison with SOTA. We compare our method with previous state-of-the-art approaches, including "S+T", "MME" <ref type="bibr" target="#b45">[46]</ref>, "UODA" <ref type="bibr" target="#b43">[44]</ref>, "BiAT" <ref type="bibr" target="#b22">[23]</ref>, "Meta-MME" <ref type="bibr" target="#b31">[32]</ref>, "APE" <ref type="bibr" target="#b27">[28]</ref>, "ECACL-P" <ref type="bibr" target="#b34">[35]</ref>, and "CDAC" <ref type="bibr" target="#b32">[33]</ref>.</p><p>Here the "S+T" refers to the model trained using labeled source and labeled target data only. <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> give the results on DomainNet and Office-Home. It can be seen that our method on all datasets and all settings can get a significant gain compared with the baseline MME * . For DomainNet, under four different settings, our method can obtain a gain of more than 5%. It strongly proves the effectiveness of our method. Furthermore, our method outperforms other methods except CDAC and ECACL-P that adopt the FixMatch technique. For fair comparison, we also add FixMatch to our method. We can see that our method combined with FixMatch can achieve new state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Semi-Supervised Learning</head><p>In fact, the applicable scenarios of PCL are not limited to the domain adaptation. As long as the features of unlabeled data cannot be clustered around the class weights, our method has a good potential to improve the performance. In this section, we consider the case where the source do- main and target domain come from the same distribution, i.e., semi-supervised learning. In particular, for the semisupervised tasks, the unlabeled features will deviate from the class weight when the labeled data is very scarce. Therefore, we consider the situation of very scarce labels. In this work, 4 samples per class is particularly adopted for evaluation.</p><p>Datasets. We conduct SSL experiments on the CIFAR-10 and CIFAR-100 datasets. CIFAR-10 (CIFAR-100) <ref type="bibr" target="#b28">[29]</ref> contains 50,000 images of size 32 ? 32 from 10 (100) classes. Similarly, we take FixMatch <ref type="bibr" target="#b48">[49]</ref> as our baseline, and the backbone and training hyperparameters are exactly the same as FixMatch. We evaluate on 5 runs with different random seeds, and then report the mean and standard variance. Experimental Details. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b48">49]</ref>, we report the performance of an EMA model and use a Wide ResNet-28-2 <ref type="bibr" target="#b72">[73]</ref> for CIFAR-10 and use a WRN-28-8 for CIFAR-100. The models are trained using SGD with a momentum of 0.9 and a weight decay of 0.0005. We follow the original papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b48">49]</ref> and train all models for 1024 epochs, using an learning rate of 0.03 with a cosine decay schedule. For the hyperparameters used in FixMatch, we follow FixMatch and set ? cls = 1, ? = 0.95, and ? = 7, B = 64. For the hyper-parameters in PCL, we set s = 4, ? = 0.02 for CIFAR-10 and s = 7 ,? = 0.05 for CIFAR-100. Comparison with SOTA. We compare our method with previous state-of-the-art approaches, including "ReMixMatch" <ref type="bibr" target="#b0">[1]</ref>, "FixMatch" <ref type="bibr" target="#b48">[49]</ref>, "CoMatch" <ref type="bibr" target="#b33">[34]</ref>, "SsCL" <ref type="bibr" target="#b77">[78]</ref>, and "Dash" <ref type="bibr" target="#b67">[68]</ref>. The quantitative evaluation results on CIFAR-10/100 are reported in <ref type="table">Table 5</ref>. In the case of extremely limited labeled data (4 samples per class), our method get significant gains on CIFAR-10 (+4.8%) and CIFAR-100 (+4.0%) compared with the baseline FixMatch. It well demonstrates the effectiveness of our method. It is worth noting that our performance is superior to SsCL and CoMatch that use feature contrast learning. In particular, CoMatch boosts the original feature contrastive learning through memory-smoothed pseudo-labeling and graph structure, while our method does not require any additional techniques. We believe these techniques can further enhance the performance of probabilistic contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head><p>In this section, we verified the effectiveness of our proposed method through extensive ablation studies. As the SSDA task combines the characteristics of both SSL and UDA, we particularly choose the SSDA task here. We conduct the experiments on DomainNet under the setting of 3shot and adopt Resnet34 as the backbone. In addition, I investigate the effectiveness of our method on the domain adaptive detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Feature Space v.s. Probability Space</head><p>In this section, we verify the effectiveness of probabilistic contrastive learning (PCL) compared to traditional feature contrast learning (FCL). <ref type="table" target="#tab_6">Table 8</ref> gives the results and we have the following observations. First, the traditional FCL can improve the performance of the baseline but the gain is limited. Second, the average gain of our probabilistic contrastive learning over FCL is more than 5%, which indicates the importance of probabilistic contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Logit Space v.s. Probability Space</head><p>According to previous analysis, the FCL does not involve the information of the class weights, and thus the features cannot be clustered around the class weights. In this   section, we explore whether it is effective to calculate the contrast loss directly using the logits output by the classifier. Therefore, we consider the features before the softmax, which is called logits contrastive learning (LCL). Following the traditional feature contrastive learning, we perform 2norm normalization on the logits. <ref type="table" target="#tab_6">Table 8</ref> gives the results. We found that LCL does not improve performance compared to traditional FCL, although the class weight information is introduced. This is because optimizing the LCL does not enforce the probability present in the form of onehot, which is important to make the features be distributed around the class weight. Compared with LCL, our PCL only introduces an additional softmax layer, but the average gain exceeds 5%. As analyzed in Sec. 3.2, PCL can force the probability of features to approximate the one-hot form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of L2-norm Normalization</head><p>In this section, we investigate whether our PCL requires 2 -norm normalization like the standard FCL. <ref type="table" target="#tab_4">Table 7</ref> gives the results. It can be seen that the accuracy would be reduced by 2% if the probabilities are normalized by the 2norm. This is because the 2 normalization on the probability only make a pair of features keep the same direction for the inner product of 1, and it is no longer necessary to enforce them keep in the one-hot form. Therefore, the 2 -norm normalization reduces the proximity of the learned  features to the class weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">PCL v.s. SFCL</head><p>In contrastive learning, due to lack of labels, there may be false negative samples that belong to the same category with positive sample data. Many methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b65">66]</ref> point out that the false negative samples are harmful to contrastive learning. Therefore, a natural question is whether we can make feature contrast learning work well in domain adaptation tasks by alleviating the false negative problem without specifically designing probabilistic contrastive learning. An appropriate way to solve the false negative problem is to use supervised feature contrastive learning (SFCL) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. Therefore, in this section, we mainly compare PCL with SFCL to verify whether PCL is necessary in domain adaptation tasks. For SFCL, in addition to the positive samples constructed by different transformations, it also selects the samples which are similar to the positive samples from the negative samples as positive samples. For convenience, we define I fi = {f i } N 1 ? {f i } N 1 \f i , which means the set of all features except f i . The loss function is defined as:</p><formula xml:id="formula_11">fi = ? fn?Q + f i log ?(sg(f i ) g(f n )) ?(sg(f i ) g(f n ))+ f k ?Q ? f i ?(sg(f i ) g(f k )) ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_12">Q + fi = ({f k |g(f k ) g(f i ) &gt; t} ? {f k |g(f k ) g(f i ) &gt; t}?{f i }) ? I fi , Q ? fi = I fi \Q + fi and ?(x) = exp(x)</formula><p>. We set t = 0.95. <ref type="table" target="#tab_6">Table 8</ref> shows the results. It can be seen that SFCL can indeed improve the performance of FCL in domain adaptation tasks. But compared with PCL, SFCL has very limited improvements over FCL. The reason is that by alleviating the false negative problem, better feature representation can be learned, but because of the domain shift, even if the features of each category of the target domain are more separated, it is difficult to be classified by class weights learned from the source domain. Therefore, in the domain adaptation task, how to relieve the feature deviation of the class weights is important and our PCL can effectively alleviate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">The Importance of InfoNCE Loss</head><p>In this section, we explore whether it is necessary to use the function form based on InfoNCE loss to approximate  the probability to the one-hot form. Therefore, we consider binary cross entropy loss (BCE) which is used in <ref type="bibr" target="#b78">[79]</ref>. For convenience, we redefine the probabilities p i andp i as p 0 i and p 1 i . Therefore, the new loss function is defined as follows: bce = ? i,j,m,n (? n,m i,j log p n,m i,j + 1 ?? n,m i,j log(1?p n,m i,j )).</p><p>(9) where p n,m i,j = p n i p m j and? n,m i,j = 1[p n,m i,j ? t or (i = j)]. We set t = 0.95.</p><p>Observing the Equation <ref type="formula">(9)</ref>, when? n,m i,j = 1, BCE loss will maximize p n,m i,j = p n i p m j . Note that p n i and p m j are probability, so p n,m i,j will take maximum if and only if p n i and p m j are equal and both of them have a one-hot form. Therefore, BCE loss can force probability to present onehot form. Based on the above analysis, we can use BCE to replace PCL, forcing probability approaching one-hot form. In addition, we also consider combining FCL and BCE to optimize the model. <ref type="table" target="#tab_8">Table 9</ref> gives the results. It can be seen that although the BCE is better than FCL, it is still behind PCL. This proves that it is necessary to use the function form based on InfoNCE loss to approximate the probability to the one-hot form. <ref type="figure" target="#fig_3">Figure 4</ref> shows the relationship between the extracted unlabeled features and the class weights for the three methods, including MME, MME+FCL, and MME+PCL, respectively. Firstly, compared to MME, MME+FCL produce more compact feature clusters for the same category and more separate feature distributions for different categories. However, for both MME+FCL and MME, the learned class weights are deviated from the feature centers. Secondly, the class weights of MME+PCL are closer to the feature centers than MME+FCL. It demonstrates that probabilistic contrastive learning is effective in enforcing the features closer to the class weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">T-SNE Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Domain Adaptive Object Detection</head><p>In this section, we conduct an extra experiment on Synthetic-to-Real scenes to simply verify whether our proposed method is effective for the object detection task. In particular, we choose the recent RPA <ref type="bibr" target="#b75">[76]</ref> as the baseline and apply our PCL in the second stage. <ref type="table" target="#tab_9">Table 10</ref> gives the   results. It can be seen that our method can still improve the performance of the baseline model on domain adaptive detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we found that the traditional feature contrastive learning can only cluster the features of similar semantics and cannot enforce the learned features to be distributed around the class weights due to the class weights are not involved during optimization. To solve this problem, we propose a novel probabilistic contrastive learning. Specifically, we use the probabilities after Softmax instead of the features, and remove the 2 -norm normalization widely used in FCL. We experimentally verified the effectiveness of our proposed methods in three tasks with multiple datasets. We believe that our PCL provides an innovative route for various visual tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Feature contrastive learning v.s. Probabilistic Contrastive learning. (a) The distribution of initial features are relatively scattered. (b) After FCL, the features of same semantic are clustered, but the learned class weights are deviated from the class center. (c) After PCL, the features with similar semantics can be not only clustered but also distributed around the class weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Framework of FCL and PCL. Different from FCL, PCL uses the output of softmax to perform contrastive learning and removes the 2-norm normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Bathtub Bee Blackberry Bottlecap Broccoli Cactus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The t-SNE visualization of learned features. We focus on the relationship between features and class weights on the C?S task of DomainNet dataset with Resnet34 under the setting of 3shot. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MethodA?C A?P A?R C?A C?P C?R P?A P?C P?R R?A R?C R?P Avg</figDesc><table><row><cell>Source-Only</cell><cell>34.9</cell><cell>50.0</cell><cell>58.0</cell><cell>37.4</cell><cell>41.9</cell><cell>46.2</cell><cell>38.5</cell><cell>31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9 46.1</cell></row><row><cell>TAT (ICML'19)</cell><cell>51.6</cell><cell>69.5</cell><cell>75.4</cell><cell>59.4</cell><cell>69.5</cell><cell>68.6</cell><cell>59.5</cell><cell>50.5</cell><cell>76.8</cell><cell>70.9</cell><cell>56.6</cell><cell>81.6 65.8</cell></row><row><cell>SymNet (CVPR'19)</cell><cell>47.7</cell><cell>72.9</cell><cell>78.5</cell><cell>64.2</cell><cell>71.3</cell><cell>74.2</cell><cell>63.6</cell><cell>47.6</cell><cell>79.4</cell><cell>73.8</cell><cell>50.8</cell><cell>82.6 67.2</cell></row><row><cell>MDD (ICML'19)</cell><cell>54.9</cell><cell>73.7</cell><cell>77.8</cell><cell>60.0</cell><cell>71.4</cell><cell>71.8</cell><cell>61.2</cell><cell>53.6</cell><cell>78.1</cell><cell>72.5</cell><cell>60.2</cell><cell>82.3 68.1</cell></row><row><cell>BNM (CVPR'20)</cell><cell>56.2</cell><cell>73.7</cell><cell>79.0</cell><cell>63.1</cell><cell>73.6</cell><cell>74.0</cell><cell>62.4</cell><cell>54.8</cell><cell>80.7</cell><cell>72.4</cell><cell>58.9</cell><cell>83.5 69.4</cell></row><row><cell>FixBi (CVPR'21)</cell><cell>58.1</cell><cell>77.3</cell><cell>80.4</cell><cell>67.7</cell><cell>79.5</cell><cell>78.1</cell><cell>65.8</cell><cell>57.9</cell><cell>81.7</cell><cell>76.4</cell><cell>62.9</cell><cell>86.7 72.7</cell></row><row><cell>ToAlign (NeurIPS'21)</cell><cell>57.9</cell><cell>76.9</cell><cell>80.8</cell><cell>66.7</cell><cell>75.6</cell><cell>77.0</cell><cell>67.8</cell><cell>57.0</cell><cell>82.5</cell><cell>75.1</cell><cell>60.0</cell><cell>84.9 72.0</cell></row><row><cell>SCDA (ICCV'21)</cell><cell>60.7</cell><cell>76.4</cell><cell>82.5</cell><cell>69.8</cell><cell>77.5</cell><cell>78.4</cell><cell>68.9</cell><cell>59.0</cell><cell>82.7</cell><cell>74.9</cell><cell>61.8</cell><cell>84.5 73.1</cell></row><row><cell>TCM (ICCV'21)</cell><cell>58.6</cell><cell>74.4</cell><cell>79.6</cell><cell>64.5</cell><cell>74.0</cell><cell>75.1</cell><cell>64.6</cell><cell>56.2</cell><cell>80.9</cell><cell>74.6</cell><cell>60.7</cell><cell>84.7 70.7</cell></row><row><cell>GVB (CVPR'20)</cell><cell>57.0</cell><cell>74.7</cell><cell>79.8</cell><cell>64.6</cell><cell>74.1</cell><cell>74.6</cell><cell>65.2</cell><cell>55.1</cell><cell>81.0</cell><cell>74.6</cell><cell>59.7</cell><cell>84.3 70.4</cell></row><row><cell cols="2">+ MetaAlign(CVPR'21) 59.3</cell><cell>76.0</cell><cell>80.2</cell><cell>65.7</cell><cell>74.7</cell><cell>75.1</cell><cell>65.7</cell><cell>56.5</cell><cell>81.6</cell><cell>74.1</cell><cell>61.1</cell><cell>85.2 71.3</cell></row><row><cell>+ Our PCL</cell><cell>59.7</cell><cell>75.9</cell><cell>80.4</cell><cell>69.3</cell><cell>75.5</cell><cell>77.1</cell><cell>67.0</cell><cell>58.3</cell><cell>81.0</cell><cell>75.2</cell><cell>63.9</cell><cell>84.6 72.3</cell></row><row><cell>+ FixMatch</cell><cell>59.8</cell><cell>78.1</cell><cell>81.3</cell><cell>67.7</cell><cell>78.2</cell><cell>76.7</cell><cell>68.7</cell><cell>60.2</cell><cell>83.9</cell><cell>75.1</cell><cell>65.5</cell><cell>86.4 73.5</cell></row><row><cell cols="2">+ Our PCL + FixMatch 60.8</cell><cell>79.8</cell><cell>81.6</cell><cell>70.1</cell><cell>78.9</cell><cell>78.9</cell><cell>69.9</cell><cell>60.7</cell><cell>83.3</cell><cell>77.1</cell><cell>66.4</cell><cell>85.9 74.5</cell></row><row><cell cols="12">Table 1. Classification accuracy (%) of different UDAs on Office-Home with ResNet-50 as backbone.</cell><cell></cell></row><row><cell>Method</cell><cell>Acc</cell><cell cols="2">Method</cell><cell></cell><cell>Acc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DAN(ICML15)</cell><cell>61.6</cell><cell cols="2">DANN(ICML'15)</cell><cell></cell><cell>57.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GTA (CVPR'18)</cell><cell>69.5</cell><cell cols="2">MDD (ICML'19)</cell><cell></cell><cell>74.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CDAN (NeurIPS'20) 70.0</cell><cell cols="2">GVB (CVPR'20)</cell><cell></cell><cell>75.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GVB*</cell><cell>75.0</cell><cell cols="2">+ FixMatch</cell><cell></cell><cell>80.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ Our PCL</cell><cell cols="5">80.8 Our PCL + FixMatch 82.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 2. Accuracies (%) of Synthetic ? Real on VisDA-2017 for unsupervised domain adaptation methods using ResNet-50.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>-shot 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot 1-shot<ref type="bibr" target="#b2">3</ref>-shot A S+T 43.3 47.1 42.4 45.0 40.1 44.9 33.6 36.4 35.7 38.4 29.1 33.3 55.8 58.7 40.0 43.4 MME (ICCV'19) 48.9 55.6 48.0 49.0 46.7 51.7 36.3 39.4 39.4 43.0 33.3 37.9 56.8 60.7 44.2 48.2 58.6 49.2 50.6 44.0 52.0 37.7 41.9 39.6 42.1 37.2 42.0 56.9 58.8 45.5 49.4 APE (ECCV'20) 47.7 54.6 49.0 50.5 46.9 52.1 38.5 42.6 38.5 42.2 33.8 38.7 57.5 61.4 44.6 48.9 CDAC ? (CVPR'21) 56.9 61.4 55.9 57.5 51.6 58.9 44.8 50.7 48.1 51.7 44.1 46.7 63.8 66.8 52.1 56.2 ECACL-P ? (ICCV'21) 55.8 62.6 54.0 59.0 56.1 60.5 46.1 50.6 54.6 50.3 45.0 48.4 62.3 67.4 52.8 57.6 MME Our PCL + FixMatch 58.2 62.5 55.9 59.3 57.5 60.6 47.1 51.2 51.9 56.0 44.9 48.8 65.2 67.8 54.4 58.0 Our PCL + FixMatch 78.1 80.5 75.2 78.1 77.2 80.3 68.8 74.1 74.5 76.5 70.1 73.5 81.9 84.1 75.1 78.2 Accuracy(%) on DomainNet under the settings of 1-shot and 3-shot using Alexnet (A) and Resnet34 (R) as backbone networks. ? means using Fixmath and * means our reimplementation. Accuracy(%) on Office-Home under the setting of 3-shot using Alexnet (A) and Resnet34 (R) as backbone networks. ? means using Fixmath and * means our reimplementation.</figDesc><table><row><cell cols="3">R?C 1-shot 3Meta-MME (ECCV'20) -Net Method 56.4</cell><cell>-</cell><cell>R?P 50.2</cell><cell cols="2">P?C 51.9</cell><cell>-</cell><cell cols="2">C?S 39.6</cell><cell>-</cell><cell>S?P 43.7</cell><cell>-</cell><cell cols="2">R?S 38.7</cell><cell>-</cell><cell>P?R 60.7</cell><cell>-</cell><cell>Mean 48.8</cell></row><row><cell></cell><cell>BiAT (IJCAI'20)</cell><cell cols="14">54.2 50.8 57.9 48.4 50.6 46.8 54.2 39.5 42.5 40.0 45.0 36.5 40.3 58.9 61.2 45.8 50.2</cell></row><row><cell></cell><cell>+ Our PCL</cell><cell cols="14">55.1 59.5 54.6 57.2 52.4 56.7 44.2 48.2 49.6 52.6 42.0 46.9 64.2 67.1 51.7 55.5</cell></row><row><cell></cell><cell>+ Fixmatch</cell><cell cols="14">57.6 60.8 52.1 56.4 57.3 59.9 46.2 49.8 51.0 53.7 43.3 46.2 65.4 67.6 53.2 56.3</cell></row><row><cell cols="2">S+T MME (ICCV'19) UODA (arXiv 2020) Meta-MME(ECCV'20) BiAT (IJCAI'20) + R APE (ECCV'20)</cell><cell cols="14">55.6 60.0 60.6 62.2 56.8 59.4 50.8 55.0 56.0 59.5 46.3 50.1 71.8 73.9 56.9 60.0 70.0 72.2 67.7 69.7 69.0 71.7 56.3 61.8 64.8 66.8 61.0 61.9 76.1 78.5 66.4 68.9 72.7 75.4 70.3 71.5 69.8 73.2 60.5 64.1 66.4 69.4 62.7 64.2 77.3 80.8 68.5 71.2 -73.5 -70.3 -72.8 -62.8 -68.0 -63.8 -79.2 -70.1 73.0 74.9 68.0 68.8 71.6 74.6 57.9 61.5 63.9 67.5 58.5 62.1 77.0 78.6 67.1 69.7 70.4 76.6 70.8 72.1 72.9 76.7 56.7 63.1 64.5 66.1 63.0 67.8 76.6 79.4 67.6 71.7</cell></row><row><cell></cell><cell>CDAC  ? (CVPR'21)</cell><cell cols="14">77.4 79.6 74.2 75.1 75.5 79.3 67.6 69.9 71.0 73.4 69.2 72.5 80.4 81.9 73.6 76.0</cell></row><row><cell></cell><cell cols="15">ECACL-P  ? (ICCV'21) 75.3 79.0 74.1 77.3 75.3 79.4 65.0 70.6 72.1 74.6 68.1 71.6 79.7 82.4 72.8 76.4</cell></row><row><cell></cell><cell>MME  *</cell><cell cols="14">71.0 71.4 68.9 70.0 69.2 72.6 59.8 62.7 65.6 68.2 63.2 64.3 77.8 77.9 67.9 69.5</cell></row><row><cell></cell><cell>+ Our PCL</cell><cell cols="14">74.8 78.1 73.9 76.5 75.5 78.6 67.6 72.5 73.4 75.6 68.9 72.5 80.6 84.6 73.5 76.9</cell></row><row><cell></cell><cell>+ Fixmatch</cell><cell cols="14">75.5 78.7 72.5 77.0 75.9 80.0 66.3 68.6 72.1 74.4 67.2 71.4 81.1 82.6 72.9 76.1</cell></row><row><cell cols="2">+ Net Method</cell><cell cols="14">R?C R?P R?A P?R P?C P?A A?P A?C A?R C?R C?A C?P Mean</cell></row><row><cell></cell><cell>S+T</cell><cell>44.6</cell><cell>66.7</cell><cell>47.7</cell><cell>57.8</cell><cell>44.4</cell><cell></cell><cell>36.1</cell><cell>57.6</cell><cell></cell><cell>38.8</cell><cell cols="2">57.0</cell><cell>54.3</cell><cell>37.5</cell><cell>57.9</cell><cell>50.0</cell></row><row><cell></cell><cell>MME (ICCV'19)</cell><cell>51.2</cell><cell>73.0</cell><cell>50.3</cell><cell>61.6</cell><cell>47.2</cell><cell></cell><cell>40.7</cell><cell>63.9</cell><cell></cell><cell>43.8</cell><cell cols="2">61.4</cell><cell>59.9</cell><cell>44.7</cell><cell>64.7</cell><cell>55.2</cell></row><row><cell></cell><cell cols="2">Meta-MME (ECCV'20) 50.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.3</cell><cell></cell><cell>40.3</cell><cell>-</cell><cell></cell><cell>44.5</cell><cell cols="2">-</cell><cell>-</cell><cell>44.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BiAT (IJCAI'20)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="2">-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.4</cell></row><row><cell>A</cell><cell>APE (ECCV'20)</cell><cell>51.9</cell><cell>74.6</cell><cell>51.2</cell><cell>61.6</cell><cell>47.9</cell><cell></cell><cell>42.1</cell><cell>65.5</cell><cell></cell><cell>44.5</cell><cell cols="2">60.9</cell><cell>58.1</cell><cell>44.3</cell><cell>64.8</cell><cell>55.6</cell></row><row><cell></cell><cell>CDAC  ? (CVPR'21)</cell><cell>54.9</cell><cell>75.8</cell><cell>51.8</cell><cell>64.3</cell><cell>51.3</cell><cell></cell><cell>43.6</cell><cell>65.1</cell><cell></cell><cell>47.5</cell><cell cols="2">63.1</cell><cell>63.0</cell><cell>44.9</cell><cell>65.6</cell><cell>56.8</cell></row><row><cell></cell><cell>ECACL-P  ? (ICCV'21)</cell><cell>55.4</cell><cell>75.7</cell><cell>56.0</cell><cell>67.0</cell><cell>52.5</cell><cell></cell><cell>46.4</cell><cell>67.4</cell><cell></cell><cell>48.5</cell><cell cols="2">66.3</cell><cell>60.8</cell><cell>45.9</cell><cell>67.3</cell><cell>59.1</cell></row><row><cell></cell><cell>MME  *</cell><cell>44.6</cell><cell>73.0</cell><cell>50.4</cell><cell>62.9</cell><cell>48.3</cell><cell></cell><cell>41.0</cell><cell>63.4</cell><cell></cell><cell>45.4</cell><cell cols="2">62.2</cell><cell>60.8</cell><cell>43.3</cell><cell>65.2</cell><cell>55.7</cell></row><row><cell></cell><cell>+ Our PCL</cell><cell>52.8</cell><cell>74.7</cell><cell>53.7</cell><cell>64.6</cell><cell>48.9</cell><cell></cell><cell>44.5</cell><cell>66.3</cell><cell></cell><cell>47.1</cell><cell cols="2">65.0</cell><cell>64.8</cell><cell>46.4</cell><cell>67.9</cell><cell>58.1</cell></row><row><cell></cell><cell>+ FixMatch</cell><cell>54.4</cell><cell>75.8</cell><cell>54.6</cell><cell>66.9</cell><cell>52.0</cell><cell></cell><cell>45.9</cell><cell>69.2</cell><cell></cell><cell>49.2</cell><cell cols="2">65.4</cell><cell>65.3</cell><cell>47.7</cell><cell>65.6</cell><cell>59.3</cell></row><row><cell></cell><cell cols="2">+ Our PCL + FixMatch 55.6</cell><cell>75.6</cell><cell>55.6</cell><cell>67.8</cell><cell>52.0</cell><cell></cell><cell>46.4</cell><cell>68.8</cell><cell></cell><cell>49.0</cell><cell cols="2">67.1</cell><cell>67.0</cell><cell>47.9</cell><cell>69.5</cell><cell>60.2</cell></row><row><cell></cell><cell>S+T</cell><cell>55.7</cell><cell>80.8</cell><cell>67.8</cell><cell>73.1</cell><cell>53.8</cell><cell></cell><cell>63.5</cell><cell>73.1</cell><cell></cell><cell>54.0</cell><cell cols="2">74.2</cell><cell>68.3</cell><cell>57.6</cell><cell>72.3</cell><cell>66.2</cell></row><row><cell></cell><cell>MME (ICCV'19)</cell><cell>64.6</cell><cell>85.5</cell><cell>71.3</cell><cell>80.1</cell><cell>64.6</cell><cell></cell><cell>65.5</cell><cell>79.0</cell><cell></cell><cell>63.6</cell><cell cols="2">79.7</cell><cell>76.6</cell><cell>67.2</cell><cell>79.3</cell><cell>73.1</cell></row><row><cell></cell><cell cols="2">Meta-MME (ECCV'20) 65.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.5</cell><cell></cell><cell>66.7</cell><cell>-</cell><cell></cell><cell>63.3</cell><cell cols="2">-</cell><cell>-</cell><cell>67.5</cell><cell>-</cell><cell>-</cell></row><row><cell>R</cell><cell>APE (ECCV'20) CDAC  ? (CVPR'21)</cell><cell>66.4 67.8</cell><cell>86.2 85.6</cell><cell>73.4 72.2</cell><cell>82.0 81.9</cell><cell>65.2 67.0</cell><cell></cell><cell>66.1 67.5</cell><cell>81.1 80.3</cell><cell></cell><cell>63.9 65.9</cell><cell cols="2">80.2 80.6</cell><cell>76.8 80.2</cell><cell>66.6 67.4</cell><cell>79.9 81.4</cell><cell>74.0 74.8</cell></row><row><cell></cell><cell>MME  *</cell><cell>66.0</cell><cell>86.0</cell><cell>72.3</cell><cell>80.4</cell><cell>64.0</cell><cell></cell><cell>67.4</cell><cell>79.8</cell><cell></cell><cell>64.0</cell><cell cols="2">77.9</cell><cell>77.1</cell><cell>66.6</cell><cell>80.0</cell><cell>73.5</cell></row><row><cell></cell><cell>+ Our PCL</cell><cell>65.4</cell><cell>86.7</cell><cell>74.5</cell><cell>83.1</cell><cell>62.9</cell><cell></cell><cell>71.0</cell><cell>82.8</cell><cell></cell><cell>63.7</cell><cell cols="2">81.0</cell><cell>81.1</cell><cell>71.0</cell><cell>83.1</cell><cell>75.5</cell></row><row><cell></cell><cell>+ FixMatch</cell><cell>67.2</cell><cell>88.1</cell><cell>76.6</cell><cell>83.0</cell><cell>66.8</cell><cell></cell><cell>73.6</cell><cell>83.8</cell><cell></cell><cell>67.3</cell><cell cols="2">80.5</cell><cell>81.1</cell><cell>71.8</cell><cell>82.7</cell><cell>76.9</cell></row><row><cell></cell><cell cols="2">+ Our PCL + FixMatch 69.1</cell><cell>89.5</cell><cell>76.9</cell><cell>83.8</cell><cell>68.0</cell><cell></cell><cell>74.7</cell><cell>85.5</cell><cell></cell><cell>67.6</cell><cell cols="2">82.3</cell><cell>82.7</cell><cell>73.4</cell><cell>83.4</cell><cell>78.1</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Ablation study on effect of different features on Domain-Net under the setting of 3-shot and Resnet34.</figDesc><table><row><cell>Method</cell><cell cols="8">R?C R?P P?C C?S S?P R?S P?R Mean</cell></row><row><cell>Baseline</cell><cell>71.4</cell><cell>70.0</cell><cell>72.6</cell><cell>62.7</cell><cell>68.2</cell><cell>64.3</cell><cell>77.9</cell><cell>69.5</cell></row><row><cell>2 -norm</cell><cell>75.1</cell><cell>74.4</cell><cell>76.2</cell><cell>70.3</cell><cell>73.5</cell><cell>69.9</cell><cell>82.5</cell><cell>74.6</cell></row><row><cell>w/o 2 -norm</cell><cell>78.1</cell><cell>76.5</cell><cell>78.6</cell><cell>72.5</cell><cell>75.6</cell><cell>72.5</cell><cell>84.6</cell><cell>76.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Ablation study on effect of applying 2-norm normalization to probability, where DomainNet under the setting of 3-shot and Resnet34 are used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Effect of SFCL on DomainNet under the setting of 3-shot and Resnet34.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Effect of BCE loss on DomainNet under the setting of 3-shot and Resnet34.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>car AP</cell></row><row><cell>RPA (CVPR'21)</cell><cell>45.7</cell></row><row><cell>RPA  *</cell><cell>45.3</cell></row><row><cell>+ Our PCL</cell><cell>47.8</cell></row><row><cell>Oracle</cell><cell>60.0</cell></row></table><note>. Detection performance (%) on Synthetic-to-Real cross- domain adaptation task, SIM10k [25] ? Cityscapes [11]. * means our reimplementation.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00224</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Debiased contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12028</idno>
		<title level="m">Parametric contrastive learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards discriminability and diversity: Batch nuclear-norm maximization under label insufficient situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradually vanishing bridge for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Good semi-supervised learning that requires a bad gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1705.09783</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05208</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional adversarial training for semisupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingshuai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimum class confusion for versatile domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Driving in the matrix: Can virtual worlds replace humangenerated annotations for real world tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rounak</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Supervised contrastive learning. NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attract, perturb, and explore: Learning a feature alignment network for semisupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Triple generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1703.02291</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online meta-learning for multi-source and semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crossdomain adaptive clustering for semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Comatch: Semisupervised learning with contrastive graph regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ecacl: A holistic framework for semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic concentration for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mixue</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangrui</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transferable adversarial training: A general approach to adapting deep classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fixbi: Bridging domain spaces for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Visda: The visual domain adaptation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Opposite structure learning for semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02545</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via structurally regularized deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Humble teachers teach better students for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Understanding the behaviour of contrastive loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Cross-domain contrastive learning for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05528</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Metaalign: Coordinating domain alignment and classification for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Toalign: Task-oriented alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Hierarchical semantic aggregation for contrastive representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">End-toend semi-supervised object detection with soft teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09018</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Dash: Semi-supervised learning with dynamic thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep co-training with task decomposition for semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">A survey on deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00550</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Transporting causal mechanisms for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<editor>Richard C. Wilson, Edwin R. Hancock, and William A. P. Smith</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Domainsymmetric networks for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rpn prototype alignment for domain adaptive object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Semi-supervised models are strong unsupervised domain adaptation learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00417</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Semi-supervised contrastive learning with similarity co-calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07387</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Neighborhood contrastive learning for novel class discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Fini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhankar</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Pseudoseg: Designing pseudo labels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09713</idno>
	</analytic>
	<monogr>
		<title level="m">Jia-Bin Huang, and Tomas Pfister</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
