<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhance the Visual Representation via Discrete Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
							<email>yuefeng.chenyf@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjie</forename><surname>Duan</surname></persName>
							<email>ranjie.drj@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gege</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alibaba</forename><surname>Group</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country>? EPFL</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhance the Visual Representation via Discrete Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial Training (AT), which is commonly accepted as one of the most effective approaches defending against adversarial examples, can largely harm the standard performance, thus has limited usefulness on industrial-scale production and applications. Surprisingly, this phenomenon is totally opposite in Natural Language Processing (NLP) task, where AT can even benefit for generalization. We notice the merit of AT in NLP tasks could derive from the discrete and symbolic input space. For borrowing the advantage from NLP-style AT, we propose Discrete Adversarial Training (DAT). DAT leverages VQGAN to reform the image data to discrete text-like inputs, i.e. visual words. Then it minimizes the maximal risk on such discrete images with symbolic adversarial perturbations. We further give an explanation from the perspective of distribution to demonstrate the effectiveness of DAT. As a plug-and-play technique for enhancing the visual representation, DAT achieves significant improvement on multiple tasks including image classification, object detection and self-supervised learning. Especially, the model pre-trained with Masked Auto-Encoding (MAE) and fine-tuned by our DAT without extra data can get 31.40 mCE on ImageNet-C and 32.77% top-1 accuracy on Stylized-ImageNet, building the new state-of-the-art. The code will be available at https://github.com/alibaba/easyrobust.</p><p>A possible way towards robust machine perception can be Adversarial Training (AT) <ref type="bibr" target="#b4">[5]</ref>, which automatically finds failure input cases of DNNs and augment online with these cases for fixing "bugs". With online augmentation of adversarial examples, AT greatly enhances the adversarial robustness, and helps for learning perceptually-aligned representations [6] with good interpretability <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and transferability <ref type="bibr" target="#b8">[9]</ref>. However, AT is double-edged, which meanwhile degrades the standard performance caused by problematic regularization <ref type="bibr" target="#b9">[10]</ref>. Such problematic regularization makes the decision boundaries over-smoothed and enlarges indecisive regions.</p><p>Surprisingly, previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> observe a strange phenomenon that AT behaves conversely in Natural Language Processing (NLP) tasks. By automatically finding adversarial textual inputs, AT will not hurt the accuracy and even benefit for both generalization and robustness of language models. This phenomenon motivates us considering whether the merit of NLP-style AT can be Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, Deep Neural Networks (DNNs) has achieved excellent performance surpassing humans in most computer vision tasks. Although remarkable progress has been made, the success of DNNs is actually a false sense when i.i.d hypothesis is not satisfied in wild. Researchers have shown that deep models fail in most circumstances including adversarial perturbations <ref type="bibr" target="#b0">[1]</ref>, common corruptions <ref type="bibr" target="#b1">[2]</ref>, colors or textures changing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, etc. There is still a long way to make DNNs closer to the robust human perception. transferred to vision tasks. We notice such merit could derive from the unique data organizing form of language models. To be specific, an adversarial image perturbed in continuous pixel space actually differs with the truly "hard" examples appeared in real world. Contrarily, text space is discrete and symbolic, where adversarial text is practically existing when a typo is made by humans. Learning on such adversarial text will obviously improve the generalization on other more texts with confusing typos. Therefore, we borrow the symbolic nature of languages, and apply it on CV tasks by discretizing continuous images into a more meaningful symbolic space. Afterwards, AT is conducted for minimizing the maximal risk on such text-like inputs with symbolic adversarial perturbations.</p><p>In this paper, we propose Discrete Adversarial Training (DAT), a new type of adversarial training which aims to improve both robustness and generalization of vision models. DAT leverages VQ-GAN <ref type="bibr" target="#b12">[13]</ref> to learn a vocabulary of visual words, also known as image codebook. For a continuous image input, each encoded patch embedding is replaced with its closest visual word in the codebook, and represented as a corresponding index. Then the image is transformed to a sequence of symbolic indices similar with language input. For generating adversarial examples based on such symbolic sequence, direct use of optimization methods in NLP like combinatorial optimization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> or synonym substitutions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> can be challenging. The reason lies in: 1) the large search space of images and 2) the non-existence of synonym in visual codebook. To make it more efficient, DAT adopts a gradient-based method which assumes the backward adversarial gradient goes straightthrough the complex discretization process, thus gradients on discretized image can be copied to original input. Then it use one-step search along the direction of estimated gradient such that the discrete representation will altered adversarially during the discretization process, resulting a discrete adversarial examples. Finally the discrete adversarial examples is fed into models for training. Different from AT which always adds l p bound on augmented adversarial examples, DAT affects the discretization process to produce diverse adversarial inputs beyond l p bound for training. We show in ablation experiment that DAT not only enhances the robustness on l p bounded attacks, but also is partly beneficial in defense of unrestricted semantic attacks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. The overall pipeline of discrete adversarial training is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We further give an analysis to explain the effectiveness of our DAT from the perspective of distribution. By comparing the distributional difference of training examples in AT and DAT with clean images, we find the discrete adversarial examples in our DAT are much closer to the clean distribution. Such ability of generating "in-distribution" adversarial examples makes DAT can improve the visual representation learning on multiple vision models and tasks, with no sacrificing of clean accuracy.</p><p>Our contributions are summarized below:</p><p>? To the best of our knowledge, we appear to be the first to transfer the merit of NLPstyle adversarial training to vision models, for improving robustness and generalization simultaneously. ? We propose Discrete Adversarial Training (DAT), where images are presented as discrete visual words, and the model is training on example which has the adversarially altered discrete visual representation. ? DAT achieves significant improvement on multiple tasks including image classification, object detection and self-supervised learning. Especially, it establishes a new state-of-the-art for robust image classification. By combining MAE <ref type="bibr" target="#b18">[19]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Adversarial Training Adversarial Training (AT) <ref type="bibr" target="#b4">[5]</ref> is first proposed to improve robustness by training models with adversarial examples. As one of the most effective defense, existing works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> have suggested a trade-off between adversarial and clean accuracy in AT. Despite great efforts <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> have been made for mitigating this trade-off, the bad generalization of AT still cannot be fully remedied till now. In opposite way, some other works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> use AT to improve the clean accuracy rather than adversarial robustness. The most close work to ours is the AdvProp <ref type="bibr" target="#b25">[26]</ref>, which splits batch norms to prevent the mixed statistics of clean and adversarial examples, thus learns better adversarial feature for generalization. Pyramid AT <ref type="bibr" target="#b27">[28]</ref> makes AT specific to ViTs by crafting pyramid perturbation with Dropout enabling, yielding imporved performance.</p><p>However, these methods are only applicable under specific models or tasks. VILLA <ref type="bibr" target="#b28">[29]</ref> is also a representation enhancement technique using AT. But contrary to us, it applies vision-style AT only for vision-and-language representation learning. AGAT <ref type="bibr" target="#b29">[30]</ref> is another kind of AT beyond pixel space, which perturbs images along attributes, however it has strict requirement of attributes annotation. Adversarial Augmentation By borrowing the idea of AT, some previous works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> propose to search augmentations adversarially for improving the hardness of training examples. Adversarial AutoAugment <ref type="bibr" target="#b30">[31]</ref> uses augmentation policy network to produce hard augmentation policies on a pre-defined policy space. AugMax <ref type="bibr" target="#b31">[32]</ref> mixes multiple randomly sampled augmentation operators like AugMix <ref type="bibr" target="#b33">[34]</ref>, by using adversarially learned mixing factors. AdA <ref type="bibr" target="#b32">[33]</ref> optimizes the parameters of image-to-image models to generate adversarially corrupted augmented images. MaxUp <ref type="bibr" target="#b34">[35]</ref> uses the worst augmented data of each data point in a set of random perturbations or transforms for training. However, these methods create adversarial inputs indirectly and rely on pre-defined augmentations or translation models. They cannot perturb the images locally. Instead, our DAT directly modifies the image with no need of pre-defined transforms, and can craft local perturbations on images, which is more elaborate. Discrete Visual Representation Learning Early technique <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> of Bag-Of-Visual-Words (BOVW) model has shown great power of discrete representation in visual understanding. VQ-VAE <ref type="bibr" target="#b37">[38]</ref> uses DNNs to learn neural discrete representations, also known as visual codebook, by generative modeling the image distribution. Recently, the idea of discrete representations learning has been widely emerged in many vision tasks. In most Masked Image Modeling (MIM) methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, visual codebook is needed for BERT-like self-supervised pretraining. For image classification, discrete representations strengthen the robustness by preserving the global structure of an object and ignoring local details <ref type="bibr" target="#b40">[41]</ref>. For image synthesis, adversarial and perceptual objective can be added to VQ-VAE for learning perceptually-aligned visual codebook <ref type="bibr" target="#b12">[13]</ref>. With growing power of generative models, now a well trained VQGAN can produce vivid images with 0.58 FID <ref type="bibr" target="#b41">[42]</ref>. In this work, we use VQGAN for the discretization process in our DAT. As a powerful generative model, VQGAN can greatly reduce the information loss in reconstruction process and ensure the high-quality of the generated discrete adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Traditional Adversarial Training</head><p>We take the typical image classification task as an example to show the formulation of Adversarial Training (AT). Consider the training image and label set D = {x i , y i } n i=1 , and a classifier F with learnable parameters ?, the classification objective is always a cross-entropy loss L(x, y, ?). Adversarial Training (AT) finds the optimal ? by solving a minimax optimization problem:</p><formula xml:id="formula_0">min ? E (x,y)?D max ? L(x + ?, y, ?) s.t. ? p &lt; ,<label>(1)</label></formula><p>where the inner optimization finds the perturbations ? on per-pixel values for maximizing the loss, and the outer minimization update ? to improve the worst-case performance of the network w.r.t. the perturbation. ? p constraints the p-norm of ? to a small value . A problem is that AT finds the failure case i.e. x + ? in continuous pixel space. However, human does not create or recognize images from complex pixel values, but from discrete semantic concepts. Although adversarial examples can successfully fool the models, they are still different from the real "hard" examples appeared in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discrete Adversarial Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Image Discretization by Visual Codebook</head><p>For discrete adversarial training, it is desirable first to learn an expressive visual codebook and represent the training image set in discrete space. We utilize VQGAN <ref type="bibr" target="#b12">[13]</ref> for image discretization. More precisely, consider a continuous image x ? R H?W ?3 , VQGAN learns an encoder Enc ? (?), decoder Dec ? (?) and quantization q Z (?). Enc ? is a convolutional model which maps x to intermediate latent vectors v = Enc ? (x) ? R (h?w)?d , where h, w is the height, width of the intermediate feature map and d is the latent dimension. Subsequently, q Z (?) learn a codebook Z = {z k |z k ? R d } K k=1 , such that each latent vector v ij ? R d can be quantized onto its closest codebook entry z k as:</p><formula xml:id="formula_1">v q = q Z (v) := arg min z k ?Z v ij ? z k ? R h?w?d ,<label>(2)</label></formula><p>where i, j present each location in feature map. Then the decoder Dec ? (?) outputs the reconstruction imagex from the quantized vectors v q by:</p><formula xml:id="formula_2">x = Dec ? (v q ) .<label>(3)</label></formula><p>VQGAN is trained by minimizing the reconstructed difference betweenx and x. More details can be referred to Appendix A. So far, given a continuous image x, we can get its corresponding discrete reconstructionx. For simplicity, we use Q to stand for the above image discretization process, and then we havex = Q(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Discrete Adversarial Training</head><p>Based on the definition in Sec 3.2.1, we can generate discrete adversarial examples in inner maximization step of AT. By slightly modifying the Eq 1, the objective of DAT is formulated as:</p><formula xml:id="formula_3">min ? E (x,y)?D max ? L(Q(x + ?), y, ?) ,<label>(4)</label></formula><p>where Q transforms the continuous pixel space to discrete input space. We delete the constraint term since there is no need to bound the per-pixel values of ?. Suppose that Q is an ideal discretizer with no information loss in discretization process. The problem lies how to find the worst ? for maximizing the classification loss. Similar with traditional AT, we can use gradient-based methods to approximate ? by:</p><formula xml:id="formula_4">? ?? x L(Q(x), y, ?),<label>(5)</label></formula><p>where ? determines the magnitude of the perturbations along the gradient direction. We set ? = 0.1 by default in DAT. To expand ? x L(Q(x), y, ?) by chain rule, we have four partial derivative terms as follows:</p><formula xml:id="formula_5">? x L(Q(x), y, ?) = ?L ?x ? ?x ?v q ? ?v q ?v ? ?v ?x<label>(6)</label></formula><p>Through analysing the feasibility of each term, we find only ?vq ?v is hard to solve as the nondifferentiable nature of Eq 2. Fortunately, as proposed in previous work, a straight-through gradient estimator <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> can be used by copying the gradients from v q to v. By replacing ?x ?vq ? ?vq ?v with ?x ?v , we can simplify the Eq 6 to ? x L(Q(x), y, ?) = ?L ?x ? ?x ?v ? ?v ?x , which has derivative everywhere. Although the solution seems workable theoretically, the huge cost makes it impractical on large-scale vision tasks. The bottleneck mainly lies on that ?x ?v and ?v ?x require the adversarial gradients backward through Enc and Dec. Actually, a generator capable of producing high-quality images always has a large amount of parameters. Compared with original adversarial training which only needs F for gradient calculation, it requires more than tripled GPU memory and computation cost. ? ? ??xL(x, y, ?) //Estimate the adversarial perturbations 5:</p><p>x adv ? Q(x + ?) //Generate discrete adversarial examples <ref type="bibr">6:</ref> Minimize the classification loss w.r.t. network parameter arg min ? L(x adv , y, ?) 7: end for</p><p>To solve this problem, we propose an efficient alternative solution. Sincex</p><p>x is empirically observed for an ideal discretizer Q, we can also use a straight-through estimator betweenx and x, which is given by</p><formula xml:id="formula_6">? x L(Q(x), y, ?) = ?L ?x ? ?x ?x ?L ?x .<label>(7)</label></formula><p>Finally, we can solve the worst ? by ?xL(x, y, ?). By this way, the computation cost of DAT has been largely reduced. Compared with original adversarial training, it only has extra computation cost on VQGAN forward, which is relatively controllable.</p><p>For clarity, let us restate the pipeline of our DAT. For each training image x, DAT first use VQGAN to get discrete reconstructionx. By feedingx to classifier F , a worst-case perturbation ? can be estimated by computing the gradient ofx towards maximizing the classification loss. The perturbed image thus can be created by adding ? on original x. Finally, x + ? is discretized by VQGAN again and acts as the adversarial input, on which F is trained by minimizing the classificaiton loss. The details of our DAT is summarized in Algorithm 1. Explaining the Effectiveness of DAT from the Perspective of Distribution We give an empirical analysis to explain why DAT can improve the robustness and generalization without sacrificing clean accuracy. Previous work <ref type="bibr" target="#b25">[26]</ref> has pointed out that the underlying distributions of adversarial examples are different from clean images. Training on both clean and adversarial images will force the Batch Normalization (BN) <ref type="bibr" target="#b44">[45]</ref> to estimate an inaccurate mixture statistics of feature distribution, and thus impact the standard performance. We study this effect by sampling 1000 mini-batches in ImageNet validation set, and generate corresponding adversarial images for AT and our DAT. For each batch we calculate the mean and variance statistics of last BN of ResNet50. Then the Pearson correlation coefficient (PCC) between the statistics on clean and adversarial examples is computed for showing the distributional difference. <ref type="figure" target="#fig_2">Figure 2</ref> shows the frequency histogram of the distributional difference on 1000 mini-batches. For training samples of DAT, the peak of the histogram is at 0.95, which is greater than AT. It suggests that DAT generates discrete adversarial examples much closer with the clean distributions. Therefore, training on these examples will reduce the shift of clean distribution in AT, yielding both the robustness and generalization improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate the versatility of our method, we experiment Discrete Adversarial Training (DAT) on multiple tasks including image classification, object detection and self-supervised learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Classification</head><p>Implementation We implement DAT on two representative architectures: ResNet50 <ref type="bibr" target="#b45">[46]</ref> and ViTs <ref type="bibr" target="#b19">[20]</ref>. For ResNet50, we first experiment DAT with vanilla training recipes using "robustness" library 1 . Then we combine DAT with other orthogonal robust training techniques: DeepAugment <ref type="bibr" target="#b46">[47]</ref> and AugMix <ref type="bibr" target="#b33">[34]</ref>. Only cross entropy loss is used for generating discrete adversarial examples. The JSD loss in AugMix is optimized merely on clean samples. For ViTs, we adopt ViT-B/16 as baseline models, which is trained by the recipes in AugReg <ref type="bibr" target="#b47">[48]</ref>. Besides, we use DAT to conduct supervised finetuning on downstream ImageNet classification task based on a self-supervised ViT-Huge pretrained by MAE <ref type="bibr" target="#b18">[19]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We report all results in <ref type="table" target="#tab_2">Table 1</ref>. For fair comparison, we add DAT on base methods without modifying the original training hyper-parameters, such that improvement is entirely attributed to the DAT. For ResNet50, DAT achieves significant improvement on both clean accuracy, adversarial and out-of-distribution robustness. The improvement seems greater when combining DAT with DeepAugment and AugMix. For ViTs, we find DAT is compatible with other complex augmentations such as MixUp <ref type="bibr" target="#b53">[54]</ref>, CutMix <ref type="bibr" target="#b54">[55]</ref> or RandAugment <ref type="bibr" target="#b55">[56]</ref> used in AugReg, yielding greater improvement. Compared with plain ViT, AugReg-ViT and ViT with discrete representation called DrViT <ref type="bibr" target="#b40">[41]</ref>, DAT with AugReg achieves better performance. The best result is from ViT-Huge pretrained by MAE <ref type="bibr" target="#b18">[19]</ref> and finetuned by DAT, which suggests DAT is also effective in downstream fine-tuning tasks.</p><p>Additionally, we also compare DAT with other robust training strategies in <ref type="table" target="#tab_5">Table 2</ref>. AugReg-ViT is adopted as the baseline model. Most strategies, e.g., AdvProp <ref type="bibr" target="#b25">[26]</ref> and Debiased <ref type="bibr" target="#b56">[57]</ref> are proposed for ResNet with auxiliary BatchNorm. We show these methods cannot work properly on ViTs with only LayerNorm. Compared with Pyramid AT <ref type="bibr" target="#b27">[28]</ref>, our DAT has lower clean accuracy but yields stronger robustness. More results of strategies comparison on ResNet50 refer to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-Supervised Learning</head><p>Implementation We experiment DAT on three self-supervised methods: SimCLR <ref type="bibr" target="#b57">[58]</ref>, Sim-Siam <ref type="bibr" target="#b58">[59]</ref> and recently proposed MoCov3 <ref type="bibr" target="#b59">[60]</ref>. The discrete adversarial training is only conducted during pre-training stage, and the learned representation is evaluated on downstream tasks by standard pipeline <ref type="bibr" target="#b60">[61]</ref>. We craft adversarial examples based on RoCL <ref type="bibr" target="#b61">[62]</ref>, which attacks the pre-training   Benchmarks For self-supervised learning, we adopt ImageNet-1K for both training and indistribution testing. Beyond that, in order to give more comprehensive assessment, we build three downstream tasks to metric the transferability of the learned visual representation. Specifically, the linear evaluation reports the top-1 accuracy on five classification datasets: Flowers, CIFAR10, Caltech101, Cars and DTD. For downstream evaluation of object detection, we present mAP, AP50, AP75 on Pascal VOC2007 <ref type="bibr" target="#b62">[63]</ref>. ADE20K <ref type="bibr" target="#b63">[64]</ref> is used for semantic segmentation task, and both the mean intersection over union (mIoU) and accuracy are reported.  Results As shown in <ref type="table" target="#tab_6">Table 3</ref>, DAT can enhance the learned representation on ImageNet and get 0.97%, 0.58% and 0.25% improvement on MoCov3, Sim-CLR, SimSiam respectively. For excluding that DAT is not just over-fitting on ImageNet, we transfer the representations on downstream recognition, object detection, semantic segmentation tasks. The results suggest that DAT enhances the self-supervised representations with better transferability. In particular, MoCov3 with DAT achieves significant improvement in all downstream tasks. This conclusion echoed with previous works <ref type="bibr" target="#b8">[9]</ref>, which find adversarially robust models often perform better on transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Object Detection</head><p>Implementation We implement DAT on two popular detectors: EfficientDet <ref type="bibr" target="#b64">[65]</ref> and YOLOv3 <ref type="bibr" target="#b65">[66]</ref>. Object detection generally has two sub-tasks: classification and localization. Det-AdvProp <ref type="bibr" target="#b66">[67]</ref> proposes to select more vulnerable sub-task to generate adversarial images, and use auxiliary BN for training. Instead, DAT regards the two sub-tasks as a whole, and attacks the overall detection loss to produce adversarial images. Besides, DAT does not modify BN in detectors. As the memory and computation cost of VQGAN increased quadratic with the input resolution, DAT become unaffordable when input size is large. Therefore, we only experiment DAT on lightweight version of EfficientDet and YOLOv3 with input size smaller than 512.  Benchmarks We train the detectors using the COCO 2017 object detection dataset <ref type="bibr" target="#b67">[68]</ref> and evaluate them on COCO's validation set. COCO-C <ref type="bibr" target="#b68">[69]</ref> is used for testing the robustness to natural corruptions. We report mAP on COCO and COCO-C as the clean and robust performance respectively. rPC <ref type="bibr" target="#b68">[69]</ref> is used to to measure relative performance degradation under corruption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare the detectors with DAT, Det-AdvProp <ref type="bibr" target="#b66">[67]</ref> and vanilla training in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablations</head><p>Discrete Perturbations vs. Pixel-Space Perturbations We compare the proposed Discrete Adversarial Examples (DAEs) with traditional Pixel-space Adversarial Examples (PAEs) in <ref type="figure" target="#fig_3">Figure 3</ref>. DAEs have following three superior properties: 1) DAEs are more realistic. By calculating the number of colors <ref type="bibr" target="#b16">[17]</ref>, we find PAEs add more invalid colors, resulting in a noisy image. While DAEs have minor changes on the color numbers. The FID score of our DAEs is 14.65, which is lower than 65.18 of AEs; 2) DAEs have less high frequency component compared with AEs in frequency analysis; 3) Discrete perturbations are more structural, showing it attends to more important object locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of the Codebook in DAT</head><p>A general understanding is that the codebook with larger K should have stronger representation power. As suggested in <ref type="table" target="#tab_12">Table 5</ref>, when the K is reduced from 16384 to 256, the FID of generated images increases 0.35. We show DAT on larger codebook size can achieve better generalization. However, the clean accuracy seems no improvement with increasing of K. We think the reason is that the reconstruction quality has already met the demand of DAT. And the improvement on clean accuracy is saturated when K further increases.</p><p>Different Types of Discretizer Q Our work is based on the hypothesis of an ideal Q with no loss in discretization process. However, such perfect Q is not existing in practice. So we study if and how the ability of Q affects the performance of our DAT in <ref type="table" target="#tab_12">Table 5</ref>. The results show DAT performs better by using discretizer with higher FID. We find clean accuracy declines 1.3% on DALL-E, showing it is important for choosing a powerful discretizer. Types of pre-training datasets have little impact. We find the discretizer pre-trained on OpenImages <ref type="bibr" target="#b69">[70]</ref> and transferred to discretize ImageNet images, can also achieve comparable results.</p><p>The Performance on Different Magnitude ? We present the DAT results on ResNet50 and ViT with different magnitude ? in <ref type="figure">Figure 4</ref>. ? = 0 means the model is only trained on images augmented by VQGAN with no adversarial training process. We show DAT with ? = 0 makes the models have  high clean accuracy but the lowest robustness. With the increase of ?, there is still a robustness and accuracy trade-off. DAT with ? = 0.4 on ResNet50 achieves the best adversarial robustness but generalization is getting worse. By contrast, we surprisingly find ViT has the lower sensibility on ?.</p><p>Even if ViT is trained on DAT with ? = 0.4, there is still no great drop of clean and OOD accuracy. We suspect that ViT is more suitable for training on discrete examples, and the strong modeling ability makes ViT greater than ResNet50. <ref type="figure">Figure 4</ref>: The performance of DAT with different magnitude ? in Eq 5.</p><formula xml:id="formula_7">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (a) Clean Accuracy ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (b) Adversarial Robustness ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (c) OOD Robustness</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Stronger Attacks</head><p>In the main experiment, only one-step FGSM is used for examining the robustness under white-box adversarial attacks. To give more comprehensive evaluation, we additionally test our DAT under stronger AutoAttack <ref type="bibr" target="#b72">[73]</ref>, and two unrestricted attacks named Adv-Drop <ref type="bibr" target="#b16">[17]</ref> and PerC-Adversarial <ref type="bibr" target="#b17">[18]</ref>. The result is shown in <ref type="figure">Figure 5</ref>. DAT brings the improvement of robustness under all three attackers. ViT trained with DAT achieved extremely high robust accuracy on PerC-Adversarial, which suggests DAT also effects well in defense against unrestricted adversarial attacks. More details can be referred to Appendix C. <ref type="figure">Figure 5</ref>: The adversarial robustness test under other stronger attackers.</p><formula xml:id="formula_8">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (a) AutoAttack [73] ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (b) AdvDrop [17] ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (c) PerC-Adversarial [18]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations and Conclusions</head><p>In this paper, we find transferring NLP-style adversarial training to vision models can enhance the learned visual representation effectively. We propose Discrete Adversarial Training (DAT), where images are presented as discrete visual words by VQGAN, and the model is training on examples which have the adversarially altered discrete visual representation. DAT needs not to modify the model architecture and works for both CNNs and ViTs across multiple tasks. DAT reports the state-ofthe-art robustness on ImageNet-C and Stylized-ImageNet, exhibiting strong generalization. However, DAT still costs increased training time, this limitation also holds for any adversarial training. The strict assumption of an ideal discretizer is another potential limitation, despite DAT has used powerful VQGAN model to approach this assumption. The effect of DAT is empirically studied without deeper theoretical explanation. All the above limitations are remained as the future optimization direction.</p><p>The Appendix is organized as follows. Appendix A extends the discussion on training of VQGAN. Appendix B presents more experimental results including comparison of robust training strategies, effect analysis on different perturbations, images corruptions and inference with the discretizer Q. Besides, we analyse the training budget of our DAT. Appendix C discusses the implementation details of the three attackers in ablation experiments. Finally we do some visualizations in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of Training VQGAN</head><p>We add more details about how VQGAN can be trained to discretize a continous image. The forward pass of VQGAN has been introduced in Sec 3.2.1. Suppose a reconstructed imagex, the training objective for VQGAN is defined betweenx and x as:</p><formula xml:id="formula_9">L VQGAN = min Enc,Dec,Z max D E x?p(x) [L VQ (Enc, Dec, Z) + L GAN ({Enc, Dec, Z}, D)]<label>(8)</label></formula><formula xml:id="formula_10">L VQ (Enc, Dec, Z) = x ?x precept + sg[Enc(x)] ? v q 2 2 + sg[v q ] ? Enc(x) 2 2 (9) L GAN ({Enc, Dec, Z}, D) = [log D(x) + log(1 ? D(x))],<label>(10)</label></formula><p>where x ?x precept is the perceptual reconstruction loss instead of L 2 loss, sg[?] denotes the stopgradient operation, and sg[v q ] ? Enc(x) 2 2 is the commitment loss <ref type="bibr" target="#b37">[38]</ref>. A patch-based discriminator D is introduced for improving the quality of generated images by adversarial learning <ref type="bibr" target="#b73">[74]</ref>.</p><p>In this work, we use the pre-trained VQGAN weights for DAT directly, which are opened on GitHub <ref type="bibr" target="#b22">23</ref> . VQGAN with f = 8, d = 4 and K = 16384 is used for main experiments. We acknowledge the authors of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42]</ref>, whose works have greatly promoted our DAT.  <ref type="table" target="#tab_13">Table 6</ref>: Comparison of DAT with other training strategies. We use ResNet50 as the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Strategies Comparison on ResNet50</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Comparing with Traditional Adversarial Training</head><p>We compare our DAT with open-sourced adversarially robust models 4 in <ref type="table" target="#tab_15">Table 7</ref>. From the results, we can see a clear quantification of the benefit of our proposed discrete AT scheme compared with traditional AT. For clean performance, traditional AT plays a negative impact. However DAT can reduce the negative impact and achieve higher accuracy on validation set of ImageNet. It even surpasses the clean performance of normal training. The results also suggest AT with a very small can slightly benefit the generalization, e.g., with =0.01 L2 AT, ImageNet-C mCE value from 76.70 drops to 75.33. But with the becoming larger, AT greatly damages the generalization, e.g. with =5.0 L2 AT, ImageNet-C mCE value increases to 88.98. In contrast, our DAT achieves significant improvement on generalization compared with traditional AT.   <ref type="table">Table 8</ref> counts for the percentage of the modified visual words with different magnitude ?. We show only 3.8% of the visual words are changed when we set ? as 0.1. This proportion does not have negative impact on the trained models, but even somewhat benefits the clean accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 DAT with Different Perturbations</head><p>With larger proportion of visual words being adversarially altered, the standerd performance and generalization are getting worse. We also experiment DAT with random perturbations. To introducing the randomness, we selects 3.8% visual words and replaces them with other words. We find this operation can slightly improvement the generalization of learned representation, but it still cannot achieve the comparable effect as our DAT. It suggests adversarially altering the visual words is a better way.  <ref type="table">Table 8</ref>: DAT with different perturbations. We use ResNet50 as the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 The Effect of DAT on Corruptions in ImageNet-C</head><p>To analyse the effect of DAT on each image corruption in ImageNet-C, we report the detailed results in  <ref type="table" target="#tab_17">Table 9</ref>: Detailed results of DAT on each image corruption in ImageNet-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Training Budget for DAT</head><p>DAT is experimented on 32 2080Ti GPUs. We compare the training cost with other robust training strategies in same setting. The results is shown in <ref type="table" target="#tab_2">Table 10</ref>. DAT only needs one attack step to generate discrete adversarial examples, which makes it less expensive than standard adversarial training. However, DAT still requires 3.5? training budget than normal training. We believe that reducing the cost of DAT is necessary, which will be remained as the future work.  In this work, we delete the discretizer Q at inference time. However, there is another option that remaining the discretizer for test inputs. To study the effect of this alternative, we report some results in <ref type="table" target="#tab_2">Table 11</ref>. Although inference with Q brings improvement on adversarial robustness, it meanwhile harms the standard performance and generalization. The inference cost also increases by the additional computation on Q. Therefore, such alternative cannot yield the best trade-off on speed and performance, which is not adopted by our DAT.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation of the Stronger Attacks in Ablation Experiments</head><p>For AutoAttack, we attack a subset of ImageNet provided in RobustBench 5 , which consists of 5000 images. Three perturbations bounded with l ? -norm of 1/255, 0.5/255 and 0.25/255 are adopted. For AdvDrop, the test dataset is 1000 random sampled images on ImageNet, which is provided in the official implementaton <ref type="bibr" target="#b5">6</ref> . The bound of the quantization table is a key factor in AdvDrop which controls the attack strength. We use three bounds with 10, 15 and 20 to regularize the quantization </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Visualization D.1 Attention Visualization</head><p>We visualize the attention of ViT trained by DAT in <ref type="figure" target="#fig_4">Figure 6(b)</ref>. For the object with unusual renditions in ImageNet-R, ViT cannot attend to semantically relevant image regions. While our DAT can locate the attention to the central object more related for the classification. This phenomenon is also reflected by the statistical average of the attention in <ref type="figure" target="#fig_4">Figure 6</ref>(c). By randomly sampling 1000 images in ImageNet-R and averaging the attention maps, we find the attentions of our DAT are more global. Compared with ViT which puts much attentions on the corners of the image found by prior work <ref type="bibr" target="#b19">[20]</ref>, our DAT additionally attends the central regions in where the classified object is often located.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original VQGAN_k16384_f8</head><p>VQGAN_k256_f8 VQGAN_k16384_f16 DALL-E VQGAN_IN_k16384_f16 <ref type="figure">Figure 7</ref>: The visualization of the discrete reconstructionx based on different discretizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=0</head><p>=0.2 =0.4 =0.6 =0.8 <ref type="figure">Figure 8</ref>: Training example visualization of DAT with different ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Shape-bias Analysis</head><p>We conduct an analysis based on shape-bias, which represents the fraction of correct decisions based on object shape. The result is shown in <ref type="figure" target="#fig_4">Figure 6</ref>(a). The averaged scores on 16 categories is denoted by the colored vertical line. We compare decisions with Humans, ResNet50 w/ and w/o DAT, ViT w/ and w/o DAT. Human decisions are highly based on shape, which achieve the best average fraction of 0.96 to correctly recognize an image by shape. By comparison, ViT and ResNet50 still have large gap with humans on the ability of learning shape features. In this work, we find our DAT can help for improving the fraction of shape-based decisions of models. It suggests DAT regularizes the models to learn texture independent shape features, and behave more like a human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Comparison of the Reconstruction Quality</head><p>We compare the reconstruction quality of different discretizers Q in <ref type="figure">Figure 7</ref>. VQGAN with k = 16384 and f = 8 can retain the most of the image details, which has the least impact on the classification models when used for training. With the growing of the downsampling factor f , some fine-grained attributes of the objects can be changed. For example, in third row the spotted texture on the peacock's tail is partly lost after reconstruction. Compared with VQGAN models, DALL-E blurs the image to a greater extent, yielding a low quality reconstructed image. Accordingly, DAT which uses DALL-E for image discretization performs worst on clean accuracy in <ref type="table" target="#tab_12">Table 5</ref>. It shows that the reconstruction quality is indeed proportional to the standard performance in DAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Visualization of Discrete Adversarial Examples</head><p>We visualize the discrete adversarial examples which is generated by DAT for training in <ref type="figure">Figure 8</ref>.</p><p>With the growing of ?, the alteration on images become larger. However, different from traditional adversarial example which essentially adds global high-frequency noise on images, we find DAT modifies the properties of the local part of the object. For example, in the fifth row of <ref type="figure">Figure 8</ref>, the discrete adversarial example is changing the eye color of the cat. Such modification is large but imperceptible and semantic-preserving. There are also some failure case where DAT changes the semantics. In sixth row of <ref type="figure">Figure 8</ref>, after discrete reconstruction, the otter in image looks more like a dog. We believe that such cases are only a minority, and will not affect the overall performance of DAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Visualization of the Straight-Through Gradients and Backward Gradients in DAT</head><p>We visualize the estimated straight-through gradients in Eq 7 and the directly backward gradients in Eq 6. As shown in <ref type="figure">Figure 9</ref>, the approximated gradient by straight-through estimator does accurately estimate the ground-truth gradients in Eq 6. It reflects the rationality of the proposed efficient straight-through gradient in DAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Straight-Through Gradients</head><p>Backward Gradients <ref type="figure">Figure 9</ref>: Visualization of the straight-through gradients and backward gradients.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FindFigure 1 :</head><label>1</label><figDesc>The overall pipeline of Discrete Adversarial Training (DAT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Pseudo code of DAT Input: Classifier F ; Pre-trained discretizer Q; A sampled mini-batch of clean images x with labels y; attack magnitude ?. Output: Learned network parameter ? of F 1: Fix the network parameters of Q 2: for each training steps do 3:x ? Q(x) //Get the discrete reconstructionx 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FrequencyFigure 2 :</head><label>2</label><figDesc>The frequency histogram of the Pearson correlation coefficient (PCC) between BN statistics on clean and adversarial images. Larger PCC value means smaller distributional difference with clean images. (a), (b) present the difference on mean and variance statistics respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of discrete perturbations and pixel-space perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>(a) The fraction of correct shape-based decisions of models w/ and w/o DAT. (b) Visualized attention on test images of ImageNet-R. (c) The heat map of averaged attention with ViT and ViT trained by our DAT.below</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The results of DAT on image classification. Bold number indicates the better performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Each of them represents a type of out-of-distribution scenario where the classifier is prone to make mistakes. IN-A [51] places the ImageNet objects in hard contexts; IN-C [52] applies a series of noise, blur, digital and weather corruptions; IN-R [47] collects online images with artificial creation, e.g., cartoons, graphics, video game renditions, etc; IN-Sketch [53] contains images described by sketches; Stylized IN [3] destroys the texture but maintains the shape feature by conducting style transfer on ImageNet images. Except for IN-C which is measured by mCE, we report the top-1 accuracy on all above datasets.</figDesc><table /><note>By default, we refer ViT to ViT-B/16 in all tables and figures. Benchmarks The trained model is evaluated in three aspects: 1) in-distribution performance on ImageNet-Validation set; 2) adversarial robustness on white-box FGSM [49] and transfer-based black-box attack dataset DamageNet [50]; 3) out-of-distribution robustness on ImageNet(IN)-A, IN-C, IN-V2, IN-R, IN-Sketch and Stylized IN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison of DAT with other training strategies. We use AugReg-ViT as the base model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Linear Evaluation</cell><cell></cell><cell></cell><cell cols="3">VOC Object Detection</cell><cell cols="2">ADE20K</cell></row><row><cell>Method</cell><cell>ImageNet</cell><cell>Flowers</cell><cell>CIFAR10</cell><cell>Caltech101</cell><cell>Cars</cell><cell>DTD</cell><cell>mAP</cell><cell>AP50</cell><cell>AP75</cell><cell>mIoU</cell><cell>Acc.</cell></row><row><cell>MoCov3</cell><cell>68.63</cell><cell>91.54</cell><cell>93.40</cell><cell>90.38</cell><cell>49.01</cell><cell>73.03</cell><cell>50.42</cell><cell>80.53</cell><cell>53.93</cell><cell>0.3508</cell><cell>75.64</cell></row><row><cell>+ DAT</cell><cell>69.60</cell><cell>93.15</cell><cell>95.16</cell><cell>91.42</cell><cell>53.09</cell><cell>73.55</cell><cell>51.92</cell><cell>80.97</cell><cell>56.04</cell><cell>0.3585</cell><cell>76.33</cell></row><row><cell>SimCLR</cell><cell>64.89</cell><cell>89.28</cell><cell>88.47</cell><cell>83.20</cell><cell>38.84</cell><cell>73.14</cell><cell>48.50</cell><cell>78.75</cell><cell>51.35</cell><cell>0.3396</cell><cell>75.61</cell></row><row><cell>+ DAT</cell><cell>65.47</cell><cell>90.14</cell><cell>89.97</cell><cell>85.09</cell><cell>39.42</cell><cell>72.93</cell><cell>48.83</cell><cell>79.27</cell><cell>51.95</cell><cell>0.3412</cell><cell>75.46</cell></row><row><cell>SimSiam</cell><cell>68.16</cell><cell>87.67</cell><cell>89.45</cell><cell>85.94</cell><cell>34.95</cell><cell>71.70</cell><cell>48.92</cell><cell>77.22</cell><cell>52.69</cell><cell>0.2212</cell><cell>68.57</cell></row><row><cell>+ DAT</cell><cell>68.41</cell><cell>86.93</cell><cell>91.70</cell><cell>87.03</cell><cell>35.10</cell><cell>73.03</cell><cell>51.73</cell><cell>79.69</cell><cell>55.74</cell><cell>0.2203</cell><cell>68.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The results of DAT on self-supervised learning.</figDesc><table /><note>objective by maximizing the contrastive loss. For preventing the cost explosion, we pre-train 200 epochs for SimCLR, and 100 epochs for both SimSiam and MoCov3. The batch size used for SimCLR, SimSiam, MoCov3 is set as 1024, 512, 2048 respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The results of DAT on object detection.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Although Det-AdvProp has been shown effective on EfficientDet with size larger than D0, we find on smaller detectors, the promotion of Det-AdvProp is subtle. On EfficientDet-Lite0 and YOLOv3-320, it even gets worse on COCO mAP than vanilla training. While our DAT achieves better result on both clean and corrupted input. EfficientDet-Lite0 with DAT get 27.3 and 17.89 mAP on COCO and COCO-C, resulting in 65.53% Relative rPC. By comparison, vanilla YOLOv3 models have lower Relative rPC, showing it is more vulnerable than EfficientDet at same clean mAP. After equipped with DAT, YOLOv3-320 can get 20.55 mAP on COCO-C, leading to 7.75% improvement on rPC.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Results of DAT based on different pretrained discretizer Q. f presents the downsampling factors. We use ResNet50 as base model and the subset of benchmarks in Sec 4.1 for evaluation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 presents</head><label>6</label><figDesc>the comparison results of DAT with other robust training strategies on ResNet50. Note that Advprop, Fast Advprop and Debiased use auxiliary BN while Pyramid AT and DAT not. Advprop achieves the best clean accuracy. The reason may lie in the auxiliary BN reduces the impact of adversarial examples on standard performance. On ResNet50, DAT is not always the best on the robustness benchmarks. By augmenting with the style transferred data, Debiased achieves best on IN-C, IN-Sketch and Stylized IN.On the contrary, such robust training method proposed for ViTs, e.g. Pyramid AT, is not incompatible with ResNet50 and obtain unsatisfactory results. However, our DAT is a general method which works for both CNNs and ViTs, and can be more practical.</figDesc><table><row><cell>Training Strategies</cell><cell>ImageNet</cell><cell cols="2">Adversarial Robustness FGSM DamageNet</cell><cell>A</cell><cell>C?</cell><cell cols="4">Out of Distribution Robustness V2 R Sketch</cell><cell>Stylized</cell></row><row><cell>Normal [48]</cell><cell>76.13</cell><cell>12.19</cell><cell>5.94</cell><cell>0</cell><cell cols="2">76.70</cell><cell>63.2</cell><cell>36.17</cell><cell>24.09</cell><cell>7.38</cell></row><row><cell>Advprop [26]</cell><cell>77.59</cell><cell>28.65</cell><cell>15.58</cell><cell>4.33</cell><cell cols="2">70.53</cell><cell>65.47</cell><cell>38.75</cell><cell>25.51</cell><cell>7.99</cell></row><row><cell>Fast Advprop [27]</cell><cell>76.6</cell><cell>17.33</cell><cell>7.45</cell><cell>2.19</cell><cell cols="2">73.31</cell><cell>64.24</cell><cell>38.17</cell><cell>25.03</cell><cell>8.3</cell></row><row><cell>Pyramid AT [28]</cell><cell>75.46</cell><cell>30.35</cell><cell>14.22</cell><cell>3.01</cell><cell cols="2">76.42</cell><cell>62.46</cell><cell>38.85</cell><cell>23.76</cell><cell>10.41</cell></row><row><cell>Debiased [57]</cell><cell>76.91</cell><cell>20.4</cell><cell>6.66</cell><cell>3.51</cell><cell cols="2">67.55</cell><cell>65.04</cell><cell>40.8</cell><cell>28.42</cell><cell>17.4</cell></row><row><cell>DAT (Ours)</cell><cell>76.52</cell><cell>30.66</cell><cell>14.42</cell><cell>4.38</cell><cell cols="2">74.16</cell><cell>65.02</cell><cell>41.9</cell><cell>27.27</cell><cell>10.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Comparison of our DAT with adversarial training models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 .</head><label>9</label><figDesc>For ResNet50, we find DAT reduces the accuracy on images with contrast and fog corruptions. It demonstrates that ResNet50 trained by DAT can be sensitive to the image lack of the hierarchy. However, for ViT, DAT can improve the performance on all corruptions. It suggests DAT works more efficiently on transformer-based vision models.</figDesc><table><row><cell>Model</cell><cell>Average</cell><cell cols="2">Blur Motion Defoc Glass Zoom Gauss Impul Shot Contr Elast JPEG Pixel Bright. Snow Fog Frost Noise Digital Weather</cell></row><row><cell>ResNet50</cell><cell>39.2</cell><cell>38.7</cell><cell>38.8 26.8 36.2 29.2 23.8 27.0 39.1 45.3 53.4 44.8 68.0 32.5 45.8 38.1</cell></row><row><cell>+DAT (Ours)</cell><cell>41.1</cell><cell>38.3</cell><cell>37.2 33.7 37.9 33.0 28.1 31.1 36.5 50.0 59.0 45.6 69.0 34.2 41.5 41.0</cell></row><row><cell>ViT</cell><cell>57.2</cell><cell>54.2</cell><cell>47.9 43.0 41.6 61.9 58.4 58.3 60.2 58.0 61.4 65.9 74.8 52.1 61.6 59.5</cell></row><row><cell>+DAT (Ours)</cell><cell>65.2</cell><cell>58.4</cell><cell>55.1 49.8 50.8 71.3 70.3 70.5 71.7 63.1 69.1 67.7 78.2 64.0 69.6 68.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Comparison of the training costs.</figDesc><table><row><cell>B.6 Inference with the Discretizer Q</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>The ablation on the discretizer Q at inference time.Traditional adversarial attacks always add constraints on perturbations. While in this work, there are no restrictions on ?. To explore if bounding ? is necessary in DAT, we add l ? bound on ? with different and re-run the DAT. The result is shown inTable 12, DAT achieves best performance when ? is not bounded. The worst result is appeared when ? is bounded with = 1/255. With larger , the results become better. Therefore, it seems bounding ? in our DAT is not necessary, and even plays negative affect on the overall performance.</figDesc><table><row><cell cols="3">B.7 Is it necessary for bounding ??</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>of l?</cell><cell>ImageNet</cell><cell cols="2">Adversarial Robustness FGSM DamageNet</cell><cell>A</cell><cell>C?</cell><cell cols="4">Out of Distribution Robustness V2 R Sketch</cell><cell>Stylized</cell></row><row><cell>1/255</cell><cell>76.10</cell><cell>29.41</cell><cell>12.00</cell><cell>3.53</cell><cell cols="2">75.53</cell><cell>64.11</cell><cell>39.05</cell><cell>25.04</cell><cell>8.69</cell></row><row><cell>2/255</cell><cell>76.16</cell><cell>29.75</cell><cell>13.24</cell><cell>3.75</cell><cell cols="2">74.87</cell><cell>64.32</cell><cell>40.38</cell><cell>25.53</cell><cell>9.31</cell></row><row><cell>4/255</cell><cell>76.47</cell><cell>31.43</cell><cell>14.25</cell><cell>4.31</cell><cell cols="2">74.12</cell><cell>65.07</cell><cell>41.68</cell><cell>26.99</cell><cell>10.62</cell></row><row><cell>? (Ours)</cell><cell>76.52</cell><cell>30.66</cell><cell>14.42</cell><cell>4.38</cell><cell cols="2">74.16</cell><cell>65.02</cell><cell>41.90</cell><cell>27.27</cell><cell>10.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :Attention Visualization on Test Images (a) Shape-bias Analysis of DAT</head><label>12</label><figDesc>The trained model on DAT when ? is bounded by different .B.8 DAT for Domain GeneralizationIn addition to training on large-scale ImageNet, we evaluate our DAT on Domain Generalization (DG) tasks. DG task is more challenging since it needs the learned model to transfer between multiple domains, using only small amount of the data. We adopt PACS dataset, which consists of four domains, namely Photo, Art Painting, Cartoon and Sketch. Each domain contains seven categories. For each trial, we train on 3 domains for generalizing to remaining unseen domain. To keep the setting consistent with previous works, we use AlexNet as the backbone. The results are shown in</figDesc><table><row><cell></cell><cell>Image</cell><cell>ViT</cell><cell>DAT (Ours)</cell><cell>Image</cell><cell>ViT</cell><cell>DAT (Ours)</cell><cell>ViT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DAT (Ours)</cell></row><row><cell>Humans</cell><cell cols="2">(b) ViT+DAT ViT ResNet50 ResNet50+DAT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) Average Attention</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 13 .</head><label>13</label><figDesc>We only compare with previously adversarial augmentation based DG methods. DAT can also achieve better domain generalization performance on PACS dataset. It has slight drop on domain of photo, but improves the transferability on other three domains.</figDesc><table><row><cell></cell><cell>ADA</cell><cell>MD-ADA</cell><cell>DAT (Ours)</cell></row><row><cell>Art Painting</cell><cell>64.3</cell><cell>67.1</cell><cell>67.3</cell></row><row><cell>Cartoon</cell><cell>69.8</cell><cell>69.9</cell><cell>71.3</cell></row><row><cell>Photo</cell><cell>85.1</cell><cell>88.6</cell><cell>87.8</cell></row><row><cell>Sketch</cell><cell>60.4</cell><cell>63.0</cell><cell>64.1</cell></row><row><cell>Average</cell><cell>69.9</cell><cell>72.2</cell><cell>72.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 13 :</head><label>13</label><figDesc>Classification accuracy (%) of our DAT on PACS dataset in comparison with the previously adversarial augmentation based DG methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>table .</head><label>.</label><figDesc>For PerC-Adversarial, we use the Perceptual Color distance Alternating Loss (PerC-AL) method to generate adversarial examples. PerC-Adversarial uses the test data of Defense Against Adversarial Attack Challenge in NeurIPS 2017. We change the attack strength by three different attack iterations: 20, 40 and 60.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/MadryLab/robustness</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/CompVis/taming-transformers 3 https://github.com/CompVis/latent-diffusion 4 https://github.com/microsoft/robust-models-transfer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/RobustBench/robustbench 6 https://github.com/RjDuan/AdvDrop</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Colorfool: Semantic adversarial colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shahin Shamsabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Sanchez-Matilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1151" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00945</idno>
		<title level="m">Learning perceptually-aligned representations via adversarial robustness</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Do adversarially robust imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3533" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards understanding the regularization of adversarial robustness on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10225" to="10235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Freelb: Enhanced adversarial training for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Achieving model robustness through discrete adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1529" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Is bert really robust? a strong baseline for natural language attack on text classification and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8018" to="8025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wordlevel textual adversarial attacking as combinatorial optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanchao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6066" to="6080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Jhang</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2890" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Advdrop: Adversarial attack to dnns by dropping information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjie</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dantong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Kai Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7506" to="7515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards large yet imperceptible adversarial image perturbations with perceptual color distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1039" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7472" to="7482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding and mitigating the tradeoff between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7909" to="7919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing excessive margin to achieve a better accuracy vs. robustness trade-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Rade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 12th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="95" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="819" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Fast advprop</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15121</idno>
		<title level="m">Dilip Krishnan, and Deqing Sun. Pyramid adversarial training improves vit performance</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attribute-guided adversarial training for robustness to natural perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rushil</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7574" to="7582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Augmax: Adversarial composition of random augmentations for robust training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Defending against image corruptions through adversarial augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Andrei Calian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>Gy?rgy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gowal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Maxup: Lightweight adversarial training with data augmentation improves neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2474" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1470" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jutta</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on statistical learning in computer vision, ECCV</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">ibot: Image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discrete representations strengthen vision transformer robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10752</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding straight-through estimator in training activation quantized neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Xin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05662</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">How to train your vit? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Universal adversarial attack on attention and the resulting dataset damagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15262" to="15271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Shape-texture debiased neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How well do self-supervised models transfer?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linus</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5414" to="5423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adversarial self-supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2983" to="2994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Robust and accurate object detection via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16622" to="16631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Benchmarking robustness in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07484</idno>
	</analytic>
	<monogr>
		<title level="m">Autonomous driving when winter is coming</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2206" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
