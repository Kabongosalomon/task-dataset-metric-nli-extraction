<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. X, XX XXXX 1 FP-Age: Leveraging Face Parsing Attention for Facial Age Estimation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yiming</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jie</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. X, XX XXXX 1 FP-Age: Leveraging Face Parsing Attention for Facial Age Estimation in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Age estimation</term>
					<term>face parsing</term>
					<term>in-the-wild dataset</term>
					<term>attention</term>
					<term>cross-dataset evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image-based age estimation aims to predict a person's age from facial images. It is used in a variety of real-world applications. Although end-to-end deep models have achieved impressive results for age estimation on benchmark datasets, their performance in-the-wild still leaves much room for improvement due to the challenges caused by large variations in head pose, facial expressions, and occlusions. To address this issue, we propose a simple yet effective method to explicitly incorporate facial semantics into age estimation, so that the model would learn to correctly focus on the most informative facial components from unaligned facial images regardless of head pose and nonrigid deformation. To this end, we design a face parsing-based network to learn semantic information at different scales and a novel face parsing attention module to leverage these semantic features for age estimation. To evaluate our method on in-the-wild data, we also introduce a new challenging large-scale benchmark called IMDB-Clean. This dataset is created by semi-automatically cleaning the noisy IMDB-WIKI dataset using a constrained clustering method. Through comprehensive experiment on IMDB-Clean and other benchmark datasets, under both intra-dataset and cross-dataset evaluation protocols, we show that our method consistently outperforms all existing age estimation methods and achieves a new state-of-the-art performance. To the best of our knowledge, our work presents the first attempt of leveraging face parsing attention to achieve semantic-aware age estimation, which may be inspiring to other high level facial analysis tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>facial representation that has greatly improved the state-ofthe-art in automatic estimation of facial age. However, most deep models are not explicitly trained to learn facial semantic information like eyes and noses, and therefore the extracted embedding may not appropriately attend to those more informative facial regions.</p><p>It has been shown that the most informative features for age estimation are located in the local regions such as eyes and mouth corners <ref type="bibr" target="#b2">[3]</ref>. On the other hand, face parsing is designed to classify each pixel into different facial regions and to give the regional boundaries. Therefore, a Convolutional Neural Network (CNN) trained for face parsing could also pick up the features around the facial regions that are also useful for determining the age. Moreover, due to the hierarchical structure of CNNs, the intermediate features can encode both local and global information that can be fused for age estimation.</p><p>To this end, we propose FP-Age for leveraging features in a face parsing network for facial age estimation. In particular, we adopt both coarse and fine-grained features from a pretrained face parsing network <ref type="bibr" target="#b3">[4]</ref> to represent facial semantic information at different levels and built a small network on top of it to predict the age. To avoid the loss of details in the high level features, we design a Face Parsing Attention (FPA) module to explicitly drive the network's attention to those more informative facial parts. The attended high-level features are then concatenated to the low-level features and fed into a small add-on network for age prediction. Since the semantic features are extracted using a pre-trained face parsing model, no additional face parsing annotations are required and thus our FP-Age network can be trained in an end-to-end fashion, similar to other age estimation networks.</p><p>We have also developed a semi-automatic approach to clean the noisy data in IMDB-WIKI, leading to a new large-scale age estimation benchmark titled IMDB-Clean. Our FP-Age network achieves state-of-the-art results on this IMDB-Clean , as well as on several other age estimation datasets, under both intra-dataset and cross-dataset evaluation protocols. To the best of our knowledge, this is the first reported effort to adopt semantic facial information for age estimation based on an attention mechanism on different facial regions. The idea of Face Parsing Attention can be inspiring to other facial analysis tasks too, and the proposed FP-Age network can be easily adapted to perform on those tasks as well, e.g. facial gesture recognition and emotion recognition.</p><p>Our main contributions are as follows:</p><p>? The IMDB-Clean dataset: a large-scale, clean image dataset for age estimation in the wild;</p><p>arXiv:2106.11145v2 [cs.CV] 4 Mar 2022</p><p>? FP-Age: a simple yet effective framework that leverages facial semantic features for semantic-aware age estimation; <ref type="bibr">?</ref> We also demonstrate that for age estimation, different facial parts have variable importance with "nose" being the least important region; ? Our FP-Age achieves new state-of-the-art results on IMDB-Clean, Morph <ref type="bibr" target="#b4">[5]</ref> and CACD <ref type="bibr" target="#b5">[6]</ref>; ? When trained on IMDB-Clean, our FP-Age also achieves state-of-the-art results on KANFace <ref type="bibr" target="#b6">[7]</ref>, FG-Net <ref type="bibr" target="#b7">[8]</ref>, Morph <ref type="bibr" target="#b4">[5]</ref> and CACD <ref type="bibr" target="#b5">[6]</ref> under cross-dataset evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image-based Biological Age Estimation</head><p>Early works on age estimations are mainly based on handcrafted features, and we refer interested readers to <ref type="bibr" target="#b8">[9]</ref> for a detailed survey. Recently, deep learning techniques have achieved significantly improved performance in this field. In this section, we briefly explain several deep learning approaches on age estimation. They are roughly organised into four categories depending on how they model the problem: regression based, classification based, ranking based and label distribution based.</p><p>Regression approaches treat facial ageing as a regression problem and directly predict true age values from facial images. Euclidean loss is therefore a popular choice among those methods. Yi et al. <ref type="bibr" target="#b9">[10]</ref> adopted mean squared loss to train a multi-scale CNN for age regression. Similarly, Wang et al. <ref type="bibr" target="#b10">[11]</ref> apply the same loss on the representation obtained by fusing feature maps from different layers of a CNN.</p><p>In contrast to regression methods, classification based works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref> formulate the age estimation as a multi-class classification problem and treat different ages as independent classes. Although such formulations make it easier to train CNNs, this ignores the correlations between different classes.</p><p>Ranking approaches inspect the ordinal property embedded in the ageing process. OR-CNN <ref type="bibr" target="#b12">[13]</ref> proposed to formulate age estimation as an ordinal regression problem and built multiple binary classification neurons on top of a CNN. Ranking-CNN <ref type="bibr" target="#b13">[14]</ref> ensembled a series of CNN-based binary classifiers and aggregated their predictions to obtain the estimated age. In SVRT <ref type="bibr" target="#b14">[15]</ref>, a strategy of triplet learning were introduced into the ranking loss. CORAL <ref type="bibr" target="#b15">[16]</ref> improved OR-CNN <ref type="bibr" target="#b12">[13]</ref> by proposing the Consistent Rank Logits framework to address the problem of classifier inconsistency.</p><p>Label Distribution Learning (LDL) <ref type="bibr" target="#b16">[17]</ref>, however, models the age prediction as a probability distribution over all potential age values. LDL-based methods have achieved the current state-of-the-art performance on various age estimation benchmarks. Dex <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> proposed to take the expectation value of output distribution as the predicted age. MV-Loss <ref type="bibr" target="#b19">[20]</ref> introduced the mean-variance loss to regularise the shape of the output distribution complementing the cross-entropy loss. DLDL <ref type="bibr" target="#b20">[21]</ref> and DLDL-v2 <ref type="bibr" target="#b21">[22]</ref> represented the age label as a Gaussian distribution and applied Kullback-Leibler divergence to measure the discrepancy between the output age distribution and the target label distribution. Shen et al. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> used an ensemble of decision trees in the LDL formulation. Akbari et al. <ref type="bibr" target="#b24">[25]</ref> proposed the distribution cognisant loss to regularise the predicted age distribution, improving the robustness against outliers. In this work, we follow the problem formulation of LDL-based methods, considering that they have consistently achieved most state-of-the-art results.</p><p>Noticeably, several approaches <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref> involve applying pre-trained face recognition models as the initialisation of ages estimation models, while in contrast, we freeze the weights of the face parsing network to avoid unnecessary computational cost. Additionally, some works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref> tackled age estimation simultaneously with other tasks like gender classification through a multi-task framework, sharing representations across different tasks. Although our network also share features, it differs from multi-task framework as it requires no semantic labels and also Face Parsing Attention is leveraged to transit semantic-level knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Face Parsing</head><p>Face parsing aims to classify each pixel in a facial image into different categories like background, hair, eyes, nose, etc. . Earlier works <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> used holistic priors and handcrafted features. Deep learning has largely improved the performance of face parsing models Liu et al. <ref type="bibr" target="#b30">[31]</ref> combined CNNs with conditional random fields and proposed a multiobjective learning method to model pixel-wise likelihoods and label dependencies. Luo et al. <ref type="bibr" target="#b31">[32]</ref> applied multiple Deep Belief Networks to detect facial parts and built a hierarchical face parsing framework. Jackson et al. <ref type="bibr" target="#b32">[33]</ref> employed facial landmarks as a shape constraint to guide Fully Convolution Networks (FCNs) for face parsing. Multiple deep methods including CRFs, Recurrent Neural Networks (RNNs) and Generative Adversarial Networks (GAN) were integrated by authors of <ref type="bibr" target="#b33">[34]</ref> to formulate an end-to-end trainable face parsing model, while the facial landmarks also served as the shape constraints for segmentation predictions. The idea of leveraging shape priors to regularise segmentation masks can also be found in the Shape Constrained Network (SCN) <ref type="bibr" target="#b34">[35]</ref> for eye segmentation. In <ref type="bibr" target="#b35">[36]</ref>, a spatial Recurrent Neural Networks was used to model spatial relations within face segmentation masks. A spatial consensus learning technique was explored in <ref type="bibr" target="#b36">[37]</ref> to model the relations between output pixels, while graph models were adopted in <ref type="bibr" target="#b37">[38]</ref> to learn implicit relationships between facial components. To better utilise the temporal information of sequential data, authors of <ref type="bibr" target="#b38">[39]</ref> integrated ConvLSTM <ref type="bibr" target="#b39">[40]</ref> with the FCN model <ref type="bibr" target="#b40">[41]</ref> to simultaneously learn the spatial-temporal information in face videos and to obtain temporally-smoothed face masks. In <ref type="bibr" target="#b41">[42]</ref>, a Reinforcement-Learning-based key scheduler was introduced to select online key frames for video face segmentation such that the overall efficiency can be globally optimised.</p><p>Most of those methods assume the target face has already been cropped out and is well aligned. Moreover, they often ignore the hair class due to the unpredictable margins for cropping the hair region. To solve this, Lin et al. <ref type="bibr" target="#b42">[43]</ref> proposed to warp the entire image using the Tanh function. However, the warping still requires not only the facial bounding boxes   The shapes of tensors are labelled by the blocks and ? means floor division. Face Parsing Attention is proposed to aggregate the semantic information into the features and improve age estimation.</p><p>but also the facial landmarks. Recently, RoI Tanh-polar transform <ref type="bibr" target="#b3">[4]</ref> has been proposed to solve face parsing in the wild. The RoI Tanh-polar transform warps the entire image to the Tanh-polar space and the only requirement is to have the target bounding box. With the Tanh-polar representation, a simple FCN architecture has already achieved state-of-the-art results <ref type="bibr" target="#b3">[4]</ref>. The proposed FP-Age builds atop of this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The overall architecture of FP-Age is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. The network at the top is an off-the-shelf, pre-trained face parsing model <ref type="bibr" target="#b3">[4]</ref> whose parameters are not updated for the training. At the bottom is the proposed age estimation network that contains the proposed face parsing attention module and some standard operational layers to predict the age. In this section, we formulate age estimation as the distribution learning problem and explain further in detail the components in the proposed FP-Age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><formula xml:id="formula_0">Let X = {(x (i) , b (i) , y (i) )} N i=1</formula><p>denote a set of N training example triplets where x (i) , b (i) and y (i) are i-th input image, its target face bounding box, and its corresponding age label, respectively. The bounding box b (i) is a four-dimensional tuple (x min , y min , x max , y max ) defined by the top-left and the bottom-right corners of the target face location. The age label y (i) is an integer from a set of age labels Y = {0, . . . , K ?1}. We denote the total number of age classes Y as K.</p><p>Our goal is to learn a mapping function f from the target face in x (i) , specified by b (i) , to the label y (i) . When learning such function using DNNs, one way is to set the last layer as one output neuron and employ an Euclidean loss function. However, it has been shown <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref> that training such DNNs are relatively unstable; outliers can cause large errors. Another way is to formulate age estimation as a Kclass classification problem and use the one-hot encoding to represent age labels. But this formulation ignores the fact that the faces with close ages share similar features, causing visual label ambiguity <ref type="bibr" target="#b16">[17]</ref>.</p><p>Considering the above, we formulate the age estimation as a label distribution learning problem <ref type="bibr" target="#b16">[17]</ref>. Specifically, we encode each scalar age label y (i) as a probability distribution</p><formula xml:id="formula_1">q (i) = [q (i) 0 , q (i) 1 , ..., q (i) K?1 ] T ? R K on the interval [0, K ? 1].</formula><p>Each element in q (i) represents the probability of the target face in x (i) having the k-th label. A Gaussian distribution centred at y (i) with a standard deviation ? is used to map y (i) to q (i) . We follow Gao et al. <ref type="bibr" target="#b21">[22]</ref> and set ? = 2 in all experiments.</p><p>Using this formulation, we use a Fully-Connected (FC) layer followed by a Softmax layer to map the DNN's output logits to the predicted distribution p (i) . The learning problem becomes</p><formula xml:id="formula_2">? * = arg min ? N i=1 L (i) [p (i) = f (x (i) , b (i) ), q (i) ]<label>(1)</label></formula><p>where f is the DNN and ? is its corresponding parameters. L denotes a loss function. The predicted age? (i) is obtained by taking the expectation over the p (i) as? (i) =</p><formula xml:id="formula_3">K?1 k=0 kp (i) k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Face Parsing Network</head><p>We use RTNet <ref type="bibr" target="#b3">[4]</ref> for extracting face parsing features. RT-Net has a simple FCN-like encoder-decoder architecture and achieves state-of-the-art results for in-the-wild face parsing tasks. The encoder contains 5 residual convolutional layers for feature extraction, similar to the original ResNet-50 <ref type="bibr" target="#b43">[44]</ref>. Two convolutional layers are used in the decoder to perform perpixel classification to obtain the face masks. In the encoder, the first three convolutional layers gradually reduce the spatial resolution to ( H 8 , W 8 ), and the last two layers use dilated convolutions <ref type="bibr" target="#b44">[45]</ref> to aggregate multi-scale contextual information without reducing the resolution.</p><p>In contrary to traditional methods that require facial landmarks to align the faces, RTNet uses RoI Tanh-polar transform to warp the entire image given the target bounding box. Some examples of the warping effect can be seen in <ref type="figure" target="#fig_2">Fig. 2</ref>. The warped representation not only retains all the information in the original image, but also amplifies the target face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face Parsing Attention</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, there are five feature maps produced by the encoder and one feature map given by the decoder. We take the third feature map in the encoder and denote it as the low-level feature L ? R H 8 ? W 8 ?256 . We consider the only feature map in the decoder as the high-level feature and denote it as</p><formula xml:id="formula_4">H ? R H 8 ? W 8 ?512 . Lastly, we denote the output C-channel face masks as M ? R H 8 ? W 8 ?C .</formula><p>We first divide H into C groups along the channel dimension. The k-th is group representation, after 1 ? 1 convolution, is denoted as</p><formula xml:id="formula_5">U k ? R H 8 ? W 8 ? 512 C for k = 1, . . . , C.</formula><p>Next, we multiply each group with the corresponding mask group:</p><formula xml:id="formula_6">U k = M k * U k .<label>(2)</label></formula><p>The representations? k for k = 1 . . . C are then concatenated along the channel dimension to get U . After that, we apply a channel attention block <ref type="bibr" target="#b45">[46]</ref> to capture the dependencies between face regions. This block is formed by a sequence of layers: AvgPool, FC, ReLU, FC and Sigmoid. And the output attention weights are a ? R C . The final output of this module</p><formula xml:id="formula_7">is V = [V 1 , V 2 , . . . , V C ] and each feature group is obtained by V k = a k?k .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Age Estimation Network</head><p>After the face parsing attention module is applied, we concatenate L and V along the channel dimension, and apply a 1?1 convolutional layer to reduce the channel number to 256. Next, 4 residual blocks <ref type="bibr" target="#b43">[44]</ref> are employed. Finally, we use a  <ref type="bibr" target="#b3">[4]</ref> warps the whole image to a fix-sized representation in the Tanh-polar space with the given bounding box.</p><p>FC layer followed by a SoftMax layer to map the output logits to the predicted distribution p. The predicted age is obtained by taking the expectation over p as? =</p><formula xml:id="formula_8">K?1 k=0 kp k . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Function</head><p>We use the weighted sum of Kullback-Leibler divergence and L1 loss as our loss function for the i-th example:</p><formula xml:id="formula_9">L (i) = K?1 k=0 q (i) k log( q (i) k p (i) k ) + ?|? (i) ? y (i) |<label>(4)</label></formula><p>where | ? | denotes taking the absolute value and ? is a weight balancing two terms. We empirically set ? = 1 for all examples <ref type="bibr" target="#b20">[21]</ref>.  <ref type="figure">Fig. 3</ref>: Some examples from IMDB-WIKI <ref type="bibr" target="#b17">[18]</ref> and our proposed IMDB-Clean. Each column shows the faces cropped from the same image using the groundtruth bounding boxes. The face detector used by IMDB-WIKI is biased towards middle-aged faces when encountering multiple faces, and fails for low-quality images. Our proposed semi-automatic cleaning method has corrected these errors (see Section IV-B for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>from 0 to 100 years old. The images were crawled from IMDB and Wikipedia, where the IMDB subset contains 460, 723 images and the Wikipedia subset contains 62, 328 images. These images, especially the IMDB subset, were mostly captured inthe-wild and thus are potentially useful for evaluating age estimation in real-world environment. However, the annotations of IMDB-Wiki are very noisy, such that the provided face box is often centred around the wrong person when multiple people are presented in the same image. Because of this, IMDB-Wiki has only been used for pre-training by existing age estimation methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b46">[47]</ref>.</p><p>2) CACD: Cross-Age Celebrity Dataset (CACD) <ref type="bibr" target="#b5">[6]</ref> is an in-the-wild dataset that has about 160, 000 facial images of 2, 000 people. These images are divided into the training set, the validation set and the test set which contain 1, 800 people, 120 people and 80 people, respectively. We adopt the common practice originally used in <ref type="bibr" target="#b23">[24]</ref> and report results on the testing set obtained by using the models trained on the training set and the validation set.</p><p>3) KANFace: KANFace <ref type="bibr" target="#b6">[7]</ref> is an in-the-wild dataset consisting of 41, 036 images from 1, 045 subjects. The age range of this dataset is from 0 to 100 years. The images are extremely challenging due to large variations in pose, expression and lightning conditions. Since the authors do not provide splits, we use this dataset only as a test set and the evaluation results obtained by models trained on other datasets. 4) Morph: Morph <ref type="bibr" target="#b4">[5]</ref> consists of 55, 134 mugshot images from 13, 617 subjects with the age ranging from 16 to 77 years old. Even though it is not an in-the-wild dataset, we report our results on it given its popularity. For intra-dataset evaluations, we follow the setting used in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>: we randomly divide the dataset into two non-overlapping sets, the training set (80%) and the testing part (20%). For cross-dataset evaluations, we use all 55, 134 images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Creating the IMDB-Clean Dataset</head><p>Although there have been efforts such as those reported in <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> to manually clean the IMDB-WIKI dataset, many images still have incorrect annotations. This is mainly because the previous efforts either relied on simple heuristics to remove low-quality images <ref type="bibr" target="#b49">[50]</ref>, or asked human raters to annotate apparent ages for the images based on their visual perception <ref type="bibr" target="#b50">[51]</ref>. The latter is a very difficult task, resulting in incorrect guesses due to low quality images and very high quality make-ups.</p><p>To identify the source of noise, we revisited the annotation process for the images in the IMDB subset <ref type="bibr" target="#b17">[18]</ref>. We concluded that a relatively weak face detector was used to provide bounding box labels and that, when multiple faces are encountered, the one with the highest detection score is selected.</p><p>The main problem with such an annotation process is that when there are multiple faces, the adopted face <ref type="bibr" target="#b51">[52]</ref> is biased towards large, frontal, middle-aged faces and give high scores to them. Another problem is that the utilised face detector fails to detect faces when the image has large variations in imaging quality, lightning, background etc. , because it has not been trained on in-the-wild images. Some errors are shown in <ref type="figure">Fig. 3</ref>.</p><p>Based on the above analysis, we cleaned the dataset following the process below: 1) For each subject, we use an advanced face detector S 3 FD [53] to detect all faces in all images of the target subject crawled from IMDB. 2) We use FAN-Face <ref type="bibr" target="#b53">[54]</ref> to map these face images into the face recognition embedding space. 3) We then use a constrained version of the DBSCAN <ref type="bibr" target="#b54">[55]</ref> clustering algorithm to cluster these faces. Here, cannotlink constraints are applied to faces occurring in the same images. 4) Because the method can yield different results when the order of the input faces is changed, we repeat the clustering process multiple times using random ordering. 5) After that, for each subject, we take the largest cluster obtained from all runs, and consider this to be the correct cluster containing the face images of the target subject. 6) For one subject, if the second largest cluster is larger than 70% of the largest cluster, we consider this an ambiguous case. These ambiguous cases (528) are manually checked and filtered. 7) Finally, we manually examine the dataset again to remove obvious mistakes caused by incorrect timestamps. <ref type="figure">Fig. 3</ref> shows some noisy examples and the cleaned results. Note that the above cleaning process is not applied to the WIKI subset because most identities in this subset have only one image crawled from their Wikipedia page.</p><p>We refer to the cleaned dataset as IMDB-Clean, which contains 287, 683 images of 7, 046 subjects with age labels ranging from 0 to 97. We split IMDB-Clean into three subjectindependent sets: training, validation and testing. The distributions of these sets are shown in <ref type="figure" target="#fig_3">Fig. 4</ref> and a comparison to other publicly available age datasets is given in <ref type="table" target="#tab_2">Table I</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>The performance of models is measured by Mean Absolute Error (MAE) and Cumulative Score (CS). MAE is calculated using the average of the absolute errors between age predictions and groundtruth labels on the testing set; CS is calculated by CS l = N l N ? 100% where N is the total number of testing examples and N l is the number of examples whose absolute error between the estimated age and the groundtruth age is not greater than l years. We report MAEs and CS 5 for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head><p>We use RoI Tanh-polar transform <ref type="bibr" target="#b3">[4]</ref> to warp each input image to a Tanh-polar representation of resolution 512 ? 512. In the training stage, we apply image augmentation techniques including horizontal flipping, scaling, rotation and translation, as well as bounding box augmentations <ref type="bibr" target="#b3">[4]</ref>. For all experiments, we employed mini-batch SGD optimiser. The batch size, the weight decay and the momentum were set to 80, 0.0005 and 0.9, respectively. The initial learning rate is 0.0001 and gradually increases to 0.01 in 5 epochs. Then the learning rate decreases exponentially at each epoch and the training is stopped either when the MAE on the validation set stops decreasing for 10 epochs or we reach 90 training epochs. During testing, the test image and its flipped copy are fed into the model and their predictions are averaged.</p><p>For the comparisons reasons, we have re-implemented the following models from scratch: Dex <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, OR-CNN <ref type="bibr" target="#b12">[13]</ref>, DLDL <ref type="bibr" target="#b20">[21]</ref>, DLDL-V2 <ref type="bibr" target="#b21">[22]</ref> and MV-Loss <ref type="bibr" target="#b19">[20]</ref> while ResNet-18 <ref type="bibr" target="#b43">[44]</ref> was used as the backbone. The pre-processing, training and testing steps follow the above procedure. For the models with open-sourced training code, i.e. C3AE <ref type="bibr" target="#b46">[47]</ref>, SVRT <ref type="bibr" target="#b14">[15]</ref>, SSRNet <ref type="bibr" target="#b55">[56]</ref> and Coral <ref type="bibr" target="#b15">[16]</ref>, we used their default training setups and hyper-parameters. RetinaFace <ref type="bibr" target="#b56">[57]</ref> was applied to detect 5 facial landmarks (left and right eye centres, nose tip, left and right mouth corners). The input images were aligned using these landmarks with the method proposed in SSRNet 1 and then resized to 256 ? 256 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS A. Can Face Parsing Mask Help?</head><p>As a motivational example, we first test whether existing age estimation methods can benefit from facial parts segmentation. This is done by simply stacking the face parsing masks to the input image and using the resulted 14-channel tensor as the input to the models. During this experiment, we re-train three state-of-the-art methods, Dex, DLDL-V2 and MV-Loss, with the modified 14-channel input and test the models on IMDB-Clean. From <ref type="table" target="#tab_2">Table II</ref> we observe that by taking the stacked representation as input, all three models can achieve better performance in terms of both MAE and CS 5 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Which Face Parsing Features to Use?</head><p>We study which face parsing features are more informative for age estimation. We remove the face parsing attention module in FP-Age and use take face parsing features directly as input. We use four kinds of features as input: 1) low-level; 2) high-level; 3) stacking low and high; and 4) stacking low, high and mask.</p><p>From <ref type="table" target="#tab_2">Table III</ref>, we observe that using high-level features gives worse performance than using low-level features. This is consistent with earlier research <ref type="bibr" target="#b2">[3]</ref> which argues that local features are more informative as they capture ageing patterns around the facial regions, such as the dropping skin around the eyes, and the wrinkles around the mouth. On the other hand, due to the dilated convolutions in RTNet, the highlevel features capture a larger perceptive field and thus the details can be lost. Stacking low-level and high-level features gives better performance which shows that these two types of features are complementary and combining them can help age estimation network.</p><p>We also observe that adding mask further improves the model. This can be attributed to the fact that face mask contains semantics about different regions and adding it as an explicit attention mechanism helps the model to effortlessly locate these regions and extract ageing patterns. Furthermore, our face parsing attention module yields better results than simple stacking, which we further investigate in section V-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. How about Other Feature Extractors?</head><p>To validate the choice of the face parsing network as the feature extractor, we replace it with other CNN-based feature extractors and compare the performance of these variants.</p><p>We adopted various generic feature extractors that are commonly used in transfer learning tasks as a replacement of face parsing network. The feature extractors include variants from the families of ResNet <ref type="bibr" target="#b43">[44]</ref>, ResNeXt <ref type="bibr" target="#b57">[58]</ref>, MobileNetV3 <ref type="bibr" target="#b58">[59]</ref>, FBNet <ref type="bibr" target="#b59">[60]</ref> and InceptionV4 <ref type="bibr" target="#b60">[61]</ref>. Their weights have been pre-trained on the ImageNet dataset and remained frozen during the training for age estimation.</p><p>We also adopted a state-of-the-art face recognition feature network, ArcFace <ref type="bibr" target="#b61">[62]</ref>, for feature extraction. The backbone of ArcFace is a customised, improved version of ResNet and it has been pre-trained on the large scale MS1M <ref type="bibr" target="#b62">[63]</ref> dataset for face recognition. The pre-trained weights remained frozen during the training for age estimation.</p><p>To ensure fair comparison, we did not use Face Parsing Attention in our model. All feature extractors adopted the same strategy for stacking deep and shallow semantic features. The age estimation sub-network and all other hyper-parameters remain the same as FPAge. <ref type="table" target="#tab_2">Table IV</ref> shows the results of using different pre-trained feature extractors on IMDB-Clean. Our first observation is that the features of ResNet50 performed the best among ImageNet pre-trained models though being less accurate on image classification tasks. Moreover, all ImageNet pre-trained models obtained MAEs larger than 7. This in turn suggests generic features encoded in the CNNs for image classification are not directly transferable to solve the age estimation problem.</p><p>Our second observation is that face recognition features resulted better performance than generic features, meaning that the encoded details for distinguishing between identities are more transferable to the age estimation problem.</p><p>Finally, face parsing features have given the best performance among all evaluated backbones. This suggests that face parsing networks, designed to classify each pixel in a face, are able to encode the most informative details for age estimating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. But Aren't There Other Attentions?</head><p>To validate the usefulness of the proposed Face Parsing Attention (FPA) module, we compare it with three generic  <ref type="table" target="#tab_6">Table V</ref> shows that, when applied on the same face parsing features, the proposed FPA has achieved the lowest MAE on IMDB-Clean. Moreover, FPA is directly derived from the face parsing map and acts as a probe to understand what the netword has learned, which we investigate in section V-F. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We conduct ablation study on the overall FP-Age model to understand the contribution of each component. We have evaluated five variants on IMDB-Clean. We replaced the face parsing network with ResNet50 which has the similar number of parameters. Next, we either removed the FPA module or replace it with the Squeeze-Excitation <ref type="bibr" target="#b45">[46]</ref> module. <ref type="table" target="#tab_2">Table VI</ref> shows that the biggest improvement comes from adopting the face parsing network as the feature extractor with MAEs improved from above 7 to 4.96. Moreover, the proposed FPA has reduced the MAE from 4.96 to 4.68, an improvement of 0.28 compared with 0.1 by the Squeeze-Excitation module. To provide a clearer picture of the function of the proposed face parsing attention module, we study the 11-class activation output of the Sigmoid layer. Specifically we show the mean and standard deviations of the activations for images in the IMDB-Clean dataset in <ref type="figure" target="#fig_4">Fig. 5</ref>. We observe that the network consistently gives higher attention weights to most inner facial regions, especially eyes ("l-eye" and "r-eye") and mouth ("upper-lip", "i-mouth", and "lower-lip"). This is in line with the observations reported in <ref type="bibr" target="#b2">[3]</ref>. Interestingly, it can also be seen that the "background" class contributes more than the "skin" class. This could be attributed to the fact that the face parsing network classifies objects like "beard", "glasses" and "accessories" as "background", and such context information could give hints about the person's age.</p><p>We have also performed the same test on separate age groups and observed the importance of different facial regions follows the same trend as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. This means that the face parsing attention allows the model to focus on informative regions that are universally important for judging different ages. Although there are some works such as <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b66">[66]</ref>- <ref type="bibr" target="#b68">[68]</ref> that used attention, we are the first to present the evidence that the network attends to specific facial parts and that such attention modelling improves age estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Effectiveness of IMDB-Clean</head><p>We conduct experiments on the effectiveness of the proposed IMDB-Clean. Specifically, we train 6 models on three datasets, i.e. IMDB-Clean, IMDB-WIKI and CACD. We then directly test them on KANFace without any fine-tuning. For IMDB-WIKI, we randomly sampled 300, 000 images for training; for the other two datasets, we used their provided training splits. <ref type="table" target="#tab_2">Table VII</ref> shows the cross-dataset evaluation results on KANFace. We observe that 1) all models have improved when they are trained on our IMDB-Clean; 2) our model outperforms other methods when trained on IMDB-Clean and IMDB-WIKI, and is comparable to DLDL-V2 when trained on CACD. H. Comparison to the State-of-the-arts 1) Intra-Dataset Evaluation: In this section, the performance of the proposed FP-Age is compared with the stateof-the-art age estimation methods under the intra-dataset evaluation protocol. Three benchmarks are used: IMDB-Clean, Morph and CACD. On IMDB-Clean, we train all the models from scratch on the same training set and test them on the testing set. For Morph and CACD, we only train our own models and compare the performance with the reported values for the other methods on the testing set.</p><p>The benchmarking results are shown in <ref type="table" target="#tab_2">Table VIII</ref>. It can be seen that our model achieves state-of-the-art results on IMDB-Clean dataset. When all model are trained under the same settings, our model achieves 4.68 in terms of MAE and 63.78% in terms of CS 5 . Additionally, the results show that IMDB-Clean is quite challenging compared to other datasets, such as Morph where the state-of-the-art MAEs have achieved below 2. We provide significance testing analysis in Appendix A which shows our results are significantly better than the other methods.</p><p>From <ref type="table" target="#tab_2">Table IX</ref>, it can be seen that our model achieves stateof-the-art results on Morph dataset. When directly trained on Morph, our model achieves 2.04 in terms of MAE and 92.8% in terms of CS 5 . When pre-trained on IMDB-Clean and finetuned the weights on Morph, FP-Age achieves a MAE of 1.90 and a CS 5 of 93.7%, which is the new state-of-the-art result. <ref type="table" target="#tab_11">Table X</ref> shows the results on the CACD dataset. Following the training protocols of CACD <ref type="bibr" target="#b22">[23]</ref>, we train our models with both the training set and the validation set, and report the MAE values on the testing set. Our model achieves 4.50 when trained on CACD-train and 5.62 when trained on CACD-val. Similar to above experiments, when pre-trained on IMDB-Clean, our model achieves 4.33 and 4.95 .</p><p>2) Cross-Dataset Evaluation: To test the generalisation ability of different models, we conduct experiments on a crossdataset evaluation protocol. Our results are compared with 9 advanced models: SSRNet, C3AE, SVRT, DLDL, DLDL-V2, Coral, Dex, MV-Loss, and OR-CNN. We train all models on IMDB-Clean and test them on 4 different testing datasets without fine-tuning. The reuslts are summarised in <ref type="table" target="#tab_2">Table XI</ref>. It can be seen that when all models are trained on IMDB-Clean, the proposed FP-Age achieves the best results on most of evaluation datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we have proposed a simple yet effective approach of exploiting face parsing semantics for age estimation. We have designed a framework to aggregate features from different levels of the face parsing network. A novel face parsing attention module is proposed to explicitly introduce facial semantics into the age estimation network. To train the model, we propose an semi-automatic clustering method for cleaning existing dataset and introduce the resulting IMDB-Clean dataset as a new in-the-wild benchmark. Thanks to the attention mechanism and the large-scale dataset, we have observed that the network focuses on certain facial parts when predicting ages. The nose region appears least informative for age estimation. Moreover, the extensive experiments have shown that our model outperforms the current state-of-the-art methods on various dataset in both intra-dataset and crossdataset evaluations. To the best of our knowledge, this is the first attempt of leveraging face parsing attention to achieve age estimation. We hope our design could inspire the readers to consider similar attention models for different deep face analysis tasks. For future work, since we have identified that all models performed less favourably in the cross-dataset evaluation, an interesting direction would be to investigate domain shifts between different datasets and find out how to mitigate them. Also, as most works focus on image-based age estimation, it would also be interesting to extend the models to videos and study how to improve the them with temporal information from videos. We will explore these ideas in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX STATISTICAL SIGNIFICANCE ANALYSIS</head><p>We conduct paired t-tests on the Absolute Error (AE) on the testing set of IMDB-Clean between FP-Age and the other eight methods, i.e. OR-CNN <ref type="bibr" target="#b12">[13]</ref>, DLDL <ref type="bibr" target="#b20">[21]</ref>, SSRNet <ref type="bibr" target="#b55">[56]</ref>, Dex <ref type="bibr" target="#b18">[19]</ref>, MV-Loss <ref type="bibr" target="#b19">[20]</ref>, DLDL-V2 <ref type="bibr" target="#b21">[22]</ref>, SVRT <ref type="bibr" target="#b14">[15]</ref> and C3AE <ref type="bibr" target="#b46">[47]</ref>. Concretely, suppose there are N images in the testing set, then FP-Age i is the AE between the predicted age of FP-Age and the groundtruth age on the i-th testing image and M i is such AE for another method M . The difference between the i-th pair is defined as</p><formula xml:id="formula_10">d i = FP-Age i ? M i . The t statistic is calculated as t = ? Nd ? d<label>(5)</label></formula><p>whered and ? d are the average and standard deviation of</p><formula xml:id="formula_11">{d i } N i=1 .</formula><p>We correct the p-values using Bonferroni correction. The alpha value is set to 0.05. <ref type="table" target="#tab_2">From Table VIII and Table XII</ref>, we observe our results are significantly better than those of the other methods. We can, thus, reject the null hypotheses.</p><p>ACKNOWLEDGMENT Data cleaning and all experiments have been conducted at Imperial College London.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Face</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>FP-Age. A pre-trained face parsing framework [4] (top) is used to extract features of the target face in the input image. A lightweight network (bottom) aggregates low-level features, high-level features and face masks to predict the age.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>RoI Tanh-polar Transform</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Age distributions of the proposed IMDB-Clean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Attention weights for facial regions induced by the face parsing attention module on IMDB-Clean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Comparison of age estimation datasets used.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell># Images</cell><cell></cell><cell># ID</cell><cell>Age</cell><cell>In-the-wild?</cell></row><row><cell>FG-Net [8]</cell><cell></cell><cell>1, 002</cell><cell></cell><cell>82</cell><cell>0-69</cell><cell>Yes</cell></row><row><cell>Morph [5]</cell><cell></cell><cell>55, 134</cell><cell cols="3">13, 618 16-77</cell><cell>No</cell></row><row><cell>CACD [6]</cell><cell></cell><cell>163, 446</cell><cell cols="2">2, 000</cell><cell>14-62</cell><cell>Yes</cell></row><row><cell>KANFace [7]</cell><cell></cell><cell>41, 036</cell><cell cols="2">1, 045</cell><cell>0-100</cell><cell>Yes</cell></row><row><cell cols="2">IMDB-Clean (ours)</cell><cell>287, 683</cell><cell cols="2">7, 046</cell><cell>0-97</cell><cell>Yes</cell></row><row><cell>6000 7000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Training Validition Testing</cell></row><row><cell>2000 3000 4000 5000 Number of samples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>20</cell><cell>40</cell><cell>Age</cell><cell>60</cell><cell>80</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Stacking images and face masks helps (evaluated on IMDB-Clean).</figDesc><table><row><cell>Method</cell><cell>MAE ?</cell><cell>CS 5 (%) ?</cell></row><row><cell>Dex [18]</cell><cell>5.34</cell><cell>58.31</cell></row><row><cell>Dex with stacked input</cell><cell>5.29</cell><cell>58.61</cell></row><row><cell>DLDL-V2 [22]</cell><cell>5.19</cell><cell>54.28</cell></row><row><cell>DLDL-V2 with stacked input</cell><cell>5.12</cell><cell>55.14</cell></row><row><cell>MV-Loss [20]</cell><cell>5.27</cell><cell>53.97</cell></row><row><cell>MV-Loss with stacked input</cell><cell>5.13</cell><cell>59.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Using Different Face Parsing Features for Age Estimation on IMDB-Clean.</figDesc><table><row><cell>Features from RTNet</cell><cell>MAE ?</cell><cell>CS 5 (%) ?</cell></row><row><cell>Low-level</cell><cell>5.01</cell><cell>60.97</cell></row><row><cell>High-level</cell><cell>5.24</cell><cell>58.30</cell></row><row><cell>Stacking Low and High</cell><cell>4.96</cell><cell>61.01</cell></row><row><cell>Stacking Low, High and Masks</cell><cell>4.90</cell><cell>61.84</cell></row><row><cell>Full Model (with Face Parsing Attention)</cell><cell>4.68</cell><cell>63.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Performance of Different Pre-trained Feature Extractors on IMDB-Clean.</figDesc><table><row><cell>Feature extractor</cell><cell>Pre-train Data</cell><cell># Params</cell><cell>MAE ?</cell><cell>CS 5 (%) ?</cell></row><row><cell>FBNet-C</cell><cell>ImageNet</cell><cell>2.9 M</cell><cell>7.45</cell><cell>42.64</cell></row><row><cell>InceptionV4</cell><cell>ImageNet</cell><cell>41.1 M</cell><cell>7.43</cell><cell>42.59</cell></row><row><cell>ResNeXt50</cell><cell>ImageNet</cell><cell>23.0 M</cell><cell>7.26</cell><cell>44.13</cell></row><row><cell>MobileNetv3-L</cell><cell>ImageNet</cell><cell>3.0 M</cell><cell>7.24</cell><cell>44.11</cell></row><row><cell>ResNeXt101</cell><cell>ImageNet</cell><cell>86.7 M</cell><cell>7.17</cell><cell>44.28</cell></row><row><cell>ResNet101</cell><cell>ImageNet</cell><cell>42.5 M</cell><cell>7.12</cell><cell>44.63</cell></row><row><cell>ResNet50</cell><cell>ImageNet</cell><cell>23.5 M</cell><cell>7.10</cell><cell>44.59</cell></row><row><cell>ArcFace [62]</cell><cell>MS1M [63]</cell><cell>30.7 M</cell><cell>5.96</cell><cell>52.19</cell></row><row><cell>Ours (w/o FPA)</cell><cell>iBugMask [4]</cell><cell>27.3 M</cell><cell>4.96</cell><cell>61.01</cell></row><row><cell>Ours (full)</cell><cell>iBugMask [4]</cell><cell>27.3 M</cell><cell>4.68</cell><cell>63.78</cell></row><row><cell cols="5">CNN attention modules, Squeeze-Excitation (SE) [46], Con-</cell></row><row><cell cols="5">volutional Block Attention Module (CBAM) [64], and Simple</cell></row><row><cell cols="5">and Parameter Free Attention Module (SimAM) [65]. To</cell></row><row><cell cols="5">ensure fair comparison, all attention modules are applied on</cell></row><row><cell cols="5">the high-level features. Other components and all other hyper-</cell></row><row><cell cols="3">parameters remain the same as FPAge.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Applying Different Attention Modules on Face Parsing Features on IMDB-Clean.</figDesc><table><row><cell>Attention</cell><cell>MAE ?</cell><cell>CS 5 (%) ?</cell></row><row><cell>Squeeze-Excitation [46]</cell><cell>4.86</cell><cell>61.47</cell></row><row><cell>CBAM [64]</cell><cell>4.83</cell><cell>62.04</cell></row><row><cell>SimAM [65]</cell><cell>4.82</cell><cell>62.03</cell></row><row><cell>Face Parsing Attention (ours)</cell><cell>4.68</cell><cell>63.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Ablation Study on IMDB-Clean.</figDesc><table><row><cell>Feature Extractor</cell><cell>Attention</cell><cell>MAE ?</cell><cell>CS 5 (%) ?</cell></row><row><cell>ResNet50</cell><cell>-</cell><cell>7.10</cell><cell>44.59</cell></row><row><cell>ResNet50</cell><cell>Squeeze-Excitation</cell><cell>7.00</cell><cell>45.50</cell></row><row><cell>Face Parsing Network</cell><cell>-</cell><cell>4.96</cell><cell>61.01</cell></row><row><cell>Face Parsing Network</cell><cell>Squeeze-Excitation</cell><cell>4.86</cell><cell>61.47</cell></row><row><cell>Face Parsing Network</cell><cell>FPA</cell><cell>4.68</cell><cell>63.78</cell></row></table><note>F. What Did FPA Learn Really?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>Effectiveness of IMDB-Clean (Testing dataset: KANFace<ref type="bibr" target="#b6">[7]</ref>).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Trained on</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">IMDB-Clean</cell><cell cols="2">IMDB-WIKI</cell><cell cols="2">CACD</cell></row><row><cell>Method</cell><cell>MAE</cell><cell>CS 5 (%)</cell><cell>MAE</cell><cell>CS 5 (%)</cell><cell>MAE</cell><cell>CS 5 (%)</cell></row><row><cell>DLDL [21]</cell><cell>9.84</cell><cell>37.37</cell><cell>12.19</cell><cell>27.20</cell><cell>11.66</cell><cell>29.20</cell></row><row><cell>DLDL-V2 [22]</cell><cell>8.05</cell><cell>41.74</cell><cell>11.46</cell><cell>28.83</cell><cell>10.88</cell><cell>30.66</cell></row><row><cell>Dex [19]</cell><cell>7.91</cell><cell>42.30</cell><cell>11.70</cell><cell>20.91</cell><cell>11.90</cell><cell>28.62</cell></row><row><cell>M-V Loss [20]</cell><cell>7.71</cell><cell>43.31</cell><cell>11.95</cell><cell>28.30</cell><cell>11.30</cell><cell>29.07</cell></row><row><cell>OR-CNN [13]</cell><cell>7.71</cell><cell>47.51</cell><cell>11.10</cell><cell>33.07</cell><cell>11.18</cell><cell>32.90</cell></row><row><cell>FP-Age (ours)</cell><cell>6.81</cell><cell>48.49</cell><cell>10.83</cell><cell>29.63</cell><cell>10.91</cell><cell>30.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII :</head><label>VIII</label><figDesc>Intra-Dataset Evaluation on IMDB-Clean.</figDesc><table><row><cell>Method</cell><cell>MAE ?</cell><cell>CS 5 (%) ?</cell><cell>Year</cell></row><row><cell>OR-CNN [13]</cell><cell>5.85</cell><cell>49.72</cell><cell>2016</cell></row><row><cell>DLDL [21]</cell><cell>6.04</cell><cell>56.94</cell><cell>2017</cell></row><row><cell>SSRNet [56]</cell><cell>7.08</cell><cell>27.87</cell><cell>2018</cell></row><row><cell>Dex [19]</cell><cell>5.34</cell><cell>58.61</cell><cell>2018</cell></row><row><cell>M-V Loss [20]</cell><cell>5.27</cell><cell>59.74</cell><cell>2018</cell></row><row><cell>DLDL-V2 [22]</cell><cell>5.19</cell><cell>54.28</cell><cell>2018</cell></row><row><cell>SVRT [15]</cell><cell>5.85</cell><cell>49.72</cell><cell>2019</cell></row><row><cell>C3AE [47]</cell><cell>6.75</cell><cell>47.98</cell><cell>2019</cell></row><row><cell>FP-Age (ours)</cell><cell>4.68  ?</cell><cell>63.78</cell><cell>-</cell></row><row><cell cols="3">Bold indicates the best and italic the second</cell><cell></cell></row><row><cell cols="4">? Our results are statistically significant according</cell></row><row><cell cols="4">to paired t-test and Bonferroni corrections (See Ap-</cell></row><row><cell>pendix A)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX :</head><label>IX</label><figDesc>Intra-Dataset Evaluation on Morph<ref type="bibr" target="#b4">[5]</ref>.Bold indicates the best and italic the second</figDesc><table><row><cell>Method</cell><cell>MAE ?</cell><cell>CS 5 (%) ?</cell><cell>Year</cell></row><row><cell>Human workers [13]</cell><cell>6.30</cell><cell>51.0</cell><cell>2015</cell></row><row><cell>OR-CNN [13]</cell><cell>3.34</cell><cell>81.5</cell><cell>2016</cell></row><row><cell>DLDL [21]</cell><cell>2.42</cell><cell>-</cell><cell>2017</cell></row><row><cell>ARN [69]</cell><cell>3.00</cell><cell>-</cell><cell>2017</cell></row><row><cell>Ranking-CNN [14]  *</cell><cell>2.96</cell><cell>85.2</cell><cell>2017</cell></row><row><cell>M-V Loss [20]</cell><cell>2.41</cell><cell>91.2</cell><cell>2018</cell></row><row><cell>DLDL-V2 [22]  ?</cell><cell>1.97</cell><cell>-</cell><cell>2018</cell></row><row><cell>BridgeNet [70]  *</cell><cell>2.38</cell><cell>-</cell><cell>2019</cell></row><row><cell>C3AE [47]  *</cell><cell>2.75</cell><cell>-</cell><cell>2019</cell></row><row><cell>AVDL [71]  *</cell><cell>1.94</cell><cell>-</cell><cell>2020</cell></row><row><cell>PML [49]</cell><cell>2.15</cell><cell>-</cell><cell>2021</cell></row><row><cell>DRF [24]</cell><cell>2.14</cell><cell>91.3</cell><cell>2021</cell></row><row><cell>FP-Age (ours)</cell><cell>2.04</cell><cell>92.8</cell><cell>-</cell></row><row><cell>FP-Age  ? (ours)</cell><cell>1.90</cell><cell>93.7</cell><cell>-</cell></row></table><note>* pre-trained on IMDB-WIKI? pre-trained on MS-Celeb-1M ? pre-trained on the proposed IMDB-Clean</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X :</head><label>X</label><figDesc>Intra-Dataset Evaluation (MAEs) on CACD<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Trained on CACD-train CACD-val</cell><cell>Year</cell></row><row><cell>Dex [19]</cell><cell>4.78</cell><cell>6.52</cell><cell>2018</cell></row><row><cell>DLDLF [23]</cell><cell>4.67</cell><cell>6.16</cell><cell>2018</cell></row><row><cell>DRF [24]</cell><cell>4.61</cell><cell>5.63</cell><cell>2021</cell></row><row><cell>FP-Age (ours)</cell><cell>4.50</cell><cell>5.62</cell><cell>-</cell></row><row><cell>FP-Age  ? (ours)</cell><cell>4.33</cell><cell>4.95</cell><cell></cell></row></table><note>- ? pre-trained on the proposed IMDB-Clean</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XI :</head><label>XI</label><figDesc>Cross-Dataset Evaluation (Training set: IMDB-Clean). inputs are pre-processed with 5-point face alignment ? inputs are pre-processed with RoI Tanh-polar Transform<ref type="bibr" target="#b3">[4]</ref> </figDesc><table><row><cell></cell><cell cols="2">FG-Net [8]</cell><cell cols="2">Morph [5]</cell><cell cols="2">KANFace [7]</cell><cell cols="2">CACD-test [6]</cell></row><row><cell>Method</cell><cell>MAE</cell><cell>CS 5 (%)</cell><cell>MAE</cell><cell>CS 5 (%)</cell><cell>MAE</cell><cell>CS 5 (%)</cell><cell>MAE</cell><cell>CS 5 (%)</cell></row><row><cell>SSRNet  *  [56]</cell><cell>12.04</cell><cell>19.86</cell><cell>7.12</cell><cell>40.77</cell><cell>11.36</cell><cell>30.11</cell><cell>11.76</cell><cell>22.01</cell></row><row><cell>C3AE  *  [47]</cell><cell>11.23</cell><cell>27.34</cell><cell>7.03</cell><cell>41.81</cell><cell>10.41</cell><cell>31.71</cell><cell>12.71</cell><cell>16.14</cell></row><row><cell>SVRT  *  [15]</cell><cell>9.77</cell><cell>23.75</cell><cell>5.87</cell><cell>43.71</cell><cell>10.89</cell><cell>27.55</cell><cell>11.73</cell><cell>14.37</cell></row><row><cell>DLDL  ? [21]</cell><cell>11.40</cell><cell>24.05</cell><cell>6.07</cell><cell>33.06</cell><cell>9.84</cell><cell>37.37</cell><cell>6.53</cell><cell>55.12</cell></row><row><cell>Coral  *  [16]</cell><cell>6.12</cell><cell>45.61</cell><cell>6.13</cell><cell>42.33</cell><cell>7.88</cell><cell>39.01</cell><cell>12.58</cell><cell>11.38</cell></row><row><cell>Dex  ? [19]</cell><cell>6.52</cell><cell>41.52</cell><cell>5.63</cell><cell>53.03</cell><cell>7.91</cell><cell>42.30</cell><cell>6.08</cell><cell>55.94</cell></row><row><cell>DLDL-V2  ? [22]</cell><cell>6.65</cell><cell>42.41</cell><cell>5.10</cell><cell>55.64</cell><cell>8.05</cell><cell>41.74</cell><cell>5.92</cell><cell>57.39</cell></row><row><cell>M-V Loss  ? [20]</cell><cell>6.49</cell><cell>42.12</cell><cell>4.99</cell><cell>56.94</cell><cell>7.71</cell><cell>43.31</cell><cell>5.88</cell><cell>57.22</cell></row><row><cell>OR-CNN  ? [13]</cell><cell>6.44</cell><cell>40.72</cell><cell>5.04</cell><cell>60.87</cell><cell>7.71</cell><cell>47.51</cell><cell>5.83</cell><cell>62.47</cell></row><row><cell>Ours  ?</cell><cell>5.60</cell><cell>48.80</cell><cell>4.67</cell><cell>60.54</cell><cell>6.81</cell><cell>48.49</cell><cell>5.60</cell><cell>60.91</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XII :</head><label>XII</label><figDesc>Paired t-Tests between FP-Age and Other Methods on IMDB-Clean. ? 10 ?115 1.23 ? 10 ?110 * indicates underflow Yiming Lin is a research scientist at Meta AI (formerly known as Facebook AI). He received his PhD degree in 2021, and his MSc degree with Distinction in Communications and Signal Processing in 2016, from Imperial College London. His research interests include face tracking, face parsing and facial attribute recognition. He is a member of the IEEE. Jie Shen is a research scientist at Meta AI and an honorary research fellow at the Department of Computing at Imperial College London. He received his B.Eng. in electronic engineering from Zhejiang University in 2005, and his MSc in advanced computing and Ph.D. from Imperial College London in 2008 and 2014. His research interests include facial analysis, computer vision, affective computing, and social robots. He is a member of the IEEE. Yujiang Wang is a postdoctoral researcher at the University of Oxford. He received his Ph.D. degree from Imperial College London in February 2021, after which he worked as a research collaborator at Meta AI until January 2022. He obtained a BSc degree in Architecture from Tsinghua University in 2010, and two MSc from University College London and Imperial College London, respectively. His research interest centres around video face parsing and clustering, word-level lip-reading, smart wearable devices, clinical AI, etc. Maja Pantic is a professor in affective and behavioural computing in the Department of Computing at Imperial College London, UK. She was the Research Director of Samsung AI Centre, Cambridge, UK from 2018 to 2020 and is currently an AI Scientific Research Lead at Meta Platforms (Facebook) London. She currently serves as an associate editor for International Journal of Computer Vision. She has received various awards for her work on automatic analysis of human behaviour including the Royal Society Roger Needham Award 2011 and IAPR Maria Petrou Award 2020. She is a fellow of the UK's Royal Academy of Engineering, the IEEE, and the IAPR.</figDesc><table><row><cell>Method</cell><cell>t-statistic</cell><cell>p-value</cell><cell>Corrected p-value</cell></row><row><cell>SSRNet [56]</cell><cell>-137.73</cell><cell>0.00  *</cell><cell>0.00  *</cell></row><row><cell>C3AE [47]</cell><cell>-84.66</cell><cell>0.00  *</cell><cell>0.00  *</cell></row><row><cell>DLDL [22]</cell><cell>-66.44</cell><cell>0.00  *</cell><cell>0.00  *</cell></row><row><cell>Dex [19]</cell><cell>-39.08</cell><cell>0.00  *</cell><cell>0.00  *</cell></row><row><cell>OR-CNN [13]</cell><cell>-33.83</cell><cell>2.08 ? 10 ?248</cell><cell>1.17 ? 10 ?243</cell></row><row><cell>DLDL-V2 [22]</cell><cell>-31.83</cell><cell>2.24 ? 10 ?220</cell><cell>1.26 ? 10 ?220</cell></row><row><cell>M-V Loss [20]</cell><cell>-28.03</cell><cell>1.21 ? 10 ?171</cell><cell>6.80 ? 10 ?167</cell></row><row><cell>SVRT [15]</cell><cell>-22.89</cell><cell>2.01</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/shamangary/SSR-Net/blob/master/data/TYY MORPH create db.py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chalearn looking at people and faces of the world: Face analysis workshop and challenge 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Torres</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Corneou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Ali</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Demographic Estimation from Face Images: Human vs. Machine Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1148" to="1161" />
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RoI Tanh-polar Transformer Network for Face Parsing in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">104190</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MORPH: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Automatic Face and Gesture Recognition (FGR06)</title>
		<imprint>
			<date type="published" when="2006-04" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face Recognition and Retrieval Using Cross-Age Reference Coding With Cross-Age Celebrity Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="804" to="815" />
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Investigating bias in deep face analysis: The KANFace dataset and empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">103954</biblScope>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward automatic simulation of aging effects on face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of research on facial ageing using the FG-NET ageing database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsapatsoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Age estimation by multi-scale convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeply-Learned Feature for Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2015-01" />
			<biblScope unit="page" from="534" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ordinal Regression with Multiple Output CNN for Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="4920" to="4928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using Ranking-CNN for Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scale-Varying Triplet Ranking with Classification Loss for Facial Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2018, ser. Lecture Notes in Computer Science</title>
		<editor>C. Jawahar, H. Li, G. Mori, and K. Schindler</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rank consistent ordinal regression for neural networks with application to age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="325" to="331" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Label Distribution Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1734" to="1748" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DEX: Deep EXpectation of Apparent Age from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="252" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Expectation of Real and Apparent Age from a Single Image Without Facial Landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mean-Variance Loss for Deep Age Estimation from a Face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="5285" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Label Distribution Learning With Label Ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Age Estimation Using Expectation of Label Distribution Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="712" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep Regression Forests for Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Differentiable Random Forests for Age Estimation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="404" to="419" />
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distribution Cognisant Loss for Cross-Database Facial Age Estimation with Sensitivity Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An All-In-One Convolutional Neural Network for Face Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heterogeneous Face Attribute Estimation: A Deep Multi-Task Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Multi-Task Learning for Joint Prediction of Heterogeneous Face Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="173" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Labelfaces: Parsing facial features by multiclass labeling with an epitome prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J D</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2481" to="2484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exemplar-based face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-objective convolutional learning for face labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3451" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2480" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A cnn cascade for landmark guided semantic part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="143" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>G??l?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>G??l?t?rk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Lier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Van Gerven</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03305</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shape constrained network for eye segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1952" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face parsing via recurrent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Sifei Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards Learning Structure via Consensus for Face Segmentation and Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mathai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Edge-aware graph representation learning and reasoning for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="258" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face mask extraction in video sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamic face video segmentation via reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face parsing with roi tanh-warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">C3AE: Exploring the Limits of Compact Model for Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ordinal Deep Feature Learning for Facial Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PML: Progressive Margin Loss for Long-tailed Age Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Apparent Age Estimation from Face Images Combining General and Children-Specialized Deep Learning Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Age Group and Gender Estimation in the Wild With Deep RoR Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="22" to="492" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">S 3 FD: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">FAN-Face: A simple orthogonal improvement to deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">DBSCAN revisited, revisited: Why and how you should (still) use DBSCAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SSR-Net: A compact soft stagewise regression network for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Hsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/150</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/150" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1078" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multi-level face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/ms-celeb-1m-dataset-benchmark-large-scale-face-recognition-2/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Simam: A simple, parameter-free attention module for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, M. Meila and T. Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="11" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<ptr target="http://proceedings.mlr.press/v139/yang21o.html" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Age estimation from facial parts using compact multi-stream convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Angeloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Freitas Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pedrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Local deep neural networks for age and gender classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08497</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Attended Endto-End Architecture for Age Estimation From Facial Expression Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dibeklioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1972" to="1984" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Anchored Regression Networks Applied to Age Estimation and Super Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="1652" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">BridgeNet: A Continuity-Aware Probabilistic Network for Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="1145" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Adaptive Variance Based Label Distribution Learning for Facial Age Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12368</biblScope>
			<biblScope unit="page" from="379" to="395" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
