<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
							<email>wanghaiyang@stu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Peng Cheng Laboratory 7 Pazhou Laboratory (Huangpu)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaocong</forename><surname>Dong</surname></persName>
							<email>shaocong@bit.edu.cnsshi@mpi-inf.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Beijing institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
							<email>liaoxue2@huawei.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
							<email>lijianan15@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Beijing institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<email>li.zhenguo@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@cls.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">MOE</orgName>
								<orgName type="department" key="dep2">School of Intelligence Science and Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel two-stage fully sparse convolutional 3D object detection framework, named CAGroup3D. Our proposed method first generates some high-quality 3D proposals by leveraging the class-aware local group strategy on the object surface voxels with the same semantic predictions, which considers semantic consistency and diverse locality abandoned in previous bottom-up approaches. Then, to recover the features of missed voxels due to incorrect voxel-wise segmentation, we build a fully sparse convolutional RoI pooling module to directly aggregate fine-grained spatial information from backbone for further proposal refinement. It is memory-and-computation efficient and can better encode the geometry-specific features of each 3D proposal. Our model achieves state-of-theart 3D detection performance with remarkable gains of +3.6% on ScanNet V2 and +2.6% on SUN RGB-D in term of mAP@0.25. Code will be available at https://github.com/Haiyang-W/CAGroup3D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a crucial step towards understanding 3D visual world, 3D object detection aims to estimate the oriented 3D bounding boxes and semantic labels of objects in real 3D scenes. It has been studied for a long time in both academia and industry since it benefits various downstream applications, such as autonomous driving <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref>, robotics <ref type="bibr">[54,</ref><ref type="bibr" target="#b36">37]</ref> and augmented reality <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. In this paper, we focus on detecting 3D objects from unordered, sparse and irregular point clouds. Those natural characteristics make it more challenging to directly extend well-studied 2D techniques to 3D detection.</p><p>Unlike 3D object detection from autonomous driving scenarios that only considers bird's eye view (BEV) boxes <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31]</ref>, most of existing 3D indoor object detectors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b50">51]</ref> typically handle this task through a bottom-up scheme, which extracts the point-wise features from input point clouds, and then groups the points into their respective instances to generate a set of proposals. However, the above grouping algorithms are usually carried out in a class-agnostic manner, which abandons semantic consistency within the same group and also ignores diverse locality among different categories. For example, VoteNet <ref type="bibr" target="#b26">[27]</ref> learns the point-wise center offsets and aggregates the points that vote to similar semantic-irrelevant local region. Though impressive, as shown in <ref type="figure" target="#fig_1">Figure  1</ref>, these methods may fail in cluttered indoor scenes where various objects are close but belong to different categories. Also, the object sizes are diverse for different categories, so that a class-agnostic seed point <ref type="table">(table)</ref> seed point <ref type="bibr">(chair)</ref> seed point (sofa) ball query vote point <ref type="table">(table)</ref> vote point (chair) vote point (sofa) (a) mis-grouping of different categories (b) partial or over coverage <ref type="figure" target="#fig_1">Figure 1</ref>: Class-agnostic grouping methods suffer from (a) mis-grouping of different categories within the same local regions, (b) partial coverage of the object surfaces; outliers from the cluttered scene.</p><p>local grouping may partially cover the boundary points of large objects and involve more noise outliers for small objects.</p><p>Hence, we propose CAGroup3D, a two-stage fully convolutional 3D object detection framework. Our method consists of two novel components. One is the class-aware 3D proposal generation module, which aims to generate reliable proposals by utilizing class-specific local group strategy on the object surface voxels with same semantic predictions. The other one is an efficient fully sparse convolutional RoI pooling module for recovering the features of the missed surface voxels due to semantic segmentation errors, so as to improve the quality of predicted boxes.</p><p>Specifically, a backbone network with 3D sparse convolution is firstly utilized to extract descriptive voxel-wise features from raw point clouds. Based on the learned features, we conduct a class-aware local grouping module to cluster surface voxels into their corresponding instance centroids. Different from <ref type="bibr" target="#b26">[27]</ref>, in order to consider the semantic consistency, we not only shift voxels of the same instance towards the same centroid but also predict per-voxel semantic scores. Given the contiguously distributed vote points with their semantic predictions, we initially voxelize them according to the predicted semantic categories and vote coordinates, so as to generate class-specific 3D voxels for different categories. The voxel size of each category is adaptive to its average spatial dimension.</p><p>To maintain the structure of fully convolution, we apply sparse convolution as grouping operation centered on each voted voxel to aggregate adjacent voxel features in the same semantic space. Note that these grouping layers are class-dependent but share the same kernel size, thus the larger classes are preferred to be aggregated with larger local regions.</p><p>Secondly, given the proposal candidates, fine-grained specific features within 3D proposals need to be revisited from 3D backbone through certain pooling operation for the following box refinement. However, state-of-the-art pooling strategies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b8">9]</ref> are memory-and-computation intensive due to the hand-crafted set abstraction <ref type="bibr" target="#b24">[25]</ref>. Besides that, its max-pooling operation also harms the geometry distribution. To tackle this problem, we propose RoI-Conv pooling module, which directly adopts the well-optimized 3D sparse convolutions to aggregate voxel features from backbone. It can encode effective geometric representations with a memory-efficient design for further proposal refinement.</p><p>In summary, our contributions are three-fold: 1) We propose a novel class-aware 3D proposal generation strategy, which considers both the voxel-wise semantic consistency within the same local group and the object-level shape diversity among different categories. 2) We present RoI-Conv pooling module, an efficient fully convolutional 3D pooling operation for revisiting voxel features directly from backbone to refine 3D proposals. 3) Our approach outperforms state-of-the-art methods with remarkable gains on two challenging indoor datasets, i.e., ScanNet V2 <ref type="bibr" target="#b6">[7]</ref> and SUN RGB-D <ref type="bibr" target="#b32">[33]</ref>, demonstrating its effectiveness and generality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>3D Object Detection on Point Clouds. Detecting 3D objects from point clouds is challenging due to orderless, sparse and irregular characteristics. Previous approaches can be coarsely classified into two lines in terms of point representations, i.e., the voxel-based methods [53, <ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b31">32]</ref> and the point-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b45">46]</ref>. Voxel-based methods are mainly applied in outdoor autonomous driving scenarios where objects are distributed on the large-scale 2D ground plane. They process the sparse point clouds by efficient 3D sparse convolution, then project these 3D volumes to 2D grids for detecting bird's eye view (BEV) bboxes by 2D ConvNet. Powered by PointNet series <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>, point-based methods are also widely used to predict 3D bounding bboxes.</p><p>Most of existing methods are in a bottom-up manner, which extracts the point-wise features and groups them to obtain object features. This pipeline has been a great success for estimating 3D bboxes directly from cluttered and dense 3D scenes. However, due to the hand-crafted point sampling and computation intensive grouping scheme applied in PointNet++ <ref type="bibr" target="#b28">[29]</ref>, they are difficult to be extended to large-scale point clouds. Hence, we propose an efficient fully convolutional bottom-up framework to efficiently detect 3D bboxes directly from dense 3D point clouds.</p><p>Feature Grouping. Feature grouping is a crucial step for bottom-up 3D object detectors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b34">35]</ref>, which clusters a group of point-wise features to generate high-quality 3D bounding boxes. Among the numerous successors, voting-based framework <ref type="bibr" target="#b26">[27]</ref> is widely used, which groups the points that vote to the same local region. Though impressive, it doesn't consider the semantic consistency, so that may fail in cluttered indoor scenes where the objects of different classes are distributed closely. Moreover, voting-based methods usually adopt a class-agonistic local region for all objects, which may incorrectly group the boundary points of large objects and involve more noise points for small objects. To address the above limitations, we present a class-aware local grouping strategy to aggregate the points of the same category with class-specific center regions.</p><p>Two-stage 3D Object Detection. Many state-of-the-art methods considered applying RCNN style 2D detectors to the 3D scenes, which apply 3D RoI-pooling scheme or its variants <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b42">43]</ref> to aggregate the specific features within 3D proposals for the box refinement in a second stage. These pooling algorithms are usually equipped with set abstraction <ref type="bibr" target="#b24">[25]</ref> to encode local spatial features, which consists of a hand-crafted query operation (e.g., ball query <ref type="bibr" target="#b24">[25]</ref> or vector query <ref type="bibr" target="#b8">[9]</ref>) to capture the local points and a max-pooling operation to group the assigned features. Therefore these RoI pooling modules are mostly computation expensive. Moreover, the max-pooling operation also harms the spatial distribution information. To tackle these problems, we propose RoI-Conv pooling, a memory-and-computation efficient fully convolutional RoI pooling operation to aggregate the specific features for the following refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this paper, we propose CAGroup3D, a two-stage fully convolutional 3D object detection framework for estimating accurate 3D bounding boxes from point clouds. The overall architecture of CAGroup3D is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. Our framework consists of three major components: an efficient 3D voxel CNN with sparse convolution as the backbone network for point cloud feature learning ( ?3.1), a class-aware 3D proposal generation module for predicting high quality 3D proposals by aggregating voxel features of the same category within the class-specific local regions ( ?3.2) and RoI-Conv pooling module for directly extracting complete and fine-grained voxel features from the backbone to revisit the miss-segmented surface voxels and refine 3D proposals. Finally, we formulate the learning objective of our framework in ?A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">3D Voxel CNN for Point Cloud Feature Learning</head><p>For generating accurate 3D proposals, we first need to learn discriminative geometric representation for describing input point clouds. Voxel CNN with 3D sparse convolution <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr">53,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref> is widely used by state-of-the-art 3D detectors thanks to its high efficiency and scalability of converting the point clouds to regular 3D volumes. In this paper, we adopt sparse convolution based backbone for feature encoding and 3D proposal generation.</p><p>3D backbone network equipped with high-resolution feature maps and large receptive fields is critical for accurate 3D bounding box estimation and voxel-wise semantic segmentation. The latter is closely related to the accuracy of succeeding grouping module. To maintain these two characteristics, inspired by the success of HRNet series <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34]</ref> in segmentation community, we implement a 3D voxel bilateral network with dual resolution based on ResNet <ref type="bibr" target="#b14">[15]</ref>. For brevity, we refer it as BiResNet. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, our backbone network contains two branches. One is the sparse modification of  ResNet18 <ref type="bibr" target="#b14">[15]</ref> where all 2D convolutions are replaced with 3D sparse convolutions. It can extract multi-scale contextual information with proper downsampling modules. The other one is a auxiliary branch that maintains a high-resolution feature map whose resolution is 1/2 of the input 3D voxels. Specifically, the auxiliary branch is inserted following the first stage of ResNet backbone and doesn't contain any downsampling operation. Similar to <ref type="bibr" target="#b38">[39]</ref>, we adopt the bridge operation between the two paths to perform the bilateral feature fusion. Finally, the fine-grained voxel-wise geometric features with rich contextual information are generated by the high-resolution branch and facilitate the following module. Experiments also demonstrate that our voxel backbone performs better than previous FPN-based ResNet <ref type="bibr" target="#b19">[20]</ref>. More architecture details are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Class-Aware 3D Proposal Generation</head><p>Given the voxel-wise geometric features generated by the backbone network, a bottom-up grouping algorithm is generally adopted to aggregate object surface voxels into their respective ground truth instances and generate reliable 3D proposals. Voting-based grouping method <ref type="bibr" target="#b26">[27]</ref> has shown great success for 3D object detection, which is performed in a class-agnostic manner. It reformulates Hough voting to learn point-wise center offsets, and then generates object candidates by clustering the points that vote to similar center regions. However, this method may incorrectly group the outliers in the cluttered indoor scenarios (e.g., votes are close together but belong to different categories), which degrades the performance of 3D object detection. Moreover, due to the diverse object sizes of different categories, class-agnostic local regions may mis-group the boundary points of large objects and involve more noise points for small objects.</p><p>To address this limitation, we propose the class-aware 3D proposal generation module, which first produces voxel-wise predictions (e.g., semantic maps and geometric shifts), and then clusters the object surface voxels of the same semantic predictions with class-specific local groups.</p><p>Voxel-wise Semantic and Vote Prediction. After obtaining the voxel features from backbone network, two branches are constructed to output the voxel-wise semantic scores and center offset vectors. Specifically, the backbone network generates a number of N non-empty voxels</p><formula xml:id="formula_0">{o i } N i=1 from backbone, where o i = [x i ; f i ] with x i ? R 3 and f i ? R C .</formula><p>A voting branch encodes the voxel feature f i to learn the spatial center offset ?x i ? R 3 and feature offset ?f i ? R C . Based on the learned spatial and feature offset, we shift voxel o i to the center of its respective instance and generate vote point p i as follow:</p><formula xml:id="formula_1">{p i | p i = [x i + ?x i , f i + ?f i ]} N i=1 .</formula><p>(1) The predicted offset ?x i is explicitly supervised by a smooth-1 loss with the ground-truth displacement from the coordinate of seed voxel x i to its corresponding bounding box center.</p><p>In parallel with the voting branch, we also construct a semantic branch to output semantic scores S = {s i } N i=1 for all the voxels over N class classes as</p><formula xml:id="formula_2">s i = MLP sem (o i ) ? [0, 1] N class , for i = 1, ? ? ?, N,<label>(2)</label></formula><p>where MLP sem (?) is a one-layer multi-layer-perceptron (MLP) network and s i indicates the semantic probability for all classes of voxel o i . We adopt focal loss <ref type="bibr" target="#b20">[21]</ref> for calculating voxel segmentation loss to handle the class imbalance issue.</p><p>Notably, the vote and semantic targets of each voxel are associated with the ground-truth 3D boxes not the instance or semantic masks, so that it can be easily generalized to the 3D object detection datasets with bounding box annotations. To be specific, for each voxel, only the ground-truth bounding boxes that includes this voxel are selected. Considering the ambiguous cases that a voxel is in multiple ground truth bounding boxes, only the box with the least volume is assigned to this voxel.</p><p>Class-Aware Local Grouping. This step aims to produce reliable 3D proposals in a bottom-up scheme based on the above voxel-wise semantic scores</p><formula xml:id="formula_3">{s i } N i=1 and vote predictions {p i } N i=1 .</formula><p>To carry out grouping with semantic predictions, we first define a score threshold ? for all categories to individually determine whether a voxel belongs to a category instead of utilizing the one-hot semantic predictions. It can allow the voxel to be associated with multiple classes and thus improve the recall of semantic voxels for each category. Given the semantic scores S = {s i } N i=1 , we iterate over all the classes, and slice a point subset from the whole scene of each class that has the score higher than the threshold ? , to form the class-dependent vote set {c j } N class j=1 ,</p><formula xml:id="formula_4">c j | c j = {p i : s j i &gt; ?, i = 1, ..., N } N class j=1 .<label>(3)</label></formula><p>The above semantic subset is generated in a contiguous euclidean space and the vote points are distributed irregularly. To maintain the structure of pure convolutions and facilitate the succeeding class-aware convolution-based local grouping module, we individually voxelize the vote points in each semantic subset to {V j } N class j=1 by employing a voxel feature encoding (VFE) layer with a class-specific 3D voxel size, which is proportional to the class average spatial dimension. Specifically, this re-voxelization process for each class can be formulated as follows:</p><formula xml:id="formula_5">{v i } |Vj | i=1 = VFE(c j , ? ? d j , Avg), for j = 1, ..., N class ,<label>(4)</label></formula><p>where c j is the class-dependent vote set of class index j and | V j | is the number of non-empty voxels after class individual re-voxelization. ? ? d j is the class-specific voxel size, where ? is a predefined scale factor and d j = (w j , h j , l j ) is the category average spatial dimension. VFE( ? , ? ? d j , Avg) means that the average pooling operation is adopted to voxelize vote features on j-th class subset with the voxel size ? ? d j . Importantly, the voxel size is adaptive among different categories, which is more diverse than the widely used FPN-based prediction structure.</p><p>Given the predicted vote voxels of j-th class</p><formula xml:id="formula_6">{v i } |Vj | i=1</formula><p>, we apply sparse convolutions with a predefined kernel size k (a) on each voxel, and automatically aggregate local context inside the class to generate class-specific geometric features A (j) as follow:</p><formula xml:id="formula_7">A (j) = {a (j) i | a (j) i = SparseConv (j) 3D (v i , {v i } |Vj | i=1 , k (a) )} |Vj | i=1 ,<label>(5)</label></formula><p>where SparseConv</p><formula xml:id="formula_8">(j)</formula><p>3D (? center , ? support voxels , ? kernel size ) is the standard sparse 3D convolution <ref type="bibr" target="#b12">[13]</ref> and specific for different classes. A shared anchor-free head is appended to the aggregated features for predicting classification probabilities, bounding box regression parameters and confidence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RoI-Conv point cloud feature pooling for 3D Proposal Refinement</head><p>Due to the semantic segmentation errors in stage-I, class-aware local grouping module will mis-group some object surface voxels. So an efficient pooling module is needed to recover the missed voxel features and also aggregate more fine-grained features from backbone for proposal refinement. Stateof-the-art 3D RoI pooling strategies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b8">9]</ref> usually adopt set abstraction operation to encode local patterns, which is computation expensive compared to traditional convolution and hand-crafted with lots of hyper-parameters (e.g., radius, the number of neighbors). Moreover, its max-pooling operation also harms the spatial distribution information.</p><p>To tackle these limitations and hold a fully convolution structure, we propose RoI-Conv pooling operation, which builds a hierarchical grouping module with well optimized 3D sparse convolutions to directly aggregate RoI-specific features from backbone for further proposal refinement. Our hierarchical structure is composed by a number of sparse abstraction block, which contains two key components: the RoI-guidence sampling for selecting a subset of input voxels within the 3D proposals, which defines the centroids of local regions, and a shared sparse convolution layer for encoding local patterns into feature vectors.</p><p>Sparse Abstraction. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the inputs to this block are a number of | I | input voxels</p><formula xml:id="formula_9">I = {l n } I n=1 and | M | proposals M = {? m } M m=1 ,</formula><p>where ? m is the proposal parameters for guiding the point sampling. The output is a number of | Q | pooling RoI-specific voxels Q = {q k } |Q| k=1 . Specifically, given the input voxels and proposals, instead of directly sampling from the whole input space, we adopt the RoI-guidence sampling to uniformly sample G</p><formula xml:id="formula_10">x ? G y ? G z grid points within each 3D proposal in voxel space, which are denoted as G = {g k ? Z 3 } Gx?Gy?Gz?M k=1</formula><p>. G x ?G y ?G z is the proposal sampling resolution, which are the hyper-parameters independent on proposal sizes. Considering the overlap of different proposals, we merge the repeated grid points and generate a unique points set G = {g k } | G| k=1 , where | G | is the number of unique grid points. Then, with the RoI-specific points set G, we exploit a sparse convolution centered on each sampled point to cover a set of neighboring input voxels (e.g.,</p><formula xml:id="formula_11">N k = {l 1 k , l 2 k , ..., l L k k } for g k ) within the kernel size k (p) as: Q = q k | q k = SparseConv 3D (g k , N k , k (p) ), if L k &gt; 0, ?, if L k = 0, for k = 1, ..., | G | (6)</formula><p>where L k is the number of neighboring voxels queried by the k-th RoI-specific point with kernel size k (p) . SparseConv 3D (? center , ? support voxels , ? kernel size ) is a shared sparse 3D convolution for all the proposals and only applied on the points that their neighboring voxel sets are non-empty. ? means empty voxel. Then, to hold the surface geometry and reduce computation cost, we abandon the empty voxels and output the RoI-specific voxels set Q.</p><p>RoI-Conv Pooling Module. Our pooling network is equipped with two-layers sparse abstraction block and progressively abstracts the voxel features within each 3D proposal from the backbone to RoI-specific features iteratively as:</p><formula xml:id="formula_12">Q 1 = SparseAbs(I, {? m } M m=1 , 7 ? 7 ? 7, 5), F = SparseAbs(Q 1 , {? m } M m=1 , 1 ? 1 ? 1, 7),<label>(7)</label></formula><p>where SparseAbs(? input voxels , ? proposals , ? sampling resolution , ? kernel size ) denotes our sparse abstraction block. Notably, to encode the voxel features of oriented proposals, we follow the transformation strategy in <ref type="bibr" target="#b31">[32]</ref> before the last block, and normalize the input voxels belonging to each proposal to its individual canonical systems. With the RoI feature F ? R C of each proposal, the refinement network predicts the size and location (i.e., bbox dimension, center and orientation) residuals relative to the proposal in Stage-I and the targets are encoded by the traditional residual-based method <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning Objective</head><p>Our proposed approach is trained from scratch with semantic loss L sem , voting loss L vote-reg , centerness loss L cntr , bounding box estimation loss L box , classification losses L cls for Stage-I and bbox refinement loss L rebox for Stage-II, which are formulated as follows:</p><formula xml:id="formula_13">L = ? sem L sem + ? vote L vote + ? cntr L cntr +? box L box + ? cls L cls + ? rebox L rebox .<label>(8)</label></formula><p>L sem-cls is a multi-class focal loss <ref type="bibr" target="#b20">[21]</ref> used to supervise voxel-wise semantic segmentation. L vote is a smooth-1 loss for predicting the center offset of each voxel. In term of the 3D proposal generation module, we follow the same loss functions L cntr , L box and L cls defined in <ref type="bibr" target="#b7">[8]</ref> to optimize object centerness, bounding box estimation and classification respectively. For the second stage, L rebox is the residual-based smooth-1 box regression loss for 3D box proposal refinement, which contains size, box center and angle refinement loss. Besides that, we also add the same IoU loss L iou as used in stage-I, and the final box refinement loss is as follows</p><formula xml:id="formula_14">L rebox = r?{x,y,z,l,h,w,?} L smooth-1 (?r * , ?r) + L iou ,<label>(9)</label></formula><p>where ?r is the predicted residual and ?r * is the corresponding ground truth. The detailed balancing factors are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metric</head><p>Our CAGroup3D is evaluated on two indoor challenging 3D scene datasets, i.e., ScanNet V2 <ref type="bibr" target="#b6">[7]</ref> and SUN RGB-D <ref type="bibr" target="#b32">[33]</ref>. For all datasets, we follow the standard data splits adopted in <ref type="bibr" target="#b26">[27]</ref>.</p><p>ScanNet V2 contains richly-annotated 3D reconstructed indoor scenes with axis-aligned bounding box for most common 18 object categories. It contains 1201 training samples and the remaining 312 scans are left for validation. We follow <ref type="bibr" target="#b26">[27]</ref> to sample point clouds from the reconstructed meshes.</p><p>SUN-RGB-D is a single-view indoor dataset which consists of 10,355 RGB-D images for 3D scene understanding. It contains ?5K training images annotated with the oriented 3D bounding boxes and the semantic labels for 10 categories. To feed the point data to our method, we follow <ref type="bibr" target="#b26">[27]</ref> and convert the depth images to point clouds using the provided camera parameters.</p><p>All the experiment results on both datasets are evaluated by a standard evaluation protocol <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8]</ref>, which uses mean average precision(mAP) with different IoU thresholds, i.e., 0.25 and 0.50.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Network Architecture Details. For both datasets, we set the voxel size as 0.02m. As for the backbone, we use the same 3D voxel ResNet18 introduced in <ref type="bibr" target="#b13">[14]</ref> as the downsample branch to extract rich contextual information and set the voxel size of high-resolution branch to 0.04m. In terms of class-aware 3D proposal generation module, we set the scale factor of re-voxelization ? to 0.15. The semantic threshold ? is initially set to 0.15 and then decreases by 0.02 every 10 epochs for ScanNet V2 and 4 epochs for SUN RGB-D until it reaches the minimum value of 0.05. Moreover, the kernel size of convolution-based grouping module k (a) is 9. We stack two sparse abstraction blocks for extracting RoI-specific representations, in where the proposal sampling resolutions are set to 7 ? 7 ? 7 and 1 ? 1 ? 1, and the kernel sizes of sparse grouping k (p) are set to 5 and 7 respectively.</p><p>Training and Evaluation Scheme. Our model is trained in an end-to-end manner by AdamW optimizer <ref type="bibr" target="#b22">[23]</ref>. Following the <ref type="bibr" target="#b7">[8]</ref>, we set batch size, initial learning rate and weight decay are 16, 0.001 and 0.0001 for both datasets. Training ScanNet V2 requires 120 epochs with the learning rate decay by 10x on 80 epochs and 110 epochs. SUN RGB-D takes 48 epochs and learning rate decayed on 32, 44 epochs. All models are trained on two NVIDIA Tesla V100 GPUs with a 32 GB memory per-card. The gradnorm clip <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b17">18]</ref> is applied to stabilize the training dynamics. We follow the same evaluation scheme in <ref type="bibr" target="#b21">[22]</ref>, which runs training for 5 times and test each trained model for 5 times. We report both the best and average metrics across all results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmarking Results</head><p>We report the comparison results with state-of-the-art 3D detection methods on ScanNet V2 <ref type="bibr" target="#b6">[7]</ref>] and SUN RGB-D <ref type="bibr" target="#b32">[33]</ref> benchmark. Same as <ref type="bibr" target="#b21">[22]</ref>, we report the best and average results of 5?5 trials. <ref type="table" target="#tab_1">Table 1</ref>, our approach leads to 75.1 in terms of mAP@0.25 and 61.3 in terms of mAP@0.50 on ScanNet V2 <ref type="bibr" target="#b6">[7]</ref>, which is +3.6 and +4.0 better than the state-of-the-art <ref type="bibr" target="#b7">[8]</ref>. For SUN RGB-D, our approaches achieves 66.8 and 50.2, which gains +2.6 and +1.3 in terms of mAP, with 3D IoU threshold 0.25 and 0.5 respectively. Moreover, our approach outperforms all the multi-sensor based methods, which is more challenging for SUN RGB-D due to its relatively poor point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies and Discussions</head><p>We conduct extensive ablation studys on the val sets of ScanNet V2 to analyze individual components of our proposed method. Following <ref type="bibr" target="#b21">[22]</ref>, we report the average performance of 25 trials by default.  Effect of class-aware local grouping module. We first ablate the effects of class-aware local grouping module in <ref type="table" target="#tab_2">Table 2</ref>, 3, 4. In <ref type="table" target="#tab_2">Table 2</ref>, the base competitor (1 st row) is the fully sparse convolutional VoteNet <ref type="bibr" target="#b26">[27]</ref> we implemented. Compared with CAGroup3D, it abandons the two stage refinement, replaces the BiResNet with FPN-based variant and groups the vote voxels with the voxel size of 0.02 in a class-agnostic manner. As evidenced in the 1 st and the 2 nd rows, by considering the semantic consistency, our model performs better, i.e., 68.22 ? 69.24, 53.17 ? 54.05 on mAP@0.25 and mAP@0.5. Combining the semantic predictions with class-specific local groups (3 rd rows), our model achieves a significant improvement, i.e., 68.22 ? 72.10, 53.17 ? 57.07. That verifies our motivation that semantic consistency within the same group and diverse locality among different categories are crucial for an effective grouping algorithm.</p><p>Our class-aware local grouping module also works well for a range of hyper-parameters, such as the scale factor ? of class-specific re-voxelization and the semantic threshold ? . <ref type="table" target="#tab_3">Table 3</ref> shows the performance of our model with different scale factors. We first set k (a) to 9 and reduce the scale factor ? gradually. With the decrease of ?, the output voxels are more fine-grained, which is beneficial to estimate accurate bounding boxes and localize small objects. However, a very small voxel size will degrade the performance. With the fixed k (a) , smaller voxel sizes will lead to smaller local regions, which may mis-group the boundary object points, and thus it is hard for the network to accurately capture local object geometry. <ref type="table" target="#tab_4">Table 4</ref> also ablates the effectiveness of different semantic threshold. We observe that the performance gradually improves with the decrease of ? . However, too small threshold can also drops the performance due to the abandonment of semantic consistency.</p><p>Effect of RoI-Conv pooling module. <ref type="table" target="#tab_2">Table 2</ref> demonstrates the effectiveness of two-stage refinement with RoI-Conv pooling module. By comparing the 4 rd and 5 th rows, we find that our refinement module can really help to detect more accurate 3D bounding boxes, especially in term of mAP@0.50, i.e., 57.18 ? 60.31. To further ablate the high performance of our RoI-Conv pooling module, we compare it to several pooling strategies <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref> widely used in 3D object detection. For a fair comparison, we only switch the RoI pooling algorithm while all other settings remain unchanged, (e.g., class-aware local grouping and BiResNet). <ref type="table" target="#tab_1">Table 14</ref> shows that our approach surpasses others on the performance of both detection scores and computation cost with a remarkable margin. Move details about the above competitors are in Appendix. <ref type="table" target="#tab_6">Table 6</ref> shows the impact of stacking different number of sparse abstraction blocks with k (p) = 5 except the last layer. We choose the relative shallow design of two layers as there are no obvious improvement by additional deepening. We further ablate the influence of proposal sampling resolution and sparse kernel size k (p) based on our two layers architecture. <ref type="table" target="#tab_7">Table 7</ref> and 8 show that both the larger G and k (p) can capture more fine-grained geometric details and lead to better performance. Considering the trade-off between memory usage and performance improvement, our model finally sets G and k (p) of the 1 st layer to 7 ? 7 ? 7 and 5.</p><p>Effect of bilateral feature learning. In <ref type="table" target="#tab_2">Table 2</ref>, we investigate the effects of bilateral backbone (BiResNet18) by replacing it with FPN-based ResNet18 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. The 3 th and 4 th rows exhibit that the performance drops, especially in term of mAP@0.25, from 73.21 ? 72.10, when adopts FPN-based backbone. This phenomenon validates that our bilateral backbone could learn much richer contextual information while maintain the high-resolution representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose CAGroup3D, a two-stage fully convolutional 3D object detector, which generates some 3D proposals by utilizing the class-aware local grouping module on the object voxels with same semantic predictions. Then, to efficiently recover the features of the missed voxels due to incorrect semantic segmentation, we design a fully sparse convolutional RoI pooling module, which  is memory-and-computation efficient and could better encode spatial information than previous maxpooling based RoI methods. Equipped with the above designs, our model achieves state-of-the-art on ScanNet V2 and SUN RGB-D benchmarks with remarkable performance gains.</p><p>Limitations. CAGroup3D mainly focuses on the inter-category locality, which is class-specific and diverse among the different classes, but ignores the intra-category discriminations. Due to the incompleteness of the point cloud and the scale variance within the classes, the object spatial dimension of the same class is also variational, which leads to the diverse intra-category locality. Although our grouping algorithm can implicitly handle this problem in some degree by the learnable convlutional aggregation module, it is still an open problem and will be studied in the future.</p><p>[53] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection. In CVPR, 2018.</p><p>[54] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017.</p><p>In the supplementary material, we first provide more implementation details of the network architecture ( ?A), then present the per-category evaluation ( ?B.1), latency and runtime memory analysis ( ?B.2), more ablation studies ( ?B.3) and visualization of quantitative results ( ?B.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>As mentioned in the main paper, the CAGroup3D architecture consists of a backbone with dual resolution named BiResNet, a class-aware 3D proposal generation module and a RoI-Conv refinement module. We first detail the backbone and proposal generation module, then elaborate the RoI-Conv refinement module as well as its competitors, and finally present the details of our loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 BiResNet Backbone</head><p>Our backbone network is built upon MinkowskiEngine <ref type="bibr" target="#b5">[6]</ref>, an auto-differentiation library for sparse tensors. In all experiments, we voxelize the original point clouds into sparse tensors with a voxel size of 0.02m and feed them into the backbone network. BiResNet contains two branches, one is the sparse modification of ResNet18 <ref type="bibr" target="#b14">[15]</ref> to extract pyramid contextual features with proper downsampling modules, the other one is an auxiliary branch to hold a high-resolution feature map whose resolution is 1/2 of the input 3D voxels. To achieve information interaction between the two streams, we construct a bilateral fusion block, which includes fusing the high-resolution branch into the low-resolution (high-to-low) and low-resolution into high-resolution (low-to-high). As for high-to-low fusion, high-resolution features are downsampled by a sparse convolution block with a specifical stride (e.g., 2 and 4 for different stages) before being added to the low-resolution feature map. Meanwhile, an interpolation operation and another channel-compression convolution are used to upsample the low-resolution feature map before being fused with the high-resolution auxiliary branch. All convolution layers are followed by batch or instance normalization and ReLU activation function. The output of the backbone network are 64-dimensional voxel-wise latent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Class-aware 3D Proposal Generation Module</head><p>The class-aware 3D proposal generation module consists of a semantic and vote prediction module, a class-aware local grouping module and an anchor-free proposal head. The detailed computation procedure is provided in Algorithm 0. The proposal head comprises three parallel sparse convolutional layers with weights shared across all class-individual feature maps. For each candidate object, theses layers output classification probabilities for each class, bounding box parameters and 3D centerness values separately. Finally, we filter out those proposal bounding boxes with score less than 0.01, then apply oriented NMS with 3D IoU threshold of 0.5 to remove overlapped bounding boxes and reduce the number of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 RoI-Conv Refinement Module</head><p>Given the proposals of Stage-I, we further select 128 proposals whose 3D IoU with ground truth are greater than 0.3 as training samples for each scene, while reserve all proposals during inference. Finally, the proposals and voxel features from backbone are fed into RoI-Conv pooling module with two stacked sparse abstraction blocks to obtain the RoI-specific features. We also provide more implementation details of other RoI pooling strategies mentioned in the main paper.</p><p>PointRCNN. We first slightly enlarge the proposals by 0.3m, then randomly take out 128 voxels {l n } 128 n=1 within each proposal for further processing. These cropped voxels are regarded as input points and fed into the hierarchical PointNet++ <ref type="bibr" target="#b28">[29]</ref> with two SA layers to obtain the final RoIspecific features. The first SA layer uses farthest point sampling (FPS) to sample 32 key points from the input and applies a set abstraction operation <ref type="bibr" target="#b24">[25]</ref> centered on each key points to encode local patterns. The radius and number of neighbors are set to 0.4m and 16. Finally the sampled key points are pooled to a feature vector by the last SA layer for further proposal refinement.</p><p>Part-A 2 . Instead of directly processing the irregular points within proposals as PointRCNN, Part-A 2 converts the contiguously distributed points into regular voxels with a fixed spatial shape, where the average pooling operation is adopted to pool the points in the same voxel. For a fair comparison, we adopt the same spatial shape (i.e., 7 ? 7 ? 7) as ours, and then several sparse convolutions are stacked </p><formula xml:id="formula_15">{p i } N i=1 = {MLP vote (o i )} N i=1 , {s i } N i=1 = {MLP sem (o i ) ? [0, 1] N class } N i=1</formula><p>/*Class-Aware Local Grouping*/ 3: for j ? 0 to N class do /*Slice a semantic subset with ? */ 4:</p><formula xml:id="formula_16">c j = {p i : s j i &gt; ?, i = 1, .</formula><p>.., N } /*Class-Aware Re-voxelization*/ 5: </p><formula xml:id="formula_17">{v i } |Vj | i=1 = VFE(c j , ? ? d j ,</formula><formula xml:id="formula_18">A (j) = {a (j) i | a (j) i = SparseConv (j) 3D (v i , {v i } |Vj | i=1 , k (a) )} |Vj | i=1</formula><p>/*Merge the subset*/ 7:</p><formula xml:id="formula_19">A append A (j)</formula><p>Not a unique OP that each loc may have multiple features. to aggregate all part features into a feature vector. Notably, we follow the original paper <ref type="bibr" target="#b31">[32]</ref> and keep the empty voxels in each proposal to encode the bounding box's geometric information.</p><p>Ours-SA. In this variant, we replace the sparse convolution operation used for encoding local patterns in our sparse abstraction block with set abstraction <ref type="bibr" target="#b28">[29]</ref>. Specifically, given the RoI-specific points set G = {g k } | G| k=1 sampled from the proposals, instead of exploiting sparse convolution centered on each points, we adopt ball query to cover neighboring voxels. Then a PointNet operation is applied on each query group to learn the local patterns. We follow the same two-layers architecture and proposal sampling resolutions as our RoI-Conv module. Their corresponding radius and number of neighbors are set to (0.3m, 2.0m) and (16, 7 ? 7 ? 7) respectively.</p><p>As mentioned in the main paper, we compare our RoI-Conv module with the above three variants both on detection scores and computation cost. Note that the computation cost is measured by training memory with the batch size of 8. The experiments show that our RoI-Conv module has significant superiority. All the experiments are run on the same workstation and environment.</p><p>We further explain how we change the depth of RoI module introduced in the main paper. To be specific, we stack different number of sparse abstraction blocks with fixed sparse kernel size k (p) = 5 and decreasing proposal sampling resolutions {G t }, t = 1, ..., n, where n is the number of blocks. For example, {G t } = {1} means we only sample one grid point (proposal center) for each proposal and aggregate input voxels from backbone by directly applying sparse convolution centered on these points; {G t } = {7, 5, 1} means the first sparse abstraction block outputs a voxel set Q 1 where the voxels are sampled from the proposals with 7 ? 7 ? 7 resolution and serve as convolution centers to encode their local patterns from the input voxels. The second sparse abstraction block further samples a smaller voxel set Q 2 from the proposals with 5 ? 5 ? 5 resolution, and similarly obtains the voxelwise output features by applying sparse convolution centered on Q 2 to cover their neighbors from Q 1 . Finally, Q 2 is fed into the last sparse abstraction block. We use a sparse convolution with k (p) = 5 to aggregate all the part features into the proposal center and get the RoI-specific feature vector. Other settings can be easily understood by analogy with the above explanation. Note that the sparse kernel size k (p) in the last block is equal to the proposal sampling resolution in the second-to-last block to aggregate all information in the proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Loss Function Details</head><p>Our model is trained end-to-end with a multi-task loss including semantic loss L sem , voting loss L vote-reg , centerness loss L cntr , bounding box estimation loss L box , classification losses L cls for Stage-I and bbox refinement loss L rebox for Stage-II.</p><formula xml:id="formula_20">L = ? sem L sem + ? vote L vote + ? cntr L cntr +? box L box + ? cls L cls + ? rebox L rebox .<label>(10)</label></formula><p>The second stage loss L rebox consists of a regression loss L smooth-1 and a iou loss L iou . For the regression loss, both the 3D proposals and their corresponding ground-truth bounding boxes are transformed into the canonical coordinate systems, which means the 3D</p><formula xml:id="formula_21">proposal b i = (x i , y i , z i , h i , w i , l i , ? i ) and ground-truth bounding box b gt i = (x gt i , y gt i , z gt i , h gt i , w gt i , l gt i , ? gt i ) would be transformed t? b i = (0, 0, 0, h i , w i , l i , 0), b gt i = (x gt i ? x i , y gt i ? y i , z gt i ? z i , h gt i , w gt i , l gt i , ? gt i ? ? i ).<label>(11)</label></formula><p>Then following the traditional residual learning method and sin-cos heading encoding strategy, we obtain the final target t as follow:</p><formula xml:id="formula_22">t = ( x gt i ? x i d , y gt i ? y i d , z gt i ? z i d , log( h gt i h i ), log( w gt i w i ), log( l gt i l i ), sin(? ? ), cos(? ? )), (12) where d = h 2 i + w 2 i + l 2 i , ? ? = ? gt i ? ? i .</formula><p>Finally the smooth-L 1 loss is adopted to compute the regression loss. For the iou loss, we get the final refined bounding boxes decoded from the prediction logits and compute their rotated IoU with ground-truth bouding boxes as used in Stage-I.</p><p>The balancing factors are set default as ? sem = 1.0, ? vote = 1.0, ? cntr = 1.0, ? box = 1.0, ? cls = 1.0, ? rebox = 0.5.    We evaluate per-category on ScanNet V2 and SUN RGB-D under different IoU thresholds. <ref type="table" target="#tab_9">Table 9</ref>, 10 report the results on 18 classes of ScanNet V2 with 0.25 and 0.5 box IoU thresholds respectively. <ref type="table" target="#tab_1">Table 11</ref>, 12 show the results on 10 classes of SUN RGB-D with 0.25 and 0.5 box IoU thresholds. Our approach outperforms the baseline VoteNet <ref type="bibr" target="#b27">[28]</ref> and previous state-of-the-art method FCAF3D <ref type="bibr" target="#b7">[8]</ref> significantly in almost every category. Notably, our model significantly performs better than prior works on tiny classes (e.g., picture: +10.80 and +13.48 better than the SOTA on ScanNet V2), which demonstrates the effectiveness of our local grouping strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Latency and Memory Analysis.</head><p>We also report the latency and memory usage of our CAGroup3D on ScanNet V2. For a fair comparison, we re-measure all the methods on the same workstation (Single NVIDIA RTX 3090 GPU card, 256G RAM, and Xeon(R) E5-2638 v3) and enviroment (Unbuntu-16.04, Python 3.7, Cuda-11.1 and Pytorch-1.8.1). The official code of other methods is used for evaluation. <ref type="table" target="#tab_1">Table  13</ref> shows that our method achieves better performance with a competitive speed. The time cost of CAGroup3D is mainly on class-aware local grouping step, which iterates over all the classes to generate high-quality 3D proposals. However, in our approach, we use semantic threshold to select a point subset for each category, which can significantly reduce the computation usage. To achieve faster running speed, we also present a light-weight version with the larger voxel size (0.04m). With this modification, our model can be faster and still maintain a high performance. In addition, we further add the inference time comparison between our RoI-Conv module and other alternatives in <ref type="table" target="#tab_1">Table 14</ref>. It can be seen that RoI-Conv pooling module is significantly more memory-and-time efficient than previous pooling operation. Hope it can be useful for the following two-stage methods.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 More Ablation Studies</head><p>The effect of feature offsets. We provide the ablative studies of the feature shifting operation in our class-aware grouping module on ScanNet V2. We can observe in <ref type="table" target="#tab_1">Table 15</ref> that feature shifting is slightly better than the variant of non-shift. As discussed in VoteNet, to generate more reliable object representations, a MLP is used to transform seeds' features extracted from backbone to vote space, so that the grouped features can align with the voted points automatically.</p><p>The effect of different loss weights. As mentioned in ?A.4, we simply set all the loss weights to 1.0 except for the bbox refinement ? rebox , which is adjusted to 0.5 for balancing the value of Stage-I box loss L box and Stage-II refinement loss L rebox . Our method is not sensitive to loss weight and causes only minimal fluctuations (e.g. less than 0.3) as shown in <ref type="table" target="#tab_1">Table 16</ref>.</p><p>More possible combinations of the imporatant modules. In <ref type="table" target="#tab_1">Table 17</ref>, we list more results of combining different modules mentioned in the main paper including Semantic Prediction, Diverse Local Group, RoI-Conv and BiResNet. It can be seen that our well-designed modules can still boost the performance with various combinations, which shows the robustness and effectiveness of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Quantitative Results</head><p>We provide the visualization of our prediction bounding boxes on ScanNet V2 and SUN RGB-D datasets. Please see <ref type="figure" target="#fig_4">Figure 3</ref> for more qualitative results. Notably, our method can even accurately detect some miss-annotated objects in SUN RGB-D as in the bottom left of the figure. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overall architecture of CAGroup3D. (a) Generate 3D proposals by utilizing class-aware local grouping on the vote space with same semantic predictions. (b) Aggregating the specific features within the 3D proposals by the efficient RoI-Conv pooling module for the following box refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Algorithm of Class-Aware 3D proposal Generation Module. Input: Seed voxels {o i } N i=1 , semantic threshold ? , kernel sizes of aggregation k (a) , Voxel sizes of different classes {d j } N class j=1 , scale factor ?. Output: Proposals P 1: Initialize class-aware aggregation results A = {}. /*Voxel-wise Semantic and Vote Prediction*/ 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Avg) /*Class-dependent SpConv Aggregation with k (a) */ 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>8: end for /*Proposal Head with NMS*/ 9: P = NMS {MLP(A l )} |V0|+...+|V N class | l=0 10: Return P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on ScanNet V2(top) and SUN RGB-D(down).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>3D detection results on ScanNet V2<ref type="bibr" target="#b6">[7]</ref> and SUN RGB-D<ref type="bibr" target="#b32">[33]</ref>. The main comparison is based on the best results of multiple experiments, and the average value of 25 trials is given in brackets. * means the multi-sensor approaches that use both point clouds and RGB images.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ScanNet V2</cell><cell cols="2">SUN RGB-D</cell></row><row><cell>Methods</cell><cell cols="5">Presened at mAP@0.25 mAP@0.5 mAP@0.25 mAP@0.5</cell></row><row><cell>F-PointNet [26]*</cell><cell>CVPR'18</cell><cell>19.8</cell><cell>10.8</cell><cell>54.0</cell><cell>-</cell></row><row><cell>GSPN [48]*</cell><cell>CVPR'19</cell><cell>30.6</cell><cell>17.7</cell><cell>-</cell><cell>-</cell></row><row><cell>3D-SIS [17]*</cell><cell>CVPR'19</cell><cell>40.2</cell><cell>22.5</cell><cell>-</cell><cell>-</cell></row><row><cell>ImVoteNet [28]*</cell><cell>CVPR'20</cell><cell>-</cell><cell>-</cell><cell>63.4</cell><cell>-</cell></row><row><cell>TokenFusion [40]*</cell><cell>CVPR'22</cell><cell>70.8(69.8)</cell><cell>54.2(53.6)</cell><cell>64.9(64.4)</cell><cell>48.3(47.7)</cell></row><row><cell>VoteNet [27]</cell><cell>ICCV'19</cell><cell>58.6</cell><cell>33.5</cell><cell>57.7</cell><cell>-</cell></row><row><cell>3D-MPA [10]</cell><cell>CVPR'20</cell><cell>64.2</cell><cell>49.2</cell><cell>-</cell><cell>-</cell></row><row><cell>HGNet [4]</cell><cell>CVPR'20</cell><cell>61.3</cell><cell>34.4</cell><cell>61.6</cell><cell>-</cell></row><row><cell>MLCVNet [41]</cell><cell>CVPR'20</cell><cell>64.5</cell><cell>41.4</cell><cell>59.8</cell><cell>-</cell></row><row><cell>GSDN [14]</cell><cell>ECCV'20</cell><cell>62.8</cell><cell>34.8</cell><cell>-</cell><cell>-</cell></row><row><cell>H3DNet [51]</cell><cell>ECCV'20</cell><cell>67.2</cell><cell>48.1</cell><cell>60.1</cell><cell>39.0</cell></row><row><cell>BRNet [5]</cell><cell>CVPR'21</cell><cell>66.1</cell><cell>50.9</cell><cell>61.1</cell><cell>43.7</cell></row><row><cell>3DETR [24]</cell><cell>ICCV'21</cell><cell>65.0</cell><cell>47.0</cell><cell>59.1</cell><cell>32.7</cell></row><row><cell>VENet [42]</cell><cell>ICCV'21</cell><cell>67.7</cell><cell>-</cell><cell>62.5</cell><cell>39.2</cell></row><row><cell>Group-free [22]</cell><cell>ICCV'21</cell><cell>69.1(68.6)</cell><cell>52.8(51.8)</cell><cell>63.0(62.6)</cell><cell>45.2(44.4)</cell></row><row><cell>RBGNet [38]</cell><cell>CVPR'22</cell><cell>70.6(69.6)</cell><cell>55.2(54.7)</cell><cell>64.1(63.6)</cell><cell>47.2(46.3)</cell></row><row><cell>HyperDet3D [52]</cell><cell>CVPR'22</cell><cell>70.9</cell><cell>57.2</cell><cell>63.5</cell><cell>47.3</cell></row><row><cell>FCAF3D [8]</cell><cell>ECCV'22</cell><cell>71.5(70.7)</cell><cell>57.3(56.0)</cell><cell>64.2(63.8)</cell><cell>48.9(48.2)</cell></row><row><cell>Ours</cell><cell>-</cell><cell>75.1(74.5)</cell><cell>61.3(60.3)</cell><cell>66.8(66.4)</cell><cell>50.2(49.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effect of Semantic Prediction, Diverse Local Group, RoI-Conv and BiResNet. Semantic Prediction Diverse Local Group BiResNet RoI-Conv mAP@0.25 mAP@0.5</figDesc><table><row><cell>68.22</cell><cell>53.17</cell></row><row><cell>69.24</cell><cell>54.05</cell></row><row><cell>72.10</cell><cell>57.07</cell></row><row><cell>73.21</cell><cell>57.18</cell></row><row><cell>74.50</cell><cell>60.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of the class-aware local group with different scale factor ?. (k (a) = 9)</figDesc><table><row><cell cols="3">scale factor ? mAP@0.25 mAP@0.5</cell></row><row><cell>1.00</cell><cell>36.38</cell><cell>24.66</cell></row><row><cell>0.60</cell><cell>61.49</cell><cell>47.41</cell></row><row><cell>0.20</cell><cell>74.21</cell><cell>58.77</cell></row><row><cell>0.15</cell><cell>74.50</cell><cell>60.31</cell></row><row><cell>0.10</cell><cell>74.01</cell><cell>59.03</cell></row><row><cell>0.05</cell><cell>72.98</cell><cell>58.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of the class-aware local group with different semantic threshold ? .</figDesc><table><row><cell cols="3">sem thres ? mAP@0.25 mAP@0.5</cell></row><row><cell>0.20</cell><cell>73.51</cell><cell>59.50</cell></row><row><cell>0.10</cell><cell>74.28</cell><cell>59.97</cell></row><row><cell>0.08</cell><cell>74.38</cell><cell>60.10</cell></row><row><cell>0.06</cell><cell>74.51</cell><cell>60.27</cell></row><row><cell>0.04</cell><cell>74.38</cell><cell>59.98</cell></row><row><cell>0.02</cell><cell>73.53</cell><cell>58.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with other RoI pooling approaches.</figDesc><table><row><cell>RoI Method</cell><cell cols="2">mAP@0.25 mAP@0.5</cell><cell>memory</cell></row><row><cell>PointRCNN [30]</cell><cell>73.65</cell><cell>57.83</cell><cell>8,054MB</cell></row><row><cell>Part-A 2 [32]</cell><cell>74.01</cell><cell>58.89</cell><cell>6,540MB</cell></row><row><cell>Ours-SA [29]</cell><cell>73.89</cell><cell>58.14</cell><cell>11,508MB</cell></row><row><cell>Ours-SpConv</cell><cell>74.50</cell><cell>60.31</cell><cell>2,468MB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablate the depth of RoI module. , t = 1... mAP@0.25 mAP@0.5</figDesc><table><row><cell>Gt{1}</cell><cell>72.33</cell><cell>58.42</cell></row><row><cell>{7, 1}</cell><cell>74.50</cell><cell>60.31</cell></row><row><cell>{7, 5, 1}</cell><cell>73.91</cell><cell>60.61</cell></row><row><cell>{7, 5, 3, 1}</cell><cell>73.72</cell><cell>59.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Effect of different proposal sampling resolution G x ? G y ? G z in the 1 st sparse abstraction.G x ? G y ? G z mAP@0.25 mAP@0.5</figDesc><table><row><cell>3 ? 3 ? 3</cell><cell>73.56</cell><cell>59.61</cell></row><row><cell>5 ? 5 ? 5</cell><cell>74.21</cell><cell>60.09</cell></row><row><cell>7 ? 7 ? 7</cell><cell>74.50</cell><cell>60.31</cell></row><row><cell>9 ? 9 ? 9</cell><cell>74.41</cell><cell>60.43</cell></row><row><cell>11 ? 11 ? 11</cell><cell>74.44</cell><cell>60.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Effect of using different grouping kernel sizes k (p) in the 1 st sparse abstraction.</figDesc><table><row><cell>k (p)</cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell>3 ? 3 ? 3</cell><cell>73.87</cell><cell>59.82</cell></row><row><cell>5 ? 5 ? 5</cell><cell>74.50</cell><cell>60.31</cell></row><row><cell>7 ? 7 ? 7</cell><cell>74.47</cell><cell>60.39</cell></row><row><cell>9 ? 9 ? 9</cell><cell>74.52</cell><cell>60.42</cell></row><row><cell>11 ? 11 ? 11</cell><cell>74.45</cell><cell>60.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>3D detection scores per category on the ScanNetV2, evaluated with mAP@0.25 IoU. 90.79 90.07 90.78 60.22 53.83 43.71 55.56 12.38 66.85 66.02 52.37 52.05 63.94 97.40 52.32 92.57 43.37 62.90 MLCVNet [41] 42.50 88.50 90.00 87.40 63.50 56.90 47.00 57.00 12.00 63.90 76.10 56.70 60.90 65.90 98.30 59.20 87.20 47.90 64.50 BRNet [5] 49.90 88.30 91.90 86.90 69.30 59.20 45.90 52.10 15.30 72.00 76.80 57.10 60.40 73.60 93.80 58.80 92.20 47.10 66.10 H3DNet [51] 49.40 88.60 91.80 90.20 64.90 61.00 51.90 54.90 18.60 62.00 75.90 57.30 57.20 75.30 97.90 67.40 92.50 53.60 67.20 Group-free [22] 52.10 92.90 93.60 88.00 70.70 60.70 53.70 62.40 16.10 58.50 80.90 67.90 47.00 76.30 99.60 72.00 95.30 56.40 69.10 FCAF3D [8] 57.20 87.00 95.00 92.30 70.30 61.10 60.20 64.50 29.90 64.30 71.50 60.10 52.40 83.90 99.90 84.70 86.60 65.40 71.50 Ours 60.37 93.00 95.25 92.32 69.95 67.95 63.60 67.29 40.70 77.01 83.87 69.43 65.65 73.00 99.97 79.70 86.98 66.12 75.12</figDesc><table><row><cell></cell><cell>cab</cell><cell>bed</cell><cell>chair sofa</cell><cell>tabl</cell><cell>door wind bkshf</cell><cell>pic</cell><cell>cntr</cell><cell>desk</cell><cell>curt</cell><cell>frig showr toil</cell><cell>sink</cell><cell>bath ofurn mAP</cell></row><row><cell>VoteNet [28]</cell><cell>47.87</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>3D detection scores per category on the ScanNetV2, evaluated with mAP@0.50 IoU. 76.10 67.20 68.80 42.40 15.30 6.40 28.00 1.30 9.50 37.50 11.60 27.80 10.00 86.50 16.80 78.90 11.70 33.50 BRNet [5] 28.70 80.60 81.90 80.60 60.80 35.50 22.20 48.00 7.50 43.70 54.80 39.10 51.80 35.90 88.90 38.70 84.40 33.00 50.90 H3DNet [51] 20.50 79.70 80.10 79.60 56.20 29.00 21.30 45.50 4.20 33.50 50.60 37.30 41.40 37.00 89.10 35.10 90.20 35.40 48.10 Group-free [22] 26.00 81.30 82.90 70.70 62.20 41.70 26.50 55.80 7.80 34.70 67.20 43.90 44.30 44.10 92.80 37.40 89.70 40.60 52.80 FCAF3D [8] 35.80 81.50 89.80 85.00 62.00 44.10 30.70 58.40 17.90 31.30 53.40 44.20 46.80 64.20 91.60 52.60 84.50 57.10 57.30 Ours 41.35 82.82 90.82 85.62 64.93 54.33 37.33 64.10 31.38 41.08 63.62 44.38 56.95 49.26 98.19 55.44 82.40 58.82 61.27</figDesc><table><row><cell></cell><cell>cab</cell><cell>bed</cell><cell>chair sofa</cell><cell>tabl</cell><cell>door wind bkshf</cell><cell>pic</cell><cell>cntr</cell><cell>desk</cell><cell>curt</cell><cell>frig showr toil</cell><cell>sink</cell><cell>bath ofurn mAP</cell></row><row><cell>VoteNet [28]</cell><cell>8.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>3D detection scores per category on the SUN RGB-D, evaluated with mAP@0.25 IoU.</figDesc><table /><note>bathtub bed bookshelf chair desk dresser nightstand sofa table toilet mAP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>3D detection scores per category on the SUN RGB-D, evaluated with mAP@0.50 IoU. bathtub bed bookshelf chair desk dresser nightstand sofa table toilet mAP</figDesc><table><row><cell>VoteNet [28]</cell><cell>45.40 53.40</cell><cell>6.80</cell><cell>56.50 5.90</cell><cell>12.00</cell><cell>38.60</cell><cell>49.10 21.30 68.50 35.80</cell></row><row><cell>H3DNet [51]</cell><cell>47.60 52.90</cell><cell>8.60</cell><cell>60.10 8.40</cell><cell>20.60</cell><cell>45.60</cell><cell>50.40 27.10 69.10 39.00</cell></row><row><cell>BRNet [5]</cell><cell>55.50 63.80</cell><cell>9.30</cell><cell cols="2">61.60 10.00 27.30</cell><cell>53.20</cell><cell>56.70 28.60 70.90 43.70</cell></row><row><cell cols="2">Group-free [22] 64.00 67.10</cell><cell>12.40</cell><cell cols="2">62.60 14.50 21.90</cell><cell>49.80</cell><cell>58.20 29.20 72.20 45.20</cell></row><row><cell>FCAF3D [8]</cell><cell>66.20 69.80</cell><cell>11.60</cell><cell cols="2">68.80 14.80 30.10</cell><cell>59.80</cell><cell>58.20 35.50 74.50 48.90</cell></row><row><cell>Ours</cell><cell>68.55 67.44</cell><cell>13.82</cell><cell cols="2">70.84 17.28 30.92</cell><cell>59.91</cell><cell>61.27 39.22 72.73 50.20</cell></row><row><cell>B More Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">B.1 Per-class Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Performance comparison of latency and runtime memory on ScanNet V2 dataset. All methods are tested on same workstation.</figDesc><table><row><cell>Method</cell><cell>1-stage</cell><cell cols="2">2-stage total latency</cell><cell>memory</cell><cell cols="2">mAP@0.25 mAP@0.5</cell></row><row><cell>VoteNet [28]</cell><cell>101.0ms</cell><cell>-</cell><cell>101.0ms</cell><cell>2,507MB</cell><cell>58.6</cell><cell>33.5</cell></row><row><cell cols="2">Group-free [22] 153.1ms</cell><cell>-</cell><cell>153.1ms</cell><cell>3,678MB</cell><cell>69.1</cell><cell>52.8</cell></row><row><cell>FCAF3D [8]</cell><cell>114.9ms</cell><cell>-</cell><cell>114.9ms</cell><cell>3,755MB</cell><cell>71.5</cell><cell>57.3</cell></row><row><cell>Ours(light)</cell><cell cols="2">111.6ms 12.3ms</cell><cell>123.9ms</cell><cell>2,947MB</cell><cell>74.0</cell><cell>60.1</cell></row><row><cell>Ours</cell><cell cols="2">144.8ms 34.5ms</cell><cell>179.3ms</cell><cell>3,544MB</cell><cell>75.1</cell><cell>61.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Comparison with other RoI pooling approaches.</figDesc><table><row><cell>RoI Method</cell><cell cols="2">mAP@0.25 mAP@0.5</cell><cell>memory</cell><cell>speed</cell></row><row><cell>PointRCNN</cell><cell>73.65</cell><cell>57.83</cell><cell>8,054MB</cell><cell>62.9ms</cell></row><row><cell>Part-A 2</cell><cell>74.01</cell><cell>58.89</cell><cell>6,540MB</cell><cell>47.9ms</cell></row><row><cell>Ours-SA</cell><cell>73.89</cell><cell>58.14</cell><cell cols="2">11,508MB 45.5ms</cell></row><row><cell>Ours-SpConv</cell><cell>74.50</cell><cell>60.31</cell><cell>2,468MB</cell><cell>34.5ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Ablation study of feature shifting.</figDesc><table><row><cell cols="2">Feature Shifting mAP@0.25 mAP@0.5</cell></row><row><cell>74.18</cell><cell>60.17</cell></row><row><cell>74.50</cell><cell>60.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Ablation study of loss weight.</figDesc><table><row><cell cols="3">? rebox mAP@0.25 mAP@0.5</cell></row><row><cell>1.0</cell><cell>74.29</cell><cell>60.15</cell></row><row><cell>0.5</cell><cell>74.50</cell><cell>60.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17 :</head><label>17</label><figDesc>Effect of Semantic Prediction, Diverse Local Group, RoI-Conv and BiResNet.</figDesc><table><row><cell cols="2">Semantic Prediction Diverse Local Group BiResNet RoI-Conv mAP@0.25 mAP@0.5</cell></row><row><cell>69.10</cell><cell>57.62</cell></row><row><cell>73.14</cell><cell>59.85</cell></row><row><cell>70.99</cell><cell>58.42</cell></row><row><cell>74.50</cell><cell>60.31</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* https://www.mindspore.cn/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Liwei Wang is supported by National Science Foundation of China (NSFC62276005), The Major Key Project of PCL (PCL2021A12), Exploratory Research Project of Zhejiang Lab (No. 2022RC0AN02), and Project 2020BD006 supported by PKUBaidu Fund. We gratefully acknowledge the support of MindSpore * .</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey of augmented reality. Presence: teleoperators &amp; virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronald T Azuma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Ogale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey of augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gun</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A hierarchical graph network for 3d object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biwen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Back-tracing representative points for voting-based 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">Minkowski convolutional neural networks. In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>4d spatio-temporal convnets</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fcaf3d: Fully convolutional anchor-free 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Rukhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vorontsova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00322</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Voxel r-cnn: Towards high performance voxel-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alireza Fathi, Bastian Leibe, and Matthias Nie?ner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Embracing single stride 3d object detector with sparse transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lue</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold sparse convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">and Laurens Van Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative sparse detection networks for 3d single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep dual-resolution networks for real-time and accurate semantic segmentation of road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanduo</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06085</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Non-convex distributionally robust optimization: Non-asymptotic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jikai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Group-free 3d object detection via transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An end-to-end transformer model for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Softgroup for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kookhoi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monocular plan view networks for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01151</idno>
		<title level="m">Collaborative visual navigation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rbgnet: Ray-based grouping for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal token fusion for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mlcvnet: Multi-level context votenet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Venet: Voting enhancement network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dening</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fusionrcnn: Lidar-camera fusion for two-stage 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaocong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10733</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Boosting 3d object detection via objectfocused image fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.10589</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Center-based 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Improved analysis of clipping algorithms for non-convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jikai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">H3dnet: 3d object detection using hybrid geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hyperdet3d: Learning a sceneconditioned 3d object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
