<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OPEN QUESTION ANSWERING OVER TABLES AND TEXT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
							<email>wenhuchen@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
							<email>mingweichang@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
							<email>eschling@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OPEN QUESTION ANSWERING OVER TABLES AND TEXT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In open question answering (QA), the answer to a question is produced by retrieving and then analyzing documents that might contain answers to the question. Most open QA systems have considered only retrieving information from unstructured text. Here we consider for the first time open QA over both tabular and textual data and present a new large-scale dataset Open <ref type="table">Table-</ref>and-Text Question Answering (OTT-QA) to evaluate performance on this task 1 . Most questions in OTT-QA require multi-hop inference across tabular data and unstructured text, and the evidence required to answer a question can be distributed in different ways over these two types of input, making evidence retrieval challenging-our baseline model using an iterative retriever and BERT-based reader achieves an exact match score less than 10%. We then propose two novel techniques to address the challenge of retrieving and aggregating evidence for OTT-QA. The first technique is to use "early fusion" to group multiple highly relevant tabular and textual units into a fused block, which provides more context for the retriever to search for. The second technique is to use a cross-block reader to model the cross-dependency between multiple retrieved evidence with global-local sparse attention. Combining these two techniques improves the score significantly, to above 27%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Open question answering considers the problem of retrieving documents from a fixed corpus with a retriever, and then analyzes retrieved evidence to provide answers to a given question with a reader. Prior open question answering systems focused only on retrieving and reading free-form passages or documents. However, a significant amount of real-world information is stored in other forms, such as semi-structured web tables due to its compact representation to aggregate related information. For example, tables are often used to hold large quantities of related facts, especially numeric facts, such as 'Career Statistics for Lebron James'. This type of detailed information is found much less frequently in unstructured text. Tables are also commonly used for collections of homogeneous entities or recurring events, like 'List of Periodic Comets' or 'List of Champions League Winners since 1966'. Hence tabular information serves as an excellent complement to textual data, especially in the open setting. Despite these advantages, no previous studies have exploited the millions of web tables to augment their open QA system.</p><p>In this paper, we describe the first study to jointly exploit tables and text for open-domain question answering. For this purpose, we construct a new dataset, Open <ref type="table">Table-</ref>and-Text Question Answering (OTT-QA). OTT-QA is built on the HybridQA dataset , and like HybridQA, OTT-QA questions are multi-hop questions which require aggregating information from both tables and text to answer. However, unlike HybridQA, OTT-QA requires the system to retrieve relevant tables and text -in contrast, in HybridQA, the ground truth tables and textual passages required for each question are given. To produce OTT-QA's questions, we begin by re-annotating the questions from HybridQA to 'decontextualize' them-i.e., we make questions suitable for the open-domain setting</p><p>The 2019-20 NBA season is the 74th season of the National Basketball Association (NBA). The season was suspended by COVID-19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2010</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COVID-19 Pandemics</head><p>Career Statistics Lebron James Career Statistics <ref type="bibr">(Regular Season)</ref> Q: How many points per game did Lebron James get in the NBA Season suspended by COVID? A: 25.3</p><p>Heterogeneous Retriever + Reader <ref type="figure">Figure 1</ref>: The problem setting: A OTT-QA model needs to retrieve from two candidate pools and then perform multi-hop reasoning to find answers.</p><p>so that unique answers can be determined from the question alone, without needing context from the provided text and tables. We then add new questions to remove potential biases. After these steps, OTT-QA contains 45K human-annotated questions that require retrieving and aggregating information over tables and text from the whole Wikipedia. Examples from OTT-QA are depicted in <ref type="figure">Figure 1</ref>. Note the table and passages contain non-overlapping information, and both of them must be understood to answer the question. For example, the question has a low lexical overlap with the passage about the 'Lakers', and it needs the table as the bridge to retrieve this passage. Such cross-modality multi-hop retrieval features OTT-QA. More examples are displayed in Appendix.</p><p>OTT-QA is distinguished from the existing QA datasets in two aspects. Existing table-based QA datasets <ref type="bibr" target="#b27">(Pasupat &amp; Liang, 2015;</ref><ref type="bibr" target="#b39">Yu et al., 2018;</ref> operates in the closed setting without requiring any retrieval, whereas most existing open QA datasets <ref type="bibr" target="#b17">(Joshi et al., 2017;</ref> require only text retrieval, not table retrieval. One dataset, Natural Questions (NQ) <ref type="bibr" target="#b20">(Kwiatkowski et al., 2019)</ref> includes some tabular information in its corpus, but the tables are nearly always of a restricted type (infobox tables with only a single row). In contrast, OTT-QA models require retrieving both tabular data and text, and unlike the NQ dataset, requires information fusion from text and tables in non-trivial ways. OTT-QA poses novel and realistic challenges to both the retriever and reader in open QA though the questions are less natural than the real queries from NQ <ref type="bibr" target="#b20">(Kwiatkowski et al., 2019)</ref>. Retrievers for OTT-QA need to consider two information formats, making the search space larger. Even worse, as questions in OTT-QA often require multi-hop inference, one round of retrieval is often not enough. Readers for OTT-QA also need to aggregate a significant amount of knowledge-intensive information, compared to other reader models: a single table in OTT-QA has an average length of over 300 words. Moreover, readers are often expected to process multiple retrieved units due to the uncertainty in retrieval, which makes it difficult to design strong reader models <ref type="bibr" target="#b22">Liu et al., 2019)</ref> with a length limit of 512 tokens.</p><p>The baseline system that we propose to address these challenges uses an iterative retriever <ref type="bibr" target="#b34">(Sun et al., 2019;</ref><ref type="bibr" target="#b29">Qi et al., 2019;</ref><ref type="bibr" target="#b24">Min et al., 2019;</ref><ref type="bibr" target="#b13">Ding et al., 2019;</ref><ref type="bibr" target="#b2">Asai et al., 2019)</ref> and a BERT reader . The iterative retriever explores multiple evidence documents iteratively, interacting with the candidate pool to gradually reformulate the query. Beam search is used to find multiple subsets of documents that may contain all the required evidence, and each subset is then fed to the BERT reader to predict the answer span. The highest-scored prediction is chosen as the answer. The iterative retriever needs to re-encode the query with a big transformer and re-search over the candidate pool, such a procedure (especially dense) can be computationally expensive. Furthermore, the BERT reader fails to capture a global overview of the retrieved documents, which leads to bad local optimum in the model prediction.</p><p>We propose a more sophisticated system that addresses these challenges with two novel strategies: namely fusion retrieval and cross-block reading. The fusion retriever first pre-aligns the table segments to their highly related passages, using entity linking. Then, the aligned table segments and passages are grouped as a fused block, which contains aggregated information from two modalities; hence, compared to the previous documents, it contains richer context to benefit the following retrieval. We view the fused block as the basic unit to be retrieved, and instead of performing multiple runs of retrieval iteratively, the fusion retriever is used once to retrieve the top K fused blocks; however, due to errors in fusion and retrieval, the retrieved top-1 fused block might not contain the necessary information. We thus also propose a cross-block reader based on a sparse-attention based transformer architecture <ref type="bibr" target="#b1">(Ainslie et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020)</ref>, which can process extremely long sequences efficiently. We use the cross-block reader to read all the top-K retrieved fused blocks jointly. Both strategies have proven effective compared to the baseline system: the best model combining the two strategies improves the accuracy of the baseline system by a huge margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>The aim of an open QA system is to extract an answer to a question q from a given large corpus.</p><p>Most open QA models are retriever-reader models, which extract answers in two steps: retrieval and reading. In the retrieval step, a retrieval model f is used to retrieve a set of passages from the text corpus. In the reading step, the reader is then used to extract the answer from them.</p><p>Retrieval Function There are two commonly-used types of retrieval function f : sparse retrievers and dense retrievers. Our sparse retriever uses a unigram-based BM-25 score to retrieve an evidence unit b from the candidate pool B. Our dense retrieval function is a dual-encoder model <ref type="bibr" target="#b4">(Bromley et al., 1994)</ref>, and we follow <ref type="bibr" target="#b14">Guu et al., 2020)</ref> for the dual encoder design. The query and the passage are encoded with separate Transformers. As in , the vector corresponding to the first token, [CLS], is used as a "pooled" representation of the sequence. The dense retrieval function is the dot product between</p><formula xml:id="formula_0">h q = BERT Q (q)[CLS] and h b = BERT B (b)[CLS]</formula><p>for each evidence block b in the candidate corpus-i.e., the scoring function is f (q, b) = h T q h b , which can viewed as finding the nearest neighbor in vector space. In the multi-hop open QA setting , an iterative retrieval function <ref type="bibr" target="#b34">(Sun et al., 2019;</ref><ref type="bibr" target="#b24">Min et al., 2019;</ref><ref type="bibr" target="#b13">Ding et al., 2019)</ref> is proposed, which defines the retrieval process as an auto-regressive formula. Our iterative retriever function is denoted as f ([q, b 1 , ? ? ? , b j?1 ], b j ), which appends the previous j ? 1 rounds of retrieval to the original q in in the j-th round of retrieval. Beam search is used in test time. Single-Block Reader Due to the uncertainty in retrieval, the top-1 document might not always contain the answer. Existing models normally retrieve the top-k documents and feed them to the reader for span selection. The standard reader <ref type="bibr" target="#b6">(Chen et al., 2017;</ref><ref type="bibr" target="#b17">Joshi et al., 2017)</ref> aims to extract a span from each of the retrieved blocks b i and assign a confidence f (q, b i )f read (a|q, b i ) to it, with f (q, b i ) indicating the retrieval probability and f read (a|q, b i ) denoting the span selection probability by reader. Multiple answers {a 1 , ? ? ? , a k } are ranked with this confidence score and the highest scored answer span? is the final answer. Note that the reader needs to run k times, once for each of the top-k retrievals. We refer to this model as the single-block reader and use it as our baseline. HybridQA HybridQA , a closed-domain QA dataset, is the most related to ours. During the annotation of HybridQA, a table T and its relevant passages {P 1 , ? ? ? , P N } (surrounding text and hyperlinked passage) are given to a crowd worker to write questions which necessarily require both the passage and table information to answer. The original dataset contains 72K multi-hop questions paired with 13K tables with their paired passages. During training/testing time, the ground-truth tables and passages are given to a model, HYBRIDER, to find the final answer. HYBRIDER also serves as an important baseline in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TASK AND DATASET</head><p>In OTT-QA, the retrieval corpus consists of a set of table candidates B T and a set of passage candidates B P . The task is to answer question q by extracting answer strings from blocks b ? B T ? B P , where b can be either textual and tabular data. We adopt the standard exact match (EM) and F1 scores  for evaluation. Different from HybridQA, OTT-QA's table candidates are web tables without hyperlinks provided. This decision was made to make the problem setting more general, as otherwise systems that solve OTT-QA could only be applied to high-quality data in Wikipedia. However, in OTT-QA, we provide hyperlinks in the training subset, but not dev/test set. Removing hyperlinks in tables makes the overall task much more challenging, but makes the final systems applicable to more general domains. Thus, an OTT-QA model needs to jointly retrieve both tables and text, without abusing gold hyperlinks, and then aggregate them to find the answer. Candidate Pool For our table collection B T , we extracted all Wikipedia regular tables with their metadata including page title, page section title, and section text. The metadata, denoted T M , is essential for de-contextualization. We obtain a table corpus containing over 400k high-quality tables with an average length of 320 words including metadata. For the text passage collection B P , we crawl English Wikipedia dump pages and filter out noisy pages. We follow HybridQA  and only keep a maximum of 12 sentences in the introduction section as the passage. We obtain a corpus containing over 5 million passages, with an average of 94 words.</p><p>Notation We define each table as a matrix T , which consists of cells T i,j with i specifying the row, and j specifying the column. Each cell T i,j could be a number, date, phrase or even sentence due to its semi-structured nature. However, a single complete table with structured representation <ref type="bibr" target="#b15">(Herzig et al., 2020)</ref> can easily exceed the 512-token limit, which poses great challenges to the downstream reader to process top-K retrieval. Hence we propose to decompose each table T into multiple rows R i , which are combined with the headers, metadata, and global max/min information from the original table as a table segment. The table segment is used as the basic retrieval block in our paper. This decomposition procedure increases candidate B T from 400k to 5 million, making the retrieval problems even more fine-grained and more challenging. Our table segment representation is described in Appendix subsection B.1. In summary, we build a candidate pool of 5 million table segments B T and a pool of 5 million passages B P . We denote as B as our full candidate pool, which our model needs to find the block b (a table segment or a passage) containing the answer span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">QUESTION AND ANSWER ANNOTATIONS</head><p>Our question and answer pairs are built upon the existing HybridQA  dataset, with several significant changes. First, crowd workers 'decontextualize' the questions so that they are not under-specified or context-dependent, and thus suitable for the open setting. Second, we add more questions to the development/test set to remove possible annotation bias. During annotation, we adopt strict quality control 2 and more details are described in Appendix section A.1.</p><p>Decontextualization Most questions in HybridQA are contextualized with a given table and several passages, with corresponding questions written by crowd workers. Often, the crowdsourced questions assume the context. For example, the questions might contain the words "the players" because the given table is about "Netherlands players". We thus needed 'decontextualize' (Parikh et al., 2020) the original context-dependent questions, so they could serve as standalone questions, specific enough to imply a unique answer relative to the corpus. To discourage excessive unwanted modification, we enforce a two-step annotation procedure, as depicted in <ref type="figure">Figure 2</ref>. In the first phase, the worker is only allowed to insert minimum words or phrases (or replace pronouns) into the questions based on the information presented by Wikipedia Title, Section Title, and Section Text to make the question have a unique answer. After this step, we often potentially obtain overly-complicated questions that are artificial and unnatural. Therefore, we manually selected the worst 25% questions and sent them back to make them more concise and natural. OTT-QA Annotation <ref type="figure">Figure 2</ref>: The 'de-contextualization' annotation phase of OTT-QA. In the first step, the annotator is restricted to add phrases from the context. In the second step, the annotator is specifically requested to make the sentence more concise and natural.</p><p>Additional Evaluation Examples As all the questions from HybridQA are based on the 13k tables from the HybridQA set, no questions are asked about the newly crawled 400k tables. This potentially generates unwanted statistical biases or artifacts for the model to exploit, and potentially biases the final evaluation results. Therefore, we randomly sampled another 1100 tables from the newly crawled tables, and follow the original annotation process used by HybridQA to re-collect 2200 new questions. These new questions were mainly used in the dev/test set. Below we refer to the subset of tables used by original HybridQA as the in-domain tables.</p><p>Distant Supervision Signals For the in-domain tables (? 8k), the cell-wise hyperlinks are provided in OTT-QA as a potential signal for supervision. We use H i,j = {b 1 , b 2 , . . . ? B P } to denote the hyperlinks in cell T i,j . Since in HybridQA the oracle fine-grained answer span is not explicitly annotated, we approximate this by traversing the table and hyperlinked pasasages to find all exact matches. This process contains some noise-a manual study reveals that it roughly contains 15% error. We use this 'weakly-supervised' fine-grained information to train our models. We denote the 'approximate' block of the answer span for answer a as b a , and use it to train our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DATASET STATISTICS</head><p>After annotation, we sampled roughly 2K questions from the in-domain HybridQA dataset, and then mix them with the newly collected out-domain questions to construct our dev and test sets. Finally, we have 41,469 questions in the training set, 2,214 questions in the dev set, and 2,158 questions in the test set. We conduct more in-detailed analysis over the reasoning types and show them in the Appendix A.3, a remarkable difference from original HybridQA is that a proportion of questions actually have multiple plausible inference chains in the open-domain setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL</head><p>Our model for OTT-QA is a retriever-reader model with new designs for both retriever and reader. As discussed briefly above, we propose to use a fusion retriever instead of using a standard iterative retrieve, and we also propose to use cross-block readers to replace a standard single-block reader. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">FUSION RETRIEVER</head><p>Iterative retrieval <ref type="figure" target="#fig_0">(Figure 3</ref>, Left) has the following issues. First, iterative retrieval training often requires having supervision signals for every retrieval step to reach good performance, which is not available in OTT-QA. The iterative retrieval also suffers from the problem of error propagation, as early mistakes can propagate to later retrieval stages. Finally, the computation cost for applying a dual-encoder for iterative retrieval is very high, as for every stage, the query embedding has to be re-encoded to include the entire retrieval history.</p><p>We propose an alternative strategy to replace multi-step retrieval, namely fusion retrieval <ref type="figure" target="#fig_0">(Figure 3</ref>, Right). In the fusion retriever, we first use an 'early fusion' strategy to group relevant heterogeneous data before retrieval. The fusion procedure groups several highly-relevant blocks from different modalities as a self-contained group (fused block), which provides more clues for the retriever to utilize. Early fusion is very important for retrieving  <ref type="figure">Figure 4</ref>: Left: Single-block reader with input shorter than 512 tokens (baseline). Right: Crossblock reader with length over 4K tokens, and? denotes the global state assigned to local block A. The single-block reader is stuck at local optimum, while cross-block reader outputs global optimum.</p><p>linking step. The problem is challenging due to the mismatch between the lexical forms from the table (which for brevity are often abbreviated) and the relevant passage titles. For example, a cell in the table of "NCAA Division I Men's Football Tournament" contains the term "Penn State". Directly matching "Penn State" against the passage corpus will lead to "Penn State University" rather than the ground-truth hyperlinked entity, named "Penn State Nittany Lions football". Therefore, we propose an additional augmentation step, which takes in a table segment block b T and generates a sequence of augmented queries q 1 , q 2 , ? ? ? , q n token by token to make the queries more similar to the passage title. The augmented queries are then used to search for nearest neighbors in the passage corpus B P using BM25 as the final entity linking step, which is depicted in Appendix. The query augmentation is implemented with a GPT-2 model , fine-tuned on the supervised pairs of (table segment, hyperlink) from the in-domain tables. Each b T is fed to find its companions b 1 P , ? ? ? , b n P , they are collectively called b F . We follow the standard dual-encoder setting (section 2) and the only difference is that we replace the input of the block encoder with</p><formula xml:id="formula_1">h b = BERT B ([b T , b 1 P , ? ? ? , b n P ])</formula><p>., which captures the cross-attention between the table and the text within a block. The fused embedding contains richer context from both modalities to complement each other. The retriever only needs to retrieve once from the candidate pool, which dramatically decreases the complexity compared to the existing iterative retrievers.</p><p>To enhance the neural retrieval system to retrieve fused blocks, we apply the Inverse Cloze Task (ICT)  pretraining task on the corpus of fused blocks. ICT is a way to generate pseudo-training data for dense retrieval. Unlike standard document-wise ICT, our fused block contains both table segments and multiple passages. Given a fused block b F , we generate the pseudoquery in the following way: 1) we first corrupt the table segment by randomly dropping half of the words from the table metadata and cells to obtain a partial table segmentb T . 2) We then randomly sample a sentenceb P from the fused passage. We combineb T andb P as a pseudo queryq and pair it with the original fused block b F as pre-training data. The pre-training data is applied to enhance the dual encoder's ability to select lexically matched documents. After pre-training, the retriever is fine-tuned on OTT-QA. Finally, at inference time, the retriever is used to retrieve the top K fused blocks for a question, which are then passed to the reader for answer prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CROSS-BLOCK READER</head><p>The reader typically needs to process the top-k retrieved blocks returned by the retriever to extract the best answer, as the top-1 block might not contain enough evidence to answer the question. As demonstrated in <ref type="figure">Figure 4</ref>, the cross-block reader aims to address this issue by using cross attention between different blocks to model their dependencies. To obtain the cross-block reader, we take the pre-trained long-range sparse attention transformer (ETC) <ref type="bibr" target="#b1">(Ainslie et al., 2020)</ref>, which can accept up to 4096 tokens as input, and then fine-tune the model on the distant supervision data. During training, the ground truth (fused) blocks are mixed with hard negative blocks from the retriever. We take the top-k retrieval results to fill the 4096 token space (roughly 15 fused blocks).</p><p>Cross-attention between blocks allows a much more powerful way to aggregate information across the k retrieved blocks compared to the single-block reader, especially when the blocks are fused. This is feasible because of the design of the sparse attention structure in ETC, which can constrain We conduct experiments with both sparse and dense retrievers using the dev set, and then select the best setting to report the test set results (as indicated by the word "Best"). Fusion-Retriever and Cross-Block Reader are combined to obtain the highest score. ? are ablations.</p><p>the attention of each token to its neighboring tokens within a local radius in its local block. Such sparse attention can decrease the attention computation complexity from quadratic O(N 2 ) to linear O(N |R|), where |R| is the local radius (where N = 4096 and |R| = 84 in our experiments). To allow cross-block interaction, ETC assigns a global state for each local block in the long sequence, and blocks can attention to each other through multiple layers of such global-local structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>All of our code is based on Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. For the retriever part, the sparse retriever is built on top of DrQA <ref type="bibr" target="#b6">(Chen et al., 2017)</ref> with unigram features, and the dense retriever is built with BERT. The single-block retriever is based on BERT-uncased, and the cross-block reader is based on ETC <ref type="bibr" target="#b1">(Ainslie et al., 2020)</ref>. Both of them consist of 12 layers with a hidden size of 768, the minor differences in the relative positional embedding used in ETC. All the models are trained with a learning rate of 1e-5 optimized by AdamW <ref type="bibr" target="#b23">(Loshchilov &amp; Hutter, 2019)</ref>. We use in-batch negatives  to train our dense retrievers. A more detailed implementation of the baseline iterative retriever is described in Appendix subsection B.2. In fusion retriever, we use the 'fused' block containing the 'approximate' answer block b a as the positive instance. In iterative retriever, since the auto-regressive model f (b j |q, b 1 , ? ? ? , b j?1 ) requires fine-grained inference chain for step-wise supervision, which is not given in OTT-QA. We apply lexical match based heuristics to synthesize inference chains as weakly supervised training data (described in Appendix). For all the dense retrievers, we pre-train with 10K steps using the generated pseudo query and then fine-tune them another 10K step using a batch size of 2048. For the cross-block reader, we fine-tune with a batch size of 64. Both are using 16 cloud TPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>In our experiments, we experiment with different types of retriever and reader models under both sparse and dense setting, the details are described as follows:</p><p>? HYBRIDER: this model, designed for closed domain HybridQA questions, is one baseline. Since this model requires a ground truth table with its hyperlinks to do modularized reasoning, we use BM25 to retrieve the most relevant table and passages to reconstruct an 'approximated' input for this model. We experiment with top-1,2,3,4 cases where we use the answer with the highest confidence as the final result. We also directly feed the ground-truth table and hyperlinks to HYBRIDER, which roughly estimates an upper limit of this task.</p><p>? Iterative-Retriever (Sparse): We use a 2-step iterative retriever: in the first step, we apply the question to retrieve the top-10 table segments and top-10 passages. In the second step, we use each retrieved table segment to retrieve its related top-5 passages and concatenate each retrieved passage title with the original question to retrieve the top-5 table segments. We merge and calculate the retrieval score of each unique block and rank them by their score. For the single-block reader, we split the retrieved blocks into 512-token chunks and feed them to the BERT reader. For the cross-block reader, we truncate the top 4096 subword tokens and only feed these tokens to reader.</p><p>? Iterative-Retriever (Dense): We use a 3-step iterative retriever. In the first step, we encode the question and retrieve the top-8 blocks (either table segment or passage); in the second step, we concatenate the previous retrieved block and the question to re-encode the query vector to further retrieve top-4 blocks; similarly, the last step retrieves top-2 blocks.</p><p>? Fusion Retriever (Iterative): We use a sparse retriever to directly retrieve the top-15 fused blocks based on bag-of-words BM25 score, and then split it into individual table segments and passage blocks. Since passage could be associated with multiple fused blocks, we merge duplicate blocks and use their summed score. Finally, we rank each block based on its merged retrieval score and truncate the first 4096 subword tokens for the next step.</p><p>? Fusion Retriever (Iterative): We use a dual-encoder dense retriever to directly retrieve the top-15 fused blocks, and then follow the same procedure as above. Without specifying the dense retriever uses ICT for pre-training by default.</p><p>? Fusion Retriever w/o ICT and w/o GPT-2: these two ablation studies are aimed to show the effectiveness of our proposed ICT pre-training and query augmentation.</p><p>The main results are presented in <ref type="table">Table 1</ref>. First, we can observe that best HYBRIDER top-2 can only achieve a comprised exact match of 9.9% while the oracle HYBRIDER can obtain a score of 44%, which reflects the difficulties of the hybrid retrieval in our dataset. We restrain the retriever to only retrieve table and text to answer the questions and report their results in <ref type="table">Table 1</ref>, even with the strong cross-block reader, the model only obtains 10% EM. These experiments demonstrate the necessity to integrate information from both forms in OTT-QA.</p><p>By combining the standard iterative retriever and single-block reader, we can slightly improve the score can to roughly 10%. By replacing the iterative retriever with the proposed sparse fusion retriever, the EM score can reach 14%, a 4.5% absolute improvement. By replacing the single-block with the proposed cross-block reader, the EM score can reach 17% , a 7% absolute improvement. However, by combining the two strategies, the final EM score can reach 28%, with an 18% absolute improvement, which is greater than the sum of individual improvements. The observation suggests the two components can affect each other in a positive way. We conjecture that the fusion retriever is more likely to retrieve mutually-supportive blocks in a group, which makes the multi-hop reasoning across different blocks easier for the following cross-block reader. In comparison, the iterative retriever retrieves isolated table segments and passages separately, which can easily miss out on the bridging evidence for building the complete reasoning chain. Thus, the cross-block reader cannot maximize its advantage in reasoning across blocks.</p><p>By removing the ICT pre-training and query augmentation, we observe that the Dev-EM score drops to 24.6%. By removing the GPT-2 query augmentation, the Dev-EM performance drops to 22.1%. These two results indicate the effectiveness of the proposed two strategies. By replacing the predicted hyperlinks with the oracle links, the fusion model performance can increase by 7% EM. This indicates that there is still plenty of room to improve for the  We consider the following linking models: a) BM25 model, which directly uses the cell value to retrieve passages based on their titles without query augmentation, b) a Dual-Encoder model, which encodes the cell value and meta information into a query vector to compute dot-product over all the passage candidate to retrieve, c) a GPT-2 model, which first augments the cell value by the context  <ref type="table">Table 2</ref>: HotpotQA Results. We conduct experiments on dev-set of HotpotQA, and then compare with the state-of-the art models with similar model size.</p><p>and then uses BM25. We demonstrate our findings in <ref type="figure">Figure 5</ref>, and evaluate with table-segment-wise F1 score. We observe that directly using BM25 leads to compromised precision of 30.3%, which is mainly due to the lack of context information. By using a dual-encoder retriever, the precision can be improved to 42%. However, many table segments have either zero or multiple linked passages and can be better modeled by an auto-regressive retrieval process.</p><p>We use HITS@4K is used to measure the retriever performance, which indicates the chance of ground truth block existing in the retrieved 4096 subword tokens. The results are reported in <ref type="figure" target="#fig_1">Figure 6</ref>. We vary the steps of iterative retrievers to show the necessity of multi-hop retrieval in OTT-QA. We observe that the 1-step retrieval has the lowest recall because the answer block in OTT-QA normally has a lower lexical overlap with the query. Adding the second retrieval step can greatly improve the recall, but adding the third retrieval hop has very little impact. In contrast to the iterative retriever, the fusion retriever can consistently improve the performance over the iterative setting for both sparse and dense setting. The sparse setting can rise from 35.8% to 48.1% indicating the advantage of 'early' fusion. The dense retriever's improvement is more dramatic (from 27.2% to 52.4%). We believe this is because the iterative retriever heavily relies on noisy synthetic inference chain data, while the fusion retriever does not require such a fine-grained supervision signal, thus less prone to noise. To better understand the retriever, we conduct detailed error analysis in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Results</head><p>To further demonstrate our model's effectiveness on more general opendomain question answering, we also test on purely text-based multi-hop question answering dataset HotpotQA . In this dataset, we follow the same procedure to group passage individual blocks as grouped blocks offline. The retrieved blocks are fed to the cross-block readers to allow interaction between retrieved blocks. We demonstrate our results on the dev-set in <ref type="table">Table 2</ref>, where we compare against the other models using similar model size (BERT-base). As can be seen, our model is able to achieve competitive performance with these state-of-the-art models. However, our model is more efficient in a sense that our model does not require any additional re-ranking step over the retrieved blocks. Both Transformer-XH <ref type="bibr">(Zhao et al., 2019)</ref> and <ref type="bibr">RNN-Retrieval (Asai et al., 2019)</ref> require expensive re-ranking over a large amount of retrieved reasoning graphs to select the most salient documents for the next stage (reader). In contrast, our model directly feeds the retrieved blocks to reader, which greatly simplifies and accelerates the system.  <ref type="bibr" target="#b35">(Sun et al., 2016;</ref><ref type="bibr" target="#b5">Chakrabarti et al., 2020)</ref>. In <ref type="bibr" target="#b35">Sun et al. (2016)</ref>, the authors have proposed a pipeline framework to first detect the topic entity and then generate a candidate chain, finally ranking chains to predict the answer cell. In <ref type="bibr" target="#b5">Chakrabarti et al. (2020)</ref>, the authors investigate different similarity matching features to retrieve tables from the web. Our paper is significantly different from these two studies in two aspects: 1) the previous papers use private small-scale datasets while we collect a large-scale dataset and release it for public use, 2) the previous studies are restricted to only using tables as evidence, while our paper considers a more realistic and challenging setting with both table and text corpus. Tables have been a ubiquitous information representation form to express semi-structured information. There has been a long-standing effort to utilize tables in natural language processing applications <ref type="bibr" target="#b27">(Pasupat &amp; Liang, 2015;</ref><ref type="bibr">Zhong et al., 2017;</ref><ref type="bibr" target="#b39">Yu et al., 2018;</ref><ref type="bibr" target="#b26">Parikh et al., 2020;</ref>. However, these existing tasks are restricted to in-domain cases without requiring any retrieval, and our paper is the first to investigate retrieving web tables for downstream tasks. Another pair of related works are TAPAS <ref type="bibr" target="#b15">(Herzig et al., 2020)</ref> and TABERT <ref type="bibr" target="#b38">(Yin et al., 2020)</ref>, which investigate joint pre-training over textual and tabular data. Our method draws inspiration from these models, and also uses special tokens and embeddings to encode spatial and logical operations inside tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Long Range Transformer: Recently, many transformer variants to resolve the O(n 2 ) attention cost have been proposed including Sparse Attention , Reformer <ref type="bibr" target="#b19">(Kitaev et al., 2020)</ref>, Routing Transformer <ref type="bibr" target="#b32">(Roy et al., 2020)</ref>, Longformer <ref type="bibr" target="#b3">(Beltagy et al., 2020)</ref> and ETC <ref type="bibr" target="#b1">(Ainslie et al., 2020)</ref>. These different transformer models apply hierarchical architecture, local-sensitive hashing, global-local state to decrease the attention complexity to nearly linear. Our cross-block reader is based on ETC <ref type="bibr" target="#b1">(Ainslie et al., 2020)</ref>, but unlike prior works that process one long document for QA, our task requires reading multiple blocks containing both structured and unstructured data.</p><p>To handle the long sequence of retrieved documents in open-domain question answering, Fusionin-Decoder <ref type="bibr" target="#b16">(Izacard &amp; Grave, 2020)</ref> has been proposed to replace the extractive model with an encoder-decoder generative model. The long sequence of passages are split and encoded independently to decrease the computation complexity, but the decoder still uses full attention over the tens of thousands of encoded vectors to generate the answer token by token. Such full-attention can decrease the decoding speed by an order of magnitude, while our sparse-attention-based cross-block reader can still maintain the same speed as the standard BERT model. Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DATASET COLLECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DATASET ANNOTATION</head><p>Filtering The original HybridQA dataset contains over 72k questions paired with 13k hyperlinked tables. We adopt two filtering heuristics to make the decontextualization easier. First, we filter out tables without enough meta-information or containing too much non-textual information 3 . Second, we filter out overly-long questions, i.e., questions longer than 30 words. These two filtering heuristics result in a cleaner subset of 46k questions paired with 9k in-domain tables.</p><p>Quality Control During annotation, we conduct strict manual quality evaluation over the decontextualized questions, with the following criteria: 1) the annotated question retains the same semantics and answer as before, 2) the annotated question still requires multi-hop reasoning over both table and passages, and 3) the annotated question is concise and fluent. The manual quality checking was performed over batches distributed to the same annotator. Each batch consists of six questions, one of which will be sampled to decide the acceptance/rejection of the whole batch. The overall acceptance rate for the crowd-sourcing job is 71%, and a rejected job was re-distributed until it was accepted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DATASET EXAMPLES</head><p>We demonstrate more examples in <ref type="figure" target="#fig_2">Figure 7</ref>, which includes more diverse inference chains, like </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lebron James Career Statistics</head><p>The 2019-20 NBA season is the 74th season of the National Basketball Association (NBA). The season was suspended by COVID-19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia Tables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2010</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COVID-19 Pandemics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Career Statistics</head><p>Lebron James Career 2010 Retrieva l</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19-20 NBA Best Players</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2019-2020</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics of 19-20 NBA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia Passages</head><p>The 2018-19 Los Angeles Lakers season was the franchise's 71st season, its 70th season in the National Basketball Association (NBA), and its ? Q1: How many points per game did Lebron James get in the COVID-19 NBA Season?</p><p>A1: COVID-19 -&gt; 19-20 Season -&gt; 25.3 Q2: Who is the coach of the team that Lebron James played in to achieve his highest score in his career? A2: 27.5 -&gt; Cleveland -&gt; J. B. Bickerstaff We randomly sampled 100 questions from the dataset to manually analyze the kinds of inference chains seen in OTT-QA and divide the major types into the following categories:  units together and represent them as the lower part demonstrated in <ref type="figure" target="#fig_3">Figure 8</ref>. We add [SEP] tokens to separate different passages and set their type id to 0. Such a flattened representation for fused block b F will be used throughout our experiments for both sparse/dense retriever and ETC reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ITERATIVE RETRIEVER</head><p>Iterative retrieval has been used in recent graph-based multi-hop retrieval models to gradually retrieve documents to find the correct supporting evidence. Specifically, the retriever conditions the i-th round retrieval on the previous round of retrieval results.</p><p>Sparse Retriever The sparse retriever uses uni-gram lexical feature to compute the BM-25 score between the q, .., b 1..j?1 over b j ? B to get the top candidates. Here we describe a two-step retrieval procedure, called here the LxM procedure. In the first step, the model calculates the BM25 score between question over all the candidates in B to select top L/2 table segments b T , and L/2 passages b P . In the second step, the question is concatenated with the retrieved table segment to form L/2 new queries [q; b T ] which are used to retrieve LM/2 passages from B. The question is also concatenated with the retrieved passage titles to form another K/2 queries [q; b P ] to retrieve LM/2 table segments. The retrieval procedure results in at most LM+L unique blocks. Each unique block aggregates its score from two rounds, denoted as f (q, b), which is used to rank the top-K candidates for the next step. We truncate the top-K candidate by thresholding their combined length.</p><p>Dual-Encoder Retriever The dual encoder uses a BERT-based encoder to compress each question, table segment, and passage into a fixed-length vector and then computes the dot product between fixed vectors to obtain the highest scored candidates from pool B. However, since the dataset does not provide an explicit supervision signal for the iterative retrieval, we heuristically synthesize some noisy retrieval chains using lexical matching. The retrieval inference chain is depicted as b 1 ? b 2 ? b K , which is used to train the model f (b k |q, b 1..k?1 ) in a supervised manner. At inference time, the dual encoder retriever will encode a query q into a fixed vector and retrieve the first L blocks from B. The blocks are appended to query q to form L new queries [q; b i ], which is re-encoded and search for LM new neighbors. We experiment with a maximum of 3-step retrieval of LxMxN to obtain a maximum of L+LxM+LMN unique blocks. Similarly, each unique block aggregates its score from different rounds to select the top-K candidates for the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 SPARSE FUSED RETRIEVER</head><p>The sparse fused retriever uses the uni-gram lexical feature to compute the BM-25 score between q over b F ? B F . The uni-gram feature of b F is based on the representation depicted in subsection B.1. Note that this BM25 feature will be much more abundant than the BM25 feature in iterative sparse retriever because it encloses more uni-grams. Instead of doing multiple rounds of retrieval, the fused retrieval once retrieve once over the candidate pool and treat all the units inside the block as the same retrieval score. Finally, We truncate the top-K candidate by thresholding their combined length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 QUERY AUGMENTATION</head><p>The query augmentation procedure is depicted in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lebron James Career Statistics</head><p>Year  <ref type="figure">Figure 9</ref>: Fusion: 1) GPT-2 query augmentation, 2) nearest neighbor search over passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 DENSE RETRIEVAL/IN-BATCH NEGATIVE</head><p>Recently, different dense-retrieval methods <ref type="bibr" target="#b14">Guu et al., 2020;</ref><ref type="bibr" target="#b18">Karpukhin et al., 2020)</ref> based on dual-encoders <ref type="bibr" target="#b4">(Bromley et al., 1994)</ref> have been shown to surpass traditional sparse retrieval in open-QA models. The query and the passage are both encoded using a Transformer, which produces a vector for every token. As in , the vector corresponding to the first token, <ref type="bibr">[CLS]</ref>, which is used as a "pooled" representation of the sequence (denoted BERT CLS ). The dense retrieval function can be represented as the dot product between BERT CLS (q) and BERT CLS (p) for each document in the text collection, much like TF-IDF <ref type="bibr" target="#b6">(Chen et al., 2017)</ref>  We measure our best model's performance (dense fusion retriever + cross-block reader) and baseline model (dense Iterative-Retriever + single-block reader) on different question types (subsection A.3) to show the breakdown statistics in <ref type="figure">Figure 10</ref> and <ref type="figure" target="#fig_4">Figure 11</ref>. As we can observe, the gap between our model vs. baseline in 1-hop question is less significant as 2-hop and 3-hop questions. The iterative retriever's performance is sensitive to the number of hops in the question, which is the largely due to the error propagation in the beam search stage. If the retriever fails to include the golden block in the earlier stage beam, the retrieval in later stage cannot recover from such failure. In contrast, our fusion retriever can group the related information prior to retrieval to retrieve all the blocks at once, which makes the model less prone to the error propagation issue. Another reason is due to the cross-block reader, which can reason over different blocks in the latent space, such implicit reasoning can also decrease the error propagation issue. To sum up, our model is more powerful to deal with complex multi-hop open questions with much less performance drop. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 RETRIEVER ERROR ANALYSIS</head><p>We conduct error analysis to see what are the major issues with the retriever and conclude the following types in <ref type="figure">Figure 12</ref>. The major issues causing the system to retrieve unrelated evidence are low lexical overlap, fusion errors, numerical reasoning and distracting passages or tables. In the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reason Example Groundtruth Block</head><p>Low Lexical Overlap Where is the NYU Alumni from 1980 who now serves as ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breakdown of Errors</head><p>Retriever Error Reader Error Correct <ref type="figure">Figure 12</ref>: The main error types in the retriever.</p><p>Low-Lexical-Overlap case, the errors are mainly coming from the abbreviation, rephrasing of the table metadata, for example, 'New York University' is shortened as 'NYU', etc. In the Fusion-Error case, the issue is mainly because the entity-linking model fails to fuse all the hyperlinked passages, the error (F1=50%) is quantitatively reflected in the entity-linker-performance figure. Numerical-Reasoning error is mainly related to the failure to find max/min/earliest/latest row in the table. The distraction error is mainly caused by some distracting passages or tables having very similar information. We sample 50 error samples from the dev-set and attribute their errors to the above categories. As shown in the left part, we found that the numerical reasoning error is not as severe as the other three types because the proportion of questions requiring it is relatively small. Besides the low-lexical overlap error, which is general across other open QA datasets like NQ and HoptpotQA, we found the fusion and distraction errors quite specific in our dataset.</p><p>? (Fusion) Questions which ask about tables which are linked to too many linked passages.</p><p>For example, a question over table "Team Record" in https://en.wikipedia.org/ wiki/Sevens_Grand_Prix_Series is hard because some table rows associate with over 10 passages, it's hard to link them and fuse all of them into a fused block. ? (Distraction) Questions which ask about topics which are contained by too many similar tables, it's hard to differentiate the true one. For example, there are over ten tables in https://en.wikipedia.org/wiki/List_of_RMIT_University_ people, these similar tables can easily distract the attention of the retriever to select the wrong one from the same page.</p><p>From our quantitative results, we can attribute the errors to retriever and reader, among all the examples, 49% of examples cannot find the correct supporting block. For the rest 51% examples with correct block retrieved, the reader fails to select the correct span for 23% of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 LENGTH SENSITIVITY ANALYSIS OF RETRIEVAL/READER</head><p>We perform sensitivity analysis for both retriever and reader in <ref type="figure" target="#fig_0">Figure 13</ref>. We gradually increase </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reader EM vs. Input Length Limit</head><p>Single-Block Cross-Block <ref type="figure" target="#fig_0">Figure 13</ref>: Analyzing retriever performance.</p><p>the length limit of retrieved evidence from 400 to 4096 to first visualize its impact on the sparse iterative and dense fusion retriever. For both fusion and iterative retriever, we can observe that both of their recall@K significantly improves as the length limit increases. With a low budget of token limit, their performance is gap is smaller because its performance is dominated by the singlehop questions in the dataset. As the length limit increases, the improvement for fusion retriever is steeper than iterative retriever because the contextualized fusion block becomes easier to retrieve than standalone table segment or passage.</p><p>We also visualize the input length's impact on the single-block The performance of single vs crossblock reader. With a low budget of token limit, both single-block and cross-block readers are comparable. However, as the limit increases to 4000, the cross-block reader can digest long input with its sparse attention mechanism to achieve better scores, while the single-block reader needs to truncate the information to read independently, which leads to a even lower EM score due to introduced noise. This observation reveals the importance of modeling cross-attention between different retrieved evidence units to reach a consistent answer. In single-block reader, dealing with different blocks independently can lead to suboptimal prediction in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D CONNECTION TO EXISTING WORK</head><p>KB and Text The problem combining structured and unstructured data has been studied in question answering. The previous approaches are mainly divided into two categories: 1) FusionNet and PullNet <ref type="bibr" target="#b33">(Sun et al., 2018;</ref>) simulate a KB-incomplete setting by masking out some triples from a knowledge graph and use textual information to complete the masked KB triples; these experiments are conducted on KB-based QA datasets. 2) DrKIT <ref type="bibr" target="#b12">(Dhingra et al., 2019)</ref> and Knowledge-Guided Retrieval <ref type="bibr" target="#b24">(Min et al., 2019)</ref> propose to use entity mentions and relations to guide the retrieval from the web. However, the KB is mainly used as an assisting tool, rather than a necessary information source. In OTT-QA, the structured data is used as necessary information in a realistic setting. The two information forms are combined in a non-trivial way, which makes the problem much harder than the other structure-unstructured QA settings.</p><p>Entity Linking Our generative entity linker is related to knowledge-enhanced language understanding , which proposes a seq2seq model to deal with different knowledge-intensive tasks like slot filling, entity linking, etc. There is a concurrent related work on auto-regressive entity linking , which also demonstrates the advantages of using an autoregressive generation model for entity retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Left: Iterative 3-step retrieval over individual blocks (baseline). Right: Fusion 1-step retrieval over fused groups, which greatly lowers the cost of iterative encoding and retrieving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Retriever performance (HITS@4K).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Q3:Figure 7 :</head><label>7</label><figDesc>What suspends the NBA season during which Lebron James has an average points per game of 25.3? A3: 25.3 -&gt; 19-20 Season -&gt; COVID-19 Q4:For the season suspended by COVID-19 and the season defeated by Warriors in Final, which season has Lebron obtained more points? The Cleveland Cavaliers, often referred to as the Cavs, are an American professional basketball team based in Cleveland. The Cavaliers compete in the National Basketball Association (NBA) as a member of the league's Eastern Conference Central Division. The team is coached by J.B. Bickerstaff. The 2017-18 NBA season was the 72nd season of the National Basketball Association (NBA). The regular season began on October 17, 2017. ended on June 8 with the Golden State Warriors defeating the Cleveland Cavaliers in the 2018 Final. A4: 25.3 &lt; 27.5 -&gt; 17-18 More examples from OTT-QA A.3 QUESTION TYPES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>The decomposition of the original table into segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 11 :</head><label>11</label><figDesc>Breakdown for fusion retriever + cross-block reader.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>NBA season was the 72nd season of the National Basketball Association (NBA). The regular season began on October 17, 2017. ended on June 8 with the Golden State Warriors defeating the Cleveland Cavaliers in the 2018 Final.</figDesc><table><row><cell></cell><cell cols="2">19-20 NBA Best Players</cell><cell cols="2">Lebron James</cell><cell>19-20 NBA</cell><cell>COVID-19 vaccine</cell></row><row><cell>Lebron James Career</cell><cell cols="2">2010 Statistics of 19-20 NBA</cell><cell></cell><cell>NBA</cell><cell cols="2">COVID-19</cell><cell>List of NBA teams</cell></row><row><cell></cell><cell cols="2">2019-2020</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Team Cleveland The 2017-18 Year 17-18 19-20 L.A. Lakers</cell><cell>Blocks Per Game 0.9 0.5</cell><cell>Points Per Game 27.5 25.3</cell><cell>Assists Per Game 9.1 10.2</cell><cell cols="2">3 Point % .367 .348</cell><cell>The 2018-19 Los Angeles Lakers Rebound Per Game 8.6 7.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>season was the franchise's 71st</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">season, its 70th season in the National</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Basketball Association (NBA)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>How many points per game did Lebron James get in the NBA Season suspended by COVID?</figDesc><table><row><cell cols="5">Ans: 25.3 Conf: 94% Ans: 30.1 Conf: 95% Q: A A B BERT BERT &lt; 512 B &lt; 512</cell><cell></cell><cell></cell><cell>Attention</cell></row><row><cell>C</cell><cell>&lt; 512</cell><cell>BERT</cell><cell>Ans: 0.9 Conf: 96%</cell><cell>C</cell><cell>Length &gt; 4K</cell><cell>ETC</cell><cell>Ans: 25.3</cell></row></table><note>table segments, which often have incomplete context by themselves. The early fusion process aims to fuse a table segment and relevant pas- sages into a group. Here we propose to fuse entities mentioned in a table segment to the appropri- ate passages for those entities; this is similar to document expansion based on a traditional entity</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table Retrieval :</head><label>Retrieval</label><figDesc>Tables are pervasive onthe Web, there have been some studies on mining web tables to answer open-domain questions</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062, 2020. Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul Bennett, and Saurabh Tiwary. Transformer-xh: Multi-evidence reasoning with extra hop attention. In International Conference on Learning Representations, 2019.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>table ?</head><label>?</label><figDesc></figDesc><table><row><cell>Year</cell><cell>Team</cell><cell>Blocks Per Game</cell><cell>Points Per Game</cell></row><row><cell>17-18</cell><cell>Cleveland</cell><cell>0.9</cell><cell>27.5</cell></row><row><cell>19-20</cell><cell>Lakers</cell><cell>0.5</cell><cell>25.3</cell></row></table><note>text; text ? table ? text; text + text ? comparative ? table. Our model is able to perform these reasoning types quite well by jointly matching a query against a fused table-text block.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>1. Single hop questions (13%) require reading one table or one passage to answer. 2. Two hop questions (57%) require reading one passage and one table to answer. These can be subclassified as 'table bridge' ? 'answer text' 4 or 'text bridge' ? 'answer table'. 3. Multi-hop questions (30%) require reading two passages and one table to answer. These mainly following the reasoning chain of 'text bridge' ? 'table bridge' ? 'answer text'.4. Questions with multiple reasoning paths: Due to information redundancy in Wikipedia, similar information can appear in both tables and text. We find that 9% of questions are answerable by reading one text passage, 18% of questions are answerable by reading two text passages and 4% of questions are answerable by reading two tables. The title/section title are prefixed to the table segment. We add the row position token '1st' and a max/min special token over the column to infuse global table information into the segmented unit. The column embedding is added as another vector to the representation. The table segment representation is relatively small and easy to deal with in the following reader model. After the table-passage alignment, we group the highly related</figDesc><table><row><cell cols="2">B MODEL DETAILS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">B.1 RETRIEVAL BLOCK REPRESENTATION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">The table decomposition is visualized in Figure 8. Lebron James Career Statistics</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Year</cell><cell></cell><cell>Team</cell><cell></cell><cell></cell><cell>Blocks</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2017</cell><cell></cell><cell cols="2">Cleveland</cell><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2018</cell><cell></cell><cell>Lakers</cell><cell></cell><cell></cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2019</cell><cell></cell><cell>Lakers</cell><cell></cell><cell></cell><cell>0.60</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[TAB]</cell><cell>[TITLE]</cell><cell>Lebron</cell><cell>James</cell><cell cols="2">Career</cell><cell cols="2">Statistics</cell><cell>[ROW]</cell><cell>1st</cell><cell>[MIN]</cell><cell>Year</cell><cell>is</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>2017</cell><cell>Team</cell><cell>is</cell><cell cols="2">Cleveland</cell><cell cols="2">[MAX]</cell><cell>Blocks</cell><cell>is</cell><cell>0.90</cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>and BM25<ref type="bibr" target="#b31">(Robertson &amp; Zaragoza, 2009)</ref> on some Open QA datasets. To train the dual-encoder, the in-batch negative trick<ref type="bibr" target="#b37">(Yih et al., 2011;</ref><ref type="bibr" target="#b18">Karpukhin et al., 2020)</ref> plays an important role, which uses B training instances in each batch and views the other B-1 instances inside the batch as the negatives. In this way, the model reuses computation and effectively trains on B 2 question/document pairs in each batch.</figDesc><table><row><cell>C PERFORMANCE ANALYSIS</cell></row></table><note>C.1 QUESTION TYPE BREAKDOWN PERFORMANCE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table :</head><label>:</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>New York University Alumni in Politics and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Science ?.</cell></row><row><cell>Error in Fusion</cell><cell></cell><cell cols="3">Who is the coach of Pittsburgh team in</cell><cell>Table: List of NCAA Division I</cell></row><row><cell></cell><cell></cell><cell cols="3">NCAA division I football team in 1980?</cell><cell>Cell: Pittsburgh</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fail to link to the passage: Pittsburgh Panthers</cell></row><row><cell cols="2">Numerical Reasoning</cell><cell cols="3">Who achieves the highest score in the Grand</cell><cell>Table: List of Grand Prix 1980s Men's ?</cell></row><row><cell></cell><cell></cell><cell cols="3">Prix 1980s Men's ?</cell><cell>The time is not in regular format, SQL operation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>cannot select the max/min row.</cell></row><row><cell>Distraction</cell><cell></cell><cell cols="3">Where of 1990 Grammy Award winner for</cell><cell>Table: Grammy Lifetime Achievement Award</cell></row><row><cell></cell><cell></cell><cell cols="2">? come from in?</cell><cell>Many tables from are about general "Grammy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Awards", the groundtruth table only differentiate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>from the others a little bit.</cell></row><row><cell></cell><cell cols="4">Retriever Error Breakdown</cell></row><row><cell>40%</cell><cell></cell><cell></cell><cell>36%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>32%</cell></row><row><cell>30%</cell><cell>24%</cell><cell></cell><cell></cell><cell>28%</cell></row><row><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell>49%</cell></row><row><cell>10%</cell><cell></cell><cell>8%</cell><cell></cell><cell>23%</cell></row><row><cell>0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Retriever</cell></row><row><cell cols="2">Lexical</cell><cell>Numerical</cell><cell>Fusion</cell><cell>Distractoin</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The collection is conducted on an established crowdsourcing platform with annotators from countries with English as the native language. The annotators were required to meet the requirements: 1) a native speaker in an English-speaking country 2) having an approval rate of over 95% and 3) having at least 500 approved jobs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Like longitude, latitude, mathematical formulae, etc 4 I.e., a table forms a bridge between a question and a textual passage, which is read in the first hop, and the answer is extracted from the text.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We focus on the problem of performing open question answering over tables and text in this paper. By proposing the fusion retriever and sparse reader, we manage the increase the model's effectiveness and efficiency by a large margin. One interesting question we would like to ask in the future is:</p><p>can we extend open question answering system to more modalities like images or audios, etc?</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Etc: Encoding long and structured data in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2020</title>
		<meeting>EMNLP 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03272</idno>
		<title level="m">Open domain question answering using web tables</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tabfact: A large-scale dataset for table-based fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hybridqa: A dataset of multi-hop question answering over tabular and textual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Findings of EMNLP 2020</title>
		<meeting>Findings of EMNLP 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00904</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Autoregressive entity retrieval. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Differentiable reasoning over a virtual knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidhisha</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cognitive graph for multi-hop reading comprehension at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Realm: Retrievalaugmented language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2020</title>
		<meeting>ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe? Krzysztof</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
		</author>
		<title level="m">Tapas: Weakly supervised table parsing via pre-training. ACL 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Knowledge guided text retrieval and reading for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03868</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revealing the importance of semantic retrieval for machine reading at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2553" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14373</idno>
		<title level="m">Totto: A controlled table-to-text generation dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02252</idno>
		<title level="m">Vassilis Plachouras, Tim Rockt?schel, et al. Kilt: a benchmark for knowledge intensive language tasks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Answering complex open-domain questions through iterative query generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Mehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2590" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1455</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1455" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="4231" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PullNet: Open domain question answering with iterative retrieval on knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1242</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1242" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="2380" to="2390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Table cell search for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="771" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning discriminative projections for text similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifteenth conference on computational natural language learning</title>
		<meeting>the fifteenth conference on computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Tabert: Pretraining for joint understanding of textual and tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingning</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanelle</forename><surname>Roman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3911" to="3921" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
