<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint 3D Human Shape Recovery and Pose Estimation from a Single Image with Bilayer Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
							<email>xiny@cs.utah.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Van Baar</surname></persName>
							<email>jeroen@merl.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
							<email>schen@merl.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah Salt Lake City</orgName>
								<address>
									<settlement>Utah</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint 3D Human Shape Recovery and Pose Estimation from a Single Image with Bilayer Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to estimate the 3D human shape and pose from images can be useful in many contexts. Recent approaches have explored using graph convolutional networks and achieved promising results. The fact that the 3D shape is represented by a mesh, an undirected graph, makes graph convolutional networks a natural fit for this problem. However, graph convolutional networks have limited representation power. Information from nodes in the graph is passed to connected neighbors, and propagation of information requires successive graph convolutions. To overcome this limitation, we propose a dual-scale graph approach. We use a coarse graph, derived from a dense graph, to estimate the human's 3D pose, and the dense graph to estimate the 3D shape. Information in coarse graphs can be propagated over longer distances compared to dense graphs. In addition, information about pose can guide to recover local shape detail and vice versa. We recognize that the connection between coarse and dense is itself a graph, and introduce graph fusion blocks to exchange information between graphs with different scales. We train our model end-toend and show that we can achieve state-of-the-art results for several evaluation datasets. The code is available at the following link, https://github.com/yuxwind/ BiGraphBody.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recovering 3D human shapes and poses from 2D images is a fundamental task for numerous real-world applications, such as animation and dressing 3D people <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. Some recent approaches restrict themselves to only estimate 3D poses <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref>, while other approaches need multiple * Work mainly done when Xin Yu was an intern at MERL. ? Corresponding author.  <ref type="figure">Figure 1</ref>: By associating the mesh graph to the input image with a skeleton graph, the bilayer graph structure will shorten the paths between remote mesh nodes (1723 nodes here), when we connect a joint with the mesh nodes it controls. With the body parts are correlated, such as the ankle and the wrist, this bilayer graph implicitly learn the interaction between joints and mesh vertices and further shorten the path among the remote body mesh vertices.</p><p>images to achieve reliable shape recovery <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>. Here we consider joint 3D human shape recovery and pose estimation from a single image.</p><p>As an undirected graph, a 3D mesh can represent a human shape, making graph-based techniques a natural fit to this task. For example, graph CMR <ref type="bibr" target="#b24">[25]</ref> deforms a template human mesh in a neutral pose to a desired shape through a graph convolutional network. Graph convolutional layers then propagate the node features over the mesh. However, this mesh-graph only approach suffers from the issue node feature propagation will be extremely slow when the mesh has dense vertices, such as 1723 nodes used for human in general. We illustrate this limitation of mesh based graph in <ref type="figure">Fig. 1</ref>. The recent work <ref type="bibr" target="#b28">[29]</ref> use transformer to reduce the distance between any two nodes to 1 via self-attention mechanisms. However, self-attention over a down-sampled 423 mesh nodes is still not efficient; positional encoding may maintain the base coordinates information in the sequential ordering of the mesh nodes, still ignores the structured correlations between body parts.</p><p>To resolve the above issues, we propose a bilayer graph structure, where one layer is a mesh graph for human shapes, and the other layer is a newly added skeleton graph for body joints. As shown in <ref type="figure">Fig. 1</ref>, the newly added skeleton graph can associate the 2D body joints estimated from an image with their coordinates in 3D space. <ref type="bibr" target="#b0">1</ref> This body-joint-based correspondence allows us to attach detailed local image features to each body joint in the skeleton graph. In previous mesh-only graph approaches, as shown in <ref type="figure">Fig. 1a</ref>, the ankle and wrist nodes in the mesh graph can only connect to each other via multiple iterations of aggregation-and-combine operations in GCN. Image feature propagation will be extremely slow when the mesh template has 1723 nodes. This bilayer graph structure (see <ref type="figure">Fig. 1b</ref>) use sparse skeleton graph to guide the mesh nodes to exchange information in a more efficient way. It further shortens the paths between remote mesh nodes when connecting them via joints. We thus leverage the spatial nonlocality of the mesh graph.</p><p>An added benefit of this two-layer graph structure is multi-tasking: achieving shape recovery and pose estimation at the same time. Two layers naturally model a human body from mesh and skeleton scales, handling shape recovery and pose estimation, respectively. The cross or fusion layer is a trainable bipartite graph that connects body joints and mesh nodes. Instead of imposing any fixed connections, such a bipartite graph can adaptively adjust the relationships between the mesh nodes and body joints. It enables the feature fusion between two scales of a human body, mutually enhancing two tasks. This is related to linear blend skinning <ref type="bibr" target="#b21">[22]</ref>, however, where skinning provides an analytical transformation, our cross layer learns a data-adaptive transformation between body joints and mesh nodes in the highdimensional feature space.</p><p>In summary, our main contributions are:</p><p>? We are the first to propose a neural network based on a two-layer graph structure that jointly achieves 3D human shape and pose recovery. The skeleton graph module propagates pose (coarser-scale) information, the mesh graph module propagates detailed shape (finer-scale) information, and the fusion graph module allows us to exchange information across the two modules.</p><p>? We propose an adaptive graph fusion block to learn the trainable correspondence between body joints and mesh nodes, promoting information exchange across two scales.</p><p>? We validate our method on several datasets (H36M, 1 2D body joints can be well estimated from an image, e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>UP-3D, LSP), and show that exchanging local and global image information from different scales provides a significant improvement and speedup over single graph methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human Shape Recovery Over the years there have been many approaches to recover 3D human shapes from images. Several methods propose to recover clothed humans from either single or multi-view images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53]</ref>. These approaches rely on well segmented humans in the images, and do not emphasize accurate 3D shape and pose. Our goal instead is to capture the 3D shape and pose accurately without relying on any prior segmentation. Other methods rely on the video input to recover 3D human shapes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>. While Our goal is to recover accurate 3D shape and pose from a single image only. To handle the alignment issue between neutral and deformed poses, <ref type="bibr" target="#b4">[5]</ref> proposes an optimization procedure to iteratively refine the estimate. The authors in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33]</ref> introduce a sequential and iterative approach from 2D poses. In this work, we propose a trainable two-layer graph structure to resolve the alignment issue which does not require iterations.</p><p>Graph CNNs for 3D Reconstruction Recently, graph convolutional neural networks (GCNs) have been used to recover 3D objects from images. In this method, objects are represented as meshes <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. Meshes are the de facto representation of 3D objects in computer graphics, and a mesh can be considered an undirected (3D) graph. The initial mesh before refinement may be a mesh obtained from a volumetric estimation <ref type="bibr" target="#b10">[11]</ref>. The authors in <ref type="bibr" target="#b8">[9]</ref> state the limitations of GCN for 3D pose estimation. To overcome this limitation, they propose learnable weights for the structure of the graph. To address the limited representation capability of GCN, we propose a two-layer-graph neural networks with adaptive edge weights to share information between two graph layers. Our work is an extension to the regression based approach called Graph CMR <ref type="bibr" target="#b24">[25]</ref>. The input to Graph CMR is a human mesh in neutral pose along with global image features. Graph CMR then relies solely on graph convolutions to propagate information between nodes, and finally provide a 3D estimate. The mesh is refined by estimating SMPL parameters. We propose a two-layer graph to more efficiently propagate information. Multi-scale graphs have been explored in some other applications. The authors in <ref type="bibr" target="#b11">[12]</ref> use multiple graph scales and exchange information via connectivity between different scales for the purpose of human parsing. The authors in <ref type="bibr" target="#b27">[28]</ref> use multi-scale graphs for the purpose of joint human action recognition and motion prediction. The information between scales is fused according to feature replication and concatenation between the different graphs. In comparison, our fusion approach relies on a learnable graph adjacency matrix, which exchanges information between two body scales.  <ref type="figure">Figure 2</ref>: Our proposed bilayer graph architecture. Given an input image, the mesh graph module (Mesh-GCN) is a regression which outputs 3D vertex coordinates, and the skeleton graph module (Skeleton-GCN) estimates a skeleton with twelve 3D joint locations. The input to the Mesh-GCN is a template mesh together with a global perceptual feature extracted using a CNN, from the bounding box around the person in the image. Each global perceptual feature is attached to the XYZ coordinates of the vertices in the mesh. For clarity, we omit the SMPL part. The input to each joint node in the Skeleton-GCN is a local perceptual feature extracted from the image regions around the 2D joints estimated by HRNet. The two modules exchange information via so-called fusion graph, which is a bipartite graph between all mesh nodes and joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>As shown in <ref type="figure">Fig. 1</ref>, to resolve the issues of lacking detailed local information and inefficient long-range interactions, we use a bilayer graph structure to jointly estimate a 3D human pose and recover a complete 3D mesh based on a single input RGB image (without knowing camera parameters). Mathematically, let I ? R H?W ?3 be an RGB image with the height H and the width W . Both 3D human pose and 3D mesh structure can be represented as a graph with a set of node coordinates and an adjacency matrix indicating their connecting relations. For the 3D pose, we denote the skeleton pose coordinates as V s ? R Ns?3 , where N s is the total number of body joints, thus the corresponding skeleton adjacency matrix is A s ? R Ns?Ns . For the 3D mesh structure, we denote the mesh node coordinates as V m ? R Nm?3 , where N m is the number of mesh nodes. Then the adjacency matrix for the mesh structure is A m ? R Nm?Nm . We aim to propose a model F(?):</p><formula xml:id="formula_0">V s , V m = F(I, A m , A s ),<label>(1)</label></formula><p>to estimate human pose V s and the recovered human mesh V m , which precisely approximate the targets V s , V m , respectively. This joint task naturally requires to model a human body at two scales: a sparse graph at the skeleton scale and a dense graph at the mesh scale. To explicitly model the vertex correlations at two scales, we introduce a fusion graph to learn how the joints control the deformation of the body mesh vertices, and vice versa. Thus we propose a two-layer graph structure that consists of a</p><formula xml:id="formula_1">skeleton graph G s (V s , A s ), a mesh graph G m (V m , A m ) and a fusion graph G f (V s ? V m , A f ), where A f ? R (Nm+Ns)?(Nm+Ns)</formula><p>is the adjacency matrix for the fusion graph. Note that A s , A m are fixed and given based on the human body prior; see the predefined graph topology in <ref type="figure">Fig. 1</ref>; while A f is data-adaptive during training.</p><p>The graph-based formulation makes Graph CNN (GCN) a natural fit for this task. We propose a Bilayer-Graph GCN to address this task effectively and efficiently. As a core of the proposed system, it brings two benefits: First, it naturally models a human body from both mesh and skeleton aspects, promoting local and non-local topology learning, which will speedup the convergence of training and improve the joint pose and shape recovery. Second, a fusion graph enables information exchange between two scales of a human body, mutually enhancing feature extraction at two scales and further improving the performances in two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Two-scale Graph Neural Network</head><p>To model the two-scale skeleton and mesh graph, and a fusion graph connecting them, we propose a bilayer graph neural network. <ref type="figure">Fig. 2</ref> shows an overview of the this architecture. In this section, we introduce the detailed implementation of each building block in our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture Overview</head><p>As shown in <ref type="figure">Fig. 2</ref>, given a single input image, an image encoder will be firstly used to extract the features from it, and a 2D-pose detector will processes the image into a skeleton graph. Then on the top part, skeleton graph module attaches local joint features, and propagate the features in skeleton graph layer. While at the bottom part, mesh graph module attach the global image features and models the mesh graph layer. Between them, fusion graph module connects between all the skeleton joints and mesh nodes, and exchange dual-scale information in a structured way. Finally, the learned joint and mesh node representation will be used to regress the 3D pose and mesh coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Encoder</head><p>The functionality of an image encoder module is to extract informative visual features from an RGB image, which would be the input for the subsequent modules. Given an input RGB image I, we use a multi-layer CNN to obtain a collection of intermediate image features from the output of each layer l, {X</p><formula xml:id="formula_2">( ) im } L =1 = F im (I), where F im (?)</formula><p>is a CNN whose architecture follows ResNet50 <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Bilayer Graph Module</head><p>We propose to employ Graph CNN to jointly regress the 3D coordinates of the mesh and skeleton vertices. It consists of three sub-graphs: the mesh graph module, the skeleton graph and the fusion graph module. For each sub-graph, we employ the same basic graph convolutions to formulate them <ref type="bibr" target="#b22">[23]</ref>, which is defined as:</p><formula xml:id="formula_3">X +1 = AX W ? R N ?d +1<label>(2)</label></formula><p>where indicates the -th convolutional layer, A ? R N ?N is a graph adjacency matrix for the (sub)graph, W ? R d ?d +1 is a convolution weight matrix, X ? R N ?d is the input feature vector. As shown in Eq. <ref type="formula" target="#formula_3">(2)</ref>, given a set of vertices initialized with input features (X 0 ) and their adjacency matrices (A), the graph convolution layer allows feature propagating and updating over the (sub)graph so that each vertex can aggregate information from its neighbors. In the following, we will introduce the functions of the three sub-graphs separately, and comparatively study the input and adjacency matrix of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Mesh Graph Module</head><p>The functionality of a mesh graph module is to regress the posed 3D mesh conditioned on the input image features. It is identical to graph layers of GraphCMR <ref type="bibr" target="#b24">[25]</ref>. We start from the template mesh in neutral (T-) pose introduced by SMPL and deform them to the shaped and posed mesh with the graph convolutions.</p><p>Inputs We employ the template coordinates as the position embedding of the mesh vertices, and attach it with the 2048-D global feature vector of ResNet-50 <ref type="bibr" target="#b13">[14]</ref> to feed in the mesh graph module. Let x g im ? R Dg be the global image feature after the average pooling layer and v T m,i ? R 3 is the 3D coordinate of a i-th template mesh vertex. For each mesh vertex, we have an initialized feature defined as:</p><formula xml:id="formula_4">X 0 m,i = F 0 m (v T m,i ? x g im ) ? R d0<label>(3)</label></formula><p>Where ? denotes feature vector concatenation and F 0 m denotes the linear layer to reduce the dimension (the typical reduced dimension is 512) of the concatenated features whose weights shared among all mesh vertices. Mesh adjacency matrix A m is initialed as a binary matrix to indicate the connectivity among the vertices as shown in <ref type="figure">Fig. 1</ref> and further row-normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Skeleton Graph Module</head><p>The functionality of a skeleton graph module is to lift the 2D pose estimated from an input RBG image to a 3D pose. As shown in <ref type="figure">Fig. 1b</ref>, the sparse skeleton graph can promote the non-local topology features of the dense mesh graph and enhance the correlations between different body parts. Furthermore, we extracted the local features around the joints for precise pose. Inputs Instead of using global image features for all the joint nodes, we use joint-aware local features for joint nodes. Given the image, we use HRNet <ref type="bibr" target="#b41">[42]</ref> off-the-shelve to estimate the 2D positions of body joints in this image. For each body joint, we crop a patch centered at the estimated 2D positions with the size of the average estimated bone length from the joints per image, using RoI Align <ref type="bibr" target="#b12">[13]</ref> from the k-th image feature map (K layers in total) x k im of ResNet-50 <ref type="bibr" target="#b13">[14]</ref>. This feature patch reflects the local visual information around the corresponding body joint. We concatenate the image feature patches with the positional embedding as the initial skeleton features as: <ref type="formula">(4)</ref> where i-th body joint estimated by HRNet as s i ? R 2 , RoI(?) returns image feature patches from x k im using RoI Align <ref type="bibr" target="#b12">[13]</ref> with the patch centered at s i . Similarly, F 0 s is a linear layer and share the weights among the skeleton vertices. We also experimented with the skeleton template coordinates as the positional embedding but we did not observe quantitative improvement in the results and thus keep the 2D embedding for all experiments. Skeleton adjacency matrix We use fixed adjacency matrix for A s . The element is initialized as the reciprocal of the Euclidean distance between two template joint vertices.</p><formula xml:id="formula_5">X 0 s,i = F 0 s (v s,i ? RoI(v s,i , x 1 im , . . . x K im )) ? R d0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Fusion Graph Module</head><p>The functionality of a fusion graph is to correlate the sparse skeleton graph and the dense mesh graph and enable information exchange between them and mutually enhance both tasks of 3D shape recovery and pose estimation. As shown in <ref type="figure">Fig. 1b</ref>, the fusion graph connection can shorten the path of two remote mesh vertices dramatically and will speedup the non-local information propagation of the mesh graph. Inputs The fusion graph consists of the vertex from both the Mesh Graph and Skeleton Graph and applies the same initial feature as those two sub-graphs. The intermediate input of this module are the intermediate features from the Skeleton Graph and Mesh Graph, which fuses those intermediate features and populates them back to both Skeleton-GCN and Mesh-GCN. Fusion adjacency matrix To fuse features from the skeleton and mesh graph, we leverage a trainable fusion graph to reflect the data-driven connectivity between body joints and mesh nodes. Besides defining a fixed connection part, denoted as A f,s , we allow an extra dynamic connection, denoted as W f , to be trainable to capture the connectivity in the hidden feature space in a data-driven manner. We define the final adjacency matrix as:</p><formula xml:id="formula_6">A f = RowNorm(A f,s W f )<label>(5)</label></formula><p>where A f,s , W f ? R (Nm+Ns)?(Nm+Ns) , RowNorm() indicate a row normalization, denotes element wise product. W f is learnable and its element is initialized as 1 for vertex-joint correlation and 0 for joint-joint and vertexvertex (the Skeleton and Mesh Graphs have cover those connections). The element of A f,s for a connection between a skeleton vertex and mesh vertex is fixed to the reciprocal of their Euclidean distance; otherwise, it is zero. We also experiment the RowNorm() with a softmax on each row and apply additive optation between A fs and W f , both of which bring minor change to the performance. We adopt the row normalization and element-wise product for their simplification of computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Architecture Implementation</head><p>Bilayer-Graph Block At the heart of this approach, we propose a Bilayer-Graph block as an elementary computational unit for feature learning and propagation based on the bilayer-graph structure. As illustrated in <ref type="figure">Fig. 3</ref>, the Skeleton-GCN Block, Fusion-GCN Block and Mesh-GCN Block apply graph convolutions on skeleton graph, fusion graph and mesh graph respectively. The Fusion-GCN Block collects features from both Mesh-GCN and Skeleton-GCN and distributes the updated features back to the other module. Each block consists of a sequence of a graph linear layer, a graph convolution layer, and another graph linear layer, with a residual connection from the input directly to the output of this block. Each layer follows a group normalization layer <ref type="bibr" target="#b47">[48]</ref> and ReLU. Please note that the graph linear layer is a special graph convolution layer, which simply substitutes the graph adjacency matrix A in the graph convolution layer ( see Eq. (2)) to an identity matrix.</p><p>In this network, we stack five Bilayer-Graph blocks for feature propagation, followed by two graph linear layers to regress the skeleton vertices and joint vertices separately. The first linear layers also follows a group normalization <ref type="bibr" target="#b47">[48]</ref> and ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Linear Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolution Layers</head><p>Graph Linear Layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Linear Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolution Layers</head><p>Graph Linear Layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Linear Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolution Layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Linear Layer</head><p>x N</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton-GCN Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion-GCN Block</head><p>Mesh-GCN Block <ref type="figure">Figure 3</ref>: A Bilayer-Graph Block consists of a Fusion-GCN block, a Mesh-GCN block and a Fusion-GCN block. The fusion block depicted here has its input from the previous Skeleton-GCN and Mesh-GCN blocks prior to this Bilayer-Graph block, and adds its output back to each branch after their respective blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMPL regressor</head><p>As the parametric representation of the human body can be very useful for down-stream tasks (e.g., body manipulation), we follow <ref type="bibr" target="#b24">[25]</ref> to train a MLP module to regress pose ( ?) and shape ( ?) parameters for a SMPL model <ref type="bibr" target="#b31">[32]</ref> from the predicted mesh V m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training</head><p>Losses To train the Bilayer GCN, we apply loss functions on the output of the Bilayer GCN and SMPL regressor and minimize the errors between the predictions and ground truths. Firstly, we use the a per-vertex L 1 loss between the ground truth V m and predicted mesh verticesV m from Mesh-GCN, denoted as L m , and between the GT and predicted joint verticesV s from Skeleton-GCN, denoted as L s . We follow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> to multiply the predicted meshV m by a predefined matrix to get 3D joints, denoted asV j3d m . L 1 loss is also applied to it and its GT V s , denoted as L j3d m . As we trained on mixed datasets consisting of both 3D and 2D data, we have additional supervision on the predict a weak perspective camera parameters from the intermediate features of the Mesh-GCN with two graph linear layers. Apply this camera parameters toV j3d m andV s , we get two sets of 2D pose and use a L 1 loss on them and the 2D GT pose, denoted as J j2d m and J j2d s respectively. Finally, we apply MSE loss on the predicted SMPL shape (?) and pose (?) parameters, denoted as L ? and L ? respectively. And we have the final loss as below:</p><formula xml:id="formula_7">L = L m + L j3d m + L j2d m + L s + L j2d s + L ? + ?L ? ,<label>(6)</label></formula><p>Focal loss for regression We observe that in the above losses on 3D vertices, the error caused by each body part varies a lot. For example, the joints on legs and arms usually have much larger error than the other parts. The intuition is that the variation for body limbs is much larger compared to torso and head. We generalizes the focal loss <ref type="bibr" target="#b29">[30]</ref>, which addresses class imbalance by down-weighting the loss for well-classified samples, to this regression tasks to addresses the imbalanced vertex error. We modify it based on the L 1 loss of the target, i.e.,</p><formula xml:id="formula_8">L f l = ?(?L) ? log(1 ? max(?, ?L)),</formula><p>where L is the L1 loss, ? is a factor to scale L to (0,1), ? &lt; 1 is a threshold that truncates ?L with a maximum value to avoid unreasonably large loss when ?L approaches 1, (?L) ? is a factor to reduce the relative loss for wellregressed vertices with ? &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Empirical Evaluations</head><p>We have evaluated our proposed method and present the results in this section. The datasets have different 2D annotations. We have selected the 12 joint annotations in common for the skeleton graph to define its graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation metrics</head><p>Human 3.6M This indoor 3D dataset <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> comprises eleven subjects performing 17 common scenarios, e.g. sitting down, talking on the phone. The training data contains ground truth 2D joints, 3D joints, and SMPL (pose and shape) parameters. The entire dataset contains 3.6M images. For training we only have access to subjects 1, 5, 6, 7, and 8 (about 1.55M images). Subjects 9 and 11 are held out for evaluation (about 0.5M images). UP-3D Unite the People 3D <ref type="bibr" target="#b26">[27]</ref> consists of images with annotations by humans doing sports and other miscellaneous activities. Besides ground truth 2d keypoints, SMPL fits have been performed on the 2D keypoints to produce ground truth SMPL parameters. About 7K images are used for training, and 639 held out for evaluation. LSP Leeds Sports Pose <ref type="bibr" target="#b17">[18]</ref> contains 2K images with 2D joint annotations of people playing sports. We use 1000 images for training, and 1000 for evaluation. COCO Common Object in Context <ref type="bibr" target="#b30">[31]</ref> also contains images of people annotated 2D keypoints. About 28K images are used for training. We do not evaluate for this dataset. MPII MPII Human Pose dataset contains images with annotated body joints of people performing 410 different activities <ref type="bibr" target="#b1">[2]</ref>. We use about 15K training images from this dataset, and do not evaluate on this dataset. Evaluation metrics For H36M we report the mean Euclidean distance (mm) between the predicted and ground truth 3D joints after root joint alignment (MPJPE), and rigid alignment error (PA-MPJPE) as in <ref type="bibr" target="#b53">[54]</ref>. For UP-3D we report MPVE, which is a mean per-vertex error between the predicted and ground truth shape, and for LSP we report accuracy (Acc.) and F1 score on foreground-background (FB seg) and part segmentation (Parts seg). We report nonparametric (np) and SMPL parametric (p) predictions for H36M P1, P2 and UP-3D datasets.   <ref type="table">Table 3</ref>: Comparison with the mesh-only graph method <ref type="bibr" target="#b24">[25]</ref> on UP-3D for estimated 3D mes (MPVE is in mm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiment Details</head><p>We use a pre-trained ResNet-50 to extract perceptual features. Our model is trained end-to-end with a batch size of 64 and learning rate of 2.5e ?4 . Mini-batches during training are assembled by selecting images from the five training datasets. The composition is 30%, 20%, 10%, 20% and 20% for Human3.6M, UP-3D, LSP, COCO and MPII respectively. The Adam optimizer is used to determine the weight updates. We train our model for fifty epochs, but we observed fewer epochs could suffice (see Section 5.3). During training, we apply the focal loss L f l (with ? = 1, ? = 1) to the estimated 3D pose from Mesh-GCN (V s ) and the coefficient before this loss term is 5.0. We mixed the ground truth and the estimated 2D joint location to get feature patches during training with a mixture ratio which gradually decreases to zero at the last epoch during training and only use the estimated 2D joints during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Main Results and Analysis</head><p>We compare our method to state-of-the-art methods on H36M, UP-3D and LSP datasets, which evaluates 3D poses, 3D mesh, and 2D projections of the mesh respectively. The results are shown in Tables 1, 3 and 2 for those three datasets. Our method either outperforms or achieves comparable performance as the prior methods on those datasets.</p><p>First of all, we aim to investigate how the bi-layer graph performs for body recovery. To this end, we first focus on the Human3.6M dataset and UP-3D dataset. The rich human activities in their images is a natural target to study the correlation between body parts, which requires long-range interactions. We evaluate the regressed mesh by our bilayer graph through 3D pose accuracy, in comparison to the mesh-only graph method <ref type="bibr" target="#b25">[26]</ref> and the self-attention in transformer <ref type="bibr" target="#b28">[29]</ref> as shown in <ref type="table">Table 1</ref>. In both cases, we outperform them in reconstruction error (PA-MPJPE), indicating that our proposed bi-layer graph uses the non-local interactions efficiently for body recovery. We also evaluate the regressed mesh and the mesh calculated from the regressed SMPL on the UP-3D dataset in <ref type="table">Table 3</ref>, which demonstrates that our method can promote the fine-grained interactions between mesh vertices for improved body shape. We also evaluate 3D shape through silhouette projection on the LSP dataset in <ref type="table" target="#tab_1">Table 2</ref>. Our proposed bi-layered graph again outperforms prior methods.</p><p>Our model aim to jointly model local vertex-vertex (defined by mesh neighbourhood), non-local vertex-joint, and joint-joint interactions. We get insight of vertex-joint intersections by the learned fusion adjacency matrix (A f ) in <ref type="figure" target="#fig_2">Fig. 4</ref>. Firstly, strong interactions between a joint and its nearby mesh vertices are encouraged, thus the joint will guide the mesh recovery. We achieve this by setting the initial values of A f , s (see Eq. (5)) as the reciprocal of the Euclidean vertex-join distance in T-pose mesh. This intersections cover a range larger than the fine-grained vertexvertex interactions predefined by mesh neighborhood. Secondly, long-range joint-vertex interactions are learnt between a joint and remote vertices near another joint, when the body part correlation happens. Please see the example of the right ankle and right wrist in the left sub-figure of <ref type="figure" target="#fig_2">Fig. 4</ref>. Our intersections differ from the transformerbased METRO <ref type="bibr" target="#b28">[29]</ref> in two ways: the local vertex-vertex intersections avoid huge computation of the brute-force selfattention; and joint-vertex intersections learnt from the Fusion Graph efficiently model the most important topology knowledge between body mesh and joints. Together with the localized image features, our model achieves comparable performance to METRO <ref type="bibr" target="#b28">[29]</ref> of the strong representation ability for the fully connected intersections. We believe that attention and knowledge-aware bi-layer graph network can be integrated to learn the interactions.</p><p>Finally, we evaluate the efficiency of the bi-layer graph structure through the convergence speed of the training compared to the mesh-only method <ref type="bibr" target="#b24">[25]</ref> (see <ref type="figure" target="#fig_3">Fig. 5</ref>). Our model (in blue) achieves a lower, more stable loss much earlier compared to the baseline <ref type="bibr" target="#b24">[25]</ref> (in orange), which indicates that the speedup of information propagation along this bi-layer graph can potentially reduce the training time.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Studies</head><p>Benefit of bi-layer graph vs. localized features As shown in <ref type="table" target="#tab_3">Table 4</ref>, instead of single-mesh only graph (A), bi-layer graph only (B) or localized image features only (C) , it is the combination (D) of Fusion Graph and localized features that jointly contribute most to the performance gain. This combination is the core difference from the transformerbased METRO <ref type="bibr" target="#b28">[29]</ref> and other hierarchical structures, such as CoMA <ref type="bibr" target="#b38">[39]</ref> (see suppl. mat. for more discussion). Benefit of Fusion Graph Since we design a bi-layer graph structure connected with fusion graph for mesh recovery, one interesting question is that whether the fusion graph is useful. In <ref type="table" target="#tab_4">Table 5</ref>, We study the fusion graph by limiting its usage in the bi-layer graph network: replacing with a simple fusion by pooling (avgpool-as-fusion and maxpool-asfusion) and restricting it applied to only the first or the last graph layer(fusion-at-first and fusion-at-last respectively). The simple fusion strategy will loss the individual interaction between a joint vertex and a mesh vertex as each joint (mesh) vertex apply an identical feature from pooling the mesh (joint) vertices feature. We compare those strategies to our fusion graph and observe the performance increase significantly with allowing more fusion connections in the network and our fusion graph works on best with the full connections in all graph layers. Weight sharing We exploit the property of GCN to share   weights between skeleton-GCN and Mesh-GCN for compact model size. In <ref type="table" target="#tab_4">Table 5</ref>, it is interesting to observe only marginal performance loss, indicating the strong representation ability of GCN on the body and skeleton topology. Focal loss To demonstrate the benefit of the focal loss for regression, we trained the model with L 1 loss on V s instead of the proposed focal loss and keep the other losses the same. In <ref type="table" target="#tab_4">Table 5</ref>, we see that L 1 loss works a bit inferior to the focal loss. We will explore the use of focal loss in future work for improving the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a dual-scale graph-based method for 3D human shape and pose recovery from a single image. A skeleton graph estimates 3D pose, and a mesh graph estimates 3D shape. A fusion graph promotes the exchange of local and global information between the two graphs. And Fusion Graph employs an adaptive adjacency matrix to learn which nodes between the two scales influence one another most. Our results show that we can outperform stateof-the-art methods. Some poses, and partial occlusions remain challenging. For future work, We would like to extend our work to take both single and multi-view images as input, which may help improve performance. In addition, 3D reconstruction of objects from images in general, not only humans, is an interesting research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Studies and Results</head><p>Per-Activity Evaluation for Human 3.6M</p><p>The Human 3.6M dataset <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> contains people performing 15 activities, such as sitting down and walking. In the main paper, we reported our best model (shown in Table 2 of the main paper), which applies No Weight Sharing .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Parametric vs Parametric</head><p>Result comparison As shown in <ref type="table" target="#tab_5">Table 6</ref> and 7, the nonparametric shape is the regression result of all mesh vertices from the Bi-layer Graph and is generally able to learn the pose better than the SMPL parametric predictions on Human 3.6M dataset. Please note the evaluation on SMPL parametric predictions is still comparable to the state-ofthe-arts as shown in <ref type="table">Table 1</ref> in the main paper. As introduced in the main paper, the Bi-layer Graph and SMPL regressor forms a pipeline, and the input of the later module depends on the output of the former module. The good performance of the later module further illustrates that our proposed Bi-layer Graph has addressed the dense regression of body mesh vertices well. Comparing the third and last rows of <ref type="figure" target="#fig_4">Figure 6</ref> and <ref type="figure">Figure ?</ref>?, we can observe that some key- More discussion about other networks METRO Both the transformer-based METRO <ref type="bibr" target="#b28">[29]</ref> and our model aim to jointly model vertex-vertex, vertex-joint, and joint-joint interactions. But they differ in the ways of representing each vertex and joint and learning those interactions. METRO uses self-attention to brute-force learn all interactions. Although powerful, the self-attention has a wellknown issue of the quadratic time and memory complexity. METRO has to down-sample the mesh to 431 vertices and train on very large mixed datasets for a long time (200 epochs) to learn all the interactions. Rather than the bruteforce self-attention, we inject prior knowledge of the mesh topology into the Mesh Graph (1723 vertices), whose adjacency matrix is naturally sparse. In this way, our model trains on a smaller amount of data for 50 epochs and converges faster: the accuracy increases rapidly in the first 12 epochs and becomes stable after 32 epochs as shown in <ref type="figure" target="#fig_3">Figure 5</ref> in the main paper. Together with the localized image features, our model achieves comparable performance to self-attention based METRO. We believe that attention and knowledge-aware bi-layer graph network can be integrated to learn the interactions.</p><p>CoMA Compared to the hierarchy GCN capturing face shape and expression at multiple scales in CoMA <ref type="bibr" target="#b38">[39]</ref>, our bi-layer graph is simple and efficient to represent non-local body mesh with the additional skeleton-scale graph. Firstly, we use the prior knowledge that the body mesh highly depends on the joint motion. Secondly, extra intermediatescale body mesh representations by down-sampling (as in CoMA) doesn't help based on our trials. Additionally, our bi-layer graph uses both vertex and joint inputs, rather than just vertices as CoMA does. Our Fusion Graph further learns dynamic vertex-joint correlations, while the transform matrices of down-sampling and up-sampling layers in CoMA are predefined and fixed. Qualitative comparison On the other side, we note that the shape of the parametric (SMPL) prediction is smooth but the shape of the non-parametric prediction may exhibit non-smooth artifacts. To demonstrate it, we shows examples of the rendered mesh for non-parametric prediction in <ref type="figure" target="#fig_7">Figure 9</ref>, and the ones for SMPL predictions in <ref type="figure" target="#fig_8">Figure 10</ref> for the evaluation images from the Human 3.6M, UP-3D and LSP datasets. To avoid some of the noise artifacts, we can apply some surface constraints, like vertex normal loss, to smooth the predicted surface for non-parametric methods in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative results on occlusion</head><p>The body is always self-occluded in a single image. It is especially challenging to predict the occluded limbs because of their rich poses from a variety of activities. This requires the methods to deduce the missing limbs from the other visible parts. Our model explicitly embed the whole joints and mesh in the bi-layer structure, enabling to learn the occluded part in a data-driven way. This bi-layer structure further learn the interactions between the body joints and mesh vertices by the fusion graph and thus guide the dense mesh with the sparse joints, which is less challenging to learn from the data. In <ref type="figure" target="#fig_7">Figure 9</ref> and 10, We render the predicted meshes from different views and demonstrate that our methods can always learn the occlusion well from the data. We also show a failure case of local hands pose in <ref type="figure" target="#fig_6">Figure 8</ref>. Although the shape and pose seem correct from the camera view of the image, when viewed from different angles, we can see that the hands are separated rather than joined.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Mesh-only graph Non-Local Connection via a Bi-Graph (b) Bilayer graphs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " X T J + M s + 4 m W R d Y H 5 7 u v a t C m G 7 Z V U = " &gt; A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b Y B F c S E l U 1 G X R j c s K 9 g F t D J P p p B 0 6 M w k z E 7 G E / I o b F 4 q 4 9 U f c + T d O 2 i 6 0 9 c D A 4 Z x 7 u W d O m D C q t O t + W 0 v L K 6 t r 6 6 W N 8 u b W 9 s 6 u v V d p q T i V m D R x z G L Z C Z E i j A r S 1 F Q z 0 k k k Q T x k p B 2 O b g q / / U i k o r G 4 1 + O E + B w N B I 0 o R t p I g V 3 p c a S H Y Z R 1 8 g c 3 y P g J z Q O 7 6 t b c C Z x F 4 s 1 I F W Z o B P Z X r x / j l B O h M U N K d T 0 3 0 X 6 G p K a Y k b z c S x V J E B 6 h A e k a K h A n y s 8 m 2 X P n y C h 9 J 4 q l e U I 7 E / X 3 R o a 4 U m M e m s k i q Z r 3 C v E / r 5 v q 6 M r P q E h S T Q S e H o p S 5 u j Y K Y p w + l Q S r N n Y E I Q l N V k d P E Q S Y W 3 q K p s S v P k v L 5 L W a c 2 7 q J 3 d n V f r 1 7 M 6 S n A A h 3 A M H l x C H W 6 h A U 3 A 8 A T P 8 A p v V m 6 9 W O / W x 3 R 0 y Z r t 7 M M f W J 8 / A C W U b A = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " f H R F K h u B 0 7 u X W y y e L p 8 t c I o X o w 4 = " &gt; A A A B + 3 i c b V D L S s N A F L 2 p r 1 p f s S 7 d D B b B h Z R E R V 0 W 3 b i s Y B / Q x j C Z T t q x k w c z E 7 G E / I o b F 4 q 4 9 U f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W e O F 3 M m l W V 9 G 6 W l 5 Z X V t f J 6 Z W N z a 3 v H 3 K 2 2 Z Z Q I Q l s k 4 p H o e l h S z k L a U k x x 2 o 0 F x Y H H a c c b X + d + 5 5 E K y a L w T k 1 i 6 g R 4 G D K f E a y 0 5 J r V f o D V y P P T b n Z v u a k 8 f s h c s 2 b V r S n Q I r E L U o M C T d f 8 6 g 8 i k g Q 0 V I R j K X u 2 F S s n x U I x w m l W 6 S e S x p i M 8 Z D 2 N A 1 x Q K W T T r N n 6 F A r A + R H Q r 9 Q o a n 6 e y P F g Z S T w N O T e V I 5 7 + X i f 1 4 v U f 6 l k 7 I w T h Q N y e y Q n 3 C k I p Q X g Q Z M U K L 4 R B N M B N N Z E R l h g Y n S d V V 0 C f b 8 l x d J + 6 R u n 9 d P b 8 9 q j a u i j j L s w w E c g Q 0 X 0 I A b a E I L C D z B M 7 z C m 5 E Z L 8 a 7 8 T E b L R n F z h 7 8 g f H 5 A w r U l H M = &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>A visualization of the learned fusion adjacency matrix (A f ). Left: the connections between the right ankle and the mesh vertices. Top row on the right: right knee, left elbow, left ankle; Bottom row on the right: right hip, right wrist, right shoulder. Red, yellow and gray color indicate strong, weak and trivial connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of training loss onV j3d m (left) and V m (right) from the common mesh graph structure of the baseline<ref type="bibr" target="#b24">[25]</ref> and our proposed model. We trained for 50 epochs and one epoch takes about 5,000 steps.Qualitative ResultsFig. 6 and 7show four successful examples and two failure cases due to challenging poses and occlusions (see suppl. mat. for more examples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results (left to right: input image, nonparametric pose and shape, two other view perspectives).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Two failure case examples (input, non-parametric, parametric).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>One example of bad pose in different 3D views. The two rows show parametric and non-parametric results respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative non-parametric results. From left to right: input image, pose and shape, two different view perspectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative SMPL parametric results. From left to right: input image, pose and shape, two different view perspectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art on LSP for 2D projection from the predicted non-parametric mesh.</figDesc><table><row><cell>Methods</cell><cell>MPVE (np) ?</cell><cell>MPVE(p) ?</cell></row><row><cell>GraphCMR [25]</cell><cell>104.5</cell><cell>122.9</cell></row><row><cell>Ours</cell><cell>59.0</cell><cell>61.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of bi-layer graph components and localized features. All has Mesh Graph with global image features as input as GraphCMR and has the same training settings.</figDesc><table><row><cell>Methods</cell><cell>H36M P2 MPJPE ?</cell><cell>UP-3D MPVE ?</cell></row><row><cell>Ours</cell><cell>34.0</cell><cell>59.0</cell></row><row><cell>avgpool-as-fusion</cell><cell>47.3</cell><cell>81.1</cell></row><row><cell>maxpool-as-fusion</cell><cell>42.9</cell><cell>77.0</cell></row><row><cell>fusion-at-first</cell><cell>36.7</cell><cell>64.3</cell></row><row><cell>fusion-at-last</cell><cell>38.3</cell><cell>71.3</cell></row><row><cell>shared weight</cell><cell>34.2</cell><cell>61.7</cell></row><row><cell>no FL</cell><cell>34.7</cell><cell>59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Evaluation results for ablation studies. See text for details. FL is for focal loss, shared weight indicates the model shares weights between skeleton and mesh graph.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Walking respectively. It is clear from the table that the performance for certain activities (highlighted in red), e.g., Walking, compares favorably to others, e.g., Sitting Down, as those poses are much more challenging and consequently have higher errors.</figDesc><table><row><cell></cell><cell cols="4">and 7 shows the evaluation of non-parametric</cell></row><row><cell cols="5">and SMPL parametric predictions on Human 3.6M for each</cell></row><row><cell cols="5">activity separately. The activities represent Providing Di-</cell></row><row><cell cols="5">rections, Having a Discussion, Eating, Greeting, Making a</cell></row><row><cell cols="5">Phone Call, Taking a Photo, Posing, Making a Purchase,</cell></row><row><cell cols="5">Sitting, Sitting Down, Smoking, Waiting, Walking a Dog,</cell></row><row><cell cols="3">Walking Together, and P1</cell><cell></cell><cell>P2</cell></row><row><cell>Act.</cell><cell>MPJPE?</cell><cell>PA-MPJPE?</cell><cell>MPJPE?</cell><cell>PA-MPJPE?</cell></row><row><cell>Directions</cell><cell>61.29</cell><cell>31.61</cell><cell>55.24</cell><cell>28.14</cell></row><row><cell>Discussion</cell><cell>59.62</cell><cell>33.84</cell><cell>56.19</cell><cell>32.30</cell></row><row><cell>Eating</cell><cell>58.43</cell><cell>34.46</cell><cell>55.34</cell><cell>33.51</cell></row><row><cell>Greeting</cell><cell>62.68</cell><cell>34.81</cell><cell>58.36</cell><cell>33.53</cell></row><row><cell>Phoning</cell><cell>60.32</cell><cell>35.48</cell><cell>58.14</cell><cell>33.50</cell></row><row><cell>Photo</cell><cell>68.23</cell><cell>39.27</cell><cell>63.66</cell><cell>38.23</cell></row><row><cell>Posing</cell><cell>61.22</cell><cell>33.43</cell><cell>57.86</cell><cell>29.98</cell></row><row><cell>Purchases</cell><cell>60.59</cell><cell>32.18</cell><cell>59.39</cell><cell>31.75</cell></row><row><cell>Sitting</cell><cell>66.63</cell><cell>40.51</cell><cell>64.92</cell><cell>42.21</cell></row><row><cell>SittingDown</cell><cell>72.74</cell><cell>49.57</cell><cell>72.18</cell><cell>45.44</cell></row><row><cell>Smoking</cell><cell>57.77</cell><cell>34.09</cell><cell>54.06</cell><cell>33.21</cell></row><row><cell>Waiting</cell><cell>61.80</cell><cell>34.23</cell><cell>57.93</cell><cell>31.51</cell></row><row><cell>WalkDog</cell><cell>58.09</cell><cell>34.50</cell><cell>59.16</cell><cell>35.31</cell></row><row><cell>WalkTogether</cell><cell>57.36</cell><cell>31.41</cell><cell>56.49</cell><cell>30.79</cell></row><row><cell>Walking</cell><cell>52.72</cell><cell>29.46</cell><cell>52.03</cell><cell>28.66</cell></row><row><cell>Overall</cell><cell>61.17</cell><cell>35.36</cell><cell>58.45</cell><cell>33.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of non-parametric predictions on Human 3.6M per activity. Certain activities (in red) result in better performance, compared to others. Numbers are MPJPE and PA-MPJPE in mm.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Evaluation of SMPL parametric prediction on Human 3.6M per activity. Certain activities (in red) result in better performance, compared to others. Numbers are MPJPE and PA-MPJPE in mm.points of the SMPL prediction, e.g. ankles, are slightly off the ground truth while the non-parametric poses are more accurate.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Any "typical" CNN auto-encoder can be used for the image feature extraction.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by MERL. Xin Yu was partially funded by the NSF grant IIS 1764071. We thank Srikumar Ramalingam for valuable discussions. We also thank the anonymous reviewers for their constructive feedback that helped in shaping the final manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tex2shape: Detailed full human body geometry from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-garment net: Learning to dress 3d people from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvita</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5420" to="5430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno>2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Moulding humans: Non-parametric 3d human shape estimation from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sebastien</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mesh rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin Johnson Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphonomy: Universal human parsing via graph transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bcnet: Learning body and cloth shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12.6</idno>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Part i: direct skinning methods and deformation primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Kavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Symbiotic graph neural networks for 3d skeleton-based human action recognition and motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2999" to="3007" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1-248:16</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cem Keskin, Shahram Izadi, and Sean Fanello. Volumetric capture of humans with a single rgbd camera via semi-parametric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pidlypenskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Texturepose: Supervising human mesh estimation with texture consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling the geometry of dressed humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Sanchez-Riera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating 3d faces using convolutional mesh autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<idno>2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pixel2mesh++: Multi-view 3d mesh generation via deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simulcap : Singleview human performance capture with cloth simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Predicting 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deephuman: 3d human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="901" to="914" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
