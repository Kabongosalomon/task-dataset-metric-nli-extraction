<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Nuriel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work has shown that convolutional neural network classifiers overly rely on texture at the expense of shape cues. We make a similar but different distinction between shape and local image cues, on the one hand, and global image statistics, on the other. Our method, called Permuted Adaptive Instance Normalization (pAdaIN), reduces the representation of global statistics in the hidden layers of image classifiers. pAdaIN samples a random permutation ? that rearranges the samples in a given batch. Adaptive Instance Normalization (AdaIN) is then applied between the activations of each (non-permuted) sample i and the corresponding activations of the sample ?(i), thus swapping statistics between the samples of the batch. Since the global image statistics are distorted, this swapping procedure causes the network to rely on cues, such as shape or texture. By choosing the random permutation with probability p and the identity permutation otherwise, one can control the effect's strength.</p><p>With the correct choice of p, fixed apriori for all experiments and selected without considering test data, our method consistently outperforms baselines in multiple settings. In image classification, our method improves on both CIFAR100 and ImageNet using multiple architectures. In the setting of robustness, our method improves on both ImageNet-C and Cifar-100-C for multiple architectures. In the setting of domain adaptation and domain generalization, our method achieves state of the art results on the transfer learning task from GTAV to Cityscapes and on the PACS benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the early successes of computer vision was a face recognition system by Sakai et al. <ref type="bibr" target="#b32">[33]</ref> that employed a simple neural network classifier. As it turns out, the network was relying on global image statistics, namely the average brightness, to perform recognition.</p><p>In this work, we demonstrate that removing the reliance on global image statistics improves classification results in modern networks. To mitigate the effect of the global statis-tics, a deliberate mismatch between the activations of a layer and its accumulated statistics is created. By normalizing with unmatched statistics, the distribution of activation values becomes unreliable as a source for label information. While changing the global statistics of an image to another was explored in the context of generation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>, we show that it is also useful in a variety of discriminative settings.</p><p>Our work is similar in spirit but different in conclusion from recent work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref> that has identified a bias toward texture at the expense of shape. Such recent methods can often improve the performance of the image classifier on the test set and have been shown to dramatically increase the accuracy of the classifier on shifted image domains, in which image transformations change the image statistics but leave most of the shape unchanged.</p><p>In our work, we also show classification and domain generalization improvements. However, we demonstrate that the increase in classification performance occurs simultaneously for both category-based image recognition and texture recognition. This suggests that while the texture is often defined as local image statistics, becoming invariant to global image statistics improves both shape and texture recognition.</p><p>We demonstrate the effectiveness of our method in a number of settings. First, we demonstrate how classification performance improves when adding our permutation-based regularization. Our method improves accuracy on both CI-FAR100 and ImageNet on multiple architectures trained in a vanilla fashion. Second, we train a linear classifier on top of a pre-trained image-classification network's representation layer and show that the accuracy of texture classification peaks exactly when the image classification results are maximized. We show that when this happens, the network's representation of shape does not deteriorate. Next, we demonstrate that our method can reduce the adverse effect of domain shift, by testing it in the setting of domain adaptation and more broadly in domain generalization. Our method achieves state of the art results on the domain adaptation from GTA5 to Cityscapes semantic segmentation and on the PACS dataset. Lastly, we show that our method allows for a greater robustness when handling corrupted images, where our method is superior to all baseline methods. In the setting of robustness, our method improves on both ImageNet-C and Cifar-100-C for multiple architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Bias towards texture. A large body of work has shown that, unlike humans, networks tend to be biased towards textures in making decisions. Gatys et al., <ref type="bibr" target="#b12">[13]</ref> have shown that training a linear classifier on top of a VGG19 texture representation achieves similar performance to training VGG19 directly on this task. Geirhos et al., <ref type="bibr" target="#b14">[15]</ref> observed this phenomenon in the context of pretrained ImageNet CNNs. They presented the 'Stylized-ImageNet' dataset, which is a version of ImageNet where the image style is altered, and show that training with this dataset forces the network to learn a shape-based representation. Hermann and Kornblith <ref type="bibr" target="#b18">[19]</ref> explored the role of different factors, such as the training objective or architecture, on reducing texture bias.</p><p>However, this resulted in a degradation in the network's performance. Unlike these methods, from the technical perspective, our method does not rely on the additional supervision in the form of extended or modified datasets and instead directly modifies the architecture of the network. Our interpretation of the results is also different. We show that manipulating the global statistics, which are directly linked to style, does not hurt texture recognition.</p><p>Several contributions attempt to alleviate texture bias, by proposing an architectural change or a new training objective. Shi et al., <ref type="bibr" target="#b34">[35]</ref> develop a Dropout-like algorithm. Wang et al., <ref type="bibr" target="#b41">[42]</ref> penalize shallow layers for having predictive power. Zhang and Zhu <ref type="bibr" target="#b46">[47]</ref> show that adversarial training reduces texture bias. Carlucci et al., <ref type="bibr" target="#b3">[4]</ref> propose to reduce texture bias, by training the network to solve jigsaw puzzles. Unlike these methods, our method makes use of a novel normalization layer, which, as shown in Sec. 3.1, directly affects the dependence on global image statistics. Normalization and style transfer. Batch Norm <ref type="bibr" target="#b23">[24]</ref> has become a standard mechanism for effectively training deep neural networks by normalizing activations by the statistics of the minibatch. To reduce minibatch dependencies, several alternatives were proposed, including Layer Normalization <ref type="bibr" target="#b0">[1]</ref>, Instance Normalization <ref type="bibr" target="#b39">[40]</ref>, and Group Normalization <ref type="bibr" target="#b43">[44]</ref>. Our work utilizes the ability to swap the style statistics of images as part of a novel normalization layer. Unlike our normalization layer, its role is not to support efficient training, but to direct the network toward the desired emphasis on shape and fine details.</p><p>Instance norm by Ulyanov et al., <ref type="bibr" target="#b39">[40]</ref>, can be seen as a form of style normalization by normalizing feature statistics. Building on this view, Huang and Belongie <ref type="bibr" target="#b20">[21]</ref> proposed Adaptive Instance Normalization (AdaIN) as a form of style transfer, by first normalizing the target image style statistics and then rescaling by source image style statistics. These style manipulations through normalization layer methods are mostly applied in the generative setting, where they can be used for texture synthesis and style transfer, while our method focuses on image recognition.</p><p>Our method builds upon AdaIN to swap the style statistics of different element activation. As far as we are aware, while other methods use style transfer to construct an improved dataset <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15]</ref> to alleviate the reliance on global image statistics, our method is the first to do so within a network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We are interested in the distinction between global image statistics on the one hand, and other global cues such as shape as well as local cues, on the other. Global statistics are statistics (such as mean and standard deviation) measured from all the pixels of the image. These include, for example, brightness, contrast, lightning and global color changes. Changing the style of the image typically changes such statistics. Global cues refer to any cues present in the entire image or large patches of it, but not in small patches. These include global statistics but also shape information (such as an edge map of a cat). Local cues refer to any information present in small patches in the image, which may include both texture and shape information within those patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>To motivate our method, we conduct a simple experiment visualizing the effect of swapping statistics of intermediate layer representations of an autoencoder. The autoencoder was trained to minimize the reconstruction error on the Stanford Car Dataset <ref type="bibr" target="#b24">[25]</ref>. We consider two image inputs, a and b and inspect the effect on the reconstruction of a, when swapping their statistics at different layers of the pretrained encoder. The decoder is left unchanged. As a baseline method, we also observe the effect of transferring the style of image b to a using the method of Gatys et al., <ref type="bibr" target="#b12">[13]</ref>.</p><p>As can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>, when swapping the image statistics used by the AdaIN module, the reconstructed image has similar global statistics, such as color and overall image appearance of b but the finer details of image a are preserved. Applying this swapping on more layers, results in a larger transfer of the global statistics of b. In contrast, when using style transfer, the fine details of a are borrowed from image b and are no longer preserved. For example in <ref type="figure" target="#fig_0">Fig. 1</ref>, when applying style transfer, the bird on a tree was given the fine details of the shark under water, and similarly a cat was given the texture of the elephant skin. Such details were not transformed by swapping the normalization parameters. We argue that preserving the fine details, while transferring the global ones, results in an augmented sample that can be utilized to improve classification accuracy and make the trained network more robust to imaging conditions and better suited for generalization to new visual domains. The last image on the right is the result of applying style transfer using the method of Gatys et al., <ref type="bibr" target="#b12">[13]</ref>. L.=layer; Recon=Reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive Instance Normalization</head><p>We begin by defining Instance Normalization (IN), as formulated in <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b20">[21]</ref>. For a given convolutional neural network, let the output activations of a given convolutional layer be x ? R N ?C?H?W , where N is the batch size, C the number of channels, H the height of the layer, and W its width. Instance Norm is then defined as:</p><formula xml:id="formula_0">IN(x) = ? x ? ?(x) ?(x) + ? ,<label>(1)</label></formula><p>where ?(x) and ?(x), both in R N ?C , are the mean and standard deviation, computed along the spatial dimensions (H ? W ) for each channel (c) and sample in the batch (n):</p><formula xml:id="formula_1">? nc (x) = 1 HW H h=1 W w=1 x nchw (2) ? nc (x) = 1 HW H h=1 W w=1 (x nchw ? ? nc (x)) 2 + . (3)</formula><p>? and ?, both in R N ?C , are the re-scaling affine parameters learned independently of x. The above operation is applied in the same manner both at train and test time. As detailed in <ref type="bibr" target="#b20">[21]</ref>, IN can be viewed as normalizing the style statistics of each input in the batch. Adaptive Instance Normalization (AdaIN) builds upon this view, by first normalizing the style statistics of an input a, thus extracting its content, and then scaling the normalized output by the statistics of a target style input b. This allows the transfer of style from b to a. Specifically, let a, b ? R C?H?W , then AdaIN is defined as:</p><formula xml:id="formula_2">AdaIN(a, b) = ?(b) a ? ?(a) ?(a) + ?(b)<label>(4)</label></formula><p>where ?(a) and ?(a) (resp. ?(b) and ?(b)) are the mean and standard deviation of a (resp. b) over its spatial dimension, computed for each channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Permuted AdaIN</head><p>Given an input activations map x ? R N ?C?H?W , let ?(x) = [x ?(1) , x ?(2) , . . . , x ?(N ) ] ? R N ?C?H?W be the result of applying a permutation ? to the elements of a given mini-batch x = x 1 , . . . , x N along the minibatch axis.</p><p>The result of applying pAdaIN on a single sample x i in the context of its batch and for a given permutation ? is:</p><formula xml:id="formula_3">p-IN ? (x i ) = AdaIN(x i , x ?(i) ) .<label>(5)</label></formula><p>pAdaIN is then defined for the entire tensor x:</p><formula xml:id="formula_4">pAdaIN(x) = x, probability p (p-IN ? (x 1 ), .., p-IN ? (x N )) otherwise</formula><p>where ? is a uniformly chosen permutation, and p is a hyperparameter fixed ahead of training. pAdaIN is only applied during training time and not at test time. We apply pAdaIN to the output activations of all convolutional layers and in particular, before applying batch normalization.</p><p>Backpropagation is applied through x but not through ?(x). Specifically setting a = x i and b = x ?(i) in Eq. 4, we regard ?(x ?(i) ) and ?(x ?(i) ) as constant and do not backpropagate through them. Performing a different update, such as one on ?(x ?(i) ) and ?(x ?(i) ) leads to sub-optimal results, as shown in Sec. 4.6. Mixing batch information in the forward pass during training is used as a regularization to the model, and is shown to improve generalization (see Sec. 4.4). Backpropagating gradients on both x and ?(x) causes the loss on a sample x i of the batch to affect the gradients of another sample x ?(i) in the batch, which is undesired. Effect of Batch Norm. Batch Norm (BN ) normalizes channel-wise statistics yet does not undo the effect of our method. To see this, we first define the BN operation:</p><formula xml:id="formula_5">? c (x) = 1 N N n=1 ? nc (x) (6) ? c (x) = 1 N HW N n=1 H h=1 W w=1 (x nchw ? ? c (x)) 2 + (7) BN (x) = ? x ? ? c (x) ? c (x) + ? .<label>(8)</label></formula><p>For some parameters ? and ?. After applying BN , we have:</p><formula xml:id="formula_6">? nc (BN (x)) = ? ? c (x) ? (? nc (x) ? ? c (x)) + ? ,<label>(9)</label></formula><formula xml:id="formula_7">? nc (BN (x)) = ? ? ? nc (x) ? c (x)<label>(10)</label></formula><p>With pAdaIN, when statistics are swapped (i.e not identity):</p><formula xml:id="formula_8">? nc (BN (pAdaIN(x))) = ? ? c ? (? ?(n)c (x) ? ? c (x)) + ? ,<label>(11)</label></formula><formula xml:id="formula_9">? nc (BN (pAdaIN(x))) = ? ? ? ?(n)c (x) ? c (x)<label>(12)</label></formula><p>Eq. 11 and Eq. 12 follow from pAdaIN shifting channelwise statistics. The statistics of channel c, for sample n, after applying pAdaIN, are the same as that of sample ?(n), beforehand. Hence BN does not undo the swapping of statistics, but rather scales them by batch-wise statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our experiments explore classification accuracy of both objects and texture, robustness to image corruption, and generalization to new domains. Unless otherwise mentioned, pAdaIN is applied with a fixed choice of p = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Classification</head><p>We evaluate pAdaIN in the context of image classification on both CIFAR100 and ImageNet. To evaluate pAdaIN, for every architecture, we add a pAdaIN layer before every use of batch normalization and after using a convolutional layer.</p><p>For CIFAR100, we consider the architectures of VGG19 <ref type="bibr" target="#b35">[36]</ref>, InceptionV4 <ref type="bibr" target="#b36">[37]</ref>, PyramidNet <ref type="bibr" target="#b15">[16]</ref>, ResNet18 and ResNet50 <ref type="bibr" target="#b16">[17]</ref>. During training, we apply a padding of 4, a random crop and a random rotation of up to 15%, resulting in images of size 32 ? 32. The networks are trained on a batch size of 128 with an SGD with a momentum of 0.9 and a weight decay of 5e ?4 . We use 200 epochs, start training with a learning rate of 0.1 and divide the learning rate by 5 at epochs 60, 120 and 160. For ImageNet, we consider the architectures of ReseNet50, ResNet101 and ResNet152 <ref type="bibr" target="#b16">[17]</ref>. We train for 300 epochs, and use standard augmentations of resizing to 256 ? 256 and applying a random crop of 224 ? 224 and then applying a random horizontal flip. The learning rate is initiated to 0.1 for ResNet50, ResNet101, and ResNet152, after which it is reduced by a factor of 10 every 75 epochs. SGD with momentum is used as the optimizer. The batch size, weight decay and momentum were set to 256, 1e ?4 and 0.9 respectively.</p><p>For all experiments, a default value of p = 0.01 is used. In Tab. 1 and Tab. 2 we compare, for the different architectures, the result of training the network with pAdaIN and  without pAdaIN (Baseline). Other than the use of pAdaIN, which does not add any learnable parameters to the network, the same architecture and training procedure is used. As can be seen, our method outperforms the baseline on the above datasets. The improvement is consistent across networks with a vastly different number of parameters, such as ResNet18 and ResNet50 for CIFAR100 and ResNet50, ResNet101, and ResNet152 for ImageNet. The improvement is also consistent across different model types, such as VGG, Inception, PyramidNet, and ResNet for CIFAR100.</p><p>We consider the effect of changing p on the overall accuracy. This is done for the ResNet18 and ResNet50 models trained on CIFAR100 and ImageNet, respectively. As can be seen in <ref type="figure" target="#fig_3">Fig. 3(d)</ref>, increasing the value of p up to 0.01 results in improved accuracy, after-which accuracy drops.</p><p>Lastly, to qualitatively analyze our method, we considered the two ResNet50 models trained on ImageNet, either with or without pAdaIN. <ref type="figure" target="#fig_2">Fig. 2</ref> depicts four examples along with the GradCAM <ref type="bibr" target="#b33">[34]</ref> visualization for the predicted class pAdaIN concentrates on the foreground and so, for (a), predicts a chainlink fence. The vanilla (baseline) model predicts an ostrich, as it relies more on global statistics. While both answers could be correct, the GT (ground truth) annotation is that of an ostrich and so this is regarded as an error of pAdaIN. In contrast, (c) depicts an image of shark on land. Our model relies less on global context (such as the shark being at sea) and so predicts a shark (which corresponds to the GT annotation). The vanilla model predicts a pillow.  Similarly, for (b) (resp. (d)) image, pAdaIN focuses on persons with military uniform (resp. hook), and the vanilla model on the firetruck (resp. stone wall) in the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Texture and Shape Representation</head><p>To evaluate the resulting feature representation for texture recognition, the texture surface dataset <ref type="bibr" target="#b21">[22]</ref> was employed. It consists of 64 classes, with a total of 8674 images. Since some textures can be similar to others we sample 10% for training and use the rest for test.</p><p>Our training procedure consists of freezing the backbone of the trained model, and training a linear classifier on top of the last representation layer of both ResNet models (before the label logits) to correctly classify the texture class. A high accuracy indicates that the model captures texture more strongly in its representation layer.</p><p>For ImageNet and CIFAR100, we consider the texture accuracy when training with pAdaIN for various values of p. As can be seen in <ref type="figure" target="#fig_3">Fig. 3 (a,b)</ref>, a value of p = 0.01 results in the best performing model. As pAdaIN is applied at each layer with probability p independently, setting p too high can result in an excessive change of statistics, thus resulting in degradation in accuracy. We note that this coincides in the value of p having the best overall accuracy for both ImageNet and CIFAR100, as shown in <ref type="figure" target="#fig_3">Fig. 3(d)</ref>.</p><p>Mostly, an increase (resp. decrease) in the value of p results in an increase (resp. decrease) of both overall accuracy and texture accuracy. To evaluate this connection in the context of previous work that aimed at eliminating texture bias, we repeat the experiment with such a method.</p><p>Specifically, we consider a model trained as described in Geirhos et al., <ref type="bibr" target="#b14">[15]</ref> on a combination of ImageNet and a stylized ImageNet. We measure its texture accuracy, as above. The ImageNet classification accuracy of the employed Shape-ResNet increases from 76.13 to 76.72. Concurrently, the texture accuracy drops from 89.2% to 88.7%. This indicates that unlike our method which preserves local cues, Geirhos et al., <ref type="bibr" target="#b14">[15]</ref> do not. Their increase in performance is due to the increased utilization of global cues at the expense of local ones. Our method improves the accuracy without reducing the recognizability of local textures.</p><p>Furthermore, we demonstrate that, for p ? 0.01, while the representation of global statistics, such as background color is distorted through the use of pAdaIN, the representation of shape is not. To show this, we consider the shape bias measure of Geirhos et al., <ref type="bibr" target="#b14">[15]</ref> on the cue conflict dataset.</p><p>This dataset was crafted to evaluate the shape bias of an ImageNet trained model and is composed of 1280 images. Each image has two labels: a texture label and a shape label. The texture and shape labels are taken from 16 different classes. Each image is the product of performing iterative style transfer <ref type="bibr" target="#b13">[14]</ref> between an image from a texture dataset, containing the texture corresponding to one of the texture classes, and a natural colored image of an object with a white background from one of the shape classes. An example can be seen with the elephant skin (texture) and the cat (shape) adopted from <ref type="bibr" target="#b14">[15]</ref> in <ref type="figure" target="#fig_0">Fig. 1</ref>. A correct prediction is considered a prediction that matches one of the two classes that compose a test image, i.e., either the shape class or the texture class. Given an ImageNet trained model, the shape bias is computed as the proportion of correct shape predictions which the model makes out of all the correct predictions (either correct texture or shape).</p><p>As can be seen in <ref type="figure" target="#fig_3">Fig. 3(c</ref>  <ref type="table">Table 3</ref>. Experimental results for unsupervised domain adaptation and domain generalization (source only) on GTA5 ? Cityscapes in the task of semantic segmentation, using DeepLabv2 <ref type="bibr" target="#b4">[5]</ref> with ResNet101 backbone architecture. up to 0.01 does not degrade the shape bias beyond that of a model with no pAdaIN (p = 0), and, in fact, increases it slightly for p = 0.001. At p = 0.01, both the texture recognition ability ( <ref type="figure" target="#fig_3">Fig. 3(b)</ref>) and the accuracy ( <ref type="figure" target="#fig_3">Fig. 3(d)</ref>) are maximal. Evidently, while our model strengthens local cues as it improves classification, its affinity toward shapebased classification is not reduced. For p &gt; 0.01, we notice a decrease in shape bias and of accuracy. This indicates that using a value of p which is too high may adversely affect the shape representation and subsequently the model's accuracy.</p><p>To further validate that pAdaIN does not support texture classification at the expense of shape, we evaluate our method on two shape oriented datasets. The first is ImageNet-Sketch <ref type="bibr" target="#b41">[42]</ref>, which consists of 50000 sketch-like images, with 50 images for each of the 1000 ImageNet classes. The second is the Edges dataset <ref type="bibr" target="#b14">[15]</ref>, which consists of 160 images of 16 different objects with a white background, processed by the Canny edge extractor <ref type="bibr" target="#b2">[3]</ref>. An ImageNet trained model with pAdaIN (p = 0.01) (no finetuning), achieves 26.0% and 26.9% accuracy on ImageNet-sketch and the Edges dataset, respectively. In contrast, an ImageNet trained model without pAdaIN achieves a lower accuracy of 24.5% and 24.4%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Domain Adaptation</head><p>Of particular interest is the ability of image classifiers to generalize in the settings where the test distribution is shifted compared to the train distribution. In the setting of domain adaption, one is given a labeled source data and an unlabeled target data and is asked to generalize well on both the source and target distributions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>We evaluate our method on the pixel-wise classification task of semantic segmentation, in the setting of domain adaption. At train time, we are given access to training images from both the source and the target domain. However, the semantic segmentation labels are only available for the images from the source domain. The objective is to maximize performance on the target domain.</p><p>We consider the state of the art method of Wang et al., <ref type="bibr" target="#b42">[43]</ref> for which a three-step approach is undertaken, and apply pAdaIN in conjunction with it. In the first step, a model is trained solely on the source domain using both input images and labels. In the second step, the model is initialized with the weights from the first step and is trained with images from the target domain in an unsupervised manner, as well as with source images in a supervised manner. Two losses are employed. The first is a domain confusion loss (class-wise adversarial loss) between the features of the source and target domain. The second loss is a cross entropy pixel-wise loss between the output of the model on source inputs and source labels. In the third step, a pseudo labeling approach is performed. Using the model trained in the second step, pseudo labels are generated for the target images. The network is retrained with these pseudo labels on the target domain.</p><p>pAdaIN is applied during all three stages with the same p = 0.01 as in all other benchmarks. In the second loss of the second step, a slight modification is made to the training procedure of <ref type="bibr" target="#b42">[43]</ref>. In the original setting, only source domain images are used in a given batch. Instead, when using pAdaIN, the inputs from the target domain are concatenated to the batch of source domain images. Thus, when applying the forward step on inputs from the source domain, pAdaIN mixes statistics from the target domain to the source domain. Note that we do not modify the target domains' image features in the forward pass, since we want to adapt to the target domain itself. Also, while normally pAdaIN is applied on a larger batch size, due to GPU memory constraints, a batch-size of two is used, with one image from each domain.</p><p>We evaluate our method on the GTAV dataset <ref type="bibr" target="#b31">[32]</ref> as our source domain and Cityscapes dataset as the target domain <ref type="bibr" target="#b5">[6]</ref> and use the official implementation and training scheme of FADA <ref type="bibr" target="#b42">[43]</ref>. GTAV is a synthetic dataset with 24, 966 urban scene images sourced from a video game rendering engine Grand Theft Auto V and Cityscapes is a real world urban scene dataset with 2975 training images and 500 validation images. An mIOU metric is used for evaluation. For a fair comparison to previous methods, we evaluate each image in a single scale. As can be seen in Tab. 3, our method improves the state of the art when applied in conjunction with FADA <ref type="bibr" target="#b42">[43]</ref>, achieving a gap of 2.3 mIOU. There is also a gap of 1.7 when adding pAdaIN to a model trained only for the first phase, i.e., without access to target domain images. We notice that the greatest improvements occur on large objects, such as buses, trains, trucks, sidewalks and walls and a degradation in the sky category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Multi-domain Generalization</head><p>A more restrictive setting than that of domain adaptation is that of domain generalization, in which the unlabeled target data is not available during training. The "source only" setting for the GTAV to Cityscapes experiment in Tab. 3, described above, is one instance of this problem.</p><p>We evaluate our method for domain generalization on the PACS dataset <ref type="bibr" target="#b25">[26]</ref>. It consists of four domains: photo, art, cartoon, and sketch. We follow the multi-source evaluation protocol of <ref type="bibr" target="#b3">[4]</ref>, training on three out of the four domains and evaluating on the fourth. We compare with the latest domain generalization methods. For the baseline method comparison, we simply train a network on the source data, without further modifications. Our models are trained with SGD, over 30 epochs, batch size 128. The learning rate is set to 0.001. For the RSC <ref type="bibr" target="#b22">[23]</ref> method, we also independently run the method using the open-source implementation as published by the authors, using the default configuration (https://github.com/DeLightCMU/RSC).</p><p>Tab. 4 shows the results of our method against that of baseline methods. As can be seen, when trained with pAdaIN, our method, on average, beats all baseline methods on both ResNet18 and ResNet50, except when considered against the reported values of RSC <ref type="bibr" target="#b22">[23]</ref>. We note that our method outperforms our independently reproduced results of RSC, which follows the official open source implementation. Our results improve over the baseline method especially in the sketch domain. Sketch images consist mostly of the outlines of the objects with no texture. This indicates, as previously shown, that performance based on global cues, such as the object's outline (shape), is enhanced in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Robustness Towards Corruptions</head><p>Convolutional neural networks tend to be sensitive to small perturbations <ref type="bibr" target="#b8">[9]</ref>. These small perturbations affect the statistics of the representation layers of the network. It is thus plausible that a model taught to be insensitive to statistical shifts of the feature space, such as our method, would be more robust towards corruptions. To test this hypothesis, we evaluate our method against ImageNet-C and Cifar-100-C <ref type="bibr" target="#b8">[9]</ref>, corrupted versions of ImageNet and CIFAR100.</p><p>First, we consider a ResNet50 model trained on Ima-geNet with or without pAdaIN. As can be seen in Tab. 5, our method improves upon the baseline method trained without pAdaIN (with p = 0.01). Next, we consider pAdaIN in conjunction with AugMix <ref type="bibr" target="#b17">[18]</ref>, which is the current state of the art. As can be seen, combining pAdaIN with Augmix exceeds Augmix and is thus state of the art. For reference, average test error for additional methods designed for corruption are reported in Tab. 6. Here we note that our smallest improvement is for the noise, blur, pixelated and JPEG cor-  ruptions, since these preserve the global statistics and have a tendency to modify the the fine details. Conversely, weather and contrast corruptions preserve texture and we, therefore, see an overall greater improvement for these categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Analysis</head><p>To further evaluate different variants of pAdaIN, we consider a ResNet18 network trained on CIFAR100, as described in Sec. 4.1. In Tab. 7 we consider the effect of using pAdaIN on specific blocks of the ResNet18 network. In all other experiments, we apply it to all layers. As can be seen, the effect of pAdaIN is most prominent when applied at the deeper blocks, specifically at blocks 3 and 4.</p><p>Next, we wish to understand the importance of using statistics, of the feature representation from natural images. Instead of swapping statistics between the feature representations of images, we swap the statistics of an image's feature representation with random statistics sampled from a normal distribution with zero mean and unit variance. We set the probability for this to happen at p = 0.01, as in the default pAdaIN setting. We observed that as the model converged to minimal loss on the training set, the validation performance was very unstable, both in terms of loss and test accuracy. The overall accuracy is 57.3, which is significantly lower. We believe this is due to the distribution shift from the statistics of natural images, happening with probability p.</p><p>As discussed in Sec. 3.1, in our method, we regard ?(x ?(i) ) and ?(x ?(i) ) as constants and do not backpropagate through them. As can be seen in Tab. 8, setting ?(x i ) and ?(x i ) as constant results in a degradation in performance.   <ref type="table">Table 6</ref>. Classification error in comparison to state of the art baselines on CIFAR-100-C for ResNext <ref type="bibr" target="#b44">[45]</ref> and DenseNet <ref type="bibr" target="#b19">[20]</ref>. pAdaIN in conjunction with Augmix <ref type="bibr" target="#b17">[18]</ref> exceeds the state of the art. Baseline indicates a network trained on CIFAR-100 without any modifications.  Applying backpropagation through ?(x ?(i) ) and ?(x ?(i) ) as well, results in unstable training. In addition, we analyze the effect of applying a fixed permutation across all layers in contrast to uniformly drawing one at each step of pAdaIN independently in the forward pass. To this end, we consider a ResNet18 model trained on CIFAR100, this reduces the accuracy to 68.02, compared to pAdaIN accuracy of 77.82 and 76.13 for the baseline model. Computational time. We measure the additional time incurred by incorporating pAdaIN into training. Using the same computational resources (4? Nvidia V100 GPUs) the training times for a ResNet50 on ImageNet for 300 epochs is 108 hours with and without pAdaIN (p=0.01). Thus training with pAdaIN does not result in increase in time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>While CNN image classifiers are extremely powerful, they are still reliant on global image statistics that are easy to manipulate without changing the image semantics. In this work, we make use of the normalization mechanism in order to remove the reliance on this bias. The method is probabilistic and has a parameter p that controls the tradeoff between training on deliberately mismatched image statistics and employing the matching global statistics. Naturally, there is exploitable information in these statistics that can help in image recognition benchmarks.</p><p>Since the texture is often defined as image statistics, and since previous work has focused on removing the bias toward texture, it is important to make the distinction between texture and global image statistics. As our motivating example shows, texture patterns are largely invariant to changes in global image statistics, even if these occur simultaneously across multiple encoding channels.</p><p>Indeed, contrary to results of methods for correcting texture bias, we demonstrate that the increase in classification performance goes hand in hand with the increase in classification capabilities on texture datasets. We do not believe this to be a misinterpretation by previous work, since we tested the performance of selected texture bias removal methods on texture datasets and observed a decrease in performance. We, therefore, view the two effects as distinct.</p><p>Despite this distinctiveness, both our and texture bias removal methods demonstrate an increase in the recognition ability in the face of domain shifts. As future work, we would like to explore combining the two approaches together.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Applying pAdaIN at inference on different layers of an encoder trained as part of an auto-encoder. The input and reconstructed images are shown on the left. The reconstructed results when applying pAdaIN on different layers of the encoder are shown subsequently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>GradCam<ref type="bibr" target="#b33">[34]</ref> visualizations and predictions for Ima-geNet trained ResNet50 models, with and without pAdaIN. Ground truth labels and model predictions appear on the bottom of each image respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>(a) Texture accuracy for different values of p for a ResNet18 model trained on CIFAR100. (b) As in (a), but for ResNet50 model trained on ImageNet. (c) Shape bias for a ResNet50 model trained on ImageNet, for a range of values of p. (d) Accuracy for models trained with pAdaIN for various values of p. In blue, a ResNet50 model trained on ImageNet and in orange, a ResNet18 model trained CIFAR100.For p values above 0.1 (not shown), accuracy drops significantly below 75% for both ImageNet and CIFAR100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>77.87 74.86 70.17 79.72 D-SAM [11] 95.30 77.33 72.43 77.83 80.72 JiGen [4] 96.03 79.42 75.25 71.35 80.51 MASF [10] 94.99 80.29 77.17 71.69 81.03</figDesc><table><row><cell>Method</cell><cell>Photo</cell><cell>Art</cell><cell>Cart Sketch</cell><cell>Avg</cell></row><row><cell cols="5">Baseline [4] 95.98 E-FCR [27] 93.90 82.10 77.00 73.00 81.50</cell></row><row><cell cols="5">MetaReg [2] 95.50 83.70 77.20 70.30 81.70</cell></row><row><cell>I-Drop [35]</cell><cell cols="4">96.11 80.27 76.54 76.38 82.32</cell></row><row><cell>RSC  *  [23]</cell><cell cols="4">95.99 83.43 80.31 80.85 85.15</cell></row><row><cell>RSC  *  *  [23]</cell><cell cols="4">94.10 78.90 76.88 76.81 81.67</cell></row><row><cell>Ours</cell><cell cols="4">96.29 81.74 76.91 75.13 82.51</cell></row><row><cell>Baseline [4]</cell><cell cols="4">97.66 86.20 78.70 70.63 83.29</cell></row><row><cell>MASF [10]</cell><cell cols="4">95.01 82.89 80.49 72.29 82.67</cell></row><row><cell cols="5">MetaReg [2] 97.60 87.20 79.20 70.30 83.60</cell></row><row><cell>RSC  *  [23]</cell><cell cols="4">97.92 87.89 82.16 83.35 87.83</cell></row><row><cell>RSC  *  *  [23]</cell><cell cols="4">93.72 81.38 80.14 82.31 84.38</cell></row><row><cell>Ours</cell><cell cols="4">97.17 85.82 81.06 77.37 85.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results on multi-source domain generalization on the PACS dataset. Top: ResNet18, Bottom: ResNet50. Highlighted are the best scores per category. We consider RSC<ref type="bibr" target="#b22">[23]</ref> reproduced scores ( * * ) and not the reported ones (</figDesc><table /><note>* ). See Sec. 4.4 for details. Cart stands for Cartoon.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG</figDesc><table><row><cell>Dataset Network</cell><cell cols="2">Architecture E mCE</cell><cell>Noise</cell><cell>Blur</cell><cell>Weather</cell><cell>Digital</cell></row><row><cell cols="4">Gauss. INet-C Baseline ResNet50 22.9 76.7 80 82 83</cell><cell cols="3">75 89 78 80 78 75 66 57 71 85 77 77</cell></row><row><cell>INet-C pAdaIN</cell><cell>ResNet50</cell><cell cols="2">22.3 72.8 78 79 81</cell><cell cols="3">70 87 74 76 74 71 64 55 65 82 66 71</cell></row><row><cell>C100-C Augmix [18]</cell><cell cols="3">DenseNet-BC 24.2 38.9 60 51 41</cell><cell cols="3">27 55 31 29 36 39 35 28 37 33 39 41</cell></row><row><cell cols="4">C100-C Augmix+pAdaIN DenseNet-BC 22.2 37.5 58 49 40</cell><cell cols="3">26 54 30 28 35 38 33 25 36 32 37 40</cell></row><row><cell>C100-C Augmix [18]</cell><cell cols="3">ResNext-29 21.0 34.4 56 48 32</cell><cell cols="3">23 49 27 25 32 35 32 24 32 30 34 37</cell></row><row><cell cols="4">C100-C Augmix+pAdaIN ResNext-29 17.3 31.6 58 48 24</cell><cell cols="3">20 54 23 21 28 30 25 19 27 27 33 36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Clean Top-1 Error (E), Mean Corruption Error (mCE) and Corruption Error values of various corruptions. First, we consider an ImageNet trained ResNet50 model with or without pAdaIN, evaluated on IMAGENET-C (INet-C). Second, we consider DenseNet and ResNext models trained on CIFAR-100 either with Augmix alone or together with pAdaIN and evaluated on CIFAR-100-C (C100-C).</figDesc><table><row><cell></cell><cell cols="4">Baseline Cutout Mixup CutMix</cell><cell>Auto-</cell><cell>Adversarial</cell><cell cols="2">Augmix pAdaIN+</cell></row><row><cell></cell><cell></cell><cell>[8]</cell><cell>[46]</cell><cell>[46]</cell><cell cols="2">Augment [7] Training [30]</cell><cell>[18]</cell><cell>Augmix</cell></row><row><cell>DenseNet-BC</cell><cell>59.3</cell><cell>59.6</cell><cell>55.4</cell><cell>59.2</cell><cell>53.9</cell><cell>55.2</cell><cell>38.9</cell><cell>37.5</cell></row><row><cell>ResNext-29</cell><cell>53.4</cell><cell>54.6</cell><cell>51.4</cell><cell>54.1</cell><cell>51.3</cell><cell>54.4</cell><cell>34.4</cell><cell>31.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Accuracy (bottom) on different block numbers (top) for which pAdaIN is applied on a ResNet18 trained on CIFAR100. 75.2 75.1 76.1 Accuracy for alternative backpropagation schemes for a ResNet18 trained on CIFAR100. Yes indicates backprop and No otherwise. We advocate for the leftmost scheme. Rightmost column is without using pAdaIn. *indicated unstable training.</figDesc><table><row><cell>?(x i ), ?(x i )</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>-</cell></row><row><cell cols="2">?(x ?(i) ), ?(x ?(i) ) No</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>-</cell></row><row><cell>Accuracy</cell><cell cols="2">77.8 77.6</cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC CoG 725974).</p><p>In addition, we would like to sincerely thank Sharon Fogel for her substantial advisory support to this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using metaregularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 26th international conference on computer communication and networks (ICCCN)</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Coelho De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6450" to="6461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain generalization with domain-specific aggregation modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Antonio D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="187" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5927" to="5935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the origins and prevalence of texture bias in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Katherine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kornblith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09071</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A compact convolutional neural network for surface defect inspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-challenging improves cross-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02454</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yi-Zhe Song, and Timothy Hospedales. Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Significance-aware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory, 20th International Conference, ALT</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="4" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Picture structure and its processing -the case of human-face photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint Conference of Electrical Engineers of Japan</title>
		<meeting>Joint Conference of Electrical Engineers of Japan</meeting>
		<imprint>
			<date type="published" when="1971-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Informative dropout for robust representation learning: A shape-bias perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04254</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="10506" to="10518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Interpreting adversarially trained convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09797</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
