<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Wang</surname></persName>
							<email>haonan1wang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
							<email>caopeng@mail.neu.edu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osmar</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
							<email>zaiane@cs.ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Amii</orgName>
								<orgName type="institution" key="instit2">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most recent semantic segmentation methods adopt a U-Net framework with an encoder-decoder architecture. It is still challenging for U-Net with a simple skip connection scheme to model the global multi-scale context: 1) Not each skip connection setting is effective due to the issue of incompatible feature sets of encoder and decoder stage, even some skip connection negatively influence the segmentation performance; 2) The original U-Net is worse than the one without any skip connection on some datasets. Based on our findings, we propose a new segmentation framework, named UC-TransNet (with a proposed CTrans module in U-Net), from the channel perspective with attention mechanism. Specifically, the CTrans (Channel Transformer) module is an alternate of the U-Net skip connections, which consists of a sub-module to conduct the multi-scale Channel Cross fusion with Transformer (named CCT) and a sub-module Channelwise Cross-Attention (named CCA) to guide the fused multiscale channel-wise information to effectively connect to the decoder features for eliminating the ambiguity. Hence, the proposed connection consisting of the CCT and CCA is able to replace the original skip connection to solve the semantic gaps for an accurate automatic medical image segmentation. The experimental results suggest that our UCTransNet produces more precise segmentation performance and achieves consistent improvements over the state-of-the-art for semantic segmentation across different datasets and conventional architectures involving transformer or U-shaped framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Medical imaging is considered as a vital technique to assist doctors to evaluate disease and to optimise prevention and control measures. Segmentation and the subsequent quantitative assessment of target object in medical images provide valuable information for the analysis of pathologies and are important for planning of treatment strategies, monitoring of disease progression and prediction of patient outcome. Recent approaches <ref type="bibr" target="#b18">(Long, Shelhamer, and Darrell 2015;</ref><ref type="bibr" target="#b19">Ni et al. 2020;</ref><ref type="bibr" target="#b28">Wen, Xie, and He 2020)</ref> to semantic segmentation typically rely on convolutional encoder-decoder architectures where the encoder generates low-resolution image features and the decoder up-samples features to segmentation maps with per pixel class scores. U-Net <ref type="bibr" target="#b22">(Ronneberger, Fischer, and Brox 2015)</ref> is the most widely used encoderdecoder network architecture for medical image segmentation, since the encoder captures the low-level and high-level features, and the decoder combines the semantic features to construct the final result. The skip connection can help propagate the spatial information that gets lost during the pooling operation to help recover the full spatial resolution through the encoding-decoding process. To investigate it, we conduct an in-depth study of U-Net and observe several major limitations according to our analysis on multiple datasets. We find that it is still challenging for U-Net with a simple skip connection scheme to model the global multi-scale context for assisting the decoding process without considering the semantic gap. It is necessary to find an effective way to fuse features for precise medical image segmentation. There essentially are two key issues for the extension of U-Net: which layers of the features in the encoders are connected to the decoders for modeling a global contexts through aggregating multi-scale features, and how to effectively fuse the features with possible semantic gap instead of simply concatenating? There exist two semantic gaps: semantic gap among the multi-scale encoder features and between the stages of the encoder and decoder, limiting the segmentation performance. To overcome this aforementioned limitation, a number of approaches have been introduced recently to alleviate the discrepancy when fusing these two incompatible sets of features. One approach is to directly replace the plain skip connections with the nested dense skip pathways for medical image segmentation. The most representative method is UNet++ <ref type="bibr" target="#b32">(Zhou et al. 2018</ref>) which narrows the semantic gap between the encoder and decoder sub-networks by introducing dense connectivity with a series of convolutions and achieves better segmentation performance. It is an improvement over the restrictive skip connections in U-Net requiring the fusion of only same-scale feature maps. The other approach focuses on strengthening the skip connections by introducing additional non-linear transformations on the features propagating from the encoder stage, which should account for or somewhat balance the possible semantic gaps <ref type="bibr" target="#b21">(Rahman 2020)</ref>.</p><p>Despite achieving good performance, both works above are still incapable of effectively exploring sufficient information from full scales. Capturing multi-scale features is essential for resolving complex scale variations in medical image segmentation. Driven by the important issues, a question arises: how to sufficiently bridge the semantic gap between the encoder and decoder through multi-scale channelwise information fusion by effectively capturing the nonlocal semantic dependencies. In this paper, we rethink the skip connection design and propose an alternative method for better connecting the features between the encoder and decoder stages. Different channels usually focus on different semantic patterns, adaptively fusing sufficient channel-wise features is favorable for the complex medical image segmentation. To this end, we propose an end-to-end deep learning network called UCTransNet, which takes U-Net as the main structure of the network. More specifically, we firstly propose a Channel-wise Cross Fusion Transformer (CCT) to fuse the multi-scale context with cross attention from the channel-wise perspective. It aims at capturing local crosschannel interaction to achieve an adaptive scheme for ef-fectively fusing the multi-scale channel-wise features with possible scale semantic gap through collaborative learning rather than independent connection. On the other hand, we propose another channel-wise cross attention (CCA) module for fusing the fused multi-scale features and the features from decoder stages to solve the inconsistent semantic level. Both cross attention modules are called CTrans (Channel Transformer), which can establishes the association between encoder and decoder by exploring the multi-scale global context and replace the original skip connections to solve the semantic gaps for improved segmentation performances. Both proposed modules can be easily embedded in and applied for the U-shape networks in medical image segmentation tasks. Extensive experiments show that UCTransNet can greatly improve conventional segmentation pipelines by the following absolute gains of 4.05% Dice, 7.98% Dice and 9.00% Dice over U-Net on GlaS, MoNuSeg and Synapse datasets, respectively. Moreover, we made a thorough analysis to investigate how the feature interactions work. Besides, previous works have combined both Transformers and U-Net to explicitly model long-range spatial dependency <ref type="bibr" target="#b30">Zhang, Liu, and Hu 2021)</ref>. The results demonstrate that channel-wise fusion transformer scheme generally leads to a better performance than the methods incorporating the transformer to replace the convolution operation. We argue that UCTransNet can serve as strong skip connection scheme for medical image segmentation.</p><p>Our contributions are three-fold. 1) Our study is the first work that sufficiently explores the potential weakness of skip connections in U-Net on multiple datasets and finds that the independent simple copying is not appropriate. 2) We suggest a new perspective to boost semantic segmentation performance, i.e. bridging the semantic and resolution gap between low-level and high-level features by a more effective feature fusion with multi-scale channel-wise cross attention for capturing more sophisticated channel-wise dependencies. 3) Our method is a more appropriate combination of U-Net and Transformer with less computational cost and higher performance. In comparison to other state-of-the-art segmentation methods, the experimental results present better performances on all the three public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works Transformers for Medical Image Segmentation</head><p>Recently, Vision Transformer (ViT) <ref type="bibr" target="#b5">(Dosovitskiy et al. 2020</ref>) achieved state-of-the-art on ImageNet classification by directly applying Transformers with global self-attention to full-sized images. Due to the success of Transformers in many computer vision fields, a new paradigm for medical image segmentation has recently evolved <ref type="bibr" target="#b31">(Zheng et al. 2020;</ref><ref type="bibr" target="#b14">Ji et al. 2021;</ref><ref type="bibr" target="#b8">Gao, Zhou, and Metaxas 2021;</ref><ref type="bibr" target="#b30">Zhang, Liu, and Hu 2021;</ref><ref type="bibr" target="#b10">Hatamizadeh et al. 2021)</ref>. TransUNet  is the first Transformer-based medical image segmentation framework. Valanarasu et al. proposed a Gated Axial-Attention model-MedT <ref type="bibr" target="#b26">(Valanarasu et al. 2021)</ref> to overcome the low number of data samples in medical imaging. Motivated by Swin Transformer ) which achieved state-of-the-art performance, Swin-Unet  proposed the first pure Transformer-based U-shaped architecture which introduced Swin Transformer to replace the convolution blocks in U-Net. However, the aforementioned methods mainly focus on the defects of convolution operation rather than the U-Net it-self, thus may cause structural redundancy and prohibitive computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skip Connections in U-shaped Nets</head><p>The skip connection mechanism was first proposed in U-Net <ref type="bibr" target="#b22">(Ronneberger, Fischer, and Brox 2015)</ref>, which was designed to bridge the semantic gap between encoder and decoder, and have proven to be effective in recovering finegrained details of the target objects <ref type="bibr" target="#b6">(Drozdzal et al. 2016;</ref><ref type="bibr" target="#b11">He et al. 2016;</ref><ref type="bibr" target="#b12">Huang et al. 2017)</ref>. Following the popularity of U-Net, many novel models have been proposed such as UNet++ <ref type="bibr" target="#b32">(Zhou et al. 2018)</ref>, Attention U-Net <ref type="bibr" target="#b20">(Oktay et al. 2018)</ref>, DenseUNet <ref type="bibr" target="#b17">(Li et al. 2018</ref>), R2U-Net <ref type="bibr" target="#b0">(Alom et al. 2018)</ref>, and UNet 3+ <ref type="bibr" target="#b13">(Huang et al. 2020)</ref>, which are specially designed for medical image segmentation and achieve expressive performance. Zhou et al. <ref type="bibr" target="#b32">(Zhou et al. 2018</ref>) believed that the same-scale feature maps from the encoder and decoder networks are semantically dissimilar and thus designed a nested structure named UNet++ which captures multi-scale features to further bridge the gap. Attention-UNet proposed cross-attention module which uses coarsescale features as gating signals to disambiguate irrelevant and noisy responses in skip connections. MultiResUNet (Rahman 2020) observed a possible semantic gap between the skipped encoder features and the decoder features in the same level, thus they introduced the Res Path with residual structure to improve the skip connections (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Skip connection settings These methods assume that each skip connection has equal contribution, however in the next section we will show that the contributions are different among all the skip connections, some may even harm the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Analysis of Skip connection</head><p>In this section, we thoroughly analyze the contribution of the skip connection to the segmentation performance on three datasets. According to the analysis, three findings are highlighted as follows:</p><p>Finding 1: The U-net without any skip connection is even better than the original U-Net. Comparing the results of <ref type="figure" target="#fig_2">Fig. 3</ref>, we can find that 'U-Net-none' shows the worst performance among the algorithms for almost all metrics on the MoNuSeg dataset. However, 'U-Net-none', although without any constraints, still achieves very competitive performance against 'U-Net-all' on the GlaS dataset. It demonstrates that the skip connection is not always beneficial for the segmentation.</p><p>Finding 2: Although UNet-all performs better than UNetnone, not all skip connections with simple copying are useful for segmentation. The contribution of each skip connection is different. We find that the performance range of each skip connection is [67.5%,76.44%] and [52.2%,62.73%] with respect to Dice and IOU on the MoNuSeg dataset. The impact variation is large for the different single skip connection. Furthermore, due to the issue of incompatible feature sets of the encoder and decoder stages, some skip connection negatively influence the segmentation performance. For example, L 1 performs worse than UNet-none in terms of Dice and IOU on the GlaS dataset. The result does not demonstrate that many features from the encoder stage are not informative. The reason behind it may be that the simple copying is not appropriate for the feature fusion.</p><p>Finding 3: The optimal combination of skip contributions is different for different datasets, which depends on the scales and appearance of the target lesions. We run several ablation experiments to explore the best side output settings. Note that we ignore the combination of two skip con- nections due to the limited space. As can be seen, the skip connections does not achieve better performance. The model w/o L 4 is best on the MoNuSeg dataset, whilst to our surprise, the L 3 with only one skip connection performs best on the GlaS dataset. These observations suggest that the optimal combination is different for different datasets which further confirms the necessity of introducing more appropriate course of action for the feature fusion rather than simple connection.</p><p>UCTransNet for Medical Image Segmentation <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates an overview of our UCTransNet framework. To the best of our knowledge, current Transformerbased segmentation methods mainly focus on improving the encoder of U-Net, based on its advantage of capturing long-range information. These methods, such as Tran-sUNet  or TransFuse , blend the Transformer with U-Net in a simple way, i.e. plugging the Transformer module into the encoder or fusing the both independent branches. However, we believe the potential limitation of the current U-Net model is the issue of the skip connection rather than the encoder of the original U-Net, which is sufficient for the most tasks. As mentioned in the section of skip connection analysis, we observe that the feature from the encoder is inconsistent with that from the decoder, i.e. in some cases, the shallower layer features with less semantic information may harm the final performance through the simple skip connection due to the semantic gap between the shallower level encoder and decoder. Inspired by it, we construct the UCTransNet framework by designing a channel-wise Transformer module between the vanilla U-Net encoder and decoder to better fuse the encoder features and reduce the semantic gap. Specifically, we propose a Channel Transformer (CTrans) to replace the skip connections in U-Net, which consists of two modules: CCT (Channel-wise Cross Fusion Transformer) for the multi-scale encoder feature fusion and CCA (Channel-wise Cross Attention) for the fusion of the decoder features and the enhanced CCT features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Cross-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCT: Channel-wise Cross Fusion Transformer for Encoder Feature Transformation</head><p>To solve the skip connection issue mentioned before, we propose a new Channel-wise Cross Fusion Transformer (CCT) to fuse the multi-scale encoder features with the advantage of the long dependency modeling in Transformer.</p><p>The CCT module consists of three steps: multi-scale feature embedding, multi-head channel-wise cross attention and Multi-Layer Perceptron (MLP).</p><p>Multi-scale Feature Embedding Given the outputs of four skip connection layers E i ? R HW i 2 ?Ci , (i = 1, 2, 3, 4), we first perform tokenization by reshaping the features into sequences of flattened 2D patches with patch sizes P, P 2 , P 4 , P 8 respectively, so that the patches can be mapped to the same areas of the encoder features in four scales. We keep the original channel dimensions through this process. Then, we concatenate the tokens of four layers T i (i = 1, 2, 3, 4), T i ? R HW i 2 ?Ci as the key and value T ? = Concat(T 1 , T 2 , T 3 , T 4 ).</p><p>Multi-head Cross-Attention The tokens are then fed into the multi-head channel cross-attention module, followed by a Multi-Layer Perceptron (MLP) with residual structure, to encode channel and dependencies for refining features from each U-Net encoder level using multi-scale features.</p><p>As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, the proposed CCT module contains five inputs, including four tokens T i as queries and a concatenated token T ? as key and value:</p><formula xml:id="formula_0">Q i = T i W Qi , K = T ? W K , V = T ? W V<label>(1)</label></formula><p>where W Qi ? R Ci?d , W K ? R C??d , W V ? R C??d are weights of different inputs, d is the sequence length (patch numbers) and C i (i = 1, 2, 3, 4) are the channel dimensions of the four skip connection layers. In our implementation C 1 = 64, C 2 = 128, C 3 = 256, C 4 = 512.</p><p>With Q i ? R Ci?d , K ? R C??d , V ? R C??d , the similarity matrix M i are produced and the value V is weighted by M i through a cross-attention (CA) mechanism:</p><formula xml:id="formula_1">CA i = M i V = ? ? Q i K ? C ? V = ? ? W Qi T i T ? W K ? C ? W V T ?</formula><p>(2) where ?(?) and ?(?) denote the instance normalization <ref type="bibr" target="#b25">(Ulyanov, Vedaldi, and Lempitsky 2017)</ref> and the softmax function, respectively.</p><p>The major difference from the original self-attention is that we conduct the attention operation along the channelaxis rather than the patch-axis (see <ref type="figure" target="#fig_3">Fig. 4</ref>), and we employ the instance normalization which can normalize the similarity matrix for each instance on the similarity maps so that the gradient can be smoothly propagated. In a N -head attention situation, the output after multi-head cross-attention is calculated as follow:</p><formula xml:id="formula_2">MCA i = (CA 1 i + CA 2 i +, . . . , +CA N i )/N (3)</formula><p>where N is the number of heads. Hereinafter, applying a MLP and residual operator, the output is obtained as follows:</p><formula xml:id="formula_3">O i = MCA i + MLP(Q i + MCA i )<label>(4)</label></formula><p>We omitted the layer normalization (LN) in the equation for simplicity. The operation in Eq. (4) is repeated L times to build a L-layer Transformer. In our implementation, N and L are both set to 4, based on series of experiments with 2, 4, 8 and 12 layers for CCT and we empirically find the one with 4 layers and 4heads can achieve the best performance on the three datasets. Finally, the four outputs of the L-th layer O 1 , O 2 , O 3 and O 4 are reconstructed though an up-sampling operation followed by a convolution layer and concatenated with the decoder features D 1 , D 2 , D 3 and D 4 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCA: Channel-wise Cross Attention for Feature Fusion in Decoder</head><p>In order to better fuse features of inconsistent semantics between the Channel Transformer and U-Net decoder, we propose a channel-wise cross attention module, which can guide the channel and information filtration of the Transformer features and eliminate the ambiguity with the decoder features. Mathematically, we take the i-th level Transformer output O i ? R C?H?W and i-th level decoder feature map D i ? R C?H?W as the inputs of Channel-wise Cross Attention. spatial squeeze is performed by a global average pooling (GAP) layer, producing vector G(X) ? R C?1?1 with its k th channel G(X) = 1 H?W H i=1 W j=1 X k (i, j). We use this operation to embed the global spatial information and then generate the attention mask:</p><formula xml:id="formula_4">M i = L 1 ? G(O i ) + L 2 ? G(D i )<label>(5)</label></formula><p>where L 1 ? R C?C and L 2 ? R C?C and being weights of two Linear layers and the ReLU operator ?(?). This operation in Eq. <ref type="formula" target="#formula_4">(5)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>We use Gland segmentation <ref type="bibr">(Sirinukunwattana et al. 2016</ref>), MoNuSeg <ref type="bibr" target="#b16">(Kumar et al. 2017</ref><ref type="bibr" target="#b15">(Kumar et al. , 2020</ref> and Synapse multiorgan segmentation dataset <ref type="bibr">(Bennett et al. 2015)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We implemented our model with PyTorch on a single NVIDIA A40 GPU card with 48 GB memory. To avoid overfitting, we also performed two kinds of online data augmentations, including horizontal flipping, vertical flipping and random rotating. We do not use any pre-trained weights to train the proposed UCTransNet. For GlaS and MoNuSeg we set the batch size to 4 following <ref type="bibr" target="#b26">(Valanarasu et al. 2021)</ref>, and for Synapse we set it to 24 following ). The input resolution and patch size P are set as 224 ? 224 and 16 for all the three datasets. We employ the Adam optimizer to train our model, where the initial learning rate is set to 0.001. We also employ the combined cross entropy loss and dice loss as our loss function to train our network. To make the results on the small datasets more convincing, we conduct a three times 5-fold cross validation (totally 15 CV), and obtain the mean result and std. A statistical test is used to indicate our method significantly outperforms the comparable methods. For GlaS and MoNuSeg datesets we use dice coefficient (Dice) and Intersection over Union (IoU) as the evaluation metrics while for Synapse we report the Dice and Hausdorff Distance (HD). Note that we use the same settings and loss function for training all the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-art Methods</head><p>To demonstrate the overall segmentation performance of the proposed UCTransNet, we compare it with other state-ofthe-art methods. We compare UCTransNet with two types of methods for comprehensive evaluation, covering three UNet based method: UNet++, Attention U-Net, MultiRe-sUNet and three state-of-the-art transformer based segmentation methods, including TransUNet, MedT, and Swin-Unet. To make a fair comparison, their originally released codes and published settings are used in the experiment. We also introduce two strategies to optimize the models of UC-TransNet. 1) Jointly training: We optimize the convolution and CTrans parameters in U-Net and the two channel-wise cross attention parameters together with a single loss; 2) Pre-training. We first train a U-Net, then the parameters in UCTransNet are further trained with the same data.</p><p>Experimental results are reported in <ref type="table">Table 1</ref> where the best results are boldfaced. <ref type="table">Table 1</ref> shows that our method has consistent improvements over prior arts. In <ref type="table" target="#tab_2">Table 2</ref>, similar observations and conclusions can be made, which once again validates that UCTransNet outperforms all others. Additionally, the pre-training scheme not only achieves a faster convergence speed, but also obtains a better performance than the competing methods, even outperforms the jointly learning scheme on the MoNuSeg dataset. These observations suggest that the two proposed modules can be incorporated into the pre-trained U-Net model for improved segmentation performance. We also provide the parameter number and GFLOPs which show that our model achieves a good trade-off between effectiveness and efficiency.</p><p>We visualize the segmentation results of the comparable models in <ref type="figure">Fig. 6 and Fig. 7</ref>. The red boxes highlight regions where UCTransNet performs better than the other methods. It shows that our UCTransNet generates better segmentation results, which are more similar to the ground truth than the results of the baseline model. It can be easily seen that our proposed method not only highlights the right salient regions   eliminating the confusing false positive lesions but also produces coherent boundaries. These observations suggest that UCTransNet is capable of finer segmentation while preserving detailed shape information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Ablation Studies on Proposed Modules As shown in Table 3, 'Base+CCT+CCA' is generally better than the other baselines on all datasets, which indicates the effectiveness of combination of the two modules. Our results shed new light on the importance of multi-scale multi-channel feature fusion in encoder-decoder framework for improving segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies on the Number of Queries and Keys</head><p>The previous experiments demonstrate that the CCT module in our model is effective for enhancing the skip connections. In the previous experiments, the multi-scale features from all encoder levels engage into the CCT module, thus the number of queries is 4 and the key is the concatenated representation consisting of the four scale features. We perform a series of experiments with respect to the amount of the skip connections between encoders and decoders as illustrated in <ref type="figure" target="#fig_6">Fig. 8</ref>. Note that the key vector is fixed, which is still consisting of the four scale features. We observe consistent improvements with the increase of the number of skip connections. The observation implies the usefulness of multi-scale features learned by different encoder levels, which validates our motivation. It is interesting that 'Q234' is slightly better than our model with all skip connections. Besides, we also keep the number of queries fixed and vary the key to verify concatenating multiscale features. From <ref type="figure" target="#fig_6">Fig. 8</ref>, it can be found that the performance improves with the scale of feature increasing until four scales, which demonstrates that more channels help capture accurate node features, which implies that transforming more scales of features to queries is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Cross Attention Matrix in CCT Module</head><p>To perform a thorough evaluation of our UCTransNet, we visualize the cross attention distributions in our CCT module in <ref type="figure" target="#fig_7">Fig. 9</ref>. It is also interesting to investigate which encoder level has more confident correlation and is more important for segmentation. It can be seen that the 'K 2 ' and 'K 3 ' have a more confident correlation with the other encoder level on the GlaS and MoNuSeg dataset, respectively. The findings are consistent with the skip connection analysis in U-Net in <ref type="figure" target="#fig_2">Fig. 3</ref>. It explains why 'L3' and 'L2' achieve better performances on GlaS and MoNuSeg dataset, respectively. The fact implies the necessity to develop a multi-scale feature fusion to tackle the semantic gap problems, which also validates our motivation to build a global multi-scale channelwise feature fusion model for effectively capturing the nonlocal semantic dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Accurate and automatic segmentation of medical images is a crucial step for clinical diagnosis and analysis. In this work, we introduced a Channel Transformer Segmentation network (UCTransNet) from the channel-wise perspective to provide precise and reliable automatic segmentation of medical images. By combining the strengths of multiscale Channel-wise Cross fusion Transformer (CCT) and recurrent neural networks and Channel-wise Cross-Attention (CCA) in an end-to-end manner, the proposed approach significantly improves the state-of-the-art results in medical image segmentation on multiple benchmark datasets. With indepth analysis and empirical evidence, we show the advantages of the UCTransNet model. It indeed successfully narrows the semantic gap and takes full advantage of the multiscale features in the encoding stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of the skip connection scheme among the proposed UCTransNet (d) and other models. The dash lines denote the skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed UCTransNet. We replace the original skip connections by CTrans consisting of two components: Channel-wise Cross fusion Transformer (CCT) and Channel-wise Cross Attention (CCA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Analysis of different skip connection layers of U-Net. 'All' represents the original U-Net, 'L1' represents only the skip connection of level one is kept and 'w/o L1' represents only the skip connection of level one is removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison between the original self-attention (a) and our proposed channel-wise cross-attention (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Multi-head Cross-Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>The qualitative comparison on the GlaS and MoNuSeg datasets. The qualitative comparison on the Synapse dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Ablation study of the number of queries and keys on GlaS dataset and MoNuSeg dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Similarity Matrix of GlaS dataset (a) and MoNuSeg dataset (b). 'K 1 ' denotes the same feature as 'Q 1 ' concatenated in key.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>50.3 85.45?1.25 74.78?1.67 76.45?2.62 62.86?3.00 UNet++ 74.5 94.6 87.56?1.17 79.13?1.70 77.01?2.10 63.04?2.54 AttUNet 34.9 101.9 88.80?1.07 80.69?1.66 76.67?1.06 63.47?1.16 MRUNet 57.2 78.4 88.73?1.17 80.89?1.67 78.22?2.47 64.83?2.87 TransUNet 105 56.7 88.40?0.74 80.40?1.04 78.53?1.06 65.05?1.28 MedT 98.3 131.5 85.92?2.93 75.47?3.46 77.46?2.38 63.37?3.11 Swin-Unet 82.3 67.3 89.58?0.57 82.06?0.73 77.69?0.94 63.77?1.15 Ours 65.6 63.2 90.18?0.71 * 82.96?1.06 * 79.08?0.67 65.50?0.91</figDesc><table><row><cell>Method</cell><cell cols="2">Param FlOPs</cell><cell>GlaS</cell><cell></cell><cell>MoNuSeg</cell></row><row><cell></cell><cell>(M)</cell><cell>(G) Dice (%)</cell><cell cols="2">IoU (%)</cell><cell>Dice (%)</cell><cell>IoU (%)</cell></row><row><cell cols="6">U-Net 14.8 Table 1: The three times 5-fold cross validation results on</cell></row><row><cell cols="6">GlaS and MoNuSeg datasets. The Dice and IoU are in</cell></row><row><cell cols="6">'mean?std' format. Symbol  *  indicates that our method sig-</cell></row><row><cell cols="6">nificantly outperformed others on that score (Student's t-test</cell></row><row><cell cols="3">at a level of 0.05 is used).</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Methods</cell><cell></cell><cell cols="2">Dice? HD?</cell></row><row><cell></cell><cell cols="2">V-Net (2016)</cell><cell></cell><cell cols="2">68.81</cell><cell>-</cell></row><row><cell></cell><cell cols="2">DARR (2020)</cell><cell></cell><cell cols="2">69.77</cell><cell>-</cell></row><row><cell></cell><cell cols="2">U-Net (2015)</cell><cell></cell><cell cols="2">71.77 53.04</cell></row><row><cell></cell><cell cols="2">R50-U-Net</cell><cell></cell><cell cols="2">74.68 36.87</cell></row><row><cell></cell><cell cols="3">R50-AttUNet (2018)</cell><cell cols="2">75.57 36.97</cell></row><row><cell></cell><cell cols="2">TransUNet (2021)</cell><cell></cell><cell cols="2">77.48 31.69</cell></row><row><cell></cell><cell cols="2">Swin-Unet (2021)</cell><cell></cell><cell cols="2">79.13 21.55</cell></row><row><cell></cell><cell cols="5">UCTransNet (w/o CCA) 78.99 30.29</cell></row><row><cell></cell><cell cols="2">UCTransNet-pre</cell><cell></cell><cell cols="2">75.54 38.97</cell></row><row><cell></cell><cell cols="2">UCTransNet</cell><cell></cell><cell cols="2">78.23 26.75</cell></row></table><note>encodes the channel-wise dependencies. Followed ECA-Net (Wang et al. 2020) which empirically</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>The resultant vector is used to recalibrate or excite O i t?O i = ?(M i ) ? O i ,where the activation ?(M i ) indicates the importance of channels. Finally, the masked? i is concatenated with the up-sampled features of the i-th level decoder.</figDesc><table><row><cell>: Comparison with state-of-the-art segmentation</cell></row><row><cell>methods on Synapse dataset. For simplicity, 'R50-U-Net'</cell></row><row><cell>and 'R50-AttUNet' denote U-Net and Attention U-Net with</cell></row><row><cell>ResNet-50 as backbone, respectively.</cell></row><row><cell>showed avoiding dimensionality reduction is important for</cell></row><row><cell>learning channel attention, we use a single Linear layer</cell></row><row><cell>and sigmoid function to build the channel attention map.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments on GlaS and MoNuSeg datasets. 'CCT' denotes the proposed Channel Transformer and 'CCA' denotes Channel-wise Cross Attention. The best results are boldfaced.</figDesc><table><row><cell></cell><cell>91</cell><cell cols="6">Ablation on Number of Queries and Keys</cell><cell>Q1</cell></row><row><cell></cell><cell>89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q2 Q3</cell></row><row><cell>Dice Value [%]</cell><cell>81 83 85 87 79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q4 Q12 Q34 Q123 Q234 K1 K12</cell></row><row><cell></cell><cell>77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K123</cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K34</cell></row><row><cell></cell><cell>73</cell><cell>Queries</cell><cell>GlaS</cell><cell>Keys</cell><cell>Queries</cell><cell>MoNuSeg</cell><cell>Keys</cell><cell>K234 CTranS</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the National Natural Science Foundation of China (No.62076059), the Fundamental Research Funds for the Central Universities (No. N2016001) and the Science Project of Liaoning province (2021-MS-105).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent Residual Convolutional Neural Network Based on U-Net (R2U-Net) for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhoubing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eugenio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentation Outside the Cranial Vault Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI: Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<title level="m">Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<title level="m">TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Importance of Skip Connections in Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M R S</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<idno>978-3-319</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Data Labeling for Medical Applications</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">10008</biblScope>
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptive relational reasoning for 3d multi-organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MIC-CAI 2020 -23rd International Conference, Proceedings</title>
		<imprint>
			<publisher>Springer Science and Business Media Deutschland GmbH</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="656" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00781</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Instance-Based Vision Transformer for Subtyping of Papillary Renal Cell Carcinoma in Histopathological Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12265</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10504</idno>
		<title level="m">UN-ETR: Transformers for 3D Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>978-1-4673-8851-1</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>978-1-5386-0457-1</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08790</idno>
		<title level="m">UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14385</idno>
		<title level="m">Multi-Compound Transformer for Accurate Biomedical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Multi-Organ Nucleus Segmentation Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">F</forename><surname>Onder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tsougenis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1380" to="1391" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation From CT Volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<idno>arXiv:1606.04797</idno>
		<title level="m">Fully Convolutional Networks for Semantic Segmentation. CVPR, 10. Milletari, F.; Navab, N.; and Ahmadi, S.-A. 2016. V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pyramid Attention Aggregation Network for Semantic Segmentation of Surgical Instruments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11782" to="11790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention U-Net: Learning Where to Look for the Pancreas</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<title level="m">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>B?hm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<imprint>
			<publisher>Cheikh</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Racoceanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R J</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00275</idno>
		<title level="m">Gland Segmentation in Colon Histology Images: The GlaS Challenge Contest</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Instance Normalization: The Missing Ingredient for Fast Stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10662</idno>
		<title level="m">Medical Transformer: Gated Axial-Attention for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno>978-1-72817-168-5</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11531" to="11539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segmenting Medical MRI via Recurrent Decoding Cell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12452" to="12459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A Multi-Branch Hybrid Transformer Networkfor Corneal Endothelial Cell Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Higashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07557</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08005</idno>
		<title level="m">TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">UNet++: A Nested U-Net Architecture for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="DLMIA/ML-CDS@MICCAI" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
