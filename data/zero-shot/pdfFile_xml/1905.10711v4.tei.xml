<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
							<email>weiyuewa@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Adobe Los Angeles</addrLine>
									<settlement>California San Jose</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
							<email>qiangenx@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Adobe Los Angeles</addrLine>
									<settlement>California San Jose</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
							<email>ceylan@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Adobe Los Angeles</addrLine>
									<settlement>California San Jose</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
							<email>rmech@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Adobe Los Angeles</addrLine>
									<settlement>California San Jose</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<email>uneumann@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Adobe Los Angeles</addrLine>
									<settlement>California San Jose</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reconstructing 3D shapes from single-view images has been a long-standing research problem. In this paper, we present DISN, a Deep Implicit Surface Network which can generate a high-quality detail-rich 3D mesh from a 2D image by predicting the underlying signed distance fields. In addition to utilizing global image features, DISN predicts the projected location for each 3D point on the 2D image and extracts local features from the image feature maps. Combining global and local features significantly improves the accuracy of the signed distance field prediction, especially for the detail-rich areas. To the best of our knowledge, DISN is the first method that constantly captures details such as holes and thin structures present in 3D shapes from single-view images. DISN achieves the stateof-the-art single-view reconstruction performance on a variety of shape categories reconstructed from both synthetic and real images. Code is available at https: //github.com/Xharlie/DISN. The supplementary can be found at https: //xharlie.github.io/images/neurips_2019_supp.pdf.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Rendered Image</p><p>OccNet DISN Real Image OccNet DISN <ref type="figure">Figure 1</ref>: Single-view reconstruction results using Occ-Net <ref type="bibr" target="#b0">[1]</ref> and DISN on synthetic and real images.</p><p>Over the recent years, a multitude of single-view 3D reconstruction methods have been proposed where deep learning based methods have specifically achieved promising results. To represent 3D shapes, many of these methods utilize either voxels <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> or point clouds <ref type="bibr" target="#b9">[10]</ref> due to ease of encoding them in a neural network. However, such representations are often limited in terms of resolution. A few recent methods <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> have explored utilizing explicit surface representations in a neural network but make the assumption of a fixed topology, limiting the flexibility of the approaches. Moreover, point-and mesh-based methods use Chamfer Distance (CD) and Earth-mover Distance (EMD) as training losses. However, these distances only provide approximated metrics for measuring shape similarity.</p><p>To address the aforementioned limitations in voxels, point clouds and meshes, in this paper, we study an alternative implicit 3D surface representation, Signed Distance Functions (SDF). SDFs have recently attracted attention from researchers and few other works <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b0">1]</ref> also choose to reconstruct 3D shapes by generating an implicit field. However, such methods either generate a binary occupancy grid or consider only the global information. Therefore, while they succeed in recovering overall shape, they fail to recover fine-grained details. After exploring different forms of the implicit field and the information that preserves local details, we present an efficient, flexible, and effective Deep Implicit Surface Network (DISN) for predicting SDFs from single-view images <ref type="figure">(Figure 1</ref>).</p><p>An SDF simply encodes the signed distance of each point sample in 3D from the boundary of the underlying shape. Thus, given a set of signed distance values, the shape can be extracted by identifying the iso-surface using methods such as Marching Cubes <ref type="bibr" target="#b16">[17]</ref>. As illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>, given a convolutional neural network (CNN) that encodes the input image into a feature vector, DISN predicts the SDF value of a given 3D point using this feature vector. By sampling different 3D point locations, DISN is able to generate an implicit field of the underlying surface with infinite resolution. Moreover, without the need of a fixed topology assumption, the regressing target for DISN is an accurate ground truth instead of an approximated metric.</p><p>While many single-view 3D reconstruction methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref> that learn a shape embedding from a 2D image are able to capture the global shape properties, they have a tendency to ignore details such as holes or thin structures. Such fine-grained details only occupy a small portion in 3D space and thus sacrificing them does not incur a high loss compared to ground truth shape. However, such results can be visually unsatisfactory.</p><p>To address this problem, we introduce a local feature extraction module. Specifically, we estimate the viewpoint parameters of the input image. We utilize this information to project each query point onto the input image to identify a corresponding local patch. We extract local features from such patches and use them in conjunction with global image features to predict the SDF values of the 3D points. This module enables the network to learn the relations between projected pixels and 3D space and significantly improves the reconstruction quality of fine-grained details in the resulting 3D shape. As shown in <ref type="figure">Figure 1</ref>, DISN is able to generate shape details, such as the patterns on the bench back and holes on the rifle handle, which previous state-of-the-art methods fail to produce. To the best of our knowledge, DISN is the first deep learning model that is able to capture such high-quality details from single-view images.</p><p>We evaluate our approach on various shape categories using both synthetic data generated from 3D shape datasets as well as online product images. Qualitative and quantitative comparisons demonstrate that our network outperforms state-of-the-art methods and generates plausible shapes with high-quality details. Furthermore, we also extend DISN to multi-view reconstruction and other applications such as shape interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been extensive studies on learning-based single-view 3D reconstruction using various 3D representations including voxels <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, octrees <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>, points <ref type="bibr" target="#b9">[10]</ref>, and primitives <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. More recently, Sinha et al. <ref type="bibr" target="#b22">[23]</ref> propose to generate the surface of an object using geometry images. Tang et al. <ref type="bibr" target="#b23">[24]</ref> use shape skeletons for surface reconstruction, however, their method requires additional shape primitives dataset. Groueix et al. <ref type="bibr" target="#b10">[11]</ref> present AtlasNet to generate surfaces of 3D shapes using a set of parametric surface elements. Wang et al. <ref type="bibr" target="#b11">[12]</ref> introduce a graph-based network Pix2Mesh to reconstruct 3D manifold shapes from input images whereas Wang et al. <ref type="bibr" target="#b12">[13]</ref> present 3DN to reconstruct a 3D shape by deforming a given source mesh.</p><p>Most of the aforementioned methods use explicit 3D representations and often suffer from problems such as limited resolution and fixed mesh topology. Implicit representations provide an alternative representation to overcome these limitations. In our work, we adopt the Signed Distance Functions (SDF) which are among the most popular implicit surface representations. Several deep learning approaches have utilized SDFs recently. Dai et al. <ref type="bibr" target="#b13">[14]</ref> use a voxel-based SDF representation for shape inpainting. Nevertheless, 3D CNNs are known to suffer from high memory usage and computation cost. Park et al. <ref type="bibr" target="#b14">[15]</ref> introduce DeepSDF for shape completion using an auto-decoder structure. However, their network is not feed-forward and requires optimizing the embedding vector during test time which limits the efficiency and capability of the approach. Chen and Zhang <ref type="bibr" target="#b15">[16]</ref> use SDFs in deep networks for the task of shape generation. While their method achieves promising results for the generation task, it fails to recover fine-grained details of 3D objects for single-view reconstruction. Mescheder et al. <ref type="bibr" target="#b0">[1]</ref> learns an implicit representation by predicting the probability of each cell in a volumetric grid being occupied or not, i.e., being inside or outside of a 3D model. By iteratively subdividing each active cell (i.e., cells surrounded by occupied and empty cells) into sub-cells and repeating the prediction for each sub-cell, they alleviate the problem of the limited resolution of volumetric grids. Finally, in concurrent work, Saito et al. <ref type="bibr" target="#b24">[25]</ref> utilize local image features to predict if a 3D point sample is inside or outside the surface of a mesh and demonstrate high quality human reconstruction results. In contrast, our method not only predicts the sign (i.e., being inside or outside) of sampled points but also the distance which is continuous. We compare our method with recent approaches in Section 4.1 and demonstrate state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given an image of an object, our goal is to reconstruct a 3D shape that captures both the overall structure and fine-grained details of the object. We consider modeling a 3D shape as a signed distance function (SDF). As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, SDF is a continuous function that maps a given spatial point p = (x, y, z) ? R 3 to a real value s ? R: s = SDF (p). Instead of more common 3D representations such as depth <ref type="bibr" target="#b25">[26]</ref>, the absolute value of s indicates the distance of the point to the surface, while the sign of s represents if the point is inside or outside the surface. An iso-surface S 0 = {p|SDF (p) = 0} implicitly represents the underlying 3D shape. In this paper, we use a feed-forward deep neural network, Deep Implicit Surface Network (DISN), to predict the SDF from an input image. DISN takes a single image as input and predicts the SDF value for any given point. Unlike the 3D CNN methods <ref type="bibr" target="#b13">[14]</ref> which generate a volumetric grid with fixed resolution, DISN produces a continuous field with arbitrary resolution. Moreover, we introduce a local feature extraction method to improve recovery of shape details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DISN: Deep Implicit Surface Network</head><p>The overview of our method is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. Given an image, DISN consists of two parts: camera pose estimation and SDF prediction. DISN first estimates the camera parameters that map an object in world coordinates to the image plane. Given the predicted camera parameters, we project each 3D query point onto the image plane and collect multi-scale CNN features for the corresponding image patch. DISN then decodes the given spatial point to an SDF value using both the multi-scale local image features and the global image features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Features</head><p>Global Features p(x,y,z) Concat <ref type="figure">Figure 3</ref>: Local feature extraction. Given a 3D point p, we use the estimated camera parameters to project p onto the image plane. Then we identify the projected location on each feature map layer of the encoder. We concatenate features at each layer to get the local features of point p.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Camera Pose Estimation</head><p>Given an input image, our first goal is to estimate the corresponding viewpoint. We train our network on the ShapeNet Core dataset <ref type="bibr" target="#b26">[27]</ref> where all the models are aligned. Therefore we use this aligned model space as the world space where our camera parameters are with respect to, and we assume a fixed set of intrinsic parameters. Regressing camera parameters from an input image directly using a CNN often fails to produce accurate poses as discussed in <ref type="bibr" target="#b27">[28]</ref>. To overcome this issue, Insafutdinov and Dosovitskiy <ref type="bibr" target="#b27">[28]</ref> introduce a distilled ensemble approach to regress camera pose by combining several pose candidates. However, this method requires a large number of network parameters and a complex training procedure. We present a more efficient and effective network illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. In a recent work, Zhou et al. <ref type="bibr" target="#b28">[29]</ref> show that a 6D rotation representation is continuous and easier for a neural network to regress compared to more commonly used representations such as quaternions and Euler angles. Thus, we employ the 6D rotation representation</p><formula xml:id="formula_0">b = (b x , b y ), where b ? R 6 ,b x ? R 3 , b y ? R 3 . Given b, the rotation matrix R = (R x , R y , R z ) T ? R 3?3 is obtained by R x = N (b x ), R z = N (R x ? b y ), R y = R z ? R x ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">R x , R y , R z ? R 3 , N (?)</formula><p>is the normalization function, '?' indicates cross product. Translation t ? R 3 from world space to camera space is directly predicted by the network. Instead of calculating losses on camera parameters directly as in <ref type="bibr" target="#b27">[28]</ref>, we use the predicted camera pose to transform a given point cloud from the world space to the camera coordinate space. We compute the loss L cam by calculating the mean squared error between the transformed point cloud and the ground truth point cloud in the camera space:</p><formula xml:id="formula_2">L cam = pw?P Cw ||p G ? (Rp w + t))|| 2 2 pw?P Cw 1 ,<label>(2)</label></formula><p>where P C w ? R N ?3 is the point cloud in the world space, N is number of points in P C w . For each p w ? P C w , p G represents the corresponding ground truth point location in the camera space and || ? || 2 2 is the squared L 2 distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">SDF Prediction with Deep Neural Network</head><p>Given an image I, we denote the ground truth SDF by SDF I (?), and the goal of our network f (?) is to estimate SDF I (?). Unlike the commonly used CD and EMD losses in previous reconstruction methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, our guidance is a true ground truth instead of approximated metrics. Park et al <ref type="bibr" target="#b14">[15]</ref> recently propose DeepSDF, a direct approach to regress SDF with a neural network. DeepSDF concatenates the location of a query 3D point and the shape embedding extracted from a depth image or a point cloud and uses an auto-decoder to obtain the corresponding SDF value. The auto-decoder structure requires optimizing the shape embedding for each object. In our initial experiments, when we applied a similar network architecture in a feed-forward manner, we observed convergence issues. Alternatively, Chen and Zhang <ref type="bibr" target="#b15">[16]</ref> propose to concatenate the global features of an input image and the location of a query point to every layer of a decoder. While this approach works better in practice, it also results in a significant increase in the number of network parameters. Our solution is to use a multi-layer perceptron to map the given point location to a higher-dimensional feature space. This high dimensional feature is then concatenated with global and local image features respectively and used to regress the SDF value. We provide the details of our network in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>(a) (b) <ref type="figure">Figure 6</ref>: Shape reconstruction results (a) without and (b) with local feature extraction.</p><p>Local Feature Extraction As shown in <ref type="figure">Figure 6</ref>(a), our initial experiments showed that it is hard to capture shape details such as holes and thin structures when only global image features are used. Thus, we introduce a local feature extraction method to focus on reconstructing fine-grained details, such as the back poles of a chair ( <ref type="figure">Figure 6</ref>). As illustrated in <ref type="figure">Figure 3</ref>, a 3D point p ? R 3 is projected to a 2D location q ? R 2 on the image plane with the estimated camera parameters. We retrieve features on each feature map corresponding to location q and concatenate them to get the local image features. Since the feature maps in the later layers are smaller in dimension than the original image, we resize them to the original size with bilinear interpolation and extract the resized features at location q.</p><p>Two decoders then take the global and local image features respectively as input with the point features and make an SDF prediction. The final SDF is the sum of these two predictions. <ref type="figure">Figure 6</ref> compares the results of our approach with and without local feature extraction. With only global features, the network is able to predict the overall shape but fails to produce details. Local feature extraction helps to recover these missing details by predicting the residual SDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions</head><p>We regress continuous SDF values instead of formulating a binary classification problem (e.g., inside or outside of a shape) as in <ref type="bibr" target="#b15">[16]</ref>. This strategy enables us to extract surfaces that correspond to different iso-values. To ensure that the network concentrates on recovering the details near and inside the iso-surface S 0 , we propose a weighted loss function. Our loss is defined by</p><formula xml:id="formula_3">L SDF = p m|f (I, p) ? SDF I (p)|, m = m 1 , if SDF I (p) &lt; ?, m 2 , otherwise,<label>(3)</label></formula><p>where | ? | is the L 1 -norm. m 1 , m 2 are different weights, and for points whose signed distance is below a certain threshold ?, we use a higher weight of m 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Surface Reconstruction</head><p>To generate a mesh surface, we firstly define a dense 3D grid and predict SDF values for each grid point. Once we compute the SDF values for each point in the dense grid, we use Marching Cubes <ref type="bibr" target="#b16">[17]</ref> to obtain the 3D mesh that corresponds to the iso-surface S 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform quantitative and qualitative comparisons on single-view 3D reconstruction with state-ofthe-art methods <ref type="bibr">[11-13, 16, 1]</ref> in Section 4.1. We also compare the performance of our method on Input 3DN AtlasNet Pix2Mesh 3DCNN IMNET OccNet Ourscam Ours GT <ref type="figure">Figure 7</ref>: Single-view reconstruction results of various methods. 'GT' denotes ground truth shapes. Best viewed on screen with zooming in.</p><p>camera pose estimation with <ref type="bibr" target="#b27">[28]</ref> in Section 4.2. We further conduct ablation studies in Section 4.3 and showcase several applications in Section 4.4. More qualitative results and all detailed network architectures can be found in supplementary.</p><p>Dataset For both camera prediction and SDF prediction, we follow the settings of <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b0">1]</ref>, and use the ShapeNet Core dataset <ref type="bibr" target="#b26">[27]</ref>, which includes 13 object categories, and an official training/testing split to train and test our method. We train a single network on all categories and report the test results generated by this network. Choy et al. <ref type="bibr" target="#b29">[30]</ref> provide a dataset of renderings of ShapeNet Core models where each model is rendered from 24 views with limited variation in terms of camera orientation. In order to make our method more general, we provide a new 2D dataset 1 composed of renderings of the models in ShapeNet Core. Specifically, for each mesh model, our dataset provides 36 renderings with smaller variation(similar to <ref type="bibr" target="#b29">[30]</ref>'s) and 36 views with a larger variation(bigger yaw angle range and larger distance variation). Unlike Choy et al., we allow the object to move away from the origin, therefore, providing more degrees of freedom in terms of camera parameters. We ignore the "Roll" angle of the camera since it is very rare in a real-world scenarios. We also render higher resolution images (224 by 224 instead of the original 137 by 137). Finally, to facilitate future studies, we also pair each rendered RGBA image with a depth image, a normal map and an albedo image as shown in <ref type="figure" target="#fig_4">Figure 8</ref>. Data Preparation and Implementation Details For each 3D mesh in ShapeNet Core, we first generate an SDF grid with resolution 256 3 using <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Models in ShapeNet Core are aligned and we choose this aligned model space as our world space where each render view in <ref type="bibr" target="#b29">[30]</ref> represents a transformation to a different camera space.</p><p>We train our camera pose estimation network and SDF prediction network separately. For both networks, we use VGG-16 <ref type="bibr" target="#b32">[33]</ref> as the image encoder. When training the SDF prediction network, we extract the local features using the ground truth camera parameters. As mentioned in Section 3.1, DISN is able to generate a signed distance field with an arbitrary resolution by continuously sampling points and regressing their SDF values. However, in practice, we are interested in points near the iso-surface S 0 . Therefore, we use Monte Carlo sampling to choose 2048 grid points under Gaussian distribution N (0, 0.1) during training. We choose m 1 = 4, m 2 = 1, and ? = 0.01 as the parameters of Equation 3. Our network is implemented with TensorFlow. We use the Adam optimizer with a learning rate of 1 ? 10 ?4 and a batch size of 16.</p><p>For testing, we first use the camera pose prediction network to estimate the camera parameters for the input image and feed the estimated parameters as input to SDF prediction. We follow the aforementioned surface reconstruction procedure (Section 3.2) to generate the output mesh.</p><p>Evaluation Metrics For quantitative evaluations, we apply four commonly used metrics to compute the difference between a reconstructed mesh object and its ground truth mesh: (1) Chamfer Distance (CD), (2) Earth Mover's Distance (EMD) between uniformly sampled point clouds, (3) Intersection over Union (IoU) on voxelized meshes, and (4) F-Score <ref type="bibr" target="#b33">[34]</ref>. The definitions of CD and EMD can be found in the supplemental.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single-view Reconstruction Comparison With State-of-the-art Methods</head><p>In this section, we compare our approach on single-view reconstruction with state-of-the-art methods: AtlasNet <ref type="bibr" target="#b10">[11]</ref>, Pixel2Mesh <ref type="bibr" target="#b11">[12]</ref>, 3DN <ref type="bibr" target="#b12">[13]</ref>, OccNet <ref type="bibr" target="#b0">[1]</ref> and IMNET <ref type="bibr" target="#b15">[16]</ref>. AtlasNet <ref type="bibr" target="#b10">[11]</ref> and Pixel2Mesh <ref type="bibr" target="#b11">[12]</ref> generate a fixed-topology mesh from a 2D image. 3DN <ref type="bibr" target="#b12">[13]</ref> deforms a given source mesh to reconstruct the target model. When comparing to this method, we choose a source mesh from a given set of templates by querying a template embedding as proposed in the original work. IMNET <ref type="bibr" target="#b15">[16]</ref> and OccNet <ref type="bibr" target="#b0">[1]</ref> both predict the sign of SDF to reconstruct 3D shapes. Since IMNET trains an individual model for each category, we implement their model following the original paper and train a single model on all 13 categories. Due to mismatch between the scales of shapes reconstructed by our method and OccNet, we only report their IoU, which is scale-invariant. In addition, we train a 3D CNN model, denoted by '3DCNN', where the encoder is the same as DISN and a decoder is a volumetric 3D CNN structure with an output dimension of 64 3 . The ground truth for 3DCNN is the SDF values on all 64 3 grid locations. For both IMNET and 3DCNN, we use the same surface reconstruction method as ours to output reconstructed meshes. We also report the results of DISN using estimated camera poses and ground truth poses, denoted by 'Ours cam ' and 'Ours' respectively. AtlasNet, Pixel2Mesh, and 3DN use explicit surface generation, while 3DCNN, IMNET, OccNet, and our methods reconstruct implicit surfaces.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, DISN outperforms all other models in EMD and IoU. Only 3DN performs better than our model on CD, however, 3DN requires more information than ours in the form of a source mesh as input. <ref type="figure">Figure 7</ref> shows qualitative results. As illustrated in both quantitative and qualitative results, implicit surface representation provides a flexible method of generating topology-variant 3D meshes. Comparisons to 3D CNN show that predicting SDF values for given points produces smoother surfaces than generating a fixed 3D volume using an image embedding. We speculate that this is due to SDF being a continuous function with respect to point locations. It is harder for a deep network to approximate an overall SDF volume with global image features only. Moreover, our method outperforms IMNET and OccNet in terms of recovering shape details. For example, in <ref type="figure">Figure 7</ref>, local feature extraction enables our method to generate different patterns of the chair backs in the first three rows, while other methods fail to capture such details. We further validate the effectiveness of our local feature extraction module in Section 4.3. Although using ground truth camera poses (i.e., 'Ours') outperforms using predicted camera poses (i.e., 'Ours cam ') in quantitative results, respective qualitative results demonstrate no significant difference. We also compute the F-score (see <ref type="table" target="#tab_2">Table 2</ref>) which measures the percentage of surface area that is reconstructed correctly and thus provides a reliable metric <ref type="bibr" target="#b33">[34]</ref>. In our evaluations, we use   <ref type="table">Table 3</ref>: Camera pose estimation comparison. The unit of d 2D is pixels.</p><formula xml:id="formula_4">F 1 = 2 * (Precision ? Recall)/(Precision + Recall)</formula><p>. We uniformly sample points from both ground truth and generated meshes. We define precision as the percentage of the generated points whose distance to the closest ground truth point is less than a threshold. Similarly, we define recall as the percentage of ground truth points whose distance to the closest generated point is less than a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Camera Pose Estimation</head><p>We compare our camera pose estimation with <ref type="bibr" target="#b27">[28]</ref>. Given a point cloud P C w in world coordinates for an input image, we transform P C w using the predicted camera pose and compute the mean distance d 3D between the transformed point cloud and the ground truth point cloud in camera space. We also compute the 2D reprojection error d 2D of the transformed point cloud after we project it onto the input image. <ref type="table">Table 3</ref> reports d 3D and d 2D of <ref type="bibr" target="#b27">[28]</ref> and our method. With the help of the 6D rotation representation, our method outperforms <ref type="bibr" target="#b27">[28]</ref> by 2 pixels in terms of 2D reprojection error.</p><p>We also train and test the pose estimation on the new 2D dataset. Even these images possess more view variation, because of the better rendering quality, we can achieve an average 2D distance of 4.38 pixels on 224 by 224 images (2.67 pixels if normalized to the original resolution of 137 by 137).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>To show the impact of the camera pose estimation, local feature extraction, and different network architectures, we conduct ablation studies on the ShapeNet "chair" category, since it has the greatest variety. <ref type="table">Table 4</ref> reports the quantitative results and <ref type="figure">Figure 9</ref> shows the qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Binarycam Binary Global Onestreamcam Onestream Twostreamcam Twostream GT <ref type="figure">Figure 9</ref>: Qualitative results of our method using different settings. 'GT' denotes ground truth shapes, and ' cam ' denotes models with estimated camera parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera Pose Estimation</head><p>As is shown in Section 4.2, camera pose estimation potentially introduces uncertainty to the local feature extraction process with an average reprojection error of 2.95 pixels. Although the quantitative reconstruction results with ground truth camera parameters are constantly superior to the results with estimated parameters in <ref type="table">Table 4</ref>, <ref type="figure">Figure 9</ref> demonstrates that a small difference in the image projection does not affect the reconstruction quality significantly.</p><p>Binary Classification Previous studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> formulate SDF prediction as a binary classification problem by predicting the probability of a point being inside or outside the surface S 0 . Even though Section 4.1 illustrates our superior performance over <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, we further validate the effectiveness of our regression supervision by comparing with classification supervision using our own network structure. Instead of producing a SDF value, we train our network with classification supervision and output the probability of a point being inside the mesh surface. We use a softmax cross entropy loss to optimize this network. We report the result of this classification network as 'Binary'. Local Feature Extraction Local image features of each point provide access to the corresponding local information that captures shape details. To validate the effectiveness of this information, we remove the 'local features extraction' module from DISN and denote this setting by 'Global'. This model predicts the SDF value solely based on the global image features. By comparing 'Global' with other methods in <ref type="table">Table 4</ref> and <ref type="figure">Figure 9</ref>, we conclude that local feature extraction helps the model capture shape details and improve the reconstruction quality by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Structures</head><p>To further assess the impact of different network architectures, in addition to our original architecture with two decoders (which we call 'Two-stream'), we also introduce a 'One-stream' architecture where the global features, the local features, and the point features are concatenated and fed into a single decoder which predicts the SDF value. Detailed structure of this architecture can be found in the supplementary. As illustrated in <ref type="table">Table 4</ref> and <ref type="figure">Figure 9</ref>, the original Two-stream setting is slightly superior to One-stream, which shows that DISN is robust to different network architectures.  <ref type="table">Table 4</ref>: Quantitative results on the category "chair". CD (?0.001), EMD (?100) and IoU (%). Shape interpolation <ref type="figure" target="#fig_5">Figure 10</ref> shows shape interpolation results where we interpolate both global and local image features going from the leftmost sample to the rightmost. We see that the generated shape is gradually transformed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Applications</head><p>Test with online product images <ref type="figure">Figure 11</ref> illustrates 3D reconstruction results by DISN on online product images. Note that our model is trained on rendered images, this experiment validates the domain transferability of DISN. <ref type="figure">Figure 11</ref>: Test our model on online product images.</p><p>Multi-view reconstruction Our model can also take multiple 2D views of the same object as input. After extracting the global and the local image features for each view, we apply max pooling and use the resulting features as input to each decoder. We have retrained our network for 3 input views and visualize some results in <ref type="figure" target="#fig_0">Figure 12</ref>. Combining multi-view features helps DISN to further address shape details. In this paper, we present DISN, a deep implicit surface network for single-view reconstruction. Given a 3D point and an input image, DISN predicts the SDF value for the point. We introduce a local feature extraction module by projecting the 3D point onto the image plane with an estimated camera pose. With the help of such local features, DISN is able to capture fine-grained details and generate high-quality 3D models. Qualitative and quantitative experiments validate the superior performance of DISN over state-of-the-art methods and the flexibility of our model. Though we achieve state-of-the-art performance in single-view reconstruction, our method is only able to handle objects with a clear background since it's trained with rendered images. To address this limitation, our future work includes extending SDF generation with texture prediction using a differentiable renderer <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of SDF. (a) Rendered 3D surface with s = 0. (b) Crosssection of the SDF. A point is outside the surface if s &gt; 0, inside if s &lt; 0, and on the surface if s = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Given an image and a point p, we estimate the camera pose and project p onto the image plane. DISN uses the local features at the projected location, the global features, and the point features to predict the SDF of p. 'MLPs' denotes multi-layer perceptrons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Camera Pose Estimation Network. 'PC' denotes point cloud. 'GT Cam' and 'Pred Cam' denote the ground truth and predicted cameras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Each view of each object has four representations correspondingly</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Shape interpolation result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 :</head><label>12</label><figDesc>Multi-view reconstruction results. (a) Single-view input. (b) Reconstruction result from (a). (c)&amp;(d) Two other views. (e) Multiview reconstruction result from (a), (c) and (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>.80 3.14 2.73 3.01 2.81 5.85 3.80 2.65 2.71 3.39 2.14 2.75 3.13 3D CNN 3.36 2.90 3.06 2.52 3.01 2.85 4.73 3.35 2.71 2.60 3.09 2.10 2.67 3.00 Ours cam 2.67 2.48 3.04 2.67 2.67 2.73 4.38 3.47 2.30 2.62 3.11 2.06 2.77 2.84 .98 6.98 13.76 17.04 13.21 7.18 38.21 15.96 4.59 8.29 18.08 6.35 15.85 13.19 Pxl2mesh 6.10 6.20 12.11 13.45 11.13 6.39 31.41 14.52 4.51 6.54 15.61 6.04 12.66 11.28 3DN 6.75 7.96 8.34 7.09 17.53 8.35 12.79 17.28 3.26 8.27 14.05 5.18 10.20 9.77 IMNET 12.65 15.10 11.39 8.86 11.27 13.77 63.84 21.83 8.73 10.30 17.82 7.06 13.25 16.61 3D CNN 10.47 10.94 10.40 5.26 11.15 11.78 35.97 17.97 6.80 9.76 13.35 6.30 9.80 12.30 Ours cam 9.96 8.98 10.19 5.39 7.71 10.23 25.76 17.90 5.58 9.16 13.59 6.40 11.91 10.98 Ours 9.01 8.32 9.98 4.92 7.54 9.58 22.73 16.70 4.36 8.71 13.29 6.21 10.87 10.17 76.9 52.6 51.5 36.2 58.0 50.5 67.2 50.3 70.9 57.4 55.3 OccNet 54.7 45.2 73.2 73.1 50.2 47.9 37.0 65.3 45.8 67.1 50.6 70.9 52.1 56.4 DISN cam 57.5 52.9 52.3 74.3 54.3 56.4 34.7 54.9 59.2 65.9 47.9 72.9 55.9 57.0 DISN 61.7 54.2 53.1 77.0 54.9 57.7 39.7 55.9 68.0 67.1 48.9 73.6 60.2 59.4 Quantitative results on ShapeNet Core for various methods.Metrics are CD (?0.001, the smaller the better), EMD (?100, the smaller the better) and IoU (%, the larger the better). CD and EMD are computed on 2048 points.</figDesc><table><row><cell></cell><cell></cell><cell>plane bench box car chair display lamp speaker rifle sofa table phone boat Mean</cell></row><row><cell></cell><cell cols="2">AtlasNet 3.39 3.22 3.36 3.72 3.86 3.12 5.29 3.75 3.35 3.14 3.98 3.19 4.39 3.67</cell></row><row><cell></cell><cell cols="2">Pxl2mesh 2.98 2.58 3.44 3.43 3.52 2.92 5.15 3.56 3.04 2.70 3.52 2.66 3.94 3.34</cell></row><row><cell></cell><cell>3DN</cell><cell>3.30 2.98 3.21 3.28 4.45 3.91 3.99 4.47 2.78 3.31 3.94 2.70 3.92 3.56</cell></row><row><cell>EMD</cell><cell cols="2">IMNET 2.90 2Ours 2.45 2.41 2.99 2.52 2.62 2.63 4.11 3.37 1.93 2.55 3.07 2.00 2.55 2.71</cell></row><row><cell cols="3">CD AtlasNet 5IoU AtlasNet 39.2 34.2 20.7 22.0 25.7 36.4 21.3 23.2 45.3 27.9 23.3 42.5 28.1 30.0 Pxl2mesh 51.5 40.7 43.4 50.1 40.2 55.9 29.1 52.3 50.9 60.0 31.2 69.4 40.1 47.3 3DN 54.3 39.8 49.4 59.4 34.4 47.2 35.4 45.3 57.6 60.7 31.3 71.4 46.4 48.7 IMNET 55.4 49.5 51.5 74.5 52.2 56.2 29.6 52.6 52.3 64.1 45.0 70.9 56.6 54.6</cell></row><row><cell></cell><cell cols="2">3D CNN 50.6 44.3 52.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>[28]</cell><cell>Ours</cell><cell>Ours new</cell></row><row><cell cols="3">d 3D 0.073 0.047</cell><cell>0.059</cell></row><row><cell>d 2D</cell><cell>4.86</cell><cell cols="2">2.95 4.38/2.67</cell></row><row><cell>: F-Score for varying thresholds (% of reconstruction</cell><cell></cell><cell></cell></row><row><cell>volume side length, same as [34]) on all categories.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Xharlie/ShapenetRender_more_variation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking reprojection: Closing the loop for pose-aware shape reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning shape priors for single-view 3d completion and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning single-view 3d reconstruction with limited pose supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Shape inpainting using 3d generative adversarial network and recurrent convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suya</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AtlasNet: A Papier-M?ch? Approach to Learning 3D Surface Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01654</idno>
		<title level="m">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3dn: 3d deformation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Shape completion using 3d-encoderpredictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05103</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02822</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Marching cubes: A high resolution 3d surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvey</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM siggraph computer graphics</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical surface prediction for 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adaptive o-cnn: A patch-based deep representation of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07917</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Duygu Ceylan, and Derek Hoiem. 3d-prnn: Generating shape primitives with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Im2struct: Recovering 3d shape structure from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Surfnet: Generating 3d shape surfaces using deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Unmesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A skeleton-bridged deep learning approach for generating meshes of complex topologies from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05172</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep rgb-d canonical correlation analysis for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suya</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Shapenet: An information-rich 3d model repository. arxiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of shape and pose with differentiable point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07035</idno>
		<title level="m">On the continuity of rotation representations in neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Signed distance fields for polygon soup meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jernej</forename><surname>Barbi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface</title>
		<meeting>Graphics Interface</meeting>
		<imprint>
			<publisher>Canadian Information Processing Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vega: non-linear fem deformable object simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fun Shing Sin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jernej</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barbi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="36" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">What do single-view 3d reconstruction networks learn?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3405" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
