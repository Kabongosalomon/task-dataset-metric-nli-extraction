<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KGPool: Dynamic Knowledge Graph Context Selection for Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
							<email>abhishek.nadgeri@rwth-aachen.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anson</forename><surname>Bastos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaiah</forename><forename type="middle">Onando</forename><surname>Mulang&amp;apos;</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Africa</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
							<email>johannes.hoffart@gs.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeedeh</forename><surname>Shekarpour</surname></persName>
							<email>sshekarpour1@udayton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Dayton 5 Cerence GmbH</orgName>
								<address>
									<addrLine>6 Goldman Sachs</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Saraswat</surname></persName>
							<email>vijay.saraswat@gs.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerotha</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rwth</forename><surname>Aachen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iit</forename><surname>Hyderabad</surname></persName>
						</author>
						<title level="a" type="main">KGPool: Dynamic Knowledge Graph Context Selection for Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel method for relation extraction (RE) from a single sentence, mapping the sentence and two given entities to a canonical fact in a knowledge graph (KG). Especially in this presumed sentential RE setting, the context of a single sentence is often sparse. This paper introduces the KGPool method to address this sparsity, dynamically expanding the context with additional facts from the KG. It learns the representation of these facts (entity alias, entity descriptions, etc.) using neural methods, supplementing the sentential context. Unlike existing methods that statically use all expanded facts, KGPool conditions this expansion on the sentence. We study the efficacy of KGPool by evaluating it with different neural models and KGs (Wikidata and Freebase). Our experimental evaluation on standard datasets shows that by feeding the KG-Pool representation into a Graph Neural Network, the overall method is significantly more accurate than state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) are the foundation for many downstream applications and are growing ever larger. However, due to the sheer volume of knowledge and the world's dynamic nature where new entities emerge and unknown facts about them are learned, KGs need to be continuously updated. Distantly supervised Relation Extraction (RE) is an important KG completion task aiming at finding a semantic relationship between two entities annotated on the unstructured text with respect to an underlying knowledge graph <ref type="bibr" target="#b36">(Ye and Ling, 2019)</ref>. In the literature, researchers mainly studied two variants in the RE: 1) multi-instance RE and 2) sentential RE. The multi-instance RE assumes that in a given bag of sentences, if two entities participate in a relation, there exists at least one sentence with these two entities, which wd:Q266569 wd:Q568631 wdt:P106 "Animator" "occupation" "Marc Davis" " American artist and animator"  <ref type="figure">Figure 1</ref>: Illustration of Knowledge Graph Context associated with the annotated entities in a sentence. Here, entity aliases do not play any role in the understanding of the sentence for finding the KG relation. a a sentence taken from <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref> may contain the target relation <ref type="bibr" target="#b18">(Riedel et al., 2010;</ref><ref type="bibr" target="#b26">Vashishth et al., 2018)</ref>. In this setting, researchers aim to incorporate contextual signals from the previous occurrences of an entity pair into the neural models to support relation extraction <ref type="bibr" target="#b36">(Ye and Ling, 2019;</ref><ref type="bibr" target="#b34">Xu and Barbosa, 2019;</ref><ref type="bibr" target="#b32">Wu et al., 2019)</ref>. In contrast, sentential RE restricts the scope of document context only to the input sentence (disregards other occurrences of entity pairs) while predicting the KG relation <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref>. Hence, sentential RE makes the multi-instance setting more difficult by limiting the available context.</p><p>Recent approaches for RE not only base KGs for relation inventory but also consider it for extending contextual knowledge for further improvement of RE task <ref type="bibr" target="#b26">(Vashishth et al., 2018;</ref><ref type="bibr" target="#b0">Bastos et al., 2021)</ref>. A few multi-instance RE methods rely on entity attributes (properties) such as descriptions, aliases, and types (as additional context) along with entity pair occurrences from previous sentences to improve the overall extraction quality <ref type="bibr" target="#b8">(Ji et al., 2017;</ref><ref type="bibr" target="#b26">Vashishth et al., 2018)</ref>. For the sentential RE, the RECON approach <ref type="bibr" target="#b0">(Bastos et al., 2021)</ref> aims to effectively encode KG context derived from the entity attributes and entity neighborhood triples. RECON employs a Graph Neural Network (GNN) as a context aggregator for combining sentential context (annotated entities and sentence) and structured KG representation. Although the additional KG context has a positive effect on the overall relation extraction in multiinstance and sentential RE settings, not all KG context forms are necessary for every input sentence. Consider <ref type="figure">Figure 1</ref>, where the task is to infer a semantic relation 'occupation' between two entities wd:Q568631 (Marc Davis) 1 and wd:Q266569 (animator). Wikidata <ref type="bibr" target="#b28">(Vrandecic, 2012)</ref> KG provides semantic information such as description, instance-of, and aliases about entities. Here, the entity alias (Marc Fraser Davis, Fraser Davis for wd:Q568631; and cartoonist for wd:Q266569) has no impact on understanding the sentence because the entities are explicitly mentioned in the sentence. Furthermore, there is empirical evidence in the literature that for several sentences, statically adding all KG context offered minimal or negative impact <ref type="bibr" target="#b0">(Bastos et al., 2021)</ref>. Hence, there are open research questions as to how an RE approach can dynamically utilize the sufficient context from KG and whether or not the selected KG context positively impacts the overall performance?</p><p>This paper studies these concerning questions proposing the KGPool approach. KGPool utilizes a self-attention mechanism in a Graph Convolution Network (GCN) <ref type="bibr" target="#b11">(Kipf and Welling, 2017)</ref> for selecting a sub-graph from the KG to extend the sentential context. The concept of dynamically mapping the structural representation of a KG to a latent representation of a sentence has not been widely studied in prior literature. In RE, KGPool is the initial attempt. The existing approaches <ref type="bibr" target="#b0">(Bastos et al., 2021;</ref><ref type="bibr" target="#b34">Xu and Barbosa, 2019;</ref><ref type="bibr" target="#b32">Wu et al., 2019;</ref><ref type="bibr" target="#b26">Vashishth et al., 2018)</ref> feed all the available context (either derived from a bag of sentences or a KG or both) into a neural model and relied on the model to figure out the consequences, resulting in limited performance in many cases <ref type="bibr" target="#b0">(Bastos et al., 2021)</ref>. Conversely, we study the efficacy of KGPool in dynamically choosing KG context for the sentential RE task using two standard community datasets (NYT Freebase <ref type="bibr" target="#b18">(Riedel et al., 2010)</ref>, and Wikidata (Sorokin 1 wd: binds to https://www.wikidata.org/wiki/ and Gurevych, 2017)). Our work makes the following key contributions:</p><p>? The KGPool approach dynamically selects structural knowledge and transform it into a representation suitable to supplement the latent representation of sentential context learned using a neural model. We deduce that KGPool is the first approach that works independently of the underlying context aggregators used in the literature <ref type="bibr">(Graph Neural Network (Zhu et al., 2019)</ref> or LSTM-based <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref>). ? We are the first to map the task of KG Context Selection to a Graph Pooling Problem. Therefore, our proposed approach legitimizes the application of graph pooling algorithms for choosing the relevant context. ? KGPool, paired with a GNN as context aggregator, significantly outperforms the existing baselines on both datasets, in one experiment increasing the precision by 12 points over to baseline (P@30 on NYT Freebase). Furthermore, our empirical results (cf., Table 3) conclude that an LSTM model paired with KGPool is able to notably outperform a GNN-based approach <ref type="bibr">(Zhu et al., 2019)</ref> and nearly all multi-instance baselines <ref type="bibr" target="#b36">(Ye and Ling, 2019;</ref><ref type="bibr" target="#b32">Wu et al., 2019;</ref><ref type="bibr" target="#b26">Vashishth et al., 2018)</ref> published in the recent years.</p><p>This paper is structured as follows: Section 2 reviews the related work. Section 3 formalizes the problem and the proposed approach is described in Section 4. Section 5 describes the experimental setup. The results are in Section 6. We conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-instance RE: a few multi-instance RE approaches use convolution neural network <ref type="bibr" target="#b19">(dos Santos et al., 2015)</ref>, attention CNN <ref type="bibr" target="#b30">(Wang et al., 2016)</ref> and attention-based recurrent neural models for relation extraction <ref type="bibr">(Zhou et al., 2016)</ref>. Other approaches such as <ref type="bibr" target="#b8">(Ji et al., 2017;</ref><ref type="bibr" target="#b26">Vashishth et al., 2018)</ref> incorporate entity descriptions, entity and relation aliases from KG to supplement context from the previous sentences. Work in <ref type="bibr" target="#b26">(Vashishth et al., 2018</ref>) employs a graph convolution network to encode entity and relation aliases derived from Wikidata. HRERE <ref type="bibr" target="#b34">(Xu and Barbosa, 2019)</ref> proposes an approach for jointly learning sentence and KG representation using cross-entropy loss function. To effectively capture the available entity context in the documents, Ye and Ling (2019) suggest an approach incorporating intra-bag and inter-bag attentions. For a detailed survey, we point readers to <ref type="bibr" target="#b23">(Smirnova and Cudr?-Mauroux, 2018)</ref>. Sentential RE: researchers <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref> utilized additional relations present in the sentence to assist the process of extracting the target relation using an LSTM-based model. <ref type="bibr">GP-GNN (Zhu et al., 2019)</ref> generates parameters of GNN based on the input sentence, which enables GNNs to process-relational reasoning on unstructured text inputs. RECON <ref type="bibr" target="#b0">(Bastos et al., 2021)</ref> is an approach that uses the entity attributes (aliases, labels, descriptions, instanceof) and KG triples to signal an underlying GNN model for sentential RE. Authors conclude that the multi-instance requirement can be relaxed provided a good representation of KG context to enrich the sentential RE model. However, RECON and multi-instance approaches <ref type="bibr" target="#b34">(Xu and Barbosa, 2019;</ref><ref type="bibr" target="#b26">Vashishth et al., 2018)</ref> utilize statically derived context from the KG, i.e., KG context does not vary depending the sentence.</p><p>Graph Pooling and Dynamic Context Selection: researchers proposed several models for the graph classification aka. graph pooling task <ref type="bibr" target="#b3">(Cangea et al., 2018;</ref><ref type="bibr" target="#b37">Ying et al., 2018;</ref><ref type="bibr" target="#b6">Gao and Ji, 2019)</ref>. These models employ various approaches such as graph topology-based <ref type="bibr" target="#b17">(Rhee et al., 2018)</ref>, and by learning the hierarchical graph-structure <ref type="bibr" target="#b37">(Ying et al., 2018)</ref>. Another graph pooling model relies on node features and topological information using self-attention  in which a specific number of nodes are always eliminated. In KGPool, the elimination of nodes depends on a context coefficient and node importance (Section 4). For context selection, a recent work focuses on dynamically selecting the KG context to optimize a Pre-Trained Language Model (PLM) for entity typing and relation classification <ref type="bibr" target="#b25">(Su et al., 2020)</ref>. KGPool has the following fundamental differences compared to <ref type="bibr" target="#b25">(Su et al., 2020)</ref>: KGPool inspires its self-attention mechanism from <ref type="bibr" target="#b27">Vaswani et al., 2017)</ref> to learn a representation of the KG context. Hence, KGPool works agnostic of the underlying model used for the context aggregation (unlike <ref type="bibr" target="#b25">Su et al. (2020)</ref>, which is tightly coupled with PLM). Approaches such as <ref type="bibr">Zhang et al., 2018;</ref><ref type="bibr" target="#b9">Kang et al., 2020)</ref> also perform dynamic context selection for respective tasks. However, these approaches are not focused on knowledge graph context selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement</head><p>We define the KG as a tuple given by KG = (E, R, T + ) where E denotes the set of all vertices in the graph representing entities, R is the set of edges representing relations, and T + ? E ? R ? E is a set of all KG triples. The RE Task predicts the target relation r c ? R between a given pair of entities e i , e j from the sentence W = (w 1 , w 2 , ..., w l ). If no relation is inferred, it returns 'NA' label. We aim for the sentential RE task which put a constraint that the sentence within which a given pair of entities occurs is the only visible sentence from the bag of sentences. We view RE as a classification task similar to <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref>. In a KG triple ? = (e h , r, e t ) ? T + , the relation r ? R, e h is the head entity (relation origin) while e t is the tail entity. For each entity, associated semantic properties such as entity label, description, instance-of, and aliases are known as entity attribute (At e ) (cf., graph construction step of <ref type="bibr">Figure 2)</ref>. We aim to model KG contextual information to improve the classification. This is achieved by learning the effective representations of the sets At e , e h , e t , and W (cf. section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KGPool Approach</head><p>KGPool consists of three components illustrated in <ref type="figure">Figure 2</ref>: 1) Graph Construction aggregates the sentence, entities and its attributes as a Heterogeneous Information Graph (HIG) for input representation. 2) Context Pooling step utilizes a self-attention mechanism in a graph convolution to calculate attention scores for entity attributes using node features and graph topology. The pooling process allows KGPool to construct a Context Graph (CG), which is a contextualized representation of HIG with lesser number of nodes. 3) Context Aggregator takes as input the sentence, entities, contextual representations of HIG, and classifies the target relation between the entities. We detail the approach in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Construction</head><p>As first step, we extract entity attributes (At e ) from public dumps of Freebase <ref type="bibr">(Bollacker et al.,</ref>   <ref type="figure">Figure 2</ref>: KGPool approach has three components to supplement sentential context with necessary KG context. <ref type="bibr" target="#b28">(Vrandecic, 2012)</ref> KGs depending on the dataset. For the KG context, we rely on the most commonly available properties of the entities as suggested by <ref type="bibr" target="#b0">(Bastos et al., 2021)</ref>: aliases, description, instance-of, and label. An example of various entity attributes is given in <ref type="figure">Figure  2</ref> at the Graph Construction step. Then, the sentence W is transformed to another representation using Bi-LSTM <ref type="bibr" target="#b20">(Schuster and Paliwal, 1997)</ref> by concatenating its word and character embeddings:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2007) and Wikidata</head><formula xml:id="formula_0">W = BiLSTM(W)<label>(1)</label></formula><p>Similar representation is created for each entity e i where i = (h, t):</p><formula xml:id="formula_1">e i = BiLSTM(e i )<label>(2)</label></formula><p>For entity e i , its KG contexts (entity attributes) At e i j (where j = [0...N ]) are independently converted into associated embedding representations:</p><formula xml:id="formula_2">At j e i = BiLSTM(At e i j )<label>(3)</label></formula><p>For a knowledge representation of the KG context concerning the sentential context (sentence and annotated entities), we introduce the special graph HIG = (A, F ), a Heterogeneous Informa-tion Graph, represented in the adjacency matrix A ? {0, 1} n?n , where n is the maximum number of neighboring nodes for an entity e i . Here, F ? R n?f is the node feature matrix assuming each node has f features learned from the Bi-LSTM in the equations 1, 2, and 3. In these equations, BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, or any other recent Transformer-based model can be used. Due to hardware limitations, we are bound to Bi-LSTM using Glove embeddings <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context Pooling</head><p>Context pooling is built upon three layers of Graph Convolutional Networks (GCN) and a readout layer associated with each of them. Moreover, the last layer of GCN is coupled with a pooling layer (cf., ablation studies for architectural design choice experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Graph Convolution</head><p>Since KGPool is expected to select the sufficient context, the Context Graph CG is a reduction of HIG using the mapping ? : HIG ?? CG. The challenge here is the no natural notion of spatial locality, i.e., it is not viable to pool together all context nodes in an "m ? m" patch on HIG be-cause the complex topological structure of graphs prevents any straightforward, deterministic definition of a "patch". Furthermore, entities nodes have a varying number of neighboring nodes, making the graph pooling challenging (similar to other graph classification problems <ref type="bibr" target="#b37">(Ying et al., 2018)</ref>).</p><p>In HIG, entity nodes do not contain information of their neighbors. Hence, we aim to enrich each entity node with the adjacent node's contextual information. Therefore, we employ a GNN variant to utilize its message-passing architecture to learn node embeddings from a message propagation function. The message propagation function depends on the adjacency matrix A, trainable parameters ?, and the node embeddings F <ref type="bibr" target="#b37">(Ying et al., 2018)</ref>. We rely on a GCN model by <ref type="bibr" target="#b11">Kipf and Welling (2017)</ref>. The GCN layer is defined as:</p><formula xml:id="formula_3">F (k) = ReLU D ? 1 2?D ? 1 2 F (k?1) ? (k?1) (4) where? = A + I,D = j? i,j and ? (k)</formula><p>is the trainable matrix. The GCN module might run k iteration and normally is in the range of two to six <ref type="bibr" target="#b37">(Ying et al., 2018)</ref>. A few graph representation learning approaches propose to use readout layer that aggregates node features to learn a fixed size representation <ref type="bibr" target="#b33">(Xu et al., 2018;</ref><ref type="bibr" target="#b3">Cangea et al., 2018)</ref>. We perform this summarizing after each block of the network (Equation 4), and aggregate all of the intermediate representation together by taking their sum. We define readout layer R as:</p><formula xml:id="formula_4">R (k) = 1 N N i=1 F (k) i N max i=1 F (k) i<label>(5)</label></formula><p>where N is the number of nodes in the graph and F is the node feature embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">KG Self-Attention Mask</head><p>Until </p><formula xml:id="formula_5">Z = tanh D ? 1 2?D ? 1 2 F (k) ? att (6)</formula><p>where ? att ? R F ?1 is the only parameter of the pooling layer. For ranking, we take the attention score and pass it through a softmax layer where Z score is the normalized self attention score.</p><formula xml:id="formula_6">Z score = exp (Z i )/ i exp (Z i )<label>(7)</label></formula><p>After Equation 7, we compute the scores for each entity attribute node. Next, we propose a node selection method in which nodes are selected on the basis of Context Coefficient ? which is a hyper parameter. The top nodes are selected as:</p><formula xml:id="formula_7">idx = max (Z score ) ? ? * ?(Z score ) (8)</formula><p>where ?(Z score ) is the standard deviation of Z score , idx represents the node selection result, and Z mask is the corresponding attention mask. Equation 8 acts as a soft constraint in selecting the context nodes for each HIG which depends on the value of ?. Learning ? during training may cause over-fitting. Hence, we decided to consider ? as a trade-off parameter similar to ? in regularization <ref type="bibr" target="#b2">(B?hlmann and Van De Geer, 2011)</ref>. Next, the Context Graph (CG) is formed by pooling out the less essential entity attribute nodes as:</p><formula xml:id="formula_8">F = F (k) idx,: , F out = F Z mask , A out = A idx,idx<label>(9)</label></formula><p>In addition to the dynamically selected nodes, we also inherit the intermediate node and graph representations of k ? 1 layers similar to ResNET <ref type="bibr" target="#b7">(He et al., 2016)</ref>. The intermediate representations (k ?1 ) and the CG (k th layer) is given as follows:</p><formula xml:id="formula_9">e h = F (1) e h ? F (2) e h ? ....F (k) e h e t = F (1) et ? F (2) et ? ....F (k) et W = F (1) W ? F (2) W ? ....F (k) W R = R (1) ? R (2) ? ....R (k)<label>(10)</label></formula><p>where in the i th layer: F (i) e l is the node embedding of e l , l = (h, t), F (i) W is the node embedding of sentence W, and R (i) is the readout. In the k th layer, F (k) is the F out from Equation 9. The ? is the concatenation among the vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Context Aggregator</head><p>Finally, KGPool combines the latent representation (sentential context) with the structured representation learned in Equation 10. As such, we employ a model M which learns latent relation vec-tor r . In the state-of-the-art approaches that use KG context, the representation of r is learned using sentential and all static KG context <ref type="bibr" target="#b26">(Vashishth et al., 2018;</ref><ref type="bibr" target="#b0">Bastos et al., 2021)</ref>. However, in KG-Pool, relation r is realized based on the sentential context and dynamically chosen KG context. Hence, we employ context aggregators similar to the baselines (section 5.3) for jointly learning the enriched KG context in the form of CG and sentential context. The final relation is:</p><formula xml:id="formula_10">P (r | e h , e t , W ) = sof tmax( MLP( r ? e h ? e t ? W ? R )) (11)</formula><p>5 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We consider two standard datasets (English version): Wikidata dataset <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref> and NYT Freebase <ref type="bibr" target="#b18">(Riedel et al., 2010)</ref>. Both datasets were annotated using distant supervision (associated stats are in <ref type="table" target="#tab_3">Table 1</ref>). Datasets include 'NA' as one of the target relations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">KGPool Configurations</head><p>KGPool is configured with two context aggregator modules. We inherit context aggregators from existing sentential RE baselines. Our experimental aim is to assess as how KGPool performs along with the state-of-the-art context aggregators (comparative study). Our two settings are: 1. KGP ool +lstm : KGPool is coupled with a context aware LSTM model from <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref> as context aggregator.</p><p>2. KGP ool +gnn : this implementation has KG-Pool plugged-in with a variant of GNN module used by <ref type="bibr">(Zhu et al., 2019;</ref><ref type="bibr" target="#b0">Bastos et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline Models</head><p>We consider the recent sentential RE approaches for our empirical study: RECON <ref type="bibr" target="#b0">(Bastos et al., 2021)</ref>: induces KG context (entity attributes and 1&amp;2 hop entity triples) along with the sentence in a GNN.  <ref type="bibr" target="#b26">(Vashishth et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Metrics and Hyper-parameters</head><p>Graph Construction step (section 4.1) use a Bi-LSTM with one hidden layer of size 50 <ref type="bibr" target="#b0">(Bastos et al., 2021)</ref>. The word embedding dimension is 50 initialized by Glove embeddings <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref>. The context pooling parameters are from . For model M , we used the default parameters provided by the authors <ref type="bibr">(Zhu et al., 2019;</ref><ref type="bibr" target="#b24">Sorokin and Gurevych, 2017)</ref>. For brevity, details are in the appendix. Metric and Optimization: Our experiment settings are borrowed from <ref type="bibr" target="#b0">(Bastos et al., 2021)</ref>. Hence, on Wikidata dataset, we use (micro) precision (P), recall (R), and F-score (F1). On the NYT Freebase dataset, (micro) P@10 and P@30 is reported. P@K here represents precision at K percent recall. We also study the effect of Context Coefficient (?) for both KGPool configurations (trained end-to-end). We ignore the probability predicted for the NA relation during testing. We employ the Adam optimizer (Kingma and Ba, 2015) with categorical cross entropy loss where each model is run three times on the whole training set. For the P/R curves (with best ? values of KGPool variants), the result from the first run of each model is selected. For ablation, we use the McNemar's test for statistical significance to find if the reduction in error in the KGPool configura- tions are significant. The differences in the models is statistically significant if the p ? value &lt; 0.05 <ref type="bibr" target="#b5">(Dietterich, 1998)</ref>. We release all experiment code and data on a public GitHub 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We conduct our experiments and analysis in response to the question RQ: "What is the efficacy of KGPool in dynamically selecting the KG context for the sentential RE task?" As such, we also compare KGPool against approaches that do not dynamically treat the context.   can observe that even when the available context is limited to entity attributes, the KGP ool +gnn variant surpasses RECON that also contains context from 1&amp;2 hop triples besides the entity attributes. RECON-EAC and KGP ool +gnn rely on entity attributes as KG context with the same context aggregator. When KGP ool +gnn variants choose KG context dynamically, they perform better than RECON-EAC. It is interesting to notice that when an LSTM model is fed with the dynamically chosen context, the performance gain is more than ten absolute points (KGP ool +lstm Vs Context-LSTM), even outperforming GP-GNN. Performance on NYT Freebase Dataset: Similar to the Wikidata dataset, the KGP ool +gnn variants significantly outperform all baselines (cf. <ref type="table" target="#tab_7">Table 3</ref>). The P@30 is comparatively high for KGP ool +gnn against baselines. The behavior could be interpreted as follows: dynamically adding context from the KG for the entity pairs   keeps the precision higher over a more extended recall range. For both datasets, KGPool configurations (KGP ool +gnn and KGP ool +lstm ) have the best-reported performance varying as per the ?. This validates our choice to introduce a soft constraint in selecting the context nodes (cf., Equation 8). The P/R curves in <ref type="figure" target="#fig_1">Figure 3</ref> show that KGPool performs better than baselines over the entire recall range. We conclude that the effective dynamic context selection by KGPool has a positive impact on the sentential RE task (which successfully answers our research question).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Studies</head><p>We conducted two ablation studies to understand the behavior of KGPool configurations: Significance of Dynamic Context Selection: we perform McNemar's test for the best KGPool configuration against the previous sentential state-ofthe-art (i.e. RECON). The results in <ref type="table" target="#tab_9">Table 4</ref> are statistically significant on both datasets, illustrating KGPool's robustness. Although KGP ool +gnn variants achieve statistically significant results against RECON, there exist several sentences for which our approach is unable to select supplementary KG context ((RW ) values in the contingency table). It requires further investigation, and we plan it for our future work. Effect on the Degree of Nodes for Entities: for studying the effect of context pooling (Section 4.2), we also conducted a study to understand the impact of KGPool on the reduction of the average degree of entity nodes (e i ) in the HIG. <ref type="table" target="#tab_11">Table 5</ref> summarizes the effect of Context Coefficient on the average degree of entity nodes. Irrespective of ?, KGPool notably reduces the degree of e i by removing less relevant nodes. Architectural Choice Experiment: In KGPool, we chose to introduce pooling in the last layer of a three-layered architecture (three blocks). To support our choice, we performed several additional experiments by introducing pooling in various layers. We employ the Wikidata dataset for our experiments. We use best configuration of our model ( KGP ool +gnn (?=1)) and created several variants of it. For instance, KGP ool +gnn (P =all) comprises the configuration where we introduce pooling in all three GCN blocks. The configuration KGP ool +gnn ( ?=2&amp;3) has no pooling in the first layer but has a pooling layer in the remaining two GCN blocks. KGP ool +gnn is the best configuration of KGPool where pooling is just in the final layer. In <ref type="table">Table 6</ref>, we observe that KGP ool +gnn with pooling only in the last GCN block has the superior performance compared to other two variants. Here, the first two layers are used to learn the node features, which are then employed with self-attention for node selection. Our experiments justify the architectural choice decision. However, with a newer graph pooling technique, such decisions will solely depend on the performance of the approach, and we can not generalize the results of these experiments.  <ref type="table">Table 6</ref>: When we introduce pooling in all three layers or in two layers, the performance of KGPool's variants drop. Hence, it justify our choice to add pooling only in the third layer that gives the best performance (values in bold). We use best configuration of our model (KGP ool +gnn (?=1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case-Studies:</head><p>To understand the KGPool's performance gain, we report a few top relations in <ref type="table" target="#tab_14">Table 7</ref>. It can be observed from this table that in a few cases, with lesser context, KGPool can perform significantly better. In the next case study, to understand the KGPool's performance while adding additional context (more noise), we induce extra context in the form of 1&amp;2-hop triples along with entity attributes. For the same, we considered KGPool's best configurations on the Wikidata dataset. The configurations KGP ool +gnn (+T ) and KGP ool +lstm (+T ) represent KGPool fed with additional triple context. For both configurations agnostic of underlying aggregator, we observe a slight increase in performance <ref type="table">(Table 8</ref>). There are several triples which are the irrelevant source of information not needed for a given sentence. KGPool can remove that information and does not suffer the performance drop due to added noise in the context. Details on error analysis, performance for worst performing individual relations, and on a human-annotated dataset are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>KGPool RECON GP-GNN vocal specialization 1.00 0.00 0.00 list of works   <ref type="table">Table 8</ref>: To scale the sources of the contexts, we induce additional triple context in the KGPool shown as (+T ) configurations. We use best configurations of our model (KGP ool +gnn (?=1) and KGP ool +lstm (?=1)). We observe a slight jump in the performance, however, KGPool is still able to pool irrelevant context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Conclusion</head><p>Although KGs are often employed for providing background context in the RE tasks (cf. Section 2), yet there is limited research about defining relevant context. In this work, we proposed KG-Pool and provide a set of experiments proving: 1) Given the limited context that is in individual sentences, dynamically bringing context from KG significantly improves the RE performance. 2) We introduced Context Coefficient (?), which acts as a soft constraint in determining the relevant entity context nodes. 3) Our approach KGPool is invariant of the context aggregator and enables us to learn effective knowledge representation of the required KG context for a given sentential context. Our evaluation concerns several key questions:</p><p>? Data quality impact on an effective knowledge representation: in spite KGPool's significant performance, there exist several sentences for which our model finds a limitation compared to the baseline (cf. <ref type="table" target="#tab_9">Table 4</ref>). One potential interpretation could be about the noise injected due to the data quality of the KG context <ref type="bibr" target="#b31">(Weichselbraun et al., 2018)</ref>. Hence, how does the quality of contextual data impact the performance of context selection approaches is an open direction.</p><p>? Impact of additional sources of KG context: In ablation, we provide a study by adding 1 &amp; 2-hop triples in addition to entity attributes. There is no significant increase in the performance, although KGPool is able to remove irrelevant context for a given sentence. Furthermore, we did not consider edge features in HIG although KGPool can be extended to support edge features using techniques such as <ref type="bibr" target="#b22">(Simonovsky and Komodakis, 2017)</ref>. Additional experiments are needed to verify that our empirical observations hold in this setting, and we leave it for future work.</p><p>Overall, KGPool provides an effective knowledge representation for set-ups where sentence context is sparse. It is interesting to observe that effective knowledge representation learned using KGPool paired with an LSTM model outperforms <ref type="bibr">GP-GNN (Zhu et al., 2019)</ref>, and nearly all multiinstance baselines. Our conclusive results open a new research direction: is it possible to apply effective context selection techniques coupled with deep learning models to other downstream NLP tasks? For example, our results can encourage researchers to extend KGPool or develop novel context selection methods for the tasks where KGs have been extensively used as additional background knowledge, such as in entity linking , KG completion <ref type="bibr" target="#b29">(Wang et al., 2020;</ref><ref type="bibr" target="#b21">Shi et al., 2017)</ref>, and recommendation system  8 Ethics/ Impact Statement:</p><p>In this work, we present significant progress in solving sentential RE task. Harvesting knowledge is an essential goal that human beings seek along with the advancement of technology. This research and many RE approaches rely on additional signals from the public KGs to design systems that extract structured knowledge from unstructured contents. When it comes to who may be disadvantaged from this research, we do not think it is applicable since our study of addressing the KG context capabilities is still at an early stage. Having said so, we are fully supporting the development of ethical and responsible AI. The potential bias in the standard public datasets that may lead to wrong knowledge needs to be cleaned or corrected with validation mechanisms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Due to page limit, we could not put several empirical results in the main paper. This section describes the remaining empirical studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Error Analysis</head><p>To understand the failure cases of KGPool, we conducted exhaustive error analysis. We calculated (micro) F1-Score of each relation in Sorokin dataset <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref>. <ref type="table" target="#tab_3">Table 10</ref> illustrates performance of ten relations on which KGPool performs the worst (ascending order of Micro-F1 score). To put the study in the right perspective, we also report all sentential RE baselines' performance on these relations. While analyzing the errors, we observe three patterns. First, all models fail in the relations for which number of instances are sparse. For example, the relation mother has only 190 instances (occurances) and the relation killed by has 48 instances.  In the main paper, we presented the effect of context pooling on KGPool's best configuration (KGP ool +gnn ). <ref type="table" target="#tab_17">Table 9</ref> describes the reduction in the average degree of nodes for KGP ool +lstm configuration for various context coefficient (?). On both datasets, there is a significant reduction in the degree of nodes. On Wikidata dataset <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref>, KGP ool +lstm with (?=1) reports the highest value among its other configurations. For the same, the average degree of nodes is reduced from 5.33 to 1.06. Please note, the degree of nodes in HIG remains the same. However, for CG, the degree of nodes differs based on the context aggregator. We train the model end to end, and due to back-propagation, context weights adjust as per the context aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Results on a Human Annotated Dataset</head><p>The employed datasets Wikidata <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref> and NYT Freebase <ref type="bibr" target="#b18">(Riedel et al., 2010)</ref>   <ref type="table" target="#tab_3">Table 10</ref>: Micro F-score of 10-worst performing Relations for KGP ool gnn (?=1) on Wikidata dataset. We also provide corresponding values of other sentential RE baselines. The main reason for limited performance across all models is the scarcity of training data for these relation types.</p><p>from Wikidata dataset <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref>. This is to verify that the distantly supervised dataset is correct for every pair of entities. Sentences accepted by all annotators are part of the human-annotated dataset. There are 500 sentences and 1846 triples in the test set. <ref type="table" target="#tab_3">Table 11</ref> reports KGP ool's performance against the sentential baselines. KGP ool +gnn continues to outperform the baselines, maintaining similar behavior as seen on test sets of original datasets. The results further re-assure the robustness of our proposed approach.      For augmenting entity attribute context, we relied on public dumps of Wikidata and Freebase. From these dumps, we automatically extracted entities and its proper-ties: labels, aliases, instance of, descriptions. For Wikidata, we used public API 5 using a SPARQL query and for Freebase, we took original depreciated dump 6 . We use the nltk english tokenizer for splitting the sentence into its corresponding tokens in the Riedel dataset. We do not do any further data preprocessing. We used 1 GPU NVIDIA TITAN X Pascal with 12GB of GPU storage to run our experiments. We train the models upto a maximum of 14 epochs and select the best performing model based on the micro F1 scores of the validation set. The tables 14, 15 and 12 detail the hyperparameter settings used in our experiments. We do not do any further hyper-parameter tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>KGPool's best configuration (Tables: 2, 3) perform better than baselines over the entire recall range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Equation 5, KGPool focuses on learning node features. Next, KGPool learns the importance of each entity attribute node using selfattention. Please note, in HIG, pooling happens The sentence W and entities e h , e t remain intact. Hence, each entity representation e h and e t is enriched by the useful attribute context (KG context). The entity attribute nodes which do not provide relevant context are excluded from the graph. To choose the relevant entity attribute nodes, we use a self-attention score Z (Lee et al., 2019) calculated as follows:</figDesc><table><row><cell>only for entity attribute nodes ( At j</cell><cell>e i</cell><cell>from Equa-</cell></row><row><cell>tion 3).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the Datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>RECON-EAC<ref type="bibr" target="#b0">(Bastos et al., 2021)</ref>: a variant of RECON contains entity attributes as only KG context (same context as KGPool). GP-GNN (Zhu et al., 2019): performs multi-hop reasoning using a GNN. Context-LSTM<ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref>: uses context from other sentential relations. Sorokin-LSTM<ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017)</ref>: the NYT dataset contains one relation per sentence, but Context-LSTM requires at least two relations in a sentence. Thus, the other setting is an LSTM model without a sentential relation context, is used as a baseline on the NYT dataset.</figDesc><table /><note>Multi-instance RE Approaches: Please note, the Wikidata dataset does not have multiple instances for an entity pair. Hence multi-instance baselines do not have values on it. We inherit the recent multi-instance baselines and all empirical values from (Bastos et al., 2021): (i) HRERE (Xu and Barbosa, 2019) (ii) Wu-2019 (Wu et al., 2019), (iii) Yi-Ling-2019 (Ye and Ling, 2019), (iii) RE- SIDE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Comparison of KGPool configurations with sentential RE models on Wikidata dataset. Best score in bold.</figDesc><table><row><cell>Performance on Wikidata Dataset: Table 2</cell></row><row><cell>summarizes the performance of KGPool variants</cell></row><row><cell>against the sentential RE models. Agnostic of</cell></row><row><cell>the underlying aggregator (LSTM or GNN), KG-</cell></row><row><cell>Pool effectively captures the KG context compli-</cell></row><row><cell>menting the sentential context. The KGP ool +gnn</cell></row><row><cell>(?=1) configuration outperforms other KGPool</cell></row><row><cell>variants along with all sentential RE baselines. We</cell></row><row><cell>2 https://github.com/nadgeri14/KGPool</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison of KGPool with sentential and multi-instance RE models on NYT Freebase dataset. Best score in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>The McNemar's test for statistical significance for KGPool's best configuration Vs previous baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Effect of Context Pooling. 'DEG' denotes average degree of an entity node (e i ). 'DEG' of entity nodes in CG is drastically reduced wrt the HIG.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Micro F-score of Top performing Relations for KGP ool gnn (?=1) (on Wikidata dataset). Dynamically chosen context significantly improves performance for many relations.</figDesc><table><row><cell>Model</cell><cell>F1</cell></row><row><cell>KGP ool+gnn (+T )</cell><cell>88.85</cell></row><row><cell>KGP ool +lstm (+T )</cell><cell>84.42</cell></row><row><cell>KGP ool+gnn</cell><cell>88.60</cell></row><row><cell>KGP ool +lstm</cell><cell>84.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Yuhao Zhang, Peng Qi, and Christopher D. Manning. 2018. Graph convolution over pruned dependency trees improves relation extraction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -November 4, 2018, pages 2205-2215. Association for Computational Linguistics.</figDesc><table><row><cell>Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen</cell></row><row><cell>Li, Hongwei Hao, and Bo Xu. 2016. Attention-</cell></row><row><cell>based bidirectional long short-term memory net-</cell></row><row><cell>works for relation classification. In Proceedings of</cell></row><row><cell>the 54th Annual Meeting of the Association for Com-</cell></row><row><cell>putational Linguistics (Volume 2: Short Papers),</cell></row><row><cell>pages 207-212.</cell></row><row><cell>Hao Zhu, Yankai Lin, Zhiyuan Liu, Jie Fu, Tat-Seng</cell></row><row><cell>Chua, and Maosong Sun. 2019. Graph neural net-</cell></row><row><cell>works with generated parameters for relation ex-</cell></row><row><cell>traction. In Proceedings of the 57th Conference of</cell></row><row><cell>the Association for Computational Linguistics, ACL,</cell></row><row><cell>pages 1331-1339.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>The scarcity in training data has made all models to fail on certain relations. Secondly, our model fails in very closed relations. For example, instead of predicting the relation drafted by 3 , our model predicts member of sport team 4 . Similarly, in case of unmarried partner, our model predicts spouse. We believe that introducing logical reasoning in the model can help these borderline cases. The third observed pattern for errors is the quality of context. It is worthwhile to mention that in GP-GNN and Context-LSTM, there is only a sentential context. RECON and KGPool use KG context. Still, performance is limited for many relations such as use and different from as reported in the table 10. The lack of quality context in the KG possibly a reason for limited performance for KGcontext-induced models in erroneous cases. Detailed exploration is needed to understand the impact of data quality on KGPool performance, and we leave it for the future work.</figDesc><table><row><cell cols="2">A.2 Effect of Context Pooling</cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell>DEG</cell><cell>DEG</cell><cell>Dataset</cell></row><row><cell></cell><cell>(HIG)</cell><cell>(CG)</cell><cell></cell></row><row><cell>KGP ool +lstm (?=1)</cell><cell>5.33</cell><cell>1.06</cell><cell>Wikidata</cell></row><row><cell>KGP ool +lstm (?=2)</cell><cell>5.33</cell><cell>2.12</cell><cell></cell></row><row><cell>KGP ool +lstm (?=3)</cell><cell>5.33</cell><cell>4.32</cell><cell></cell></row><row><cell>KGP ool +lstm (?=4)</cell><cell>5.33</cell><cell>4.81</cell><cell></cell></row><row><cell>KGP ool +lstm (?=1)</cell><cell>6.34</cell><cell>1.23</cell><cell>NYT</cell></row><row><cell>KGP ool +lstm (?=2)</cell><cell>6.34</cell><cell>1.74</cell><cell>Freebase</cell></row><row><cell>KGP ool +lstm (?=3)</cell><cell>6.34</cell><cell>3.05</cell><cell></cell></row><row><cell>KGP ool +lstm (?=4)</cell><cell>6.34</cell><cell>6.30</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>Effect of Context Pooling. 'DEG' denotes average degree of an entity node (e i ). We observe a reduction in the degree of entity nodes in CG compared to the HIG.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc>Sentential RE performance on Human Annotation Dataset. KGPool again outperforms the baselines. We report Micro P,R, and F1 values. (Best score in bold)</figDesc><table><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell>learning rate</cell><cell>0.001</cell></row><row><cell>batch size</cell><cell>50</cell></row><row><cell>hidden state size</cell><cell>128</cell></row><row><cell>context coefficient (?)</cell><cell>1,2,3,4</cell></row><row><cell cols="2"># of propagation layers 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 12 :</head><label>12</label><figDesc>Hyper-parameters for Context Pooling module A.4 Datasets and Hyper-parameters We augmented two datasets Wikidata dataset and Riedel Freebase dataset with our proposed KG context. The Wikidata dataset has 353 unique relations, 372,059 sentences in training, 123824 sentences in validation and 360,334 for testing. The number of sentences in the training and test</figDesc><table><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell>learning rate</cell><cell>0.001</cell></row><row><cell>batch size</cell><cell>50</cell></row><row><cell>dropout ratio</cell><cell>0.5</cell></row><row><cell>hidden state size</cell><cell>256</cell></row><row><cell>non-linear activation</cell><cell>relu</cell></row><row><cell cols="2"># of propagation layers 3</cell></row><row><cell>entity embedding size</cell><cell>8</cell></row><row><cell>adjacent matrices</cell><cell>untied</cell></row><row><cell>optimizer</cell><cell>adam</cell></row><row><cell>?1</cell><cell>0.9</cell></row><row><cell>?2</cell><cell>0.999</cell></row><row><cell>?</cell><cell>1e-08</cell></row><row><cell>pretrained embeddings</cell><cell>glove</cell></row><row><cell>word embedding dim</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 13 :</head><label>13</label><figDesc>Hyper-parameters for GNN-Aggregator module</figDesc><table><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell>learning rate</cell><cell>0.001</cell></row><row><cell>batch size</cell><cell>50</cell></row><row><cell>dropout ratio</cell><cell>0.5</cell></row><row><cell>hidden state size</cell><cell>256</cell></row><row><cell>non-linear activation</cell><cell>relu</cell></row><row><cell># of layers</cell><cell>1</cell></row><row><cell>optimizer</cell><cell>adam</cell></row><row><cell>pretrained embeddings</cell><cell>glove</cell></row><row><cell>word embedding dim</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 14 :</head><label>14</label><figDesc>Hyper-parameters for ContextAware-Aggregator module</figDesc><table><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell>learning rate</cell><cell>0.001</cell></row><row><cell>batch size</cell><cell>50</cell></row><row><cell>initial embedding size</cell><cell>50</cell></row><row><cell>final embedding size</cell><cell>50</cell></row><row><cell>pretrained embeddings</cell><cell>glove</cell></row><row><cell># of layers</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 15 :</head><label>15</label><figDesc>Hyper-parameters for Graph Construction module set are 455,771 and 172,448 respectively in the Riedel dataset. No explicit validation set has been provided for Riedel dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.wikidata.org/wiki/ Property:P647 4 https://www.wikidata.org/wiki/ Property:P54</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://query.wikidata.org/ 6 https://developers.google.com/ freebase</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recon: Relation extraction using knowledge graph context in a graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anson</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaiah</forename><forename type="middle">Onando</forename><surname>Mulang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeedeh</forename><surname>Shekarpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>long papers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A Shared Database of Structured General Human Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Tufts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Statistics for high-dimensional data: methods, theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards sparse hierarchical graph classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalina</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Jovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<idno>abs/1811.01287</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised classification learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-06" />
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic context selection for document-level neural machine translation via reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2242" to="2254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context selection for embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4816" to="4825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating the impact of knowledge graph context on entity disambiguation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaiah</forename><surname>Onando Mulang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitali</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2157" to="2160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Encoding knowledge graph entity aliases in attentive neural network for wikidata entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Isaiah Onando Mulang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeedeh</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Esther</forename><surname>Shekarpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soren</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Information Systems Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="328" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIG-DAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Sungmin Rhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/490</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3527" to="3534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15939-8_10</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6323</biblScope>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p15-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with triple context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangquan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2299" to="2302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.11</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relation extraction using distant supervision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisa</forename><surname>Smirnova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Cudr?-Mauroux</surname></persName>
		</author>
		<idno type="DOI">10.1145/3241741</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contextaware representations for knowledge base relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1188</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1784" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Contextual knowledge selection and embedding towards enhanced pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13964</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">RESIDE: improving distantly-supervised neural relation extraction using side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiranjib</forename><surname>Sai Suman Prayaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1157</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1257" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wikidata: a new platform for collaborative data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrandecic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st World Wide Web Conference, WWW 2012 (Companion Volume)</title>
		<meeting>the 21st World Wide Web Conference, WWW 2012 (Companion Volume)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1063" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Entity context and relational paths for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mining and leveraging background knowledge for improving named entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Weichselbraun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kuntschik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian Mp</forename><surname>Bra?oveanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics</title>
		<meeting>the 8th International Conference on Web Intelligence, Mining and Semantics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving distantly supervised relation extraction with neural noise converter and conditional optimal selector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33017273</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7273" to="7280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge with heterogeneous representations for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1323</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3201" to="3206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Contextualized graph attention network for recommendation with item knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11529</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distant supervision relation extraction with intra-bag and inter-bag attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Zhi-Xiu Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1288</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2810" to="2819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
