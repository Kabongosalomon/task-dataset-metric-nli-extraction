<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Embracing Single Stride 3D Object Detector with Sparse Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lue</forename><forename type="middle">Fan</forename><surname>Casia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><forename type="middle">Zhang</forename><surname>Cmu</surname></persName>
							<email>tianyuaz@andrew.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
							<email>hangzhao@mail.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Feng</surname></persName>
							<email>feng.wff@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tusimple</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tusimple</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
							<email>zhaoxiang.zhang@ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casia</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Ziqi Pang UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Embracing Single Stride 3D Object Detector with Sparse Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In LiDAR-based 3D object detection for autonomous driving, the ratio of the object size to input scene size is significantly smaller compared to 2D detection cases. Overlooking this difference, many 3D detectors directly follow the common practice of 2D detectors, which downsample the feature maps even after quantizing the point clouds. In this paper, we start by rethinking how such multi-stride stereotype affects the LiDAR-based 3D object detectors. Our experiments point out that the downsampling operations bring few advantages, and lead to inevitable information loss. To remedy this issue, we propose Singlestride Sparse Transformer (SST) to maintain the original resolution from the beginning to the end of the network. Armed with transformers, our method addresses the problem of insufficient receptive field in single-stride architectures. It also cooperates well with the sparsity of point clouds and naturally avoids expensive computation. Eventually, our SST achieves state-of-the-art results on the largescale Waymo Open Dataset. It is worth mentioning that our method can achieve exciting performance (83.8 LEVEL 1 AP on validation split) on small object (pedestrian) detection due to the characteristic of single stride. Codes will be released at https://github.com/TuSimple/SST. Voxelized feature map Sparse voxel set 1 ? 2 ? 4 ? 1 ? 1 ? 1 ? Multi-stride 3D object detectors Single-stride Sparse Transformer (Ours)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>LiDAR-based 3D object detection for autonomous driving has been benefiting from the progress of image-based object detection. The mainstream 3D detectors quantize the 3D space into a stack of pseudo-images from Bird <ref type="figure">Figure 1</ref>. Compared with previous multi-stride 3D detectors, our model is single-stride and operates sparsely on the non-empty voxels. We paint the vehicle bounding boxes on the input point cloud to show the tiny object size compared to the input scene size.</p><p>Eye's View (BEV), which makes it convenient to borrow advanced techniques from the 2D counterparts. Many works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b64">65]</ref> are proposed under this paradigm and achieve competitive performance. However, 3D and 2D spaces have intrinsic distinction in their relative object scales, where the objects in 3D spaces have much smaller relative sizes (See <ref type="figure">Fig. 2</ref>). For example, in Waymo Open Dataset <ref type="bibr" target="#b49">[50]</ref>, the perception range is usually 150m ? 150m, while a vehicle is only about 4m long, even a pedestrian occupies as little as 1m in length. Such a tiny pedestrian equivalently translates to an object of size 8 ? 8 pixels in a 1200?1200 image, suggesting that object detection on such a tiny scale is one of the challenges in 3D object detection. Different from the above challenge of small scales in the 3D space, 2D detectors have to consider the handling of the objects with varied scales. It is observed in <ref type="figure">Fig. 2</ref> that the scales of objects in 2D images exhibit a long-tail distribution, while in 3D space they are quite concentrated due to the non-projective transformation used in voxelization. To handle the varied scales, 2D detectors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref> usually build multi-scale features with a series of downsampling and upsampling operations. Such multi-scale architecture is also widely inherited in 3D detectors (See <ref type="figure">Fig. 1</ref>) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b69">70]</ref>. Since the object size in 3D object detectors is usually tiny while no large objects exist, a question naturally arises: do we really need downsampling in 3D object detectors ?</p><p>With this question in mind, we make an exploratory attempt on the single-stride architecture with no downsampling operators. The single-stride network maintains the original resolution throughout the network. However, it is challenging to make such a design feasible. The discard of downsampling operators leads to two issues: 1) the increase of computation cost; 2) the decrease of receptive field. The former constrains the applicability to the real-time system and the latter hinders the capability of object recognition. For the issue of computation, sparse convolution seems to be a solution, but the sparse connectivity between voxels 1 makes the decrease of receptive field even more severe (See <ref type="table">Table 7</ref>). For the issue of receptive field, we experimentally show that some commonly adopted techniques do not meet our needs (See <ref type="table" target="#tab_0">Table 1</ref>): the dilated convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b65">66]</ref> is not friendly to small objects, and the larger kernel leads to unaffordable computational overhead in the single stride architecture. Therefore, we are getting into a dilemma, where it is difficult to design a convolutional network simultaneously satisfying the three aspects: single stride architecture, sufficient receptive field, and acceptable computation cost.</p><p>These difficulties naturally lead us to think out of the paradigm of CNN, and the attention mechanism emerges as a better option because of the following two reasons: 1) The attention-based model is better at capturing large context and build sufficient receptive field. 2) Due to the capability of modeling dynamic data, the attention-based model fits well into the sparse voxelized representation of point clouds, where only a small portion of voxels are occupied. This property guarantees the efficiency of our single stride network. Although the attention mechanism is efficient on sparse data, computing attentions on a global scale is still <ref type="bibr" target="#b0">1</ref> We provide a clear illustration for this in our supplementary materials.  <ref type="figure">Figure 2</ref>. Distribution of the relative object size S rel in COCO dataset <ref type="bibr" target="#b27">[28]</ref> and Waymo Open Dataset (WOD). S rel is defined as Ao/As, where Ao denotes the area of 2D objects (COCO) and the BEV area of 3D objects (WOD). As is the image area in COCO, and 150m ? 150m in WOD. In COCO 73.03% objects in COCO have a S rel larger than 0.04, while only 0.54% objects in WOD have a S rel larger than 0.04. unaffordable and undesirable. So we partition the voxelized 3D space into many local regions and apply self-attention inside each of them. Eventually, this local attention mechanism, named as Sparse Regional Attention (SRA), enjoys the best of two worlds. By stacking SRA layers, we make the single-stride network feasible and obtain a transformerstyle network, called Single-stride Sparse Transformer (SST). Extensive experiments are conducted on the largescale Waymo Open Dataset <ref type="bibr" target="#b49">[50]</ref>. We summarize our contributions as follows:</p><p>? We rethink the architecture of current mainstream LiDAR-based 3D detectors. With pilot experiments, we point out that the network stride is an overlooked design factor for LiDAR-based 3D detectors.</p><p>? We propose the Single-stride Sparse Transformer (SST). With its local attention mechanism and capability of handling sparse data, we overcome receptive field shrinkage in the single-stride setting and avoid heavy computational overhead.</p><p>? Our method achieves state-of-the-art performance on the large-scale Waymo Open Dataset. Thanks to the characteristic of single stride, our method obtains exciting results on tiny objects like pedestrians (83.8 LEVEL 1 AP on the validation split).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D LiDAR-based Detection There are three major representations for point cloud learning in autonomous driving, Point-based, Voxel-based, and Range View. Point based representation backed by PointNet families <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> are widely adopted for feature learning of small region of irregular points <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. Voxel-based representation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b69">70]</ref> combined with convolutions are the most popular treatment. As explored in several recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34]</ref>, range view enjoys computational advantages over voxels, especially for long-range LiDAR sensors. Some hybrid approaches investigate how to combine different types of representations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b57">58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformers in Visual Recognition</head><p>The success of transformer architectures in NLP <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b53">54]</ref> and speech recognition <ref type="bibr" target="#b7">[8]</ref> has inspired lots of work to investigate the power of attention in visual recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b66">67]</ref>. The pioneering work ViT <ref type="bibr" target="#b11">[12]</ref> splits an image into patches, and then feeds sequences of patches to multiple transformer blocks for image classification. DeiT <ref type="bibr" target="#b52">[53]</ref> explores training strategies for data-efficient learning of vision transformers. Swin-Transformer <ref type="bibr" target="#b29">[30]</ref> exploits the power of local attention to build high-performance transformer-based image backbones. Several works have investigated the use of transformers for point cloud perceptions. Some of them focus on the indoor scene such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b67">68]</ref>. For autonomous driving scenarios, Pointformer <ref type="bibr" target="#b35">[36]</ref> proposes a point-based local and global attention module directly operating on point clouds. In addition, VoTr <ref type="bibr" target="#b31">[32]</ref> uses the local self-attention module to replace the sparse convolution <ref type="bibr" target="#b16">[17]</ref> for voxel processing, where each voxel serves as a query and attends with its neighbor voxels.</p><p>Small Object Detection Small object detection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b70">71</ref>] is a challenging track in 2D object detection. The mainstream of current methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b62">63]</ref> focuses on increasing the resolution of the input and output features, while none of them gives up the multi-stride architectures. Some other methods adopt the scale-aware training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b48">49]</ref> and strong data augmentations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b71">72]</ref>. To the best of our knowledge, there is no method specialized for small object detection in 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Discussion of Network Stride</head><p>The stride of a network is a simple but critical aspect in the architecture design. Some previous works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b64">65]</ref> in 3D detection have found that the performance can benefit from the recovery of output resolution by upsampling. However, they do not delve into this phenomenon. Therefore, we conduct a simple pilot study to reveal the influence of network stride on 3D detectors and motivate the design of our network.</p><p>For generality, we adopt the widely used PointPillars <ref type="bibr" target="#b19">[20]</ref> in MMDetection3D <ref type="bibr" target="#b8">[9]</ref> as our base model. The experiments are conducted on Waymo Open Dataset <ref type="bibr" target="#b49">[50]</ref>. We uniformly sample 20% training data (32K frames) 2 and adopt 1? schedule (12 epochs).</p><p>Based on the standard PointPillars model D 2 , we extend it to three more variants: D 3 , D 1 , and D 0 , and they only differ in the network stride. From D 3 to D 0 , the set of strides of their four stages for each model are {1, 2, 4, 8}, {1, 2, 4, 4}, {1, 2, 2, 2} and {1, 1, 1, 1}, respectively. Since the output feature maps of the four stages will be upsampled to the original resolution by an FPN-like module, our modification does not change the resolution of feature maps in the detection head. Except for the resolution of feature maps, all the four models have the same hyper-parameters. To reduce memory overhead, we change the filter number from 256 to 128 in convolution layers.</p><p>The main results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Performances of all three classes improve from D 3 to D 1 , and there is a significant boost from D 2 to D 1 . The performance boost from D 3 to D 1 supports our motivation that Smaller strides are better for 3D detection.</p><p>However, from D 1 to D 0 , the vehicle performance has a significant drop, while the performance drop in pedestrian is slight and performance of cyclist keeps going up. We conjecture that the limited receptive field of D 0 hinders the performance improvement from D 1 to D 0 since the pedestrian and cyclist have smaller sizes than vehicles.</p><p>To verify our conjecture, we add two more variants:</p><formula xml:id="formula_0">D dilation 0 and D 5?5 0 . D dilation 0</formula><p>adopts dilated convolutions with dilation as 2 in the last two stages. D 5?5 0 increases the kernel size in last two stages to 5?5. <ref type="table" target="#tab_0">Table 1</ref> shows that, dilation increases the performance of vehicle class while decreases performances of pedestrian and cyclist, indicating that it indeed enlarges the receptive field, however misses fine-grained details. Meanwhile, larger kernel consistently improves the performance of all three classes but unfortunately has the highest latency. Above studies support our major motivation of single-stride 3D detectors, and it also reveals the another important aspect in our network design: Sufficient receptive field is crucial. In summary, above experiments verify two motivations of 3D object detector designs:</p><p>? The single stride architecture has a great potential in LiDAR-based 3D detection for autonomous driving.</p><p>? The key to make single stride architecture feasible lies in appropriately addressing the shrinkage of receptive field and reducing computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overall Architecture</head><p>So far, we know the keys to make single stride architecture feasible are sufficient receptive field and acceptable computational cost. However, as we discussed in Sec. 1, it is difficult to simultaneously satisfy the two factors with convolutional single stride architecture. So we turn to the attention mechanism in Transformer <ref type="bibr" target="#b53">[54]</ref>, and present our method as follows.</p><p>We build up our Single-stride Sparse Transformer (SST) as in <ref type="figure">Fig. 4</ref>. SST voxelizes the point clouds and extracts voxel features following prior work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b69">70]</ref>. For each voxel and its features, SST treats them as "tokens." SST first partitions the voxelized 3D space to fixed-size nonoverlapping regions (Sec. 4.2). Then SST applies Sparse Regional Attention (SRA) to voxel tokens in each region (Sec. 4.3). To handle the objects scattering multiple regions and capture useful local context, we adopt Region Shift (Sec. 4.4), which is inspired by the shifted window in Swin-Transformer <ref type="bibr" target="#b29">[30]</ref>. The backbone preserves the number of voxels as well as their spatial locations, thus satisfying the single-stride property, and can be integrated with mainstream detection heads (Sec. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Regional Grouping</head><p>Given the input voxel tokens, Regional Grouping divides the 3D space into non-overlapping regions, so that the selfattentions only interact with tokens coming from the same regions. The regional grouping not only maintains sufficient receptive field, but also avoids expensive computation overhead in global attentions. We illustrate it intuitively in <ref type="figure" target="#fig_2">Fig. 3</ref>. Each regional grouping divides the input tokens into groups according to their physical locations, where the tokens belonging to the same regions (green rectangles) are assigned to the same group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sparse Regional Attention</head><p>Sparse Regional Attention (SRA) operates on the regional sparse sets of voxel tokens coming from regional grouping. For a group of tokens F and their corresponding spatial (x, y, z) coordinates I, SRA follows conventional transformers as follows</p><formula xml:id="formula_1">F = MSA(LN(F), PE(I)) + F F = MLP(LN(F )) + F<label>(1)</label></formula><p>where PE(?) stands for the absolute positional encoding function used in <ref type="bibr" target="#b1">[2]</ref>, MSA(?) denotes the Multi-head Self-Attention, and LN(?) represents Layer Normalization. This manner of SRA well exploits the sparsity of point clouds, because it only computes the voxels with actual LiDAR points.</p><p>Region Batching for Efficient Implementation Due to the sparsity of point cloud, the number of valid tokens in each region varies. To utilize the parallel computation of modern devices, we batch regions with similar number of tokens together. In practice, if a region contains the tokens with number N token , satisfying:</p><formula xml:id="formula_2">2 i N token &lt; 2 i+1 , i ? {0, 1, 2, 3, 4, 5, 6}, (2)</formula><p>then we pad the number of tokens to 2 i+1 . With padded tokens, we can divide all the regions into several batches, and then process all regions in the same batch in parallel. As the padded tokens are masked in the computation as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b53">54]</ref>, they have no effect on other valid tokens. In this way, it is easy to implement an efficient SRA module in current popular deep learning frameworks without engineering efforts as taken in the sparse convolution <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b61">62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Region Shift</head><p>Though SRA can cover a considerably large region, there are some objects inevitably truncated by the grouping. To tackle this issue and aggregate useful context, we further use Region Shift in our design, which is similar to the shifting mechanism in Swin Transformer for information communication. Supposing the size of regions in regional grouping is (l x , l y , l z ), the Region Shift moves the original regions by (l x /2, l y /2, l z /2) and groups the tokens according to this new set of regions, as illustrated in "Shifted regional grouping" of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Integration with Detection</head><p>To work with the existing detector heads, SST places the sparse voxel tokens back to dense feature maps according to their spatial locations. Unoccupied locations are filled with zeros. As LiDAR only captures points on object surfaces, 3D object centers are likely to reside on the empty locations with zero features, which is unfriendly to the current designs of detection heads. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b64">65]</ref>. So we add two 3 ? 3 convolutions to fill most of the holes on the object centers.</p><p>As for the detection head and loss function, we adopt the same settings as PointPillars <ref type="bibr" target="#b19">[20]</ref> for simplicity. Specifically, we use the SSD <ref type="bibr" target="#b28">[29]</ref> head, the smooth L1 bounding box localization loss L loc , the classification loss L cls in the form of focal loss <ref type="bibr" target="#b26">[27]</ref>, and the direction loss L dir penalizing wrong orientations. The final loss function is Eq 3, where N p is the number of positive samples. We leave the detailed setting in supplementary materials.  For an incoming set of tokens, Regional Grouping first groups them according to the partitions of regions (in Sec. 4.2). Second, Sparse Regional Attention (SRA) deals with each group of tokens separately (Sec. 4.3). Third, the tokens are grouped another time according to Region Shift, and a second SRA processes the new groups of tokens (Sec. 4.4). These three steps complete the computation of a block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Voxelization</head><p>Block # 1</p><p>Regional Grouping</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shifted Regional</head><p>Grouping</p><formula xml:id="formula_3">SRA 2 1 ? ? Dense Feature Map Recovery Detection Head SRA 1 1</formula><p>Block # T Regional Grouping</p><p>Shifted Regional Grouping  <ref type="figure">Figure 4</ref>. Architecture overview for Single-stride Sparse Transformer (SST). It begins from voxelizing an input point cloud, then processes the voxels with T blocks, and eventually recover a dense feature map. Inside each block, we consecutively append regional grouping on the voxel tokens and Sparse Regional Attention (SRA) to process them. Details in Sec. 4.1.</p><formula xml:id="formula_4">L = 1 N p (? loc L loc + ? cls L cls + ? dir L dir )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Two Stage SST</head><p>Although our main contribution lies in the design of the single stride architecture in the first stage, there is a considerable gap between the single stage detector and the two stage detector. To match the performance with current two stage detectors, we apply LiDAR-RCNN <ref type="bibr" target="#b24">[25]</ref> as our second stage. LiDAR-RCNN is a lightweight second stage network consists of a simple PointNet <ref type="bibr" target="#b37">[38]</ref> for feature extraction, only taking the raw point cloud inside proposal as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Discussion</head><p>Because of the distinctions between point clouds and RGB images, there are several differences in the design choices and motivations between our design and Swin-Transformer <ref type="bibr" target="#b29">[30]</ref> as highlighted here.</p><p>? Our SST network follows the single-stride guideline, while Swin-Transformer follows the hierarchical structure with multi-stride, which uses "token merge" to increase the receptive field.</p><p>? The tokens for our region-based attention scatter sparsely because of the sparsity of point clouds, while the tokens in vision transformers have dense layouts. This is one of the reasons for the efficiency of SST even in the single stride architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>We conduct our experiments on Waymo Open Dataset (WOD) <ref type="bibr" target="#b49">[50]</ref>. The dataset contains 1150 sequences in total (more than 200K frames), 798 for training, 202 for validation and 150 for test. Each frame covers a scene with a size of 150m ? 150m. It is a very challenging dataset and adopted as the benchmark in many recent state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We implement our model based on the popular 3D object detection codebase -MMDetection3D <ref type="bibr" target="#b8">[9]</ref>, which provides standard and solid baselines. Please refer to supplementary materials for more details.</p><p>Model Setup For generality, we build our Single-stride Sparse Transformer (SST) on the basis of popular Point-Pillars <ref type="bibr" target="#b19">[20]</ref>. We replace its backbone with 6 consecutive Sparse Regional Attentions (SRA) blocks, and each block contains 2 attention modules as <ref type="figure">Fig. 4</ref> shows. All the attention modules are equipped with 8 heads, 128 input channels, and 256 hidden channels. In Regional Grouping, each region covers a volume with size 3.84m ? 3.84m ? 6m. As for other parts, SST follows the implementation of Point-Pillars in MMDetection3D. We use the BEV pillar size of 0.32m ? 0.32m ? 6m, which can be easily extended to the 3D voxels with smaller heights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Variants</head><p>We develop several variants of SST in our experiments. SST 1f: basic single-stage model using 1-frame point cloud. SST 3f: consecutive 3 frame point clouds are used as model input, and the point cloud in different frames are concatenated together after aligning the egopose. SST TS 1f and SST TS 3f: two stage model based on above models, using a standard LiDAR-RCNN <ref type="bibr" target="#b24">[25]</ref> for refinement.</p><p>Training Scheme We train our model for 24 epochs (2?) on WOD with AdamW optimizer and cosine learning rate scheduler. The maximum learning rate is 0.001, and the weight decay is 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-art Detectors</head><p>We compare our SST with state-of-the-art methods in <ref type="table">Table 2</ref> (vehicle) and <ref type="table">Table 3</ref> (pedestrian). We divide current methods into the branches of one-stage and two-stage detectors for fair comparison. <ref type="table">Table 2</ref> shows the results on vehicles, where our models achieve competitive performances. With a lightweight second stage for refinement, our two-stage detectors are comparable with state-of-the-art methods. <ref type="table">Table 3</ref> shows the results on pedestrians. Due to the tiny size and non-rigid property, pedestrian detection is more challenging than vehicle detection. Networks are prone to confuse pedestrians with other slim objects, like poles and trees, leading to a high false positive rate. Under such cases, our best model outperforms all other methods in the challenging pedestrian class. SST TS 3f is 4.4 AP ahead of the second best RSN with the same temporal information (3 frames). We owe such leading performance to the singlestride characteristic of SST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Deep Investigation of Single Stride</head><p>Single-stride models better use dense observations. First, SST has more advantages in short-range metrics (0m -30m) than in long-range metrics (50m -inf): In <ref type="table" target="#tab_4">Table 4</ref>, SST 1f outperforms the PointPillars counterpart in shortrange metric by 12.8 AP for pedestrian class, but the margin is not that significant over PointPillars in the long-range metric. Second, SST benefits more from multi-frame data. In <ref type="table" target="#tab_4">Table 4</ref>, RSN <ref type="bibr" target="#b50">[51]</ref> got improved by 6.4 AP in long-range metric from RSN 1f to RSN 3f, while the performance of SST in long-range metric gets more significantly improved by 10.4 AP from SST 1f to SST 3f.</p><p>Does the single stride model fail on large vehicles? As smaller strides reduce the receptive fields, it would be a major concern whether our model has sufficient receptive fields for extreme cases, e.g., extremely large vehicles. We therefore divide all the vehicles into three groups according to the  <ref type="table">Table 2</ref>. Performances of vehicle detection on the Waymo Open Dataset validation split. We mark the best result in red, and the second result in blue. ?: RSN <ref type="bibr" target="#b50">[51]</ref> is not a typical two stage detector, we put it here because it uses a segmentation network to remove background first. * : re-implemented by MMDetection3D. ?: from <ref type="bibr" target="#b2">[3]</ref>. ?: from <ref type="bibr" target="#b43">[44]</ref>.</p><p>lengths of their ground-truth boxes, and evaluate the recalls of SST on them. Please refer to supplementary materials for the evaluation details. In <ref type="table" target="#tab_5">Table 5</ref>, our SST outperforms the PointPillars baseline for all vehicles, even those longer than 8m. This supports that our attention mechanism provides proper receptive fields in the single stride architecture.</p><p>Localization quality test with stricter IoU thresholds. By preserving the original resolution, our SST is supposed to localize objects more precisely as in <ref type="bibr" target="#b20">[21]</ref>. To verify this, we evaluate SST with higher 3D IoU thresholds (0.8 for vehicle, 0.6 for pedestrian). In <ref type="table">Table 6</ref>, we compare our models with the PointPillars baseline and other models with available results from <ref type="bibr" target="#b39">[40]</ref>, then a couple of interesting findings emerge:   <ref type="table">Table 3</ref>. Performance of pedestrian detection on the Waymo Open Dataset official validation split. Please refer to <ref type="table">Table 2</ref> for the meanings of the notions in this table.</p><p>1. Comparing MVF++ <ref type="bibr" target="#b39">[40]</ref> with our SST 1f on vehicles, MVF++ is slightly better than SST 1f under the normal threshold, while SST 1f is better with the stricter threshold. This suggests the single stride structure enables more precise localization of vehicles.</p><p>2. The 3DAL <ref type="bibr" target="#b39">[40]</ref> is an offboard method using all the past and future frames in a sequence (around 200 frames) and is equipped with tracking <ref type="bibr" target="#b58">[59]</ref>. Nonetheless, our best model SST TS 3f surprisingly surpasses 3DAL on pedestrian on both IoU thresholds with as few as 3 frames of point clouds.</p><p>These findings suggest that the single-stride architecture  is capable of better localizing objects with full and finegrained information.</p><p>Comparison with other alternatives. There are some potential alternatives to our SST in order to preserve the input resolution. Here we make a comprehensive comparison. We first introduce these alternative models as follows. PointPillars-SS: The single stride version of Point-Pillars introduced in Sec. 3. SparsePillars-SS: We replace all the standard 2D convolutions in backbone of PointPillars-SS with Submanifold Sparse Convolutions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b61">62]</ref>. Due to the sparsity, SparsePillars-SS also faces the issue of "empty hole" (details in Sec. 4.5) as in SST, so we add two more 2D convolutions before its detection head.  <ref type="table">Table 6</ref>. Localization quality test with stricter IoU threshold. The normal and strict thresholds for vehicles are 0.7 and 0.8, and are 0.5 and 0.6 for pedestrians. * : the results are from <ref type="bibr" target="#b39">[40]</ref>. TTA: testtime data augmentation. ?: the offline setting using all the past and future frames in a point cloud sequence.</p><p>PointPillars, we reduce the stride of the first two convolutions in HRNet from 2 to 1. All the alternatives have the same setting with SST 1f except their backbones. <ref type="table">Table 7</ref> shows the comparison between different models.  <ref type="table">Table 7</ref>. Comparison with alternatives to SST. Using 20% data for training. The latency is evaluated with standard benchmarking script in MMDetection3D on 2080Ti GPUs. ?: The size of all kernels in sparse convolution increases to 5 ? 5 or 7 ? 7.</p><p>In <ref type="table">Table 7</ref>, our method outperforms all other alternatives with relatively low latency. Besides, two things need to be noticed: (1) SparsePillars-SS is much worse than other models in vehicle class. Due to the properties of submanifold sparse convolution, this model suffers from more severe receptive field shrinkage than PointPillars-SS. For example, if a vehicle part is isolated with all the surrounding voxels being in empty, it can not perceive information from other parts in the whole forward process. On the contrary, the attention mechanism in SST well addresses this issue while maintaining sparsity. (2) HRNetV2p-W18 allocates too much computation on the high-stride (low resolution) branches which is not needed in 3D object detection. So the capacity of its high-resolution branch is limited, leading to its inferior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Analysis of Sparse Attention</head><p>We visualize the attention weights in <ref type="figure" target="#fig_4">Fig. 5</ref> and list our observations as follows.</p><p>Sufficient Coverage In <ref type="figure" target="#fig_4">Fig. 5 (a)</ref> Complete Vehicle, the query token (pink dot) in the car has strong relation with all other parts of the car. In other words, this single token can effectively cover the whole car. This demonstrates that the attention mechanism is indeed effective to enlarge the receptive field.</p><p>Semantic Discrimination In <ref type="figure" target="#fig_4">Fig. 5 (b)</ref> Person near a Wall, the query token on the person builds strong dependency with other body parts, but has little relations with background points, e.g., wall. In <ref type="figure" target="#fig_4">Fig. 5 (c)</ref> Person beside a Vehicle, the pedestrian standing next to the vehicle attends only with itself. These two cases reveal that the learned sparse attention weight is discriminative between different semantic classes. This property helps distinguish pedestrians from other slim objects and reduces false positives.</p><p>Instance Discrimination In the crowded cases, such as <ref type="figure" target="#fig_4">Fig. 5 (d)</ref> Multiple Pedestrians, the query token in a person mainly focuses on the same person. Due to the high semantic similarity, it also slightly attends to other people. In <ref type="figure" target="#fig_4">Fig. 5 (e)</ref> Multiple Vehicles, the query token in the vehicle almost has no dependency on the nearby vehicles. These two cases suggest that the learned sparse attention weights are discriminative for different instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Hyper-parameter Ablation</head><p>Region Size We show the performance under different region sizes for Regional Grouping in <ref type="table" target="#tab_8">Table 8</ref>. SST is in general robust to the region size and slightly better with larger regions. Especially, SST has the best performance in pedestrian detection with the largest local region size. It suggests that the local context is helpful to recognize pedestrians. For example, pedestrians are more likely to appear on the sidewalks than on vehicle lanes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Depth SST is relatively shallow by design thanks to the large receptive fields from the attention mechanism. In <ref type="table" target="#tab_9">Table 9</ref> we show the impact of model depths on SST. In general, SST is robust to different depths, and the performance of pedestrian class is even slightly better with fewer layers. This demonstrates our method does not rely on a very large or deep model, thus it is easier to build efficient single-stride models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Limitations</head><p>In this paper, we analyze the impact of the network stride on 3D object detectors for autonomous driving, and empirically show that 3D object detectors do not really need downsampling. To build a single-stride network, we adopt the sparse regional attention to address the problem of insufficient receptive fields and avoid expensive computation. By stacking the sparse attention modules, we propose the Single-stride Sparse Transformer, achieving state-of-the-art performance on the Waymo Open Dataset. Due to the single stride structure, our models obtain remarkable performance on the challenging pedestrian class. Without elaborated optimization, our model uses slightly more memory than baseline models, and we will pursue a more memoryfriendly model in the future. We wish our work could break the stereotype in the backbone design of point cloud data, and inspire more thoughts on the specialized architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Submission on Test Server</head><p>Due to the submission frequency limit of the Waymo test server, we only report the results of our best model. We compare SST with the three most competitive methods and report their performances in the multi-frame setting from the official leaderboard. The results are shown in <ref type="table">Table A and Table B</ref>. The performance of SST on vehicle class is comparable with these methods, and the performance of SST on pedestrian class significantly outperforms other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion of Sparse Operations</head><p>Due to the space limit of the main paper, we leave the discussion on sparse operations in the supplementary materials. In this section, we discuss two problems for sparse operations: (1) insufficient receptive field of submanifold sparse convolution (SSC) <ref type="bibr" target="#b16">[17]</ref>, and (2) the difficulties of downsampling/upsampling in sparse data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Insufficient Receptive Field of Submanifold Sparse Convolution (SSC)</head><p>In Sec. 1 and <ref type="table">Table 7</ref> in our main paper, we briefly point out that the SSC-based single-stride architecture faces a severe problem of the insufficient receptive field. We demonstrate this issue here in <ref type="figure">Fig. A</ref> by comparing the behaviors of SSC and standard 2D convolution in sparse data. Both the SSC and standard convolutions have two layers with a kernel size of three. However, the SSC could not reach the voxel on the top-left corner from the voxel marked with a star, while the standard convolution is capable of doing this. This example intuitively illustrates the insufficiency of receptive fields for SSC, and we explain the reasons in detail as follows.</p><p>The SSC do not "fill" empty voxels for the sake of efficiency, which largely constrains the information communication between voxels. Under such conditions, in <ref type="figure">Fig. A  (a)</ref>, only one voxel (the pink one) in has information communication with the one marked by a red star if the kernel size is 3 ? 3. On the contrary, <ref type="figure">Fig. A (b)</ref> shows that the 2D convolution can gradually enlarge the receptive field by involving the empty voxels in the convolution process, which is more effective for aggregating information compared to the SSC.</p><p>To give an experimental illustration, we conduct experiments on the class of vehicles, which require sufficient receptive field for detection. In the <ref type="table">Table 7</ref> of the main paper, replacing the 3 ? 3 standard convolutions with SSC will cause a significant drop of AP from 64.69 to 51.57. We further increase the receptive field by expanding the kernel size of SSC to 5 ? 5 and 7 ? 7. These improve the performance from the 3D AP 51.57 to 55.40 and 56.77, but there is still a large gap to the variant using standard convolutions. Therefore, these numbers support our analyses on the insufficient receptive fields of SSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Downsampling/Upsampling in Sparse Data</head><p>Although downsampling and upsampling are common in dense data, e.g., pooling in CNN, token merge in Swin-Transformer, it is non-trivial to transfer these techniques to sparse data like point clouds. A variant of SSC named Sparse Convolution (SC) follows the standard convolution to implement the downsampling and upsampling in sparse data. With such implementation, data loses sparsity rapidly <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b64">65]</ref> and this leads to high computational overhead.</p><p>In our sparse Transformer, downsampling/upsampling by token merge <ref type="bibr" target="#b29">[30]</ref> also needs careful consideration. First, the downsampling operation is still an open problem for point clouds: what is the best way to merge the varied number of tokens scattered in different spatial locations? Second, the upsampling operation is also non-trivial and requires future research: how to recover a couple of tokens in different locations from a single token effectively and efficiently? In developing the SST, we encounter these challenges and find it difficult to offer satisfying solutions. Although we have bypassed these difficulties by adopting the single-stride architecture, we hope future research may work on this downsampling/upsampling question and better utilizes sparse data. In SSC, only the information of the voxel (the pink one) covered by the kernel can reach to the red star. In 2D convolution, all non-empty voxels can reach to the red star after 2 convolution layers, because the empty locations are "filled" by the convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Potential Improvements</head><p>In order to rule out unimportant factors and present a clean architecture, we only inherit the basic framework of PointPillars <ref type="bibr" target="#b19">[20]</ref>. So there is a large room for further performance improvements, and we list some of them as follows. We will adopt these techniques in our future work.</p><p>IoU Prediction. In detection, the classification score of a bounding box are not always consistent with the real regression quality. So many recent methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b64">65]</ref> use another branch to predict the IoU between output bounding boxes and the corresponding ground-truth boxes, and use the predicted IoU to correct the classification scores.</p><p>More Powerful Second Stage.</p><p>We use LiDAR-RCNN <ref type="bibr" target="#b24">[25]</ref> as our second stage, which is a lightweight PointNet-like module only takes the raw point cloud as input. So it has no effect on our first stage and is convenient for our analysis of single-stride architecture. However, its performance is inferior to some other elaborately designed RCNNs, e.g., CenterPoint <ref type="bibr" target="#b64">[65]</ref>, PartA2 <ref type="bibr" target="#b45">[46]</ref>, PVR-CNN <ref type="bibr" target="#b42">[43]</ref>, PyramidRCNN <ref type="bibr" target="#b30">[31]</ref>, which reuse the features from the single stage to achieve better refinement. With the point-level features interpolated from feature maps in the first stage, SST can be equipped with most of these methods and aim for better abilities.</p><p>Incorporating Advanced Techniques in Vision Trans-former. We have witnessed the fast progress of vision transformers. Many advanced techniques can be borrowed to enhance the performance of SST. (1) Better efficiency: There are a lot of techniques can be adopted to improve our efficiency, for example, token selection <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b56">57]</ref>, attention simplication <ref type="bibr" target="#b55">[56]</ref>. (2) Better efficacy: Some techniques can be used to make SST more effective, e.g., relative positional encoding <ref type="bibr" target="#b59">[60]</ref>, different attention mechanism <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Complexity Compared with Convolutions</head><p>We investigate the computational complexity of the SST architecture and convolutional architectures. Our analyses demonstrate that SST has a unique advantage in efficiency by utilizing the sparsity of point clouds and the regional grouping.</p><p>Following the calculation in Swin-Transformer <ref type="bibr" target="#b29">[30]</ref>, we inspect the computational complexities of convolutional architectures and SST. For an input scene size of h?w, a convolution layer with kernel size k ? k and channel number C has the complexity as Equation A. On the same scene, an SRA operation has the complexity as Equation B, where it has H-heads, region size of R ? R, and the average sparsity as S, which is the ratio for non-empty voxels <ref type="bibr" target="#b2">3</ref> .</p><formula xml:id="formula_5">?(Conv) = hwk 2 C 2 , (A) ?(SRA) = 4ShwC 2 + 2HS 2 R 2 hwC,<label>(B)</label></formula><p>As shown in the equations, the computational complexities for convolutions and SRA operations are all O(hw), thus are both linear to the scale of input. However, the SRA operations have the linear factor of S, which is generally small due to the sparsity of point clouds. According to our statistics, S roughly equals to 0.09 on Waymo Open Dataset with our voxelization. Such an analysis indicates that our SRA operations is efficient by properly exploiting the sparsity of LiDAR data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Computation of an example block in SST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of the learned sparse regional attention. Each figure shows the attention weight distribution between the query token (pink dot) and all other tokens in the local region. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Illustration of receptive enlarging in the 3 ? 3 submanifold sparse convolution (SSC) and the standard 3 ? 3 convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>? 0.35 60.85 ? 0.03 47.52 ? 0.44 60ms D1 66.03 ? 2.02 65.06 ? 4.21 52.97 ? 5.45 91ms D0 64.69 ? 1.34 64.32 ? 0.74 53.02 ? 0.05 185ms D dilation 0 66.26 ? 1.57 63.51 ? 0.81 50.95 ? 2.07 192ms D 5?5 0 66.42 ? 1.77 65.71 ? 1.41 53.70 ? 0.68 340ms Results of pilot study on Waymo Open Dataset validation split. Latency is evaluated in 2080Ti GPU with 2000 samples after a cold start of 500 samples. For Dn, the arrows indicate the performance changes based on Dn+1. For D dilation</figDesc><table><row><cell>Models</cell><cell>Vehicle</cell><cell>Pedestrian</cell><cell cols="2">Cyclist Latency</cell></row><row><cell>D3</cell><cell>63.66</cell><cell>60.82</cell><cell>47.08</cell><cell>58ms</cell></row><row><cell>D2</cell><cell cols="3">64.01 0</cell><cell>and D 5?5 0</cell><cell>, the</cell></row><row><cell cols="6">arrows indicate performance changes based on D0. Best viewed</cell></row><row><cell>in color.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>86.1? 13.2 81.2? 5.3 73.8? 3.2 Distance-conditioned pedestrian detection performance. Our SST has larger advantages in short-range metrics and the multi-frame setting, where points are more dense. The increases and decreases are calculated based on PointPillars baseline. ? 0.71 80.85 ? 7.74 13.41 ? 2.82</figDesc><table><row><cell>Method</cell><cell>Overall</cell><cell cols="3">LEVEL 1 Pedestrian AP 0-30m 30-50m 50m-inf</cell></row><row><cell>RSN 1f</cell><cell>77.8</cell><cell>83.9</cell><cell>74.1</cell><cell>62.1</cell></row><row><cell>RSN 3f</cell><cell>79.0</cell><cell>84.5</cell><cell>78.1</cell><cell>68.5</cell></row><row><cell>PointPillars</cell><cell>70.6</cell><cell>72.5</cell><cell>71.9</cell><cell>63.8</cell></row><row><cell>PointPillars 3f</cell><cell>73.7</cell><cell>72.9</cell><cell>75.9</cell><cell>70.6</cell></row><row><cell>SST 1f</cell><cell cols="4">78.7? 8.1 85.3? 12.8 77.0? 5.1 63.4? 0.4</cell></row><row><cell cols="5">SST 3f 82.4? 8.7 Methods Vehicle Recalls (IoU=0.7) [0m, 4m] [4m, 8m] [8m, +?]</cell></row><row><cell cols="2">PointPillars 40.60</cell><cell>73.11</cell><cell>10.59</cell><cell></cell></row><row><cell>SST 1f</cell><cell>41.31</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Recalls for vehicles with different lengths. The vehicles with lengths in [0m, 4m] and [8m, +?] are rare (7.3% and 1.6% in WOD) and hardly get sufficient training, so their performances are relatively low.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Ablation of the region size. Using 20% data for training.</figDesc><table><row><cell>Region Size</cell><cell>Max number of voxels</cell><cell>LEVEL 1 AP/APH Vehicle Pedestrian</cell></row><row><cell>3.20m</cell><cell>100</cell><cell>66.9/66.4 70.4/56.9</cell></row><row><cell>3.84m</cell><cell>144</cell><cell>67.9/67.3 70.9/57.3</cell></row><row><cell>4.48m</cell><cell>196</cell><cell>67.8/67.3 70.6/56.5</cell></row><row><cell>5.12m</cell><cell>256</cell><cell>66.9/66.3 71.1/57.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Ablation of the network depth. Using 20% training data.</figDesc><table><row><cell>Number of</cell><cell cols="2">LEVEL 1 AP/APH</cell></row><row><cell>SRA blocks</cell><cell>Vehicle</cell><cell>Pedestrian</cell></row><row><cell>5</cell><cell cols="2">67.7/67.1 71.1/57.9</cell></row><row><cell>6</cell><cell cols="2">67.9/67.3 70.9/57.3</cell></row><row><cell>7</cell><cell cols="2">67.8/67.3 70.6/56.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Training with 20% data is a setting for efficient validation adopted in<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">52]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our calculation is approximate because we assume non-empty voxels uniformly scatter in the space.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Range Conditioned Dilated Convolutions for Scale Invariant 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09927</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-toend Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno>ECCV, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">To the Point: Efficient 3D Object Detection in the Range Image With Graph Convolution Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open MMLab Detection Toolbox and Benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<title level="m">Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-View 3D Object Detection Network for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast Point R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MMDetection3D: Open-MMLab Next-generation Platform for General 3D Object Detection</title>
		<ptr target="https://github.com/open-mmlab/mmdetection3d" />
	</analytic>
	<monogr>
		<title level="m">MMDetection3D Contributors</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09681</idno>
		<title level="m">Cross-Covariance Image Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RangeDet: In Defense of Range View for LiDAR-Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lue</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">DSSD: Deconvolutional Single Shot Detector. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhou</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangzhuang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12671</idno>
		<title level="m">AFDet: Anchor Free One Stage 3D Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold Sparse Convolutional Networks</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<title level="m">PCT: Point Cloud Transformer. Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Augmentation for Small Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mate</forename><surname>Kisantal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Murawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07296</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PointPillars: Fast Encoders for Object Detection from Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting Objects as Paired Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Vehicle Detection from 3D Lidar Using Fully Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From Voxel to Point: IoU-guided 3D Object Detection for Point Cloud with Voxel-to-Point Decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-MM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scale-Aware Trident Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LiDAR R-CNN: An Efficient and Universal 3D Object Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SSD: Single Shot Multibox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Voxel Transformer for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Voxel Transformer for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">K</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An End-to-End Transformer Model for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d Object Detection with Pointformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuran</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Offboard 3D Object Detection from Point Cloud Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stand-Alone Self-Attention in Vision Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02034</idno>
		<title level="m">DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00463</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PointR-CNN: 3D Object Proposal Generation and Detection from Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Beyond Skip Connections: Topdown Modulation for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An Analysis of Scale Invariance in Object Detection -SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SNIPER: Efficient Multi-Scale Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scalability in Perception for Autonomous Driving: Waymo Open Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamaleldin</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">OpenPCDet: An Opensource Toolbox for 3D Object Detection from Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openpcdet</forename><surname>Development Team</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<ptr target="https://github.com/open-mmlab/OpenPCDet" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training Data-efficient Image Transformers &amp; Distillation through Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deep High-resolution Representation Learning for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with Linear Complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Belinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">PnP-DETR: Towards Efficient Visual Analysis with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pillarbased Object Detection for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03961</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">A Baseline for 3D Multiobject Tracking. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Rethinking and Improving Relative Position Encoding for Vision Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">DOTA: A Large-scale Dataset for Object Detection in Aerial Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely Embedded Convolutional Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Query-Det: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhongyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09136</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">3D-MAN: 3D Multi-Frame Attention Network for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11275</idno>
		<title level="m">Centerbased 3D Object Detection and Tracking</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Exploring Self-attention for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Point Transformer. In ICCV</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Visdrone-det2018: The Vision Meets Drone Object Detection in Image Challenge Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinqin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning Data Augmentation Strategies for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
