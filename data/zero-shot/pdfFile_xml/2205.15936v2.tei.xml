<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton-based Action Recognition via Temporal-Channel Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengqin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongji</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglin</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
						</author>
						<title level="a" type="main">Skeleton-based Action Recognition via Temporal-Channel Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based action recognition methods are limited by the semantic extraction of spatio-temporal skeletal maps. However, current methods have difficulty in effectively combining features from both temporal and spatial graph dimensions and tend to be thick on one side and thin on the other. In this paper, we propose a Temporal-Channel Aggregation Graph Convolutional Networks (TCA-GCN) to learn spatial and temporal topologies dynamically and efficiently aggregate topological features in different temporal and channel dimensions for skeleton-based action recognition. We use the Temporal Aggregation module to learn temporal dimensional features and the Channel Aggregation module to efficiently combine spatial dynamic channel-wise topological features with temporal dynamic topological features. In addition, we extract multi-scale skeletal features on temporal modeling and fuse them with an attention mechanism. Extensive experiments show that our model results outperform state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action recognition is an essential component of computer vision and a very active research topic. The development of sensors <ref type="bibr" target="#b27">Liu et al. 2020a</ref>) and advanced algorithms for human pose estimation <ref type="bibr" target="#b10">(Fang et al. 2017;</ref><ref type="bibr" target="#b1">Cao et al. 2019</ref>) has made obtaining accurate 3D skeletal data easier, especially when the skeletal data are relatively computationally small and robust against complex backgrounds and changing conditions such as body size, viewpoint, and motion speed <ref type="bibr" target="#b51">(Yan, Xiong, and Lin 2018a;</ref><ref type="bibr" target="#b32">Plizzari, Cannici, and Matteucci 2021;</ref><ref type="bibr" target="#b16">Ji et al. 2013;</ref><ref type="bibr" target="#b14">Herath, Harandi, and Porikli 2017)</ref>. Skeleton-based human action recognition is thus of great interest. The joints of the skeletal spatio-temporal graph of human action are correlated. <ref type="bibr" target="#b51">Yan et al. (Yan, Xiong, and Lin 2018a)</ref> applied graph convolutional networks (GCNs) to the field of skeleton-based action recognition for the first time. They used GCNs and temporal convolution to extract motion features and graph structures to model the correlation between human joints. For example, the idea of learnable skeletal edge weights was proposed, and the problem of different importance of different joints in action recognition was initially explored. Based on this, * *Corresponding author <ref type="figure">Figure 1</ref>: Processing spatio-temporal skeletal features with TCA-GCN. A simplified analysis is performed with the red joints as target nodes. <ref type="figure">Figure (a)</ref> shows the aggregation operation in space through a priori topological relations (purple arrows). In time the node uses fixed (green arrows) node characteristics. <ref type="figure">Figure (b)</ref> shows our TCA module aggregation in space using a combination of dynamic and static topology (different colored arrows). In time node uses features that are dynamic (different node sizes), which are obtained by aggregating weights on the time dimension. some approaches <ref type="bibr" target="#b36">(Shi et al. 2019;</ref><ref type="bibr" target="#b7">Cheng et al. 2020c;</ref><ref type="bibr" target="#b33">Qin et al. 2021</ref>) exploit second-order information of skeletal data (length and orientation of bones, angles) and use data augmentation in the form of multi-stream networks. However, these approaches make it difficult to model the relationship between unnaturally connected joints, limiting the representational power of the model. Some recent approaches <ref type="bibr" target="#b36">(Shi et al. 2019;</ref><ref type="bibr" target="#b32">Plizzari, Cannici, and Matteucci 2021;</ref><ref type="bibr" target="#b60">Zhang et al. 2020a;</ref><ref type="bibr" target="#b31">Peng et al. 2020</ref>) use attentional or other mechanisms to model the human skeletal structure adaptively, however, the variability of the different channel topologies is not taken into account. <ref type="bibr">Yu et al. (Chen et al. 2021b)</ref> proposed channel topology refinement graph convolution (CTR-GC) to learn topologies and aggregate features in different channel dimensions dynamically. However, these methods ignore the balance of spatial and temporal dimensions, and they mostly favor modeling in the spatial dimension. Although using an attention mechanism in the temporal dimension can effectively model the correlation of key points in the temporal dimension, the combination of spatial and temporal features is still simple.</p><formula xml:id="formula_0">t t+1 t-1 t t+1 t-1 (b) t t+1 t-1 (a)</formula><p>In the previously proposed graph convolutional neural network, it mainly consists of a series of operations on the filter parameter matrix, the input feature matrix, and the ad-arXiv:2205.15936v2 [cs.CV] 8 Aug 2022 jacency matrix, the diagonal matrix of the graph structure. For filter parameter matrix in the field of skeletal action recognition, there are various processing methods. In the area of spatial modeling, Spatial-Temporal Graph Convolutional Networks (ST-GCN) <ref type="bibr" target="#b52">(Yan, Xiong, and Lin 2018b)</ref> first applied the graph convolutional neural network to the field of skeletal action recognition, which proposed the idea of learnable skeletal edge weights to solve the problem of different importance of different parts due to the grouping change between joints during human movement, for example, during the upper body joint parts may play a more important role than the lower body joint parts during writing. For other models <ref type="bibr" target="#b36">(Shi et al. 2019;</ref><ref type="bibr" target="#b49">Wu, Wu, and Kittler 2019;</ref><ref type="bibr" target="#b4">Chen et al. 2021b</ref>), most of them adopt a simple multichannel convolution method to achieve a high-dimensional representation to solve the problem of feature transformation of the input data. However, for this part of the processing of the actual method mostly makes the skeletal sequence matrix has the disadvantage of temporal invariance, for each frame of the skeleton is constantly changing dynamically, can not be more targeted for the adaptive weight calibration characteristics of the input feature matrix. For the temporal modeling aspect of the skeletal sequence, for the skeletal spatio-temporal map constructed by connecting joints between consecutive frames, many existing methods <ref type="bibr" target="#b52">(Yan, Xiong, and Lin 2018b;</ref><ref type="bibr" target="#b20">Li et al. 2019a;</ref><ref type="bibr" target="#b36">Shi et al. 2019</ref><ref type="bibr" target="#b37">Shi et al. , 2020a</ref><ref type="bibr" target="#b22">Li et al. 2019b</ref>) have used a fixed temporal kernel size method to process, with the multi-scale learning method proposed in Disentangling and Unifying Graph Convolutions (MS-G3D) <ref type="bibr" target="#b30">(Liu et al. 2020b</ref>) makes the model obtain a larger feeling field, realizes the modeling of different time sequences, and uses residual connections <ref type="bibr" target="#b13">(He et al. 2016)</ref> to facilitate training. To reduce the inference speed, a multiscale temporal modeling module with less analysis as well as residual connectivity was used in Channel-wise Topology Refinement Graph Convolution (CTR-GCN) <ref type="bibr" target="#b4">(Chen et al. 2021b</ref>). However, for temporal scale learning, it is difficult to solve issues such as skeletal action semantics for better effective modeling by simply adding to achieve feature fusion. For the multi-stream fusion framework problem, most of the previous methods <ref type="bibr" target="#b4">(Chen et al. 2021b;</ref><ref type="bibr" target="#b6">Cheng et al. 2020b;</ref><ref type="bibr" target="#b54">Ye et al. 2020</ref>) of fixed weights, we found that there is a problem of inconsistent use of stream in the case of optimal use results, and for stream using the same weights, which hardly reflects the importance of different streams.</p><p>To address the above problem, we propose a method (temporal-channel adaptive aggregation) that can dynamically learn spatio-temporal features of the skeleton and effectively combine the two. Specifically, we use contextual information in the temporal dimension of the skeletal map to assist in calibrating the weights generated by the input adaptively and perform feature aggregation in the temporal dimension. This helps us establish skeletal associations across different time series and finally obtain a dynamic temporal feauture representation in high-dimensional space. On the other hand, we use channel-wise topology modeling to learn the dynamic spatial topology of the input information, which is then aggregated with the previously obtained dynamic temporal topology in the channel dimension. In addition, we use a temporal modeling module with a multi-scale feature fusion mechanism as a complement. It first performs multiscale features extraction of the aggregated spatio-temporal topology to obtain a larger perceptual field, and then performs feature fusion with an attention mechanism. The temporal modeling module can effectively implement topological feature fusion to help us complete the modeling of the action. Its framework is shown in <ref type="figure">Figure 1</ref></p><formula xml:id="formula_1">(b).</formula><p>Overall, our main contributions are mainly in the following aspects:</p><p>? We propose a Temporal-Channel Aggregation Graph Convolutional Networks (TCA-GCN) to learn the topology in space and time dynamically and efficiently aggregate joint features in different temporal and channel dimensions for skeleton-based action recognition. Our approach can effectively balance features in the temporal and spatial dimensions of the human skeletal graph. ? We validated the validity of our model on three publicly available datasets, NTU RGB+D , NTU RGB+D 120 <ref type="bibr" target="#b27">(Liu et al. 2020a)</ref>, and NW-UCLA <ref type="bibr" target="#b47">(Wang et al. 2014)</ref>, and performed extensive ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Skeleton-based Action Recognition. Most of the early skeleton-based motion recognition was done by hand-made feature-based methods <ref type="bibr" target="#b46">(Vemulapalli, Arrate, and Chellappa 2014;</ref><ref type="bibr" target="#b12">Fernando et al. 2015)</ref> to simulate human motion. However, these methods mainly used the idea of translation between joints and 3D rotation, which suffers from incomplete considerations and performance problems <ref type="bibr" target="#b57">(Zeng et al. 2021b</ref>). In recent years, with the development of deep learning, more and more neural network structures have been applied in skeleton-based motion recognition, among which the more widely used ones are classified as recurrent neural networks (RNN), convolutional neural networks (CNN), and graph convolutional networks (GCN). RNN-based methods are mainly targeted at processing sequential data, and these methods extract features of bones and then construct skeletal sequence-related dependencies <ref type="bibr" target="#b9">(Du, Wang, and Wang 2015;</ref><ref type="bibr" target="#b24">Li et al. 2017b;</ref><ref type="bibr" target="#b29">Liu et al. 2016;</ref><ref type="bibr" target="#b35">Shahroudy et al. 2016;</ref><ref type="bibr" target="#b59">Zhang et al. 2017)</ref>. Shahrou et al. ) proposed a generic action recognition dataset and also proposed a part-aware LSTM model to model the long-term temporal correlation of each body part feature. <ref type="bibr" target="#b59">Zhang et al. (Zhang et al. 2017</ref>) proposed an adaptive recurrent neural network based on LSTM structure based on action recognition related features that can adapt to the most appropriate observation view. The RNN-based approach is more difficult to train and less parallel. CNN-based methods for skeletal action recognition have higher efficiency. CNN-based methods <ref type="bibr" target="#b26">(Liu, Tu, and Liu 2017;</ref><ref type="bibr" target="#b19">Kim and Reiter 2017;</ref><ref type="bibr" target="#b18">Ke et al. 2017;</ref><ref type="bibr" target="#b21">Li et al. 2017a;</ref><ref type="bibr" target="#b1">Bo et al. 2017</ref>) usually convert the skeletal sequence coordinates into three channels and then classify the features extracted through the network. Xu et al. <ref type="bibr" target="#b50">(Xu et al. 2022</ref>) proposed a pure CNN structure that can improve the modeling of irregular skeleton topology. However, these methods are difficult to achieve full utilization of the natural topology of the bones, and GCN can make full use of the spatio-temporal information of the skeleton to model the topology more effectively.</p><p>The ST-GCN proposed by Yan et al. <ref type="bibr" target="#b51">(Yan, Xiong, and Lin 2018a)</ref> introduced graph covolutional networks (GCNs) to the field of skeletal action recognition for the first time. The ST-GCN uses three joint segmentation methods by utilizing data information such as skeletal space-time. The method also proposes the Temporal Convolutional Network (TCN) for modeling the temporal dimension. Based on this approach a large number of methods based on this <ref type="bibr" target="#b51">(Yan, Xiong, and Lin 2018a;</ref><ref type="bibr" target="#b53">Yang et al. 2021;</ref><ref type="bibr" target="#b23">Li et al. 2021;</ref><ref type="bibr" target="#b31">Peng, Hong, and Zhao 2020)</ref> have been generated. <ref type="bibr" target="#b30">Liu et al. (Liu et al. 2020b)</ref> proposed MS-G3D, which enables direct information propagation using intertemporal edges, making an effective improvement to spatio-temporal topological modelling. Chen et al. <ref type="bibr" target="#b3">(Chen et al. 2021a</ref>) proposed a hop-aware hierarchical channel compression fusion layer based on the inspiration of feature compression <ref type="bibr" target="#b63">(Zou et al. 2020;</ref><ref type="bibr" target="#b55">Yu and Huang 2019;</ref><ref type="bibr" target="#b56">Zeng et al. 2021a)</ref>, which can effectively extract relevant information from neighboring nodes information from neighboring nodes efficiently. Dynamic skeletal topologies were also constructed to model the changes of actions over time better. Chen et al. <ref type="bibr" target="#b4">(Chen et al. 2021b</ref>) proposed a channel topology refinement graph convolution nonshared topology as well as the idea of dynamic graph convolution, which can spatially implement dynamic topology modeling.</p><p>Attention Mechanisms. Methods with attention mechanisms are increasingly used in various fields, and HU et al. <ref type="bibr" target="#b17">(Jie et al. 2017)</ref> proposed Squeeze-and-excitation networks, which are able to obtain scores on relevant dimensions. The transformer structure with self-attention <ref type="bibr" target="#b44">(Vaswani et al. 2017</ref>) was proposed and applied to text-based tasks such as translation. With increasing attention, such methods started to be applied to the vision domain.</p><p>In skeletal-based action recognition tasks. Song et al. <ref type="bibr" target="#b40">(Song et al. 2017</ref>) first proposed spatial-temporal attention Long Short-Term Memory method for modelling skeletal joint distinctions. Chiara et al. <ref type="bibr" target="#b32">(Plizzari, Cannici, and Matteucci 2021)</ref> proposed a novel Spatial-Temporal Transformer network (ST-TR), which uses spatio-temporal Transformer self-attention to model the representation of relationships between joints. Cheng et al. ) added an attention guided drop mechanism to the model as a means of enhancing the regularisation effect and effectively improving the accuracy of action recognition. Shi et al. <ref type="bibr" target="#b26">(Liu, Tu, and Liu 2017)</ref> proposed an attention map method to represent the strength of the connection between two nodes. Ye et al. <ref type="bibr" target="#b54">(Ye et al. 2020)</ref> proposed the Dynamic GCN method, which also proposed an attention mechanism method to capture spatial dependencies. Qiu et al. <ref type="bibr" target="#b34">(Qiu et al. 2022</ref>) proposed the spatio-temporal tuples transformer method to capture dependencies between different joints. In the recent work, song et al <ref type="bibr" target="#b41">(Song et al. 2022a</ref>) proposed a Spatial Temporal Joint Attention module that enables the finding of key joints for spatio-temporal sequence graphs to better achieve effective modelling of topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce the proposed skeleton-based action recognation method as shown in <ref type="figure">Figure 2</ref>(a). Section 3.1 describes the relevant preparatory work, including the data representation and the graph convolution structure used. We detail the specific architecture of the used TCA module in Section 3.2. In Section 3.3, we provide a detailed description of the TF module. Finally, we illustrate our fusion method for different streams in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Skeletal Data. The graph on the skeletal sequence can be represented by G = (V, E, X) <ref type="bibr" target="#b51">(Yan, Xiong, and Lin 2018a;</ref><ref type="bibr" target="#b32">Plizzari, Cannici, and Matteucci 2021;</ref><ref type="bibr" target="#b23">Li et al. 2021</ref>). The set V represents all the nodes. The set of edges is denoted by E, the degree of connectivity between different joints is usually represented by the adjacency matrix A ? R N ?N . The set of feature X ? R N ?C is represented as an input feature. Its dimension is the number of joints. In addition, we use a spatial partitioning method based on centrifugal and centripetal motions <ref type="bibr" target="#b51">(Yan, Xiong, and Lin 2018a)</ref>, the spatiotemporal skeleton is spatially partitioned, which is represented as (D, V, V ), where D is the number of delineated joint sets. Skeleton-based Graph Convolution. Since the birth of graph convolution <ref type="bibr" target="#b48">(Welling and Kipf 2016)</ref>, based on the ability to effectively model skeletal spatio-temporal maps, it has made a wide range of applications in the field of action recognition <ref type="bibr" target="#b51">(Yan, Xiong, and Lin 2018a)</ref>:</p><formula xml:id="formula_2">f out (v i ) = vj ?Bi 1 Z ij f in (v j ) ? w(l i (v j )),<label>(1)</label></formula><p>where f out (v i ) is output characteristics of the i-th joint, B i is the set of neighborhoods divided by the partitioning method, Z ij is equal to the number of bases in the corresponding subset, w(l i (v j )) is weighting function by index tensor. We used the spatial configuration partitioning method for skeletal sequence segmentation, l i (v j ) can be divided into three different values <ref type="bibr" target="#b51">(Yan, Xiong, and Lin 2018a;</ref><ref type="bibr" target="#b53">Yang et al. 2021;</ref><ref type="bibr" target="#b23">Li et al. 2021;</ref><ref type="bibr" target="#b31">Peng, Hong, and Zhao 2020;</ref><ref type="bibr" target="#b4">Chen et al. 2021b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TCA Module</head><p>Channel-wise Topology Modeling. In this part, we focus on spatial skeletal features. We use channel-wise topology modeling <ref type="bibr" target="#b4">(Chen et al. 2021b</ref>) to dynamically acquire the channel topology of a space, which mainly consists of correlation modeling function M(?) and refinement function R(?), where the correlation modeling function uses the input features X ? R T ?N ?C to model the channel correlations between joints to obtain channel-specific correlations Q ? R N ?N ?C 1 , and the refinement function sums the channel-specific correlations Q with the adjacency matrix obtained from spatial configuration partitioning to obtain channel-wise topologies S ? R N ?N ?C 1 , the framework of which is shown in <ref type="figure">Figure 2</ref>(f). In this paper, we use the following specific method formula <ref type="bibr" target="#b4">(Chen et al. 2021b)</ref>:   <ref type="figure">Figure 2</ref>: The pipeline of the proposed method. Our proposed network consists of 10 TCAF blocks, and each TCAF block consists of three TCA modules and one TF module. The TF module consists of multi-scale skeleton feature fusion containing MSCONV and feature fusion with attention mechanism. Detail structure of the TCA module: The TCA module consists of three parts. It contains temporal aggregation, channel-wise topology modeling and channel aggregation.</p><formula xml:id="formula_3">Skeletal Sequence Classification Joint&amp; Bone flow 10 ? TCAF Blocks (a) baseline T ? N ? C C? ? C ? 1 ? 1 T ? N ? C? G (? ) Initial Weights W (? ) T ? N ? C G (? ) Generate function W (? ) Reduced Dimension Functions T ? N ? C N ? N K (? ) Tanh (? ) Conv? (? ) Conv? (? ) ? (? ) Conv? (? ) K (? ) Averaging ? (? ) Normalization (d) TCA module details N ? N ? C? T ? N ? C? (e) Temporal Aggregation (f) Channel-wise Channel Aggregation Time Dimension Aggregation (b) TCA module Temporal Aggregation Channel-wise T ? N ? C? N ? N ? C? T ? N ? C? (b) TCA module Temporal Aggregation Channel-wise T ? N ? C? N ? N ? C? T ? N ? C? T ? N ? C Channel Aggregation (b) TCA module Temporal Aggregation Channel-wise T ? N ? C? N ? N ? C? T ? N ? C? T ? N ? C Channel Aggregation T? ? N ? C? (c) TF module MS CONV T ? N ? C? M (? ) (c) TF module MS CONV T ? N ? C? M (? ) T? ? N ? C? (c) TF module MS CONV T ? N ? C? M (? ) TCAF</formula><formula xml:id="formula_4">T ? N ? C C? ? C ? 1 ? 1 T ? N ? C? G (? ) Initial Weights W (? ) T ? N ? C G (? ) Generate function W (? ) Reduced Dimension Functions T ? N ? C N ? N K (? ) Tanh (? ) Conv? (? ) Conv? (? ) ? (? ) Conv? (? ) K (? ) Averaging ? (? ) Normalization</formula><formula xml:id="formula_5">S = ? ? Q + ? (A k ) ,<label>(2)</label></formula><p>where A k is k-th channel adjacency matrix, ?(?) is expressed as the normalization of the third-order adjacency matrix and the dimensional transformation operation and ? is a trainable parameter to represent the joint connection strength. Temporal Aggregation. In this part, we mainly consider the time dimension skeletal feature processing, inspired by Temporally-Adaptive Convolutions <ref type="bibr" target="#b15">(Huang et al. 2021)</ref>, we propose the temporal adaptive weight aggregation method, which is capable of generating calibrated weights in the temporal dimension based on the input features and aggregating them with the prior topology in time dimension to complete the temporal dynamic topological representation. Its framework is shown in <ref type="figure">Figure 2</ref>(e). Specifically, we first generate the temporal weights by using the skeletal sequence as the input X ? R T ?N ?C , using the generating function to multiply with the initial weights and perform the dimensional transformation to generate the temporal weights, which are temporally aggregated with the prior topology to obtain the new high-dimensional feature representation A out ? R T ?N ?C 1 , as shown in the following equation:</p><formula xml:id="formula_6">A out = T A(W (W ), X) = (W 1 X 1 ) ... (W T X T ),<label>(3)</label></formula><p>where W i X i is specified as:</p><formula xml:id="formula_7">(W i X i ) ab = C j w(j) ? x(j),<label>(4)</label></formula><p>where T A(?) indicates aggregation function, indicates aggregation operation, W (?) is the dimensionality reduction function, W (W ) ? R C?C 1 ?T is the temporal weight feature, whose expression is <ref type="bibr" target="#b15">(Huang et al. 2021)</ref>:</p><formula xml:id="formula_8">W = ? t ? W 0 ,<label>(5)</label></formula><p>where ? t = G(x 1 , ..., x t ) <ref type="bibr" target="#b15">(Huang et al. 2021)</ref>, and ? t is the calibrated weights with output channels, we use a different dimensional approach based on skeletal features, W 0 ? R C 1 ?C?1?1 is the initial weight. Channel Aggregation. In this part, we focus on aggregating the spatio-temporal features of the first two parts to complete the balancing process, we aggregate them in the channel dimension, whose graph convolutional representation is formulated as shown below:</p><formula xml:id="formula_9">f out (v i ) = vj ?Bi A out (v j ) ? S(v j ),<label>(6)</label></formula><p>where A out denotes the output feature of adaptive aggregation, S denotes the output feature of channel-wise topology, v i denotes the i-th joint, the aggregated ground in the channel dimension is specified as follows:</p><formula xml:id="formula_10">F out = CA(A out , S) = (A out1 S 1 ) ... (A outC 1 S C 1 ),<label>(7)</label></formula><p>where A outj S j is specified as:</p><formula xml:id="formula_11">(A outj S j ) ab = N i a out (i) ? s(i),<label>(8)</label></formula><p>where CA(?) indicates channel aggregation function, N denotes the number of joints, and F out ? R T ?N ?C 1 is the final output result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TF Module</head><p>Multi-scale Skeleton Feature Fusion. For feature fusion problems in temporal modeling, we use a method with an attention mechanism <ref type="bibr" target="#b8">(Dai et al. 2021)</ref> for feature fusion, unlike that, we use an input branch, which is used to solve the problem of feature fusion of contextual skeletal feature information to improve the effectiveness of modeling. For the temporal modeling feature, we use multi-scale convolutional <ref type="bibr" target="#b4">(Chen et al. 2021b</ref>) learning to augment the temporal convolutional layer. As shown in <ref type="figure">Figure 2(c)</ref>, the specific equation is expressed as:</p><formula xml:id="formula_12">Z out = sk (M SCON V (F out )) ,<label>(9)</label></formula><p>where F out denotes the features after TCA module, M SCON V (?) <ref type="bibr" target="#b4">(Chen et al. 2021b</ref>) is the multi-scale convolution, sk(?) is an attentional feature fusion method with a single branch input (i.e. M SCON V (?) ? M (?)). The specific formula for M (?) <ref type="bibr" target="#b8">(Dai et al. 2021</ref>) is expressed as:</p><formula xml:id="formula_13">Sig(l(Z) ? g(Z)),<label>(10)</label></formula><p>where Z is the input feature matrix, g(?) and l(?) is the global channel context and local channel context, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dynamic Fusion of Models</head><p>In order to obtain higher accuracy, many methods <ref type="bibr" target="#b4">(Chen et al. 2021b;</ref><ref type="bibr" target="#b7">Cheng et al. 2020c;</ref><ref type="bibr" target="#b54">Ye et al. 2020</ref>) nowadays use uniform fusion weights and use four streams of fusion: bone, bone motion, joint and joint motion. This leads to the problem of difficulty in reflecting the differences between different datasets and the inability to use all flow models. Therefore, to better address the variability between datasets as well as to better utilize all the flow models, we dynamically adjust the four flow models to achieve the better results. We established specific dynamic fusion planning equations, the details of which are placed in the supplementary material, and eventually obtained accuracy by Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we perform an experimental evaluation of our model in skeleton-based action recognition. In Section 4.1, We conducted experiments on three datasets namely Northwestern-UCLA <ref type="bibr" target="#b47">(Wang et al. 2014)</ref>, NTU RGB+D , and NTU RGB+D 120 <ref type="bibr" target="#b27">(Liu et al. 2020a</ref>). Section 4.2 describes the specific configuration used to conduct our experiments. In Section 4.3, to validate the generality of our model, we perform a final evaluation on these three datasets and compare them with other existing state-of-the-art methods. In Section 4.4, We provide ablation experiments to discuss the impact of the different components of our proposed approach, the ablation experiments are placed on the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Northwestern-UCLA. Northwestern-UCLA dataset <ref type="bibr" target="#b47">(Wang et al. 2014</ref>) is captured from multiple angles with three cameras. The dataset contains 1,494 video clips containing 10 action categories, which are performed by ten different objects. The training set used was from two of the cameras and the test set was from the other Kinect camera.</p><p>NTU RGB+D. NTU RGB+D ) contains 60 categories of actions, with a total of 56,880 samples, of which 40 categories are daily behavioral actions, 9 categories are health-related actions, and 11 categories are two-player mutual actions. 60 categories of actions are performed by 40 people ranging in age from 10 to 35 years old. It was acquired by the Microsoft Kinect v2 sensor and used three different camera angles, and in this paper we use data in the form of 3D skeletal information. It uses two criteria in dividing the training and test sets: (1) cross-subject (Xsub): the specified 20 people are used as the training set and the rest as the test set. (2) cross-view (X-view): the data collected by camera 1 is used as the training set, and the data collected by the remaining two cameras are used as the test set. NTU RGB+D 120. NTU RGB+D 120 <ref type="bibr" target="#b27">(Liu et al. 2020a</ref>) is the largest 3D joint dataset of human movements available. This dataset expands the NTU RGB+D dataset by adding 57,367 skeletal sequences and 60 additional action categories. This dataset was captured with three cameras, with a total of 113,945 samples, by 106 volunteers. The dataset was also divided between the training and test sets using two different division criteria: (1) cross-subject (X-sub): the training data is from 53 subjects, and the test set is from the remaining subjects. <ref type="formula" target="#formula_5">(2)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>All experiments are conducted with the PyTorch deep learning framework, and we use three Tesla M40 GPUs. Crossentropy method is used as the loss function. Our models are trained with Stochastic Gradient Descent (SGD) with momentum(0.9). When training the model on the three datasets, we used the warmup strategy for the first 5 epochs. About learning rate, we set to 0.1 and use decays at 35 epoch  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-Art</head><p>In order to fairly compare the results, we not only used the previous multi-stream fusion model <ref type="bibr" target="#b4">(Chen et al. 2021b;</ref><ref type="bibr" target="#b6">Cheng et al. 2020b;</ref><ref type="bibr" target="#b54">Ye et al. 2020</ref>). In addition, we have also included the results of Section 3.4 experiments named 4sD. The details of the implementation are included in the supplementary material. Comparison with SOTAs. As shown in <ref type="table">Table 1</ref>, 2, 3, our models obtain state-of-the-art results on almost all benchmarks. In NTU60, our method takes into account the dynamic features in time to better demonstrate the spatio-temporal variation of skeletal sequences and to better achieve the degree of differentiation of features. <ref type="table" target="#tab_4">Table 2</ref> shows our accuracy is 0.2% and 0.1% higher than CTR-GCN <ref type="bibr" target="#b4">(Chen et al. 2021b</ref>) in X-Sub and X-View criteria, respectively. In NTU120, our method is not only able to consider spatio-temporal features and effectively combine time-space modeling, but also has more advantages for large dataset identification. <ref type="table" target="#tab_6">Table 3</ref> shows our accuracy is 0.3% and 0.1% higher than CTR-GCN <ref type="bibr" target="#b4">(Chen et al. 2021b</ref>) in X-Sub and X-Set criteria, respectively. Finally, after conducting the new fusion experiments, our model: TCA-GCN(4sD), it has improved to some extent over all previous fusion methods. For example, our accuracy  is 0.5% higher than state-of-the-art method in NTU120. This is because we not only make full use of the stream framework, but also can perform dynamic weight fusion according to the importance of different streams. Comparison of attentional mechanisms. To explore the advantages of our model over methods with attentional mechanisms, the model is compared with other methods with attentional mechanisms. We perform experimental comparisons not only on single streams, but also on multistream fusion in terms of specific actions. The experimental results show that, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>: In terms of accuracy for single streams, our model fares better than S-TR (Plizzari, Cannici, and Matteucci 2021) in terms of full action recognition, which is particularly evident in the top right cor-  ner of the figure, indicating that our method also has some advantages in terms of some similar action recognition, and that our method is better able to learn topologically on single streams. This suggests that our spatio-temporal aggregation model has a better advantage, mainly because our model is able to model the spatio-temporal topology effectively and better learn effectively based on the topology. The results of specific actions are shown in <ref type="table" target="#tab_8">Table 4</ref>: <ref type="formula" target="#formula_2">(1)</ref> In general, our model has some advantages in recognizing difficult actions, for example, our model is 2.4%, 1.1%, and 1.0% higher than other methods in eating meal, which is mainly attributed to our spatio-temporal topology modeling can better solve the effective extraction of spatio-temporal features.</p><p>(2) From the individual approaches, our method is not only able to surpass the attentional methods in time and space in a single stream, but also has certain advantages in the combination of multiple streams. Finally, the experimental accuracy is further improved by comparing both approaches, which indicates that our temporal aggregation approach not only integrates well with the attention mechanism, but also shows better performance than the attention mechanism. This is because for skeletons, aggregation is better at modeling the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We demonstrate the advantages of our proposed model and the effectiveness of each component. Since the NTU RGB+D 120 data is the largest 3D joint annotation dataset for human motion recognition and is more general and researchable, we describe this data as two benchmarks unless otherwise stated. We put the experiment in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a temporal-channel dimension aggregation-based graph neural network that includes TCA module and TF module. The network is capable of dynamic spatio-temporal topological feature extraction and their effective aggregation, as well as attention feature fusion for multi-scale features, to complete the modeling more effectively. Extensive experiments show that our model has advanced performance on different types of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Specific Configuration of Model Architecture</head><p>Modeling of Spatial Dimensions. In the spatial modeling module, we use three TCA modules for dynamic spatiotemporal feature extraction, and we use the output feature results of this module as the input features X ? R T ?N ?C of the temporal modeling module. Specifically, channelwise topology modeling is used for spatial skeletal topology optimization modeling to realize the combination of spatial a priori skeletal knowledge and a posteriori knowledge, and the temporal aggregation method is used to complete the high-dimensional feature representation with temporal adaptive aggregation, and then they are channel aggregated, and finally, the results of the three channels of this module are summed to form the final output features.</p><p>Modeling of Temporal Dimensions. We use the multi-scale convolutional skeleton feature fusion method as a temporal modeling module (TF module) that can consider action semantics for different skeletal features, better feature fusion of skeletal information, and improved modeling capabilities.</p><p>The module takes F out ? R T ?N ?C 1 as input and obtains the output Z out ? R T ?N ?C 1 , the framework of which is shown in <ref type="figure">Figure 2(a)</ref>. Finally, the spatial modeling module is connected with the temporal modeling module for residuals to form the TCAF block. Our network uses ten TCAF blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dynamic Fusion Planning Equations</head><p>The data flow fusion problem can be viewed as a functional optimization process, where the optimal fusion effect is the objective function and the individual flow models are used as decision variables to construct the fusion-based nonlinear programming equations. Here we use the control variable method. We carried out a quantitative process for making full use of the four streams fusion weights for comparison, we put the initial values of the four weights greater than 0 into the model constraint part to achieving the importance of different streams. We set four relations with small weights and put them into constraints. In addition, we output the maximum accuracy that satisfies the condition, the pseudo-nonlinear programming equation is shown below:</p><formula xml:id="formula_14">goal : max(right/zong),<label>(11)</label></formula><formula xml:id="formula_15">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? y i = a ? r 11i + b ? r 22i + c ? r 33i + d ? r 44i , right = Sum (M ax (Index (y i )) == Label i ) , r 11i ? r 1i , r 22i ? r 2i , r 33i ? r 3i , r 44i ? r 4i , b &gt; a &gt; c &gt; d, a, b, c, d ? (0, 0.05, 1] , i = 1, 2, ..., zong,<label>(12)</label></formula><p>where y i denotes the multi-stream fusion score, a, b, c, d denotes the weights of different streams respectively, r 11i denotes the score of a single stream, Label i denotes the true label of the action, Index(?) is the index value where the solved action score is located, in order to be able to correspond with the true value. ? denotes the extraction operation operator, zong denotes the number of actions for the test set, right indicates the number of correct predicted actions. To facilitate efficient solving of nonlinear programming equations, we set up a pseudo code, as shown in the Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Efficient of Multi-scale Skeleton Feature Fusion</head><p>To verify the efficient of multi-scale convolutional skeleton feature fusion in NTU60(X-Sub) , we compared its time consumption with the current state-ofthe-art model <ref type="bibr" target="#b4">(Chen et al. 2021b)</ref>. As shown in the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Two Interactive Performance</head><p>For some skeletal sequences, there are two people moving with each other. The movements have occlusion as well as the difference in spatio-temporal position, which causes the problem of multi-semantic movements and makes it difficult to recognize such movements. For example, the action in <ref type="figure" target="#fig_2">Figure 4</ref>, it can be found that one person's action changes more obviously, and the other person's action is not obvious. To verify how well our model handles this problem, we conducted experiments on bone streams and compared the results with the current best method <ref type="bibr" target="#b4">(Chen et al. 2021b</ref>). Our model improves the effect of such actions as knock over and follow by 5% and 4%. This is because our proposed method is able to model multi-scale information with a priori skeletal knowledge to solve the action semantics problem. This implies the feasibility of our method to deal with this type of problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Dynamic Fusion of Models</head><p>To solve the problem of uneven use of the flow framework and the weight assignment problem, we use a mathematical approach to solve the problem, using CTR-GCN <ref type="bibr" target="#b4">(Chen et al. 2021b</ref>) and our proposed model with the same configuration as a control experiment. As can be seen from   our proposed dynamic fusion uses all four streams, and in addition, the weights we use are consistent with the real situation of each stream. For example, if the result of our bone mode is better than that of the joint mode, then it should be given more weight than the joint mode when fusion is performed, and the results of the method are improved on different data sets, which also indicates the good applicability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Single-stream Comparative Ablation Experiments</head><p>To compare the combined effect of our model based on four streams, including bone, joint, bone motion, and joint motion. Based on the NTU RGB+D 120 dataset, the comparison is made one by one with those of the publicly available state-of-the-art methods <ref type="bibr" target="#b4">(Chen et al. 2021b)</ref>, as shown in <ref type="table" target="#tab_14">Table 7</ref>, our model is higher than the state-of-the-art methods by 1.1%, 0.2%, 0.5%, 0.2% for the four streams, respectively. The efficiency as well as the applicability of our proposed skeleton-based action recognition model for each stream is verified.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Exploring Model Configurations</head><p>To explore our model in dealing with the spatio-temporal sequence characteristics of the skeleton, we conducted experiments on the settings of the components, and the results are tabulated below, including the reduction rate r of the temporally channel adaptive aggregation, the activation function of each spatial modeling module, and the number of model blocks l. As shown in the <ref type="table" target="#tab_16">Table 8</ref>, after the experiments, we found that the results were better than the state-of-theart methods <ref type="bibr" target="#b4">(Chen et al. 2021b)</ref>, which also confirmed the robustness of our method, as follows, (1) by modifying the activation function of the spatial modeling part, the results are very different and have good performance, indicating the applicability and compatibility of our method.</p><p>(2) By modifying the activation function in the spatial modeling part, the difference in results is very small, and both have good performance, so we use this activation function considering that the ReLU function is easier to learn to optimize. (3) In addition, we also modeled the number of basic blocks because the sensory field of the joint is crucial for graph convolution, and the results show that having a better modeled sensory field at k=10, the larger the sensory field contains more advanced skeletal features, which is very detrimental to the recognition of subtle movements. We finally chose method A as our basic configuration, which has an accuracy of 86.8%.   proposed adaptive aggregation transformation method for skeletal sequences. The left side is squat down, the right side is eat meal, and we choose the same time dimension for visualization in order to show the convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Visualization of Joint Characteristics</head><p>We show the feature-transformed nodal features of the two actions and the aggregated nodal features on the skeletal sequence proposed in this paper on the dataset, using the number of nodal points and the time series dimensions for feature visualization, respectively. The visualization results which are shown in <ref type="figure" target="#fig_3">Figure 5</ref>, where (a), (b) show the previously used method <ref type="bibr" target="#b4">(Chen et al. 2021b)</ref>, and (c), (d) show the method we used, and (b), (d) are the eating actions. We observe that (1) in terms of joint feature representation, our method has a wider range of features because our temporally channel adaptive aggregation method can dynamically adjust the weights and thus perform temporal aggregation, indicating that our method is better at representing joint features, indicating that our method is better at representing joints, (2) the visualization results we show are different for different actions, indicating that our method is able to perform adaptive aggregation learning for different actions, and (3) when the same action is represented, our model visualization is more differentiated, indicating that that our method is able to make dynamic adjustments to action timeliness based on time series. (4) From a practical point of view, the importance of the movement of the hands is obviously more expressive when performing eat meal, and when performing squat down, the characteristics of each part change significantly with time, which is also in line with the actual situation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Confusion matrix with attention mechanism and our methods. (a), (b) are S-TR and our method (only joint).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The skeleton visualization of knock over action. For the sake of conciseness and clarity of the general change process, three time periods are selected for display in accordance with the chronological order of the action. The blue color indicates the upper limb part of the skeletal frame. The person on the left has a larger change in movement, while the other person on the right has a smaller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of joint features. (a), (b) are the previous feature transformation methods and (c), (d) is our</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1: Dynamic fusion of models solver. Input: four streams of action score: {r lli } 4 ll=1 ; Label i : the true label of the action. Output: Action accuracy. 1: Initialize the fusion weights (a, b, c, d) ?? the previous method; 2: Import the test scores of the four streaming frameworks {r ki }</figDesc><table><row><cell cols="2">4 k=1 ; 3: while b&gt;a&gt;c&gt;d and (a, b, c, d) ?? (0,1] do</cell></row><row><cell>4:</cell><cell>Calculate the correct rate of action prediction acc</cell></row><row><cell></cell><cell>from Eq. 11, calculate r and the correct number of</cell></row><row><cell></cell><cell>right according to the Eq. 12;</cell></row><row><cell>5:</cell><cell>if the model accuracy improve then</cell></row><row><cell>6:</cell><cell>Save new fusion weights and results;</cell></row><row><cell>7:</cell><cell>else</cell></row><row><cell>8:</cell><cell>Keep the parameters;</cell></row><row><cell>9:</cell><cell>end if</cell></row><row><cell cols="2">10: end while</cell></row><row><cell cols="2">11: return acc;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>cross-setup (X-setup): the training data set from samples with even set IDs, and the test data set are from samples with other set IDs.</figDesc><table><row><cell>Methods</cell><cell>N-UCLA</cell></row><row><cell>Lie Group (2015)</cell><cell>74.2</cell></row><row><cell>HBRNN-L (2015)</cell><cell>78.5</cell></row><row><cell>Glimpse Clouds (2018)</cell><cell>87.6</cell></row><row><cell>VA-fusion (2018)</cell><cell>88.1</cell></row><row><cell>Action Machine (2018)</cell><cell>92.3</cell></row><row><cell>AGC-LSTM (2019)</cell><cell>93.3</cell></row><row><cell>SGN cite (2020b)</cell><cell>92.5</cell></row><row><cell>Shift-GCN (2020c)</cell><cell>94.6</cell></row><row><cell cols="2">DC-GCN+ADG (2020a) 95.3</cell></row><row><cell>CTR-GCN (2021b)</cell><cell>96.5</cell></row><row><cell>Ta-CNN (2022)</cell><cell>96.1</cell></row><row><cell>Ta-CNN+ (2022)</cell><cell>97.2</cell></row><row><cell>TCA-GCN</cell><cell>96.8</cell></row><row><cell>TCA-GCN(4sD)</cell><cell>97.0</cell></row><row><cell cols="2">Table 1: Our model is compared with the state-of-the-art ap-</cell></row><row><cell cols="2">proach on the NW-UCLA dataset, where 4sD denotes the</cell></row><row><cell cols="2">result of four-stream fusion in Section 3.4.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Our model is compared with the state-of-the-art method on the NTU RGB+D dataset, where 4sD denotes the result of four-stream fusion in Section 3.4. and 55 epoch, ending the training at 65 epoch. For the NTU-RGB+D 60&amp;120 dataset, we used the preprocessing), setting the batch size to 64. For the Northwestern-UCLA dataset, we set the batch size to 16. In multi-stream fusion for different datasets, applying the previous fusion method<ref type="bibr" target="#b4">(Chen et al. 2021b;</ref><ref type="bibr" target="#b7">Cheng et al. 2020c;</ref><ref type="bibr" target="#b54">Ye et al. 2020</ref>).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Our model is compared with the state-of-the-art method on the NTU RGB D+120 dataset, where 4sD denotes the result of four-stream fusion in Section 3.4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Accuracy of specific action classification under dif-</cell></row><row><cell>ferent methods, where E-B0 represents for EfficientGCN-B0</cell></row><row><cell>and put into bag represents for put object into bag.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Comparison of efficiency with state-of-the-art models. Experiments on the same GPU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Comparison of dynamic nonlinear fusion settings. Where denotes the SOTA model selected to be compared, Number of Streams indicates the number of stream used, Same data set configuration indicates whether to use the previous model fusion weights, Using Dynamic Fusion indicates whether weights are dynamically assigned for different streams.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>A comparison of the combined performance of our models on different stream.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>Comparison of the results of our model with different part settings.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
		<editor>Realtime Multi-Person 2D</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Pose Estimation using Part Affinity Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning multi-granular spatiotemporal graph network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4334" to="4342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13359" to="13368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition With Shift Graph Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition With Shift Graph Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attentional feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gieseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oehmcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RMPE: Regional Multi-person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-Scale Spatial Temporal Graph Neural Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Going Deeper into Action Recognition: A Survey. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="4" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">TAda! Temporally-Adaptive Convolutions for Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H A</forename></persName>
		</author>
		<idno>abs/2110.06178</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A New Representation of Skeleton Sequences for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interpretable 3D Human Action Analysis with Temporal Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Routing for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8561" to="8568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Skeleton-based Action Recognition with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Symbiotic graph neural networks for 3d skeleton-based human action recognition and motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3316" to="3333" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive RNN Tree for Large-Scale Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1453" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Hong; Mengyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition: The Journal of the Pattern Recognition Society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Two-stream 3d convolutional neural network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<title level="m">NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2684" to="2701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Skeleton-Based Online Action Prediction Using Scale Selection Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kot Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatiotemporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aaai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition via spatial and temporal transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plizzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cannici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page">103219</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fusing higher-order features in graph neural networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01563</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02849</idno>
		<title level="m">Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition With Directed Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition With Directed Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<editor>Singh, S.</editor>
		<editor>and Markovitch, S.</editor>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Constructing stronger and faster baselines for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Constructing stronger and faster baselines for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<title level="m">Differential Recurrent Neural Networks for Action Recognition. international conference on computer vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Crossview Action Modeling, Learning and Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1405.2941</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">J. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CVF International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Topologyaware Convolutional Neural Network for Efficient Skeletonbased Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>McIlraith, S. A.</editor>
		<editor>and Weinberger, K. Q.</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Feedback graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="164" to="175" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dynamic gcn: Context-enriched topology learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Autoslim: Towards one-shot architecture search for channel numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11728</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Hopaware dimension optimization for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv, 2105: 2</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning skeletal graph neural networks for hard 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11436" to="11445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Action machine: Rethinking action recognition in trimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05770</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">High-order Graph Convolutional Networks for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st British Machine Vision Conference 2020</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2020-09-07" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
