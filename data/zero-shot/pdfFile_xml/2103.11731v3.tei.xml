<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-DETR: Image-Level Few-Shot Object Detection with Inter-Class Correlation Exploitation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
							<email>gongjiezhang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Luo</surname></persName>
							<email>zhipeng001@e.ntu.edu.sgkaiwen001@e.ntu.edu.sgshijian.lu@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-DETR: Image-Level Few-Shot Object Detection with Inter-Class Correlation Exploitation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot object detection has been extensively investigated by incorporating meta-learning into region-based detection frameworks. Despite its success, the said paradigm is constrained by several factors, such as (i) low-quality region proposals for novel classes and (ii) negligence of the inter-class correlation among different classes. Such limitations hinder the generalization of base-class knowledge for the detection of novel-class objects. In this work, we design Meta-DETR, a novel few-shot detection framework that incorporates correlational aggregation for meta-learning into DETR detection frameworks. Meta-DETR works entirely at image level without any region proposals, which circumvents the constraint of inaccurate proposals in prevalent few-shot detection frameworks. Besides, Meta-DETR can simultaneously attend to multiple support classes within a single feed-forward. This unique design allows capturing the inter-class correlation among different classes, which significantly reduces the misclassification of similar classes and enhances knowledge generalization to novel classes. Experiments over multiple few-shot object detection benchmarks show that the proposed Meta-DETR outperforms state-of-the-art methods by large margins. The implementation codes will be released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computer vision has experienced significant progress in recent years. However, there still exists a huge gap between current computer vision techniques and the human visual system in learning new concepts from very few examples: most existing methods require a large amount of annotated samples, while humans can effortlessly recognize a new concept even with very few instructions <ref type="bibr" target="#b13">(Landau, Smith, and Jones 1988)</ref>. Such human-like capability to generalize from limited examples is highly desirable for machine vision systems, especially when sufficient training samples are unavailable or their annotations are hard to obtain.</p><p>In this work, we explore the challenging task of few-shot object detection, which requires detecting novel objects with only a few training samples. With extremely limited supervision from annotated samples, the key is to exploit knowledge from base classes and generalize it to novel classes. To this ? denotes equal contribution.</p><p>* denotes corresponding author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Classes</head><p>Inter-Class Correlation Proposal Generation ???? <ref type="bibr">Region-wise Prediction</ref> Image-level Prediction <ref type="figure">Figure 1</ref>: Comparison of few-shot object detection pipelines: Prior works (upper part) perform region-level detection, which are often constrained by inaccurate region proposals for novel classes. Besides, they can only deal with one support class at one go and overlook the correlation among different classes. The proposed Meta-DETR (lower part) works at image level without any proposals. It captures interclass correlation by learning from multiple support classes simultaneously, which suppresses confusion among similar classes and enhances model generalization greatly.</p><p>end, many works <ref type="bibr">(Kang et al. 2019;</ref><ref type="bibr" target="#b31">Yan et al. 2019;</ref><ref type="bibr" target="#b30">Xiao and Marlet 2020;</ref><ref type="bibr" target="#b6">Fan et al. 2020;</ref><ref type="bibr" target="#b9">Hu et al. 2021</ref>) incorporate meta-learning into generic object detection frameworks, mostly Faster R-CNN <ref type="bibr" target="#b22">(Ren et al. 2015)</ref>, and have achieved very promising results. Despite their success, there still exist two underlying limitations that hinder better exploitation of base-class knowledge, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. First, region-based detection frameworks rely on region proposals to produce final predictions, thus are sensitive to low-quality region proposals. Unfortunately, as investigated by <ref type="bibr" target="#b6">Fan et al. (2020)</ref> and <ref type="bibr" target="#b35">Zhang, Wang, and Forsyth (2020)</ref>, it is not easy to produce highquality region proposals for novel classes with limited supervision under the few-shot detection setup. Such a gap proposals) for novel classes is clearly lower than that of base classes, as shown in (a). This hinders the knowledge generalization to novel classes. Additionally, object classes with similar appearances are highly correlated in feature space as shown in <ref type="bibr">(b)</ref>, which tend to be misclassified if the learning does not incorporate the correlation among them, as illustrated in <ref type="bibr">(c)</ref>.</p><p>in the quality of region proposals obstructs the generalization from base classes to novel classes. Second, most existing meta-learning-based approaches <ref type="bibr">(Kang et al. 2019;</ref><ref type="bibr" target="#b31">Yan et al. 2019;</ref><ref type="bibr" target="#b6">Fan et al. 2020;</ref><ref type="bibr" target="#b30">Xiao and Marlet 2020)</ref> adopt 'feature reweighting' or its variants to aggregate query and support features, which can only deal with one support class (i.e., target class to detect) at a time and essentially treat each support class independently. Without seeing multiple classes within a single feed-forward, they largely overlook the important inter-class correlation among different support classes. This limits the ability to distinguish similar classes (e.g., distinguishing from cows and sheep) and to generalize from related classes (e.g., learning to detect cows by generalizing from detecting sheep). To mitigate the above limitations, we design Meta-DETR, an innovative few-shot object detector that achieves metalearning at image level and at the same time explicitly exploits the inter-class correlation among different support classes. To our best knowledge, this is the first work that explores incorporating meta-learning into the recently proposed DETR detection frameworks <ref type="bibr" target="#b1">(Carion et al. 2020;</ref><ref type="bibr" target="#b37">Zhu et al. 2021b)</ref>, which can skip proposal generation and directly perform detection at image level. With image-level meta-learning, the proposed Meta-DETR lifts the constraint of inaccurate region proposals as in prevalent few-shot detection frameworks. In addition, as shown in <ref type="figure">Fig. 1</ref>, Meta-DETR can attend to multiple support classes at one go instead of class-by-class meta-learning with repeated runs as in most existing methods. By integrating detection tasks that involve multiple classes into meta-learning, Meta-DETR can explicitly leverage the inter-class correlation, including (i) the inter-class commonality to facilitate generalization among related classes and (ii) the inter-class uniqueness to reduce misclassification among similar classes.</p><p>In summary, the contributions of this work are threefold. First, we propose Meta-DETR, an innovative few-shot object detection framework that incorporates metalearning into DETR detection frameworks. Being the first pure image-level meta-detector, Meta-DETR circumvents the gap of inaccurate region proposals for novel-class objects, enabling better generalization to novel classes. Second, we design a novel correlational aggregation module for few-shot object detection, which allows aggregating query features with multiple support classes simultaneously. It enables effective exploitation of the inter-class correlation, which greatly reduces misclassification and enhances model generalization. Third, extensive experiments show that, without bells and whistles, the proposed Meta-DETR outperforms state-of-the-art methods by large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object Detection Generic object detection ) is a joint task on object localization and classification. Modern object detectors are mostly region-based and can be broadly classified into two categories: two-stage and single-stage detectors. Two-stage detectors include Faster R-CNN <ref type="bibr" target="#b22">(Ren et al. 2015)</ref> and its variants <ref type="bibr" target="#b10">(Hu et al. 2018;</ref><ref type="bibr" target="#b0">Cai and Vasconcelos 2018;</ref><ref type="bibr" target="#b32">Zhang, Lu, and Zhang 2019)</ref>, which first adopt a Region Proposal Network (RPN) to generate region proposals, and then produce final predictions based on the proposals. Differently, single-stage detectors <ref type="bibr" target="#b18">(Liu et al. 2016;</ref><ref type="bibr" target="#b21">Redmon and Farhadi 2017;</ref><ref type="bibr" target="#b33">Zhang et al. 2018</ref>) employ densely placed anchors as region proposals and directly make predictions over them. Recently, another line of research featuring DETR <ref type="bibr" target="#b1">(Carion et al. 2020</ref>) and its variants <ref type="bibr" target="#b37">(Zhu et al. 2021b;</ref><ref type="bibr" target="#b4">Dai et al. 2021)</ref> has received vast attention, thanks to the merits of pure image-level framework, fully end-to-end pipeline, and comparable or even better performance. However, these aforementioned generic detectors still heavily rely on large amounts of annotated training samples, thus will suffer from drastic performance drop when directly applied to few-shot object detection.  <ref type="figure">Figure 3</ref>: The framework of the proposed Meta-DETR. Query and support images are processed by a weight-shared feature extractor to produce query and support features. To leverage the inter-class correlation in meta-learning, the correlational aggregation module (CAM) first matches the query features with multiple support classes simultaneously, then introduces task encodings to differentiate these support classes. Finally, few-shot detection results are obtained via a class-agnostic transformer architecture that predicts objects' locations and corresponding task encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot Object Detection</head><p>Existing works on few-shot object detection can be categorized into two paradigms: transfer learning and meta-learning. Transfer-learning-based methods include LSTD <ref type="bibr" target="#b2">(Chen et al. 2018)</ref>, TFA , MPSR <ref type="bibr" target="#b29">(Wu et al. 2020)</ref>, and FSCE <ref type="bibr" target="#b25">(Sun et al. 2021)</ref>, where novel concepts are learned via fine-tuning. Differently, meta-learning-based methods <ref type="bibr">(Kang et al. 2019;</ref><ref type="bibr" target="#b31">Yan et al. 2019;</ref><ref type="bibr" target="#b28">Wang, Ramanan, and Hebert 2019;</ref><ref type="bibr" target="#b20">Perez-Rua et al. 2020;</ref><ref type="bibr" target="#b30">Xiao and Marlet 2020;</ref><ref type="bibr" target="#b6">Fan et al. 2020;</ref><ref type="bibr" target="#b9">Hu et al. 2021</ref>) extract knowledge that can generalize across various tasks via 'learning to learn', i.e., learning a class-agnostic predictor on various auxiliary tasks. Our proposed Meta-DETR falls under the umbrella of meta-learning, but differs from existing approaches by achieving image-level meta-learning and effectively leveraging the correlation among various support classes. To the best of our knowledge, Meta-DETR is the first work that incorporates meta-learning into the recently proposed DETR frameworks; It is also the pioneering work to explicitly integrate the inter-class correlation among support classes into meta-learning-based few-shot object detection frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Problem Definition Given two sets of classes C base and C novel , where C base ? C novel = ?, a few-shot object detector aims at detecting objects of C base ? C novel by learning from a base dataset D base with abundant annotated objects of C base and a novel dataset D novel with very few annotated objects of C novel . In the task of K-shot object detection, there are exactly K annotated objects for each novel class in D novel .</p><p>Rethink Region-Based Detection Frameworks Most existing works on few-shot object detection are developed on top of Faster R-CNN <ref type="bibr" target="#b22">(Ren et al. 2015)</ref>, a region-based object detector, thanks to its robust performance and easy optimization. However, by relying on region proposals to produce detection results, these approaches are inevitably con-strained by the inaccurate proposals for novel classes due to very limited supervision under the few-shot detection setup. As illustrated in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>, there is a clear gap in the quality of region proposals for base and novel classes, hindering region-based detection frameworks from fully exploiting base-class knowledge to generalize to novel classes. Though several studies <ref type="bibr" target="#b6">(Fan et al. 2020;</ref><ref type="bibr" target="#b35">Zhang, Wang, and Forsyth 2020)</ref> attempt to acquire more accurate region proposals, this issue still remains as it is rooted in the region-based detection frameworks under the few-shot learning setup. Rethink Meta-Learning via Feature Reweighting To meta-learn a class-agnostic detector that can generalize across various classes, most existing methods <ref type="bibr">(Kang et al. 2019;</ref><ref type="bibr" target="#b31">Yan et al. 2019;</ref><ref type="bibr" target="#b6">Fan et al. 2020;</ref><ref type="bibr" target="#b30">Xiao and Marlet 2020)</ref> adopt 'feature reweighting' or its variants to aggregate query features with support class information, acquiring class-specific meta-features to detect objects corresponding to the support class. However, such aggregation approaches can deal with only one support class within each feed-forward process, i.e., C repeated runs are required to detect C classes within each query image. More importantly, by treating each support class independently, 'feature reweighting' overlooks the essential inter-class correlation among different support classes. As shown in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>, many object classes with similar appearances are highly correlated. Intuitively, their correlation can effectively facilitate the distinction and the generalization among similar classes. However, as shown in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>, in existing methods, we observe that objects misclassified as highly correlated classes constitute a major source of error due to the negligence of inter-class correlation.  <ref type="bibr" target="#b37">(Zhu et al. 2021b</ref>), a fully end-to-end Transformer-based <ref type="bibr" target="#b26">(Vaswani et al. 2017)</ref> detector, as the basic detection framework to bypass the constraint of region-wise prediction. Besides, during metalearning, Meta-DETR aggregates query features with multiple support classes simultaneously, thus can exploit the inter-class correlation among different classes to reduce misclassification and boost generalization.</p><p>Specifically, given a query image and a set of support images with instance annotations, a weight-shared feature extractor first encodes them into the same feature space. Subsequently, the correlational aggregation module (CAM), which will be introduced in detail later, performs matching between the query features and the set of support classes. CAM further maps the set of support classes to a set of pre-defined task encodings that differentiate these support classes in a class-agnostic manner. Finally, detection results are obtained via a transformer architecture that predicts objects' locations and corresponding task encodings. As the detection targets are dynamically determined by support classes and their mappings to task encodings, the proposed Meta-DETR is trained as a meta-learner to extract generalizable knowledge not specific to certain classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Correlational Aggregation Module</head><p>The correlational aggregation module (CAM) is the key component in Meta-DETR, which aggregates query features with support classes for the subsequent class-agnostic prediction. CAM differs from existing aggregation methods in that it can aggregate multiple support classes simultaneously, which enables it to capture their inter-class correlation to reduce misclassification and enhance model generalization. Specifically, as illustrated in <ref type="figure">Fig. 4</ref>, given the query and support features, a weight-shared multi-head attention module first encodes them into the same feature space, and the prototype for each support class is obtained by applying RoIAlign ) followed by average pooling on the support features. CAM then performs feature matching and encoding matching, which will be elaborated in the remainder of this subsection, to match the query features with support features and task encodings, respectively. Their results are summed together and processed by a feed-forward network (FFN) to produce the final output.</p><p>Feature Matching Feature matching is accomplished by a single-head attention mechanism. Specifically, given a query feature map Q ? R HW ?d and the support class prototypes S ? R C?d , the matching coefficients are obtained via:</p><formula xml:id="formula_0">A = Attn(Q, S) = Softmax( (QW)(SW) T ? d ),<label>(1)</label></formula><p>where HW is the spatial size, C is the number of support classes, d is the feature dimensionality, and W is a linear projection shared by Q and S, which ensures they are embedded into the same feature space. Subsequently, the output of the feature matching module can be obtained via:</p><formula xml:id="formula_1">Q F = A?(S) Q,<label>(2)</label></formula><p>where ?(?) denotes sigmoid function and denotes Hadamard product. ?(S) serves as feature filters for each</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Features</head><p>Multi-Head Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Features Task Encodings</head><formula xml:id="formula_2">Q K V Multi-Head Attention Q K V (HW,d) (C,HW,d) (C,HW,d) (C,d) C BG-Prototype (C+1,d) (1,d)</formula><p>Single-Head Attention <ref type="bibr">(HW,d)</ref> Single-Head Attention  <ref type="figure">Figure 4</ref>: The architecture of the Correlational Aggregation Module (CAM). It performs two matching processes: feature matching filters out query features that are unrelated to support classes, while encoding matching maps support classes to a set of pre-defined task encodings that differentiate the support classes in a class-agnostic manner.</p><formula xml:id="formula_3">Q V K K Q V C</formula><p>individual support class with the function of extracting only class-related features from query features. By applying the matching coefficients A to ?(S), we filter out features not matched to any support class, producing a feature map Q F that highlights objects belonging to the given support classes.</p><p>Encoding Matching To achieve meta-learning that requires class-agnostic prediction, we introduce a set of predefined task encodings and map the given support classes to these task encodings, so that final predictions can be made on the task encodings instead of specific classes. We implement task encodings T ? R C?d with sinusoidal functions, following the positional encodings of the Transformer <ref type="bibr" target="#b26">(Vaswani et al. 2017)</ref>. Encoding matching uses the same matching coefficients as feature matching, and the matched encodings Q E are obtained via:</p><formula xml:id="formula_4">Q E = AT.<label>(3)</label></formula><p>Modeling Background for Open-Set Prediction Object detection features an open-set setup where background, which does not belong to any of the target classes, often takes up most of the space in a query image. Therefore, as shown in <ref type="figure">Fig. 4</ref>, we additionally introduce a learnable prototype and a corresponding task encoding (fixed to zeros), denoted as BG-Prototype and BG-Encoding respectively, to explicitly model the background class. This eliminates the matching ambiguity when query does not match any of the given support classes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Objective</head><p>Target Generation The detection targets of Meta-DETR are dynamically determined by the support classes and their mappings to task encodings. Concretely, given a query image, C support images representing different support classes are randomly sampled. Only ground truth objects belonging to the sampled support classes are kept as detection targets. Besides, the classification target for each object is the task encoding of the ground truth class instead of the ground truth class itself. We empirically set C as 5 according to our ablation study on this hyper-parameter in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The loss functions for our proposed Meta-DETR follow Deformable DETR <ref type="bibr" target="#b37">(Zhu et al. 2021b)</ref>, which adopts a set-based Hungarian loss that forces unique predictions for each object via bipartite matching. Following Meta R-CNN <ref type="bibr" target="#b31">(Yan et al. 2019)</ref>, we additionally introduce a cosine similarity cross-entropy loss  to classify the class prototypes obtained by our designed CAM. It encourages prototypes of different classes to be distinguished from each other. Please refer to appendix for a detailed description of the loss functions. Marlet 2020), we also include objects from base classes to prevent performance drop for base classes. In both stages, the network is optimized in an end-to-end manner with the same training objective described in Section 4.3. Efficient Inference Unlike the training stage, there is no need to repeatedly sample support images and extract their features. We can first compute the prototypes for each support class once and for all, then directly use them for every query image to predict. This promises the efficient inference of our proposed Meta-DETR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training and Inference Procedure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We follow the well-established data setups for few-shot object detection <ref type="bibr">(Kang et al. 2019;</ref><ref type="bibr" target="#b27">Wang et al. 2020</ref>). Concretely, two widely used few-shot object detection benchmarks are adopted in our experiments. Pascal VOC <ref type="bibr" target="#b5">(Everingham et al. 2010)</ref> consists of images with object annotations of 20 classes. We use trainval 07+12 for training and perform evaluations on test 07. We use 3 novel / base class splits, i.e., ("bird", "bus", "cow", "motorbike", "sofa" / others), ("aeroplane", "bottle","cow","horse","sofa" / others) and ("boat", "cat", "motorbike","sheep", "sofa" / others). The number of shots is set to 1, 2, 3, 5 and 10. Mean average precision (mAP) at IoU threshold 0.5 is used as the evaluation metric. Results are averaged over 10 randomly sampled support datasets. MS COCO <ref type="bibr" target="#b16">(Lin et al. 2014</ref>) is a more challenging object detection dataset, which contains 80 classes including those 20 classes in Pascal VOC. We adopt the 20 shared classes as novel classes, and adopt the remaining 60 classes as base classes. The number of shots is set to 1, 3, 5, 10, and 30. We use train 2017 for training, and perform evaluations on val 2017. Standard evaluation metrics for MS COCO are adopted. Results are averaged over 5 randomly sampled support datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We adopt the commonly used ResNet-101 <ref type="bibr" target="#b8">(He et al. 2016)</ref> as the feature extractor. The network architectures and hyperparameters remain the same as Deformable DETR <ref type="bibr" target="#b37">(Zhu et al. 2021b)</ref>. We implement our model in single-scale version for fair comparison with other works. We also follow FsDetView <ref type="bibr" target="#b30">(Xiao and Marlet 2020)</ref> to implement the aggregation with a slightly more complex scheme compared with solely feature reweighting. Following Deformable DETR, we train our model with 8 x Nvidia V100 GPUs, using the AdamW <ref type="bibr" target="#b19">(Loshchilov and Hutter 2019)</ref> optimizer with an initial learning rate of 2?10 ?4 and a weight decay of 1?10 ?4 . Batch size is set to 32. In the base training stage, we train the model for 50 epochs for both Pascal VOC and MS COCO. Learning rate is decayed at the 45 th epoch by 0.1. In the few-shot fine-tuning stage, the same settings are applied to fine-tune the model until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with State-of-the-Art Methods</head><p>Pascal VOC <ref type="table" target="#tab_3">Table 1</ref> shows the few-shot detection performance for novel classes of Pascal VOC. It can be seen that Meta-DETR consistently outperforms existing methods across various setups. With multiple runs over randomly sampled support datasets to reduce randomness, our method achieves the best average performance across all setups, with a large margin of + 4.6% mAP compared with the secondbest. The strong performance demonstrates the superiority and robustness of our proposed method. MS COCO <ref type="table" target="#tab_6">Table 2</ref> shows the results on MS COCO. It can be seen that, although MS COCO is much more challenging than Pascal VOC with higher complexity like occlusions and large scale variations, Meta-DETR still outperforms all existing methods under all setups by even larger margins. This can be potentially attributed to the effective exploitation of the correlations among more classes in MS COCO. In addition, Meta-DETR performs exceptionally well compared with other region-based methods under the stricter metric AP 0.75 , which implies our method can effectively lift the constraint of inaccurate region proposals, thus producing more accurate detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>We conduct comprehensive ablation studies to verify the effectiveness of our design choices. All results are averaged over 10 runs with different randomly sampled support datasets on the first class split of Pascal VOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region-Level vs. Image-Level</head><p>From <ref type="table" target="#tab_3">Table 1</ref> and Table 2, we can find that fine-tuning Deformable DETR (Deformable-DETR-ft-full) generally outperforms finetuning Faster R-CNN (FRCN-ft-full), especially in the MS COCO dataset, where it is much harder to obtain accurate region proposals for novel classes due to higher complexity (see <ref type="figure" target="#fig_0">Fig. 2(a)</ref>). This observation aligns well with our insight that region-based frameworks tend to suffer from inaccurate regional proposals for novel classes. To further verify the superiority of image-level few-shot object detection, we adopt FsDetView <ref type="bibr" target="#b30">(Xiao and Marlet 2020)</ref>, a state-of-the-art meta-learning-based few-shot detector built on top of Faster  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Correlational Aggregation Module (CAM)</head><p>As shown in <ref type="table" target="#tab_9">Table 4</ref>, when incorporating CAM into our model, even if we keep the number of support classes as 1, which means CAM cannot explicitly leverage interclass correlation among different support classes, CAM can still boost few-shot detection performance under all set-    <ref type="table">Table 5</ref>: Ablation study on the effectiveness of our designed CAM on region-based detection frameworks. C denotes the number of support classes to aggregate simultaneously.</p><p>tings. This demonstrates CAM's strong capacity in aggregating query and support information. When multiple support classes are available, CAM can further enable the exploitation of their inter-class correlation to boost few-shot detection performance under lower-shot (?5) settings, especially under 1-shot (+ 4.8% mAP) and 2-shot (+ 5.0% mAP). No clear performance gain is observed for 10-shot, which implies that, when more training samples are available, the detector can already recognize novel classes and differentiate them from similar classes without explicitly modeling their inter-class correlation. We also apply our designed CAM to the commonly used region-based meta-detector FsDetView and report the results in <ref type="table">Table 5</ref>. Its steady performance gain demonstrates CAM's strong adaptability. <ref type="figure">Fig. 6</ref> further shows the visualization of objects of different classes in the feature space learned with and without the explicit exploitation of inter-class correlation. As shown, with CAM introduced to capture inter-class correlation, object classes are better separated from each other, which affirms our motivation of leveraging inter-class correlation to reduce misclassification among similar classes. Number of Classes for Correlational Aggregation Meta-DETR receives a fixed number of support classes and simultaneously aggregates them with query features to capture the inter-class correlation among different support classes. <ref type="figure">Fig. 5</ref> investigates the impact of the number of support classes to aggregate at a time. As the number of support classes increases from 1 to 10, the lower-shot (?5) detection performance first improves and then drops, while 10-shot performance first saturates and then drops. This also validates the effectiveness of leveraging inter-class correlation under lower-shot settings. We conjecture the performance drop with a large number of support classes for correlational aggregation is due to the model's limited capacity to differentiate so many support classes at one go. Based on the results, we set our method's number of support classes as 5 in all other experiments unless otherwise stated. Impact of Explicitly Modeling Background <ref type="table" target="#tab_9">Table 4</ref> also validates the effectiveness of explicitly modeling a prototype and a task encoding for background, which allows our method to better handle the 'no match' scenario where the query features do not match any of the support classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents a novel few-shot object detection framework, namely Meta-DETR. The proposed framework achieves (i) pure image-level meta-learning, which lifts the constraints caused by novel classes' inaccurate region proposals, and (ii) effective exploitation of inter-class correlation, which reduces misclassification and enhances generalization among similar or related classes. Despite its simplicity, our method achieves state-of-the-art performance over multiple few-shot object detection setups, outperforming prior works by large margins. We hope this work can offer good insights and inspire further researches in few-shot object detection.</p><p>This section provides more details of our proposed method and experimental results, which are omitted in the main paper due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Detailed Architecture of Meta-DETR</head><p>The transformer encoder and decoder in the proposed Meta-DETR have similar setups as Deformable DETR <ref type="bibr" target="#b37">(Zhu et al. 2021b)</ref>. Concretely, both transformer encoder and decoder have 6 layers and adopt the multi-scale deformable attention module, with the proposed correlational aggregation module (CAM) counted as one encoder layer. The channel dimension d is 256, and the intermediate dimension of fullyconnected layers (FC) inside the transformer is 1024. The dropout probability is set to 0.1. The number of attention heads is 8. The number of object queries N is 300. <ref type="figure">Fig. 7</ref> illustrates the prediction head that produces final predictions. The prediction head locates after the transformer encoder-decoder architecture, and is omitted for simplicity in <ref type="figure">Fig. 3</ref> of the main paper. It consists of a 1-layer MLP for confidence prediction and a 3-layer MLP for box prediction. The prediction head is shared for all the embeddings generated from the transformer decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Training Objective of Meta-DETR</head><p>Section 4.3 only provides a brief description of the training objective. Here, we provide a mathematically formulated description of the training objective in detail.</p><p>Target Generation Let us denote the fixed number of object queries as N , which means Meta-DETR infers N predictions within a single feed-forward process. Let us denote by x query the query image, and y = {y i } N i=1 the ground truth set of objects within the query image, which is a set of size N . When y i indicates an object, y i = (c i , b i ), where c i denotes the target class label and b i denotes the bounding box of the object. When y i indicates no object, y i = (?, ?).</p><p>Meta-DETR dynamically conditions its detection targets on support classes and their mappings to the task encodings. As discussed in Section 4, Meta-DETR predicts over C support classes (i.e., target classes) simultaneously. The C support classes are randomly sampled, denoted as c supp = {s i } C i=1 . Besides, these support classes are further mapped to a set of task encodings. We denote the mapping function from the labels of support classes to the labels of task encodings as ?(?). A specific case of ?(?) can be formulated as:</p><formula xml:id="formula_5">?(s i ) = i i ? {1, 2, ? ? ? , C}.</formula><p>(4) Therefore, the detection targets of Meta-DETR can be formulated as:</p><formula xml:id="formula_6">y = {y i } N i=1 = {(c i , b i )} N i=1 = {?(y i , c supp )} N i=1</formula><p>, (5) where ?(y i , c supp ) acts to filter out irrelevant object annotations and to map the labels of target classes to the labels of the corresponding task encodings, which can be formulated as:  <ref type="figure">Figure 7</ref>: Illustration of the prediction head after the transformer encoder-decoder architecture to produce final predictions. It is shared for all the embeddings generated from the transformer decoder.</p><formula xml:id="formula_7">?(y i , c supp ) = ? ? ? (?, ?), if y i = (?, ?) (?, ?), if c i / ? c supp . (?(c i ), b i ), if c i ? c supp<label>(6)</label></formula><p>Note that y can completely consist of (?, ?) when there is no objects that belong to the provided support classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function Assume the N predictions for target class made by Meta</head><formula xml:id="formula_8">-DETR are? = {? i } N i=1 = (? i ,b i ) N i=1 .</formula><p>We adopt a pair-wise matching loss L match (y i ,? ?(i) ) to search for a bipartite matching between? and y with the lowest cost:? = arg min</p><formula xml:id="formula_9">? N i=1 L match (y i ,? ?(i) ),<label>(7)</label></formula><p>where ? denotes a permutation of N elements, and? denotes the optimal assignment between predictions and targets. Since the matching should consider both classification and localization, the matching loss is defined as:</p><formula xml:id="formula_10">L match (y i ,? ?(i) ) =1 {c i =?} L cls (c i ,? ?(i) ) + 1 {c i =?} L box (b i ,b ?(i) ) .<label>(8)</label></formula><p>With the optimal assignment? obtained with Eq. 7 and Eq. 8, we optimize the network using the following loss function:</p><formula xml:id="formula_11">L(y ,?) = N i=1 L cls (c i ,?? (i) ) + 1 {c i =?} L box (b i ,b? (i) ) ,<label>(9)</label></formula><p>where we adopt sigmoid focal loss <ref type="bibr" target="#b15">(Lin et al. 2017)</ref> for L cls and adopt a linear combination of 1 loss and GIoU loss <ref type="bibr" target="#b23">(Rezatofighi et al. 2019)</ref> for L box . Similar to Deformable DETR <ref type="bibr" target="#b37">(Zhu et al. 2021b</ref>), L(y ,?) is applied to every layer of the transformer decoder.</p><p>Following Meta R-CNN <ref type="bibr" target="#b31">(Yan et al. 2019)</ref>, we additionally introduce a cosine similarity cross-entropy loss  to classify the class prototypes obtained by our designed CAM. It encourages prototypes of different classes to be distinguished from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Additional Comparison with State of the Art</head><p>We also present results taking base classes into consideration in <ref type="table" target="#tab_11">Table 6</ref>. While achieving good performance for novel classes with limited training samples, Meta-DETR can still detect objects of base classes with competitive performance. TFA     <ref type="table">Table 7</ref>: Ablation study on the location of our designed CAM. Results are averaged over 10 repeated runs on the first class split of Pascal VOC.</p><p>for base classes since it works more like conventional detectors with fine-tuning, thus having relatively constrained capacity in generalizing on novel classes. We also wish to highlight that our proposed Meta-DETR achieves the best base-class and novel-class performance of all the compared meta-learning-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Additional Ablation Study</head><p>Early Aggregation vs. Late Aggregation We conduct experiments to study the location of our designed correlational aggregation module (CAM) to place. As shown in <ref type="table">Table 7</ref>, it is preferable to place CAM at the beginning of the transformer encoder, which implies the importance of learning a deep class-agnostic predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Qualitative Results</head><p>We provide multiple qualitative visualizations of Meta-DETR's few-shot detection results in Figs. 8-11, which give a straightforward illustration of the performance of our method. Note that only detection results of novel classes are presented, as the major focus is to detect objects of novel classes. In addition, we only show results with confidence scores higher than 0.25. White boxes indicate correct detections, red solid boxes indicate false positives, and red dashed boxes indicate false negatives. It can be observed that the proposed Meta-DETR is able to detect novel objects at a satisfactory performance even with scarce training samples.</p><p>In addition, we also provide a demo video attached in the supplementary materials, which consists of several short clips with Meta-DETR's predictions on novel classes as a reference.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Existing few-shot detection frameworks tend to suffer from inaccurate region proposals and under-exploitation of inter-class correlation. Due to very limited training samples, the proposal quality (measured by Average Recall on top 1000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Overview Fig. 3 presents the architecture of the proposed Meta-DETR. Motivated by previous discussions, Meta-DETR employs the recently proposed Deformable DETR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Meta-DETR w/o CAM Meta-DETR w/ CAM Figure 6: t-SNE visualization of objects learned in the feature space with and without our designed CAM. Results are obtained on split 1 of Pascal VOC under the 2-shot setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of Meta-DETR's 10-shot object detection results on Pascal VOC class split 1. Novel classes include bird, bus, cow, motorbike, and sofa. For simplicity, only results of novel classes are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of Meta-DETR's 10-shot object detection results on Pascal VOC class split 2. Novel classes include aeroplane, bottle, cow, horse, and sofa. For simplicity, only results of novel classes are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Visualization of Meta-DETR's 10-shot object detection results on Pascal VOC class split 3. Novel classes include boat, cat, motorbike, sheep, and sofa. For simplicity, only results of novel classes are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Visualization of Meta-DETR's 10-shot object detection results on MS COCO. Novel classes include person, bicycle, car, motorcycle, airplane, bus, train, boat, bird, cat, dog, horse, sheep, cow, bottle, chair, couch, potted plant, dining table, and tv. For simplicity, only results of novel classes are illustrated. White boxes indicate correct detections. Red solid boxes indicate false positives. Red dashed boxes indicate false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Results over a single run: LSTD<ref type="bibr" target="#b2">(Chen et al. 2018)</ref> 8.2 1.0 12.4 29.1 38.5 11.4 3.8 5.0 15.7 31.0 12.6 8.5 15.0 27.3 36.3 17.1 RepMet (Schwartz et al. 2019) ? 26.1 32.9 34.4 38.6 41.3 17.2 22.1 23.4 28.3 35.8 27.5 31.1 31.5 34.4 37.2 30.8 Meta-YOLO (Kang et al. 2019) 14.8 15.5 26.7 33.9 47.2 15.7 15.3 22.7 30.1 40.5 21.3 25.6 28.4 42.8 45.9 28.4 Meta Det (Wang, Ramanan, and Hebert 2019) 18.9 20.6 30.2 36.8 49.6 21.8 23.1 27.8 31.7 43.0 20.6 23.9 29.4 43.9 44.1 31.0 Meta R-CNN (Yan et al. 2019) 19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5 41.2 48.1 31.1 TFA w/ cos (Wang et al. 2020) ? 39.8 36.1 44.7 55.7 56.0 23.5 26.9 34.1 35.1 39.1 30.8 34.8 42.8 49.5 49.8 39.9 MPSR (Wu et al. 2020) ? 41.7 43.1 51.4 55.2 61.8 24.4 29.5 39.2 39.9 47.8 35.6 40.6 42.3 48.0 49.7 43.3 TFA w/ cos + Halluc (Zhang and Wang 2021) ? 45.1 44.0 44.7 55.0 55.9 23.2 27.5 35.1 34.9 39.0 30.5 35.1 41.4 49.0 49.3 40.6 CME (Li et al. 2021) ?</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Class Split 1</cell><cell></cell><cell></cell><cell cols="3">Class Split 2</cell><cell></cell><cell></cell><cell cols="3">Class Split 3</cell><cell>Avg.</cell></row><row><cell>Method \ Shot</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell></cell><cell cols="15">41.5 47.5 50.4 58.2 60.9 27.2 30.2 41.4 42.5 46.8 34.3 39.6 45.1 48.3 51.5 44.4</cell></row><row><cell>SRR-FSD (Zhu et al. 2021a)  ?</cell><cell cols="15">47.8 50.5 51.3 55.2 56.8 32.5 35.3 39.1 40.8 43.8 40.1 41.5 44.3 46.9 46.4 44.8</cell></row><row><cell>FSCE (Sun et al. 2021)  ?</cell><cell cols="15">44.2 43.8 51.4 61.9 63.4 27.3 29.5 43.5 44.2 50.2 37.2 41.9 47.5 54.6 58.5 46.6</cell></row><row><cell>Meta-DETR (Ours)</cell><cell cols="15">40.6 51.4 58.0 59.2 63.6 37.0 36.6 43.7 49.1 54.6 41.6 45.9 52.7 58.9 60.6 50.2</cell></row><row><cell>Results averaged over multiple random runs:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FRCN-ft-full (Ren et al. 2015)  ?</cell><cell cols="15">9.9 15.6 21.6 28.0 35.6 9.4 13.8 17.4 21.9 29.8 8.1 13.9 19.0 23.9 31.0 19.9</cell></row><row><cell>Deformable-DETR-ft-full (Zhu et al. 2021b)  ?</cell><cell cols="15">5.6 13.3 21.7 34.2 45.0 10.9 13.0 18.4 27.3 39.4 7.3 16.6 20.8 32.2 41.8 23.2</cell></row><row><cell>TFA w/ cos (Wang et al. 2020)  ?</cell><cell cols="15">25.3 36.4 42.1 47.9 52.8 18.3 27.5 30.9 34.1 39.5 17.9 27.2 34.3 40.8 45.6 34.7</cell></row><row><cell>FsDetView (Xiao and Marlet 2020)</cell><cell cols="15">24.2 35.3 42.2 49.1 57.4 21.6 24.6 31.9 37.0 45.7 21.2 30.0 37.2 43.8 49.6 36.7</cell></row><row><cell>MPSR (Wu et al. 2020)  ?</cell><cell cols="15">34.7 42.6 46.1 49.4 56.7 22.6 30.5 31.0 36.7 43.3 27.5 32.5 38.2 44.6 50.0 39.1</cell></row><row><cell>DCNet (Hu et al. 2021)  ?</cell><cell cols="15">33.9 37.4 43.7 51.1 59.6 23.2 24.8 30.6 36.7 46.6 32.3 34.9 39.7 42.6 50.7 39.2</cell></row><row><cell>FSCE (Sun et al. 2021)  ?</cell><cell cols="15">32.9 44.0 46.8 52.9 59.7 23.7 30.6 38.4 43.0 48.5 22.6 33.4 39.5 47.3 54.0 41.2</cell></row><row><cell>Meta-DETR (Ours)</cell><cell cols="15">35.1 49.0 53.2 57.4 62.0 27.9 32.3 38.4 43.2 51.8 34.9 41.8 47.1 54.1 58.2 45.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Few-shot detection performance (mAP@0.5) on Pascal VOC test 07 for novel classes. ? indicates methods using multi-scale features. indicates re-evaluated results using official codes. indicates usage of external data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Two-Stage Training ProcedureThe training procedure consists of two stages. The first stage is base training stage. During this stage, the model is trained on the base dataset D base with abundant training samples for each base class. The second stage is few-shot fine-tuning stage. In this stage, we train the model on both base and novel classes with limited training samples. Only K object instances are available</figDesc><table /><note>for each novel category in K-shot object detection. Follow- ing prior works (Yan et al. 2019; Wang et al. 2020; Xiao and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Few-shot detection performance on MS COCO val 2017 for novel classes. ? indicates methods using multiscale features. ? indicates results averaged on multiple runs.</figDesc><table><row><cell>R-CNN, as a solid baseline to compare with our method.</cell></row><row><cell>For a fair comparison, we add a deformable transformer to</cell></row><row><cell>FsDetView to rule out the performance difference brought</cell></row><row><cell>by the transformer architecture. Furthermore, we replace our</cell></row><row><cell>proposed CAM in Meta-DETR with the feature aggregation</cell></row><row><cell>module in FsDetView (denoted as Meta-DETR w/o CAM).</cell></row><row><cell>As shown in Table 3, even with aligned network architecture</cell></row><row><cell>and aggregation scheme, Meta-DETR w/o CAM still outper-</cell></row><row><cell>forms FsDetView + Deform Transformer under most setups.</cell></row><row><cell>The results validate the superiority of solving few-shot ob-</cell></row><row><cell>ject detection at image level.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Ablation study over the number of support classes for correlational aggregation under different few-shot setups.</figDesc><table><row><cell>1-shot</cell><cell></cell><cell>2-shot</cell><cell></cell><cell></cell><cell>3-shot</cell><cell>5-shot</cell><cell>10-shot</cell></row><row><cell>mAP(%)</cell><cell>mAP(%)</cell><cell></cell><cell></cell><cell>mAP(%)</cell><cell>mAP(%)</cell><cell>mAP(%)</cell></row><row><cell>Novel</cell><cell>Novel</cell><cell></cell><cell></cell><cell>Novel</cell><cell>Novel</cell><cell>Novel</cell></row><row><cell>Number of Support Classes</cell><cell cols="3">Number of Support Classes</cell><cell></cell><cell>Number of Support Classes</cell><cell>Number of Support Classes</cell><cell>Number of Support Classes</cell></row><row><cell cols="5">Figure 5: Novel mAP@0.5</cell><cell></cell></row><row><cell>Method \ Shot</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell cols="6">FsDetView (Xiao and Marlet 2020) 24.2 35.3 42.2 49.1 57.4</cell></row><row><cell cols="6">FsDetView + Deform Transformer 28.0 36.3 41.8 48.9 57.4</cell></row><row><cell>Meta-DETR w/o CAM</cell><cell cols="5">27.2 42.1 50.5 52.9 59.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="6">: Performance comparison between region-level and</cell></row><row><cell cols="6">image-level meta-learning-based few-shot object detection.</cell></row><row><cell>Method</cell><cell>CAM</cell><cell>Modeling C Background</cell><cell>1</cell><cell>Novel mAP@0.5 2 3 5</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell cols="4">1 27.2 42.1 50.5 52.9 59.3</cell></row><row><cell>Meta-DETR</cell><cell></cell><cell cols="4">1 30.3 44.0 52.1 55.7 62.0 5 32.6 45.6 51.3 56.1 60.9</cell></row><row><cell></cell><cell></cell><cell cols="4">5 35.1 49.0 53.2 57.4 62.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Ablation study to evaluate the effectiveness of our designed CAM and its design choices. C denotes the number of support classes to aggregate simultaneously.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Novel mAP@0.5</cell><cell></cell></row><row><cell>Method \ Shot</cell><cell>C 1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell cols="6">FsDetView (Xiao and Marlet 2020) 1 24.2 35.3 42.2 49.1 57.4</cell></row><row><cell>FsDetView w/ CAM</cell><cell cols="5">5 30.1 41.1 45.2 51.4 57.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Kang et al. 2019)  66.4 64.8 63.4 63.6 14.8 26.7 33.9  47.2 FsDetView (Xiao and Marlet 2020) ? 64.2 69.4 69.8 71.1 24.2 42.2 49.1 57.4 TFA w/ cos (Wang et al. 2020) ? 77.6 77.3 77.4 77.5 25.3 42.1 47.9 52.9 MPSR (Wu et al. 2020) ? 60.6 65.9 68.2 69.8 34.7 46.1 49.4 56.7 FSCE (Sun et al. 2021) ? 75.5 73.7 75.0 75.2 32.9 46.8 52.9 59.7 Meta-DETR (Ours) ? 67.2 70.0 73.0 73.5 35.1 53.2 57.4 62.0</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Base Classes</cell><cell></cell><cell></cell><cell cols="2">Novel Classes</cell><cell></cell></row><row><cell>Method \ Shot</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell>Meta-YOLO (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>produces outstanding performance</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Few-shot detection performance (mAP@0.5) for both base and novel classes on the first split of Pascal VOC. ? indicates results averaged on multiple runs.</figDesc><table><row><cell>CAM's Location</cell><cell></cell><cell cols="3">Novel mAP@0.5</cell><cell></cell></row><row><cell>@ Encoder Layer</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell>1</cell><cell cols="5">35.1 49.0 53.2 57.4 62.0</cell></row><row><cell>3</cell><cell cols="5">27.1 42.9 50.6 54.0 59.2</cell></row><row><cell>6</cell><cell cols="5">15.2 31.5 37.7 50.3 53.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LSTD: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Closer Look at Few-shot Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">UP-DETR: Unsupervised Pre-training for Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN. In ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Few-shot Object Detection via Feature Reweighting</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The importance of shape in early lexical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Development</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="321" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond Max-Margin: Class Margin Equilibrium for Fewshot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Learning for Generic Object Detection: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incremental Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">YOLO 9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">RepMet: Representative-based metric learning for classification and one-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FSCE: Few-shot object detection via contrastive proposal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frustratingly Simple Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Metalearning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-Scale Positive Sample Refinement for Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Few-Shot Object Detection and Viewpoint Estimation for Objects in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta R-CNN: Towards General Solver for Instancelevel Low-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CAD-Net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10015" to="10024" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hallucination Improves Few-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10142</idno>
		<title level="m">Cooperating RPN&apos;s Improve Few-Shot Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic relation reasoning for shot-stable few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deformable DETR: Deformable Transformers for End-to-End Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
