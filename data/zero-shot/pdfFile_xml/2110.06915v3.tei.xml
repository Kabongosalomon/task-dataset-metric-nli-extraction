<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object-Region Video Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<addrLine>2 UC Berkeley</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Ben-Avraham</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<addrLine>2 UC Berkeley</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<addrLine>2 UC Berkeley</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country>NVIDIA Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<addrLine>2 UC Berkeley</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object-Region Video Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>+ 1 Our focus is different from papers on two-stream models in vision, that are not object-centric (see Sec. 2).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Our ORViT model incorporates object information into video transformer layers. The figure shows the standard (uniformly spaced) transformer patch-tokens in blue, and object-regions corresponding to detections in orange. In ORViT any temporal patch-token (e.g., the patch in black at time T ) attends to all patch tokens (blue) and region tokens (orange). This allows the new patch representation to be informed by the objects. Our method shows strong performance improvement on multiple video understanding tasks and datasets, demonstrating the value of a model that incorporates object representations into a transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recently, video transformers have shown great success in video understanding, exceeding CNN performance; yet existing video transformer models do not explicitly model objects, although objects can be essential for recognizing actions. In this work, we present Object-Region Video Transformers (ORViT), an object-centric approach that extends video transformer layers with a block that directly incorporates object representations. The key idea is to fuse object-centric representations starting from early layers and propagate them into the transformer-layers, thus affecting the spatio-temporal representations throughout the network. Our ORViT block consists of two object-level streams: appearance and dynamics. In the appearance stream, an "Object-Region Attention" module applies selfattention over the patches and object regions. In this way, visual object regions interact with uniform patch tokens and enrich them with contextualized object information. We further model object dynamics via a separate "Object-Dynamics Module", which captures trajectory interactions, and show how to integrate the two streams. We evaluate our model on four tasks and five datasets: compositional and few-shot action recognition on SomethingElse, spatio-temporal action detection on AVA, and standard action recognition on Something-Something V2, Diving48 and Epic-Kitchen100. We show strong performance improvement across all tasks and datasets considered, demonstrating the value of a model that incorporates object representations into a transformer architecture. For code and pretrained models, visit the project page at https:// roeiherz.github.io/ORViT/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider the simple action of "Picking up a coffee cup" in <ref type="figure">Figure 1</ref>. Intuitively, a human recognizing this action would identify the hand, the coffee cup and the coaster, and perceive the upward movement of the cup. This highlights three important cues needed for recognizing actions: what/where are the objects? how do they interact? and how do they move? The above perception process allows easy generalization to different compositions of actions. For example, the process of "picking up a knife" shares some of the components with "picking up a coffee cup", namely, the way the object and hand move together. More broadly, representing image semantics using objects facilitates compositional understanding, because many perceptual components remain similar when one object is swapped for another. Thus, a model that captures this compositional aspect potentially requires less examples to train.</p><p>It seems intuitively clear that machine vision models should also benefit from such object-focused representations, and indeed this has been explored in the past <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b68">69]</ref> and more recently by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b82">83]</ref>, who utilize bounding boxes for various video understanding tasks. However, the central question of what is the best way to process objects information still remains. Most object-centric methods to video understanding take a post-processing approach. Namely, they compute object descriptors using a backbone and then reestimate those based on other objects via message passing or graph networks without propagating the object information into the backbone. Unlike these approaches, we argue that objects should influence the spatio-temporal representations of the scene throughout the network, starting from the early layers (i.e., closer to the input). We claim that self-attention in video transformers is a natural architecture to achieve this result by enabling the attention to incorporate objects as well as salient image regions. Video transformers have recently been introduced as powerful video understanding models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b64">65]</ref>, motivated by the success of transformers in language <ref type="bibr" target="#b16">[17]</ref> and vision <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>. In these models, each video frame is divided into patches, and a self-attention architecture obtains a contextualized representation for the patches. However, this approach has no explicit representation of objects. Our key observation is that self-attention can be applied jointly to object representations and spatio-temporal representations, thus offering an elegant and straightforward mechanism to enhance the spatio-temporal representations by the objects.</p><p>Motivated by the above, our key goal in this paper is to explicitly fuse object-centric representations into the spatio-temporal representations of video-transformer architectures <ref type="bibr" target="#b1">[2]</ref>, and do so throughout the model layers, starting from the earlier layers. We propose a general approach for achieving this by adapting the self-attention block <ref type="bibr" target="#b18">[19]</ref> to incorporate object information. The challenge in building such an architecture is that it should have components for modeling the appearance of objects as they move, the interaction between objects, and the dynamics of the objects (irrespective of their visual appearance). An additional desideratum is that video content outside the objects should not be discarded, as it contains important contextual information. In what follows, we show that a self-attention architecture can be extended to address these aspects. Our key idea is that object regions can be introduced into transformers in a similar way to that of the regular patches, and dynamics can also be integrated into this framework in a natural way. We refer to our model as an "Object-Region Video Transformer" (or ORViT).</p><p>We introduce a new ORViT block, which takes as input bounding boxes and patch tokens (also referred to as spatiotemporal representations) and outputs refined patch tokens based on object information. Within the block, the information is processed by two separate object-level streams:</p><p>an "Object-Region Attention" stream that models appearance and an "Object-Dynamics Module" stream that models trajectories. <ref type="bibr" target="#b0">1</ref> The appearance stream first extracts descriptors for each object based on the object coordinates and the patch tokens. Next, we append the object descriptors to the patch tokens, and self-attention is applied to all these tokens jointly, thus incorporating object information into the patch tokens (see <ref type="figure">Figure 1</ref>). The trajectory stream only uses object coordinates to model the geometry of motion and performs self-attention over those. Finally, we re-integrate both streams into a set of refined patch tokens, which have the same dimensionality as the input to our ORViT block. This means that the ORViT block can be called repeatedly. See <ref type="figure">Figure 2</ref> and <ref type="figure">Figure 4</ref> for visualizations.</p><p>We evaluate ORViT on several challenging videounderstanding tasks: compositional and few-shot action recognition on SomethingElse <ref type="bibr" target="#b62">[63]</ref>, where bounding boxes are given as part of the input; spatio-temporal action detection on AVA <ref type="bibr" target="#b27">[28]</ref>, where the boxes are obtained via an off-the-shelf detector that was provided by previous methods; and in a standard action recognition task on Something-Something V2 <ref type="bibr" target="#b25">[26]</ref>, Diving48 <ref type="bibr" target="#b53">[54]</ref> and Epic-Kitchen100 <ref type="bibr" target="#b15">[16]</ref>, where we use class-agnostic boxes from an off-the-shelf detector. Through extensive empirical study, we show that integrating the ORViT block into the video transformer architecture leads to improved results on all tasks. These results confirm our hypothesis that incorporating object representations starting from early layers and propagating them into the spatio-temporal representations throughout the network, leads to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object-centric models. Recently object-centric models have been successfully applied in many computer vision applications: visual relational reasoning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b94">95]</ref>, representation learning <ref type="bibr" target="#b90">[91]</ref>, video relation detection <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b75">76]</ref>, vision and language <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b76">77]</ref>, humanobject interactions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b86">87]</ref>, and even image generation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42]</ref>. The advances and the success of objectcentric models in these domains inspired various videobased tasks, such as action localization <ref type="bibr" target="#b63">[64]</ref>, video synthesis <ref type="bibr" target="#b3">[4]</ref>, and action recognition <ref type="bibr" target="#b98">[99]</ref>. The latter was the focus of varied recent works that designed different object interactions approaches for convolutional models. A line of works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b74">75]</ref> focused on capturing spatial object interactions while ignoring the temporal interactions. STRG <ref type="bibr" target="#b82">[83]</ref> and ORN <ref type="bibr" target="#b4">[5]</ref> used spatio-temporal interactions with two consecutive frame interactions, while STAG <ref type="bibr" target="#b33">[34]</ref> considered long-range temporal interaction. Last, Unified <ref type="bibr" target="#b2">[3]</ref> tried to generalize all these models and propose long spatio-temporal object interactions. While all these works focused solely on interactions of visual appearance information, recently STIN <ref type="bibr" target="#b62">[63]</ref> introduced an object-centric model based on object trajectories by modeling bounding box movement. Our ORViT approach directly combines object appearance, object trajectories, and the overall video, by mapping all computations to spatio-temporal patch tokens. This is particularly natural to do in a transformer framework, as we show here, and results in state-of-the-art performance. Transformers in action recognition. Ranging from the early works that employ optical flow based features <ref type="bibr" target="#b19">[20]</ref>, to recent transformer based approaches <ref type="bibr" target="#b29">[30]</ref>, a wide variety of approaches have been proposed for action recognition. In broad brushstrokes, the proposed approaches have evolved from using temporal pooling for extracting features <ref type="bibr" target="#b43">[44]</ref> to using recurrent networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b93">94]</ref>, through to 3D spatiotemporal kernels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b79">[80]</ref><ref type="bibr" target="#b80">[81]</ref><ref type="bibr" target="#b81">[82]</ref>, and two-stream networks that capture complementary signals (e.g., motion and spatial cues <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b72">73]</ref>). Unlike these approaches, our work uses two separate object-level streams to leverage object-centric information. In parallel to developments in video understanding, Vision Transformers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b78">79]</ref> propose a new approach to image recognition by discarding the convolutional inductive bias entirely and instead employing self-attention operations. Specialized video models such as TimeSformer <ref type="bibr" target="#b6">[7]</ref>, ViViT <ref type="bibr" target="#b1">[2]</ref>, Mformer (MF) <ref type="bibr" target="#b64">[65]</ref> and MViT <ref type="bibr" target="#b29">[30]</ref> form the latest epoch in action recognition models. By generalizing the vision transformers to the temporal domain through the use of spatio-temporal attention, the obtained video transformers are very competitive with their convolutional counterparts both in terms of performance as well scaling behaviour to large data. However, none of the video transformer models leverage object cues, a persistent shortcoming that we aim to address in ORViT.</p><p>We also note that <ref type="bibr" target="#b84">[85]</ref> adopts a similar object-centric approach to video understanding. However, our work differs conceptually since <ref type="bibr" target="#b84">[85]</ref> models only the object parts, which is similar to the STRG baselines we consider in the paper. On the other hand, our work introduces objects into transformer layers while keeping the entire spatio-temporal representation. Also, <ref type="bibr" target="#b84">[85]</ref> pretrains the transformer in a self-supervised fashion on a large dataset (MovieClips), and therefore its empirical results cannot be directly compared to models that are not pretrained in this manner. Spatio-temporal action detection. The task of action detection requires temporally localizing the action start and end times. A wide variety of methods have been proposed for it, such as actions modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b95">96]</ref>, temporal convolutions <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b71">72]</ref>, boundaries modeling <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>, attention <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b91">92]</ref>, structure utilization <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b97">98]</ref>, detection based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b87">88]</ref>, end-to-end approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref>, recurrent neural networks <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b89">90]</ref>, and even using lan- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The ORViT model</head><p>We next present the Object-Region Video Transformer (ORViT) model, which explicitly models object appearance and trajectories within the transformer architecture. We begin by reviewing the video transformer architecture, which our model extends upon, in Section 3.1, and present ORViT in Section 3.2. A high-level overview of ORViT is shown in <ref type="figure">Figure 2</ref> and detailed in <ref type="figure">Figure 4</ref>. Briefly, ORViT repeatedly refines the patch token representations by using information about both the appearance and movement of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Video Transformer Architecture</head><p>Video transformers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30]</ref> extend the Vision Transformer model to the temporal domain. Similar to vision transformers, the input is first "patchified" but with temporally extended 3-D patches instead of 2-D image patches producing a down-sampled tensor X of size T ?H ?W ?d. Then, spatio-temporal position embeddings are added for providing location information. Finally, a classification token (CLS) is appended to X, resulting in T HW + 1 tokens in R d , to which self-attention are applied repeatedly to produce the final contextualized CLS feature vector. <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The ORViT Block</head><p>There are two inputs to the ORViT block. The first is the output of the preceding transformer block, represented as a set of spatio-temporal tokens X ? R T HW ?d . The second <ref type="figure">Figure 3</ref>. We visualize the attention allocated to the object tokens in the ORViT block (red, green, and blue) in each frame of a video describing "moving two objects away from each other". It can be seen that each of the two "remote control" objects affects the patch-tokens in its region, whereas the hand has a broader map. For more visualizations, please see Section E in supplementary.</p><p>input is a set of bounding boxes for objects across time 3 , denoted by B ? R T O?4 . The output of the ORViT block is a set of refined tokens Y ? R T HW ?d contextualized with object-centric information. Thus, the ORViT block can be viewed as a token representation refining mechanism using the object-level information.</p><p>As mentioned, we argue that the key cues for recognizing actions in videos are: the objects in the scene, their interactions, and their movement. To capture these cues, we design the ORViT block with the following two object-level streams. The first stream models the appearance of objects, and their interactions. We refer to it as "Object-Region Attention" and denote it by R. The second "Object-Dynamics Module" stream (denoted by D) models the interactions between trajectories, independently of their appearance. Importantly, the output of each of the streams is T HW token vectors, which can also be interpreted as refined patch representations based on each source of information.</p><p>The D stream only models object dynamics, and thus only uses bounding boxes B as input. We therefore denote its output by D(B). The stream R models appearance and thus depends on both the token representation X, and the bounding boxes B, and produces R(X, B). The final output of the ORViT block Y is simply formed by the sum of the two streams and an input residual connection:</p><formula xml:id="formula_0">Y ? := R(X, B) + D(B) + X Y := Y ? + MLP(LN(X))<label>(1)</label></formula><p>where LN denotes a LayerNorm operation. Next, we elaborate on the two components separately.</p><p>Object-Region Attention. The goal of this module is to extract information about each object and use it to refine the patch tokens. This is done by using the object regions to extract descriptor vectors per region from the input tokens, resulting in T O vectors in R d , which we refer to as object tokens. These vectors are then concatenated with the T HW patch tokens and serve as the keys and values, while the queries are only the patch tokens. Finally, the output of the block is T HW patch tokens. Thus, the key idea is to fuse object-centric information into spatio-temporal representations. Namely, inject the T O object region tokens into T HW patch tokens. An overview of our approach is depicted in <ref type="figure">Figure 4</ref>. We provide further details below. Given the patch token features X and the boxes B, our first goal is to obtain vector descriptors in R d per object and frame. The natural way to do this is via an RoIAlign <ref type="bibr" target="#b30">[31]</ref> layer, which uses the patch tokens X and box coordinates B to obtain object region crops. This is followed by an MLP and max-pooling to obtain the final object representation in R d :</p><formula xml:id="formula_1">O := MaxPool(MLP(RoIAlign(X, B)))<label>(2)</label></formula><p>Since this is done per object and per frame, the result is OT vectors in R d (i.e., O ? R T O?d ). Importantly, this extraction procedure is performed in each instance of an ORViT block, so that it will produce different object tokens at each layer. We also add positional embeddings but leave the details to Section B.1 in supplementary.</p><p>At this point, we would like to allow the object tokens to refine the patch tokens. We concatenate the object tokens O with the patch tokens X, resulting in C ? R T (HW +O)?d . Next C and X are used to obtain queries, keys and values as follows:</p><formula xml:id="formula_2">Q := XW q K := CW k V := CW v Where W q , W k , W v ? R d?d<label>(3)</label></formula><p>Finally, there are several ways to perform spatio-temporal self-attention (e.g., joint and divided attention over space and time, or the recently introduced trajectory attention <ref type="bibr" target="#b64">[65]</ref>). We use trajectory attention because it performs well empirically. We compare different self-attention versions in <ref type="table" target="#tab_7">Table 5c</ref> in supplementary. <ref type="figure">Figure 3</ref> also visualizes the "Object-Region Attention" learned by our model. Object-Dynamics Module. To model object dynamics, we introduce a component that only considers the boxes B. We first encode each box via its center coordinate, height and width, and apply an MLP to this vector to obtain a vector in R d . Applying this to all boxes results in L ? R T O?d . Next we add a learnable object-time position embedding P ? R T O?d , resulting in B := L + P . We refer to this as the "Coordinate Embedding" step in <ref type="figure">Figure 4</ref>. Its output can be viewed as T O tokens in R d , and we apply self-attention to those as follows:</p><formula xml:id="formula_3">Attention D ( Q, K, V ) := Softmax Q K T ? d k V , where: Q := BW q , K := BW k , V := BW v and W q , W k , W v ? R d?d . The self-attention ! ? " ? ? ? ?4 ? ? ROI ALIGN Attention Q K/V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Coordinates Embedding</head><p>Box Position Encoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patches Objects Patches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Box Position Encoder</head><formula xml:id="formula_4">: Input Patch Tokens ? ?( + )? ? ? Sampler ? # ? ! # "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object-Region Attention Object-Dynamics Module</head><formula xml:id="formula_5">? ? : Object Boxes ? ?4 1 2 1 2</formula><p>Refined Patch Tokens ? <ref type="figure">Figure 4</ref>. ORViT Block architecture. The block consists of two object-level streams: an "Object-Region Attention" that models appearance, and an "Object-Dynamics Module" that models trajectories. The two are combined to produce new patch tokens. The "Box Position Encoder" maps the output of the trajectory stream to the dimensional of patch tokens.</p><p>output is in R T O?d . Next, we would like to transform the objects with a T ? d vector into a spatial volume of T HW ? d. This is done using the Box Position Encoder described below. Box Position Encoder. The returned features of the ORViT model should have the same dimensions as the input, namely T HW ? d. Thus, our main challenge is to project the object embeddings into spatial dimensions, namely T O ? d into T HW ? d. The naive approach would be to ignore the boxes by expanding every object with vector T ? d into T HW ? d. However, since the object trajectories contain their space-time location, a potentially better way to do it would consider the object locations. Hence, for each object with corresponding T ? d tokens, we generate a spatial feature HW ? d by placing the object representation vector according to the matching bounding box coordinates using a bilinear interpolation sampler operation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42]</ref>. <ref type="bibr" target="#b3">4</ref> Finally, the output in HW ? d is the sum of all objects from all frames representing the coarse trajectory of the object in spatial dimensions. The process is shown in <ref type="figure">Figure 4</ref> (right). We show empirically that this approach is better than the naive approach described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The ORViT model</head><p>We conclude by explaining how to integrate ORViT into transformer-based video models. The advantage of ORViT is that it takes as input the standard spatio-temporal tokens in R T HW ?d and outputs a refined version of those with the same dimensions. Thus, it acts as a standard transformer layer in terms of input and output, and one can take any transformer and simply add ORViT layers to it. This is important since it highlights that ORViT can eas- <ref type="bibr" target="#b3">4</ref> Features outside of an object region are set to zeros.</p><p>ily leverage any video transformer pretrained model, obviating the need for pretraining ORViT. We experiment with three video transformer models: TimeSformer <ref type="bibr" target="#b6">[7]</ref>, Mformer (MF) <ref type="bibr" target="#b64">[65]</ref>, and MViT <ref type="bibr" target="#b29">[30]</ref>. We show that for these, using ORViT layers improves performance. The only design choice is which layers to apply ORViT to, while the training methodology remains. We found that it is very important to apply it in early layers while repeated applications keep propagating the information throughout the network. Since the RoIAlign extracts object representations from spatio-temporal representations in each ORViT layer, multiple ORViT layers allow the model to consider different object representations throughout the network. In our experiments we apply it in layers 2, 7, 11, replacing the original layers without adding depth to the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our ORVIT block on several video understanding benchmarks. Specifically, we consider the following tasks: Compositional Action Recognition (Section 4.1), Spatio-Temporal Action Detection (Section 4.2) and Action Recognition (Section 4.3). Datasets. We experiment with the following datasets: (1) Something-Something v2 (SSv2) <ref type="bibr" target="#b25">[26]</ref> is a dataset containing 174 action categories of common human-object interactions.</p><p>(2) SomethingElse <ref type="bibr" target="#b62">[63]</ref> exploits the compositional structure of SSv2, where a combination of a verb and a noun defines an action. We follow the official compositional split from <ref type="bibr" target="#b62">[63]</ref>, which assumes the set of noun-verb pairs available for training is disjoint from the set given at test time.</p><p>(3) Atomic Visual Actions (AVA) <ref type="bibr" target="#b27">[28]</ref> is a benchmark for human action detection. We report Mean Average Precision (mAP) on AVA-V2.2. (4) Epic Kitchens 100 (EK100) <ref type="bibr" target="#b15">[16]</ref>   <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref> for multi-object tracking to find correspondence between the objects in different frames (no training data is required), see Section A.1 in supp. We set the number of objects to 4 in SSv2 and EK100, 6 in AVA, and 10 in Diving48. These numbers were chosen by taking the max number of objects per video (as induced by the tracker) across all videos in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Compositional &amp; Few-Shot Action Recognition</head><p>Several video datasets define an action via a combination of a verb (action) and noun (object). In such cases, it is more challenging to recognize combinations that were not seen during training. This "compositional" setting was explored in the "SomethingElse" dataset <ref type="bibr" target="#b62">[63]</ref> relevance for object-centric models like ORViT, which can potentially better handle compositional actions. The dataset contains annotated bounding boxes that are commonly used as additional input to the model; this allows us to perform a fair comparison against previous methods <ref type="bibr" target="#b62">[63]</ref>. We also evaluate on the few-shot compositional action recognition task in <ref type="bibr" target="#b62">[63]</ref> (see Section A.6 in supplementary for details). <ref type="table" target="#tab_1">Table 1</ref> reports the results for these tasks. ORViT outperforms all models for both the Compositional and Few-shot task. Interestingly, the MF baseline is relatively strong compared to the previous methods (STRG and STIN). ORViT provides large improvement over both the previous methods and the baseline MF model. We also include results for a strong combined version (MF+STRG+STIN) of the previous methods with MF as the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spatio-temporal Action Detection</head><p>Next, we evaluate ORViT on the task of spatio-temporal action detection on the AVA dataset. In the literature, the action detection task on AVA is formulated as a two stage prediction procedure. The first step is the detection of bounding boxes, which are obtained through an off-the-shelf pretrained person detector. The second step involves predicting the action being performed at each detected bounding box. The performance is benchmarked on the end result of these steps and is measured with the Mean Average Precision (MAP) metric. Typically, for fair comparison, the detected person boxes are kept identical across approaches and hence, the final performance depends directly on the ability of the approach to utilize the video and box information.</p><p>We follow <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b83">84]</ref>, using their proposed procedure and the bounding boxes they provided for both ORViT and evaluation. This enables fair comparison with all previous methods following this standard procedure. <ref type="bibr" target="#b4">5</ref> This task presents an ideal benchmark for evaluating the benefit of ORViT since all baselines, as well as our model, operate on the same boxes. We trained the MViT-B,16?4 and MViT-B,32?3 models on Kinetics-400 <ref type="bibr" target="#b45">[46]</ref> and report these results.   <ref type="table" target="#tab_4">Table 3</ref> reports results on the standard action recognition task for several datasets. In contrast to the other tasks presented in this paper, using bounding boxes in action recognition is not part of the task definition. Thus, the comparison should be made carefully while differentiating between non-box and box-based methods. For box-based methods, we consider STIN, STRG and their combination on top of the same backbone as ORViT. Next, we explain how the boxes are extracted. For more details about datasets and evaluation see Section A of Supplementary. Box Input to ORViT. For SSv2, we finetune Faster-RCNN <ref type="bibr" target="#b66">[67]</ref> using the annotated boxes as in <ref type="bibr" target="#b62">[63]</ref>. For EK100 and Diving48 we use Faster-RCNN <ref type="bibr" target="#b66">[67]</ref> pre-trained on MS COCO <ref type="bibr" target="#b59">[60]</ref>. We only use the detector bounding boxes ignoring the object classes. There is no weight sharing between the detector and our model. SSv2. <ref type="table" target="#tab_4">Table 3a</ref> shows that ORViT outperforms recent methods. The improvement is 1.4% for both MF and MF-L, while ORViT also outperforms other box-based methods, such as MF+STIN, MF+STRG and their combination. We note that these models do not improve over MF, suggesting that using boxes is non-trivial on large datasets. We also experimented with using manually annotated boxes (as opposed to those obtained by a detector) as an "oracle" upper bound to see the potential with annotated box inputs. The results for this oracle evaluation (see Section D.2 in supp) are improvements of 7.3% and 6.7% over MF and MF-L respectively. This indicates that future improvements in object detectors will benefit object-centric approaches. Diving48. Here we build ORViT on top of the TimeSformer model, which was previously reported on this dataset (this demonstrates the ease of adding ORViT to any transformer model). <ref type="table" target="#tab_4">Table 3b</ref> shows that our ORViT TimeSformer model outperforms the state-of-the-art methods, including TQN <ref type="bibr" target="#b96">[97]</ref> by a significant margin of 6.2%. We obtain an improvement of 8.0% over the baseline TimeS-former model to which ORViT blocks were added. This again indicates the direct improvement due to the ORViT block. We note that ORViT achieves these results using only 32 frames, significantly less than the previous best results, TimeSformer-L, which uses 96 frames. ORViT outperforms box-based methods, including TimeS-former+STIN+STRG (4.5%), TimeSformer+STIN (7.0%), and TimeSformer+STRG (9.9%). EK100. <ref type="table" target="#tab_4">Table 3c</ref> reports results on EK100. Here we add ORViT blocks to the MF-HR model (which is the best performing model on EK100 in <ref type="bibr" target="#b64">[65]</ref>). Results show that our ORViT MF-HR model improves the accuracy for all three sub-tasks (with a smaller improvement for nouns). We believe the improvements on EK100 are less impressive than on the other datasets for two main reasons: (a) EK100 is an ego-centric dataset, making the camera movement a significant challenge for our method to model meaningful object interactions. (b) EK100 contains short 2-3 seconds videos, thus temporal reasoning is less effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Action Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablations</head><p>We perform a comprehensive ablation study on the compositional action recognition task <ref type="bibr" target="#b62">[63]</ref> in the "Some-thingElse" dataset to test the contribution of the different ORViT components <ref type="table">(Table 4</ref>). We use the MF as the baseline architecture for ORViT. For more ablations, see Section C in supplementary.</p><p>Components of the ORViT model. We consider the following versions of our model. (i) Single ORViT block (no ODM stream 6 ). We first consider a single application of the ORViT block, but without the ODM stream. We also compare different video transformer layers at which to apply our ORViT block (namely, the video transformer layer from which to extract the RoI descriptors). We refer to models applied at layer X as ORViT[L:X]. (ii) Single ORViT block (with ODM stream). Here we augment the single ORViT   <ref type="table">Table 4</ref>. Albations. We report top-1 and top-5 action accuracy on the SomethingElse split. We show (a) Contribution of ORViT components (with parameters number in 10 6 and GFLOPS in block, with the ODM stream. We refer to these models as ORViT[L:X]+ODM. (iii) Multiple ORViT blocks (with ODM stream). This is the version of ORViT used in all our experiments. It applies the ORViT block at multiple layers. We chose layers 2,7 and 11 of the video transformer model to apply ORViT block at. All the ablations were performed on the compositional split in SomethingElse. In the ablation table, we refer to this as ORViT[L:2,7,11]+ODM. In the rest of the experiments this is simply referred to as ORViT.</p><p>The results are shown in <ref type="table">Table 4a</ref>. It can be seen that a single ORViT layer already results in considerable improvement (66.7%), and that it is very important to apply it in the earlier layers rather than at the end. This is in contrast to the current practice in object-centric approaches (e.g., STRG and STIN) that extract RoIs from the final layer. It can also be seen that the ODM stream improves performance (by 2.1% from 66.7% to 68.8%). Finally, multiple applications of the layer further improve performance to 69.7%. Object-Centric Baselines. ORViT proposes an elegant way to integrate object region information into a video transformer. Here we consider two other candidate models to achieve this goal. (i) MF+RoIAlign uses RoIAlign over the last video transformer layer to extract object features. Then, it concatenates the CLS token with max-pooled object features to classify the action using an MLP. (ii) MF+Boxes uses coordinates and patch tokens. We use the CLS token from the last layer of MF, concatenated with trajectory embeddings. To obtain trajectory embeddings, we use a standard self-attention over the coordinates similar to our ODM stream. The first captures the appearance of objects with global context while the latter captures the trajectory information with global context, both without fusing the object information several times back to the backbone as we do. The results are shown in <ref type="table">Table 4b</ref>. MF+RoIAlign does not improve over the baseline, while MF+Boxes improves by 3.5%, which is still far from ORViT (69.7%).  <ref type="table">Table 4c</ref> for results. We observe a large drop in performance for all these baselines, which confirms the important role of the object regions in ORViT. Finally, we ask whether tracking information is important, as opposed to just detection. We find that this results in degradation from 69.7 to 68.2, indicating that the model can perform relatively well with only detection information. Decreasing Model Size. Next, we show that model size can be significantly decreased, incurring a small performance loss. Most the parameters added by ORViT over the baseline MF are in the ODM, and thus it is possible to use a smaller embedding dimension in ODM (see B in Section 3.2). <ref type="table">Table 4d</ref> reports how the dimension affects the performance, demonstrating that most of the performance gains can be achieved with a model that is close in size to the original MF. More in D.1&amp;D.2 in supp. We would like to highlight that "Object-Region Attention" alone (set dimension size to 0; thus ODM is not used) is the main reason for the improvement with only 2% additional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Limitations</head><p>Objects are a key element of human visual perception, but their modeling is still a challenge for machine vision. In this work, we demonstrated the value of an object-centric approach that incorporates object representations starting from early layers and propagates them into the transformerlayers. Through extensive empirical study, we show that integrating the ORViT block into video transformer architecture leads to improved results on four video understanding tasks and five datasets. However, we did not put effort into the object detection and used externally provided boxes, which is a limitation of our work. Replacing the externally provided boxes with boxes that the model generates without supervision will be interesting. We leave this challenge to future work. We believe our approach holds a positive social impact, mainly because it can add compositionality, an essential property for reasoning and commonsense, similar to humans. We do not anticipate a specific negative impact, but we recommend to exercise caution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary file, we provide additional information about our model, implementation details, experimental results, and qualitative examples. Specifically, Section A provides additional implementation details, Section B provides additional model details, Section C provides additional ablations of our approach, Section D provides more experiment results, and we show qualitative visualizations to demonstrate our approach in Section E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Implementation Details</head><p>We add ORViT to multiple existing transformer models Mformer (MF) <ref type="bibr" target="#b64">[65]</ref>, MViT <ref type="bibr" target="#b29">[30]</ref>, and Times-Former <ref type="bibr" target="#b6">[7]</ref>. These are all implemented based on the Slow-Fast <ref type="bibr" target="#b20">[21]</ref> library (available at https://github.com/ facebookresearch/SlowFast), and we implement ORViT based on this repository. Next, we elaborate on how we extract the object regions, and for each dataset, we add additional implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Detector and Tracker</head><p>Detector. In all action recognition datasets we used Faster R-CNN detector <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b66">67]</ref> with ResNet-50 backbone <ref type="bibr" target="#b31">[32]</ref> and Feature Pyramid Network (FPN) <ref type="bibr" target="#b58">[59]</ref> that is pre-trained on the MS COCO <ref type="bibr" target="#b59">[60]</ref> dataset. We used the Detectron2 <ref type="bibr" target="#b85">[86]</ref> implementation. In SSv2, the detector is finetuned using the bounding boxes annotation. During finetuning the class categories are hand and object. For AVA, we used the provided detection boxes for the spatio-temporal action detection task that were first obtained by Faster-RCNN pre-trained over MS COCO and then fine-tuned on AVA, as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b83">84]</ref>. We set the number of objects in our model to 4 in SSv2 and EK100, 6 in AVA, and 10 in Diving48. If fewer objects are presented, we set the object coordinates with a zero vector. These numbers were chosen by taking the max number of objects (after removing the outliers) per video (as induced by the tracker output) across all videos in the training set. Tracker. Once we have the detector results, we apply multi-object tracking to find correspondence between the objects in different frames. We use SORT <ref type="bibr" target="#b7">[8]</ref>: a simple tracker implemented based on Kalman Filter <ref type="bibr" target="#b42">[43]</ref> and the Hungarian matching algorithm (KM) <ref type="bibr" target="#b49">[50]</ref>. At each step, the Kalman Filter predicts plausible instances in the current frame based on previous tracks. Next, the predictions are matched with single-frame detections by the Hungarian matching algorithm. It is important to note that the tracker does not require any training and does not use any additional data. If an object does not appear in one of the frames, we set the coordinates in these frames to zeros. We provide some stats about the accuracy of the tracking step. Based on two consecutive frames of SSv2, SORT yields an 90.1% exact match between boxes.</p><p>Choosing the O on each dataset. We mentioned that O is set to the maximum number of objects per video (generated by the tracker) across all videos in the training set. In addition, we also tested an approach that does not require setting O ahead of time: for each batch take the maximum number of objects in any clip in the batch, and pad all clips to this number. This results in a very similar performance (-0.1%). This padding approach reduces the pre-processing step of finding the numbers for any dataset by choosing a fixed and large enough number. We also measured the inference runtime (milliseconds per clip) with and without SORT, and it increased by x1.18 (from 60.9ms to 71.6ms). Additionally, the object tokens add x1.03 FLOPS and x1.05 run-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Something-Something v2</head><p>Dataset. The SomethingElse <ref type="bibr" target="#b25">[26]</ref> contains 174 action categories of common human-object interactions. We follow the official splits from <ref type="bibr" target="#b25">[26]</ref>. Optimization details. For the standard SSv2 <ref type="bibr" target="#b62">[63]</ref> dataset, we trained 16 frames with sample rate 4 and batch-size 48 on 8 RTX 3090 GPUs. We train our network for 35 epochs with Adam optimizer <ref type="bibr" target="#b46">[47]</ref> with a momentum of 0.9 and Gamma 0.1. Following <ref type="bibr" target="#b64">[65]</ref>, we use lr = 5e ? 5 with ?10 decay steps at epochs 0, 20, 30. Additionally, we used Automatic Mixed Precision, which is implemented by PyTorch. We initialize from a Kinetics-400 pre-trained model <ref type="bibr" target="#b45">[46]</ref>. For the ORViT MF-L model, we fine-tuned from the SSv2 pre-trained model provided by <ref type="bibr" target="#b64">[65]</ref> and train with 32 frames. The optimization policy is similar to the above, except we used a different learning rate: 1e ? 5 for the pre-trained parameters, and 1e ? 4 for the ORViT parameters.</p><p>For the compositional action recognition task, we trained on the SomethingElse splits <ref type="bibr" target="#b62">[63]</ref>. We train with a batch size of 32 and a learning rate of 3e ? 5. Regularization details. We use weight decay of 0.05, a dropout <ref type="bibr" target="#b35">[36]</ref> of 0.5 before the final classification, dropout of 0.3 after the ORViT block, and DropConnect <ref type="bibr" target="#b36">[37]</ref> with rate 0.2. Training details. We use a standard crop size of 224, and we jitter the scales from 256 to 320. Additionally, we use RandAugment <ref type="bibr" target="#b13">[14]</ref> with maximum magnitude 20 for each frame separately. Inference details. We take 3 spatial crops per single clip to form predictions over a single video in testing as done in <ref type="bibr" target="#b64">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. EpicKitchens100</head><p>Dataset. EK100 <ref type="bibr" target="#b15">[16]</ref> contains 700 egocentric videos of daily kitchen activities. This dataset includes 300 noun and 97 verb classes, and we report verb, noun, and action top-1 accuracy, while the highest-scoring of the verb and noun pairs constitutes the action label.</p><p>Optimization details. We trained over videos of 16 frames with sample rate 4 and batch-size 16 on 8 Quadro RTX 8000 GPUs. We train our network for 35 epochs with Adam optimizer <ref type="bibr" target="#b46">[47]</ref> with a momentum of 0.9 and Gamma 0.1. Following <ref type="bibr" target="#b64">[65]</ref>, we use lr = 5e ? 5 with ?10 decay steps at epochs 0, 30, 40. Additionally, we used Automatic Mixed Precision, which is implemented by PyTorch. We initialize from a Kinetics-400 pre-trained model <ref type="bibr" target="#b45">[46]</ref>. Training details. We use crop size of 336 for the ORViT MF-HR. We jitter the scales from 384 to 480. Additionally, we use RandAugment <ref type="bibr" target="#b13">[14]</ref> with maximum magnitude 20. Inference details. We take 3 spatial crops with 10 different clips sampled randomly to aggregate predictions over a single video in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Diving48</head><p>Dataset. Diving48 <ref type="bibr" target="#b53">[54]</ref> contains 16K training and 3K testing videos spanning 48 fine-grained diving categories of diving activities. For all of these datasets, we use standard classification accuracy as our main performance metric. Optimization details. We trained over videos of 32 frames with sample rate 8 and batch-size 8 on 8 Quadro RTX 8000 GPUs. We train our network for 35 epochs with Adam optimizer <ref type="bibr" target="#b46">[47]</ref> with a momentum of 0.9 and Gamma 0.1. We use lr = 3.75e?5 with ?10 decay steps at epochs 0, 20, 30. Additionally, we used Automatic Mixed Precision, which is implemented by PyTorch. We initialize from a Kinetics-400 pre-trained model <ref type="bibr" target="#b45">[46]</ref>. Training details. We use a standard crop size of 224 for the standard model and jitter the scales from 256 to 320. Additionally, we use RandomFlip augmentation. Finally, we sampled the T frames from the start and end diving annotation time, followed by <ref type="bibr" target="#b96">[97]</ref>. Inference details. We take 3 spatial crops per single clip to form predictions over a single video in testing same as in <ref type="bibr" target="#b6">[7]</ref>. Object-Dynamics Module. As we show in <ref type="table" target="#tab_7">Table 5d</ref>, we compared different self-attention mechanisms, and the standard self-attention usually performed better. However, we observe a slight improvement when we perform a trajectory self-attention <ref type="bibr" target="#b64">[65]</ref> instead of the standard self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. AVA</head><p>Architecture. SlowFast <ref type="bibr" target="#b20">[21]</ref> and MViT <ref type="bibr" target="#b29">[30]</ref> are using a detection architecture with a RoI Align head on top of the spatio-temporal features. We followed their implementation to allow a direct comparison. Next we elaborate on the RoI Align head proposed in SlowFast <ref type="bibr" target="#b20">[21]</ref>. First, we extract the feature maps from our ORViT MViT model by using the RoIAlign layer. Next, we take the 2D proposal at a frame into a 3D RoI by replicating it along the temporal axis, followed by a temporal global average pooling. Then, we max-pooled the RoI features and fed them to an MLP classifier for prediction. Optimization details. To allow a direct comparison, we used the same configuration as in MViT <ref type="bibr" target="#b29">[30]</ref>. We trained 16 frames with sample rate 4, depth of 16 layers and batchsize 32 on 8 RTX 3090 GPUs. We train our network for 30 epochs with an SGD optimizer. We use lr = 0.03 with a weight decay of 1e ? 8 and a half-period cosine schedule of learning rate decaying. We use mixed precision and finetune from an MViT-B, 16 ? 4 pre-trained model. Training details. We use a standard crop size of 224 and we jitter the scales from 256 to 320. We use the same groundtruth boxes and proposals that overlap with ground-truth boxes by IoU &gt; 0.9 as in <ref type="bibr" target="#b20">[21]</ref>. Inference details. We perform inference on a single clip with 16 frames. For each sample, the evaluation frame is centered in frame 8. We use a crop size of 224 in test time. We take 1 spatial crop with 10 different clips sampled randomly to aggregate predictions over a single video in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Few Shot Compositional Action Recognition</head><p>We also evaluate on the few-shot compositional action recognition task in <ref type="bibr" target="#b62">[63]</ref>. For this setting, we have 88 base action categories and 86 novel action categories. We train on the base categories (113K/12K for training/validation) and finetune on few-shot samples from the novel categories (for 5-shot, 430/50K for training/validation; for 10-shot, 860/44K for training/validation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. SomethingElse</head><p>Dataset. The SomethingElse <ref type="bibr" target="#b62">[63]</ref> contains 174 action categories with 54,919 training and 57,876 validation samples. The proposed compositional <ref type="bibr" target="#b62">[63]</ref> split in this dataset provides disjoint combinations of a verb (action) and noun (object) in the training and testing sets. This split defines two disjoint groups of nouns {A, B} and verbs {1, 2}. Given the splits of groups, they combine the training set as 1A + 2B, while the validation set is constructed by flipping the combination into 1B + 2A. In this way, different combinations of verbs and nouns are divided into training or testing splits. Optimization details. We trained 16 frames with sample rate 4 and batch-size 32 on 8 RTX 3090 GPUs. We train our network for 35 epochs with Adam optimizer <ref type="bibr" target="#b46">[47]</ref> with a momentum of 0.9 and Gamma 0.1. We use lr = 3e ? 5 with ?10 decay steps at epochs 0, 20, 30. Additionally, we used Automatic Mixed Precision, which is implemented by PyTorch. We initialize from a Kinetics-400 pre-trained model <ref type="bibr" target="#b45">[46]</ref>. Regularization details. We use weight decay of 0.05, a dropout <ref type="bibr" target="#b35">[36]</ref> of 0.5 before the final classification, dropout of 0.3 after the ORViT block, and DropConnect <ref type="bibr" target="#b36">[37]</ref> with rate 0.2.   Training details. We use a standard crop size of 224, and we jitter the scales from 256 to 320. Inference details. We take 3 spatial crops per single clip to form predictions over a single video in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Model details B.1. Object-Region Attention</head><p>As explained in section 3.2, there are two inputs to the ORViT block. The first is the output of the preceding transformer block, represented as a set of spatio-temporal tokens X ? R T HW ?d . The second input is a set of bounding boxes for objects across time, denoted by B ? R T O?4 . Object-Region Attention. Given the patch token features X and the boxes B, we use RoIAlign <ref type="bibr" target="#b30">[31]</ref> layer, which uses the patch tokens X and box coordinates B to obtain object region crops. This is followed by max-pooling and an MLP. To these features we add a learnable object-time position encoding P ? R T O?d to encode the positional object information. We also use a coordinate embedding by applying an MLP on the boxes coordinates, resulting in L ? R d :</p><formula xml:id="formula_6">L := MLP(B)<label>(4)</label></formula><p>where B ? R T ?O?d is the boxes coordinates. This leads to an improved object features:</p><p>O := MLP(MaxPool <ref type="figure">(RoIAlign(X, B)</ref>)) + L + P <ref type="bibr" target="#b4">(5)</ref> where the token features are X ? R T HW ?d . We pass these features into the self-attention layers as explained in the "Object-Region attention" subsection in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Ablations</head><p>We perform an ablation study of each of the components in <ref type="table" target="#tab_7">Table 5</ref> to show the effectiveness of the different components of our model. All ablations are on the Somethin-gElse <ref type="bibr" target="#b62">[63]</ref> dataset and we use Mformer (MF) as the baseline architecture for ORViT unless stated otherwise. We also note that we refer to the "Object-Dynamics Module" as ODM stream. Contribution of appearance and motion streams. In Table 5a, we show the "Object-Region Attention" is an important factor for the improvement, responsible for a 7.2% gain improvement, with less than 2% additional parameters over MF (only 2M parameters addition compared to the baseline). This highlights our contribution that object interactions are indeed crucial for video transformers. Additionally, adding trajectory information with coordinates in the "Object-Dynamics Module" (ODM) improved by another 2.3% but with a cost of 36% additional parameters. We show later (see in Section D.1) that we can reduce the ODM size with smaller dimensions. ORViT blocks. In <ref type="table" target="#tab_7">Table 5b</ref>, we show which layers are most important for adding the ORViT block. The experiments show that adding this information at the network's beginning, middle, and end is the most effective (layer 2, 7, 11). This experiment demonstrates that it is important to fuse object-centric representations starting from early layers and propagate them into the transformer-layers, thus affecting the spatio-temporal representations throughout the network. Different self-attention in "Object-Region Attention".</p><p>In <ref type="table" target="#tab_7">Table 5c</ref>, we compared different self-attention mechanisms (as defined in <ref type="bibr" target="#b64">[65]</ref>): joint space-time, divided spacetime, and trajectory attention to the MF baseline, which uses trajectory attention in all layers. We observed that trajectory  attention is slightly better. However, it can be seen that our object region approach is not sensitive to these choices, indicating that the generic approach is the main reason for the observed improvements.</p><p>Replacing ORViT with Trajectory Attention. We observe that joint and divided self-attention layers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> have similar results to the trajectory attention <ref type="bibr" target="#b64">[65]</ref>, as seen in <ref type="table" target="#tab_7">Table 5c</ref>. However, we would like to demonstrate that trajectory attention is not the main reason for the improvement when using ORViT with TimeSformer <ref type="bibr" target="#b6">[7]</ref> or MViT <ref type="bibr" target="#b29">[30]</ref>. Thus, we replace our ORViT with a standard trajectory attention on the Diving48 and AVA datasets. The top1 accuracy on Diving48 are improved by 4.5% (from 80.0 to 84.5) with trajectory attention, while using our ORViT+TimeSformer achieves 88.0 (3.5% improvements on top of that). The MAP on AVA are the same as the baseline with trajectory attention (25.5), while using our ORViT+MViT-B achieves 26.6 (1.1 improvements on top of the baseline). We note that our MF is the baseline on EK100, SSv2, and SomethingElse, and therefore the trajectory attention is already part of the model, and hence this demonstration is not needed. Processing trajectory information. In <ref type="table" target="#tab_7">Table 5d</ref>, we compared our self-attention (see "Object-Dynamics Module" in Section 3.2) with other standard baseline models: GCN <ref type="bibr" target="#b47">[48]</ref> and trajectory self-attention <ref type="bibr" target="#b64">[65]</ref>. For the GCN, we use a standard implementation with 2 hidden layers, while for the trajectory attention, we treat the O objects as the spatial dimension. We can see that self-attention is slightly better than trajectory self-attention (+0.3%) and significantly better than GCN (+2.0%). Components on Diving48. Following our components ablations in <ref type="table">Table 4a</ref>, we also validate our hypothesis on the Diving48 dataset. We used the TimeSformer <ref type="bibr" target="#b6">[7]</ref> trained on videos of 32 frames as a baseline for a fair comparison. It can be seen that a single layer version of the model already results in considerable improvement (85.4%) and that it is important to apply it in the earlier layers of transformers than at the end (86.8% compared to 85.4%). Additionally, the "Object-Dynamics Module" improves performance to 87.5%. Finally, multiple applications of the layer further improve performance to 88.0%. Box Position Encoder. Our "Box Position Encoder" transforms from a tensor of size T O to size T HW . Our implementation of this transformation uses box information so that each object is mapped to the "correct" region in space.</p><p>A simpler approach would have been to expand the shape of T O to T HW without using boxes. We refer to the latter as a standard tensor expansion. Comparing the two methods, we find out that our approach obtains 69.7 compared to 68.4, showing that our box-based encoding performs better.</p><p>Combine the two streams in ORViT Block. As part of the ORViT block, we examined other operations to combine the two streams of information. We explored the following methods: element-wise multiplication, gating (with conv), and our simple sum. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experiments</head><p>Here we present additional experiments, including demonstrating a lightweight version of the "Object-Dynamics Module" that significantly reduces the model parameters without losing significant performance and complete results on the standard action recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Light-weight ORViT</head><p>In <ref type="table" target="#tab_7">Table 5a</ref>, we show that the "Object-Dynamics Module" improves by 2.3% the top-1 accuracy with an additional 39M parameters (148M Vs. 109M). We would like to demonstrate that model size can be significantly decreased, incurring a small performance loss. Most the parameters added by ORViT over the baseline MF are in the ODM, and thus it is possible to use a smaller embedding dimension in ODM. Here, we present a light-weight version of the module that reduces the embedding dimensions without losing significant accuracy. See <ref type="table" target="#tab_9">Table 6</ref>.</p><p>As mentioned in the main paper (see Section 3), we use B for the coordinate embeddings in the "Object-Dynamics</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Visualizations</head><p>To provide insight into the inner representation of ORViT we provide further visualization next. See <ref type="figure">Figure 5</ref> and Figure 6. In <ref type="figure">Figure 5</ref>, we visualize the attention map of the CLS token on all spatial tokens. It can be seen that object-regions indeed affect these spatial maps. For example, "Tearing something into two pieces" (top left corner) demonstrates that ORViT+MF successfully separates the two pieces of the paper, while the MF baseline does not. Next, in <ref type="figure">Figure 6</ref> we visualize the attention allocated to each of the object keys. It can be seen that the object keys in ORViT indeed affect their corresponding spatial tokens. <ref type="table">Table 7</ref>. Comparison to the state-of-the-art on video action recognition. We report pretrain, param (10 6 ), GFLOPS (10 9 ) and top-1 (%) and top-5 (%) video action recognition accuracy on SSv2. On Epic-Kitchens100 (EK100), we report top-1 (%) action (A), verb (V), and noun (N) accuracy. On Diving48 we report pretrain, number of frames, param (10 6 ) and top-1 (%) video action recognition accuracy. Difference between baselines (MF/MF-L for SSv2, TimeSformer for Diving48, MF-HR for EK100) and ORViT is denoted by (+X). We denote methods that do not use bounding boxes with ? .</p><p>(a) Something-Something V2 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>10 9 ). (b) Other Object-centric baselines. (c) ORViT with different boxes input, and (d) The effect of "Object-Dynamics Module" (ODM) embedding dimension. More ablations are in Section C in supplementary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>How important are the object bounding boxes. Since ORViT changes the architecture of the base video transformer model, we want to check whether the bounding boxes are indeed the source of improvement. We consider several variations where the object bounding boxes are replaced with other values. (i) All boxes: all boxes are given the coordinates of the entire image ([0, 0, 1, 1]). (ii) Null boxes: boxes are initialized to zeros. (iii) Grid boxes: each of the 4 bounding boxes is one fourth of the image. (iv) Random boxes -each box is chosen uniformly at random. See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Compositional and Few-Shot Action Recognition on the "SomethingElse" dataset.contains 700 egocentric videos of kitchen activities. This dataset includes noun and verb classes, and we report verb, noun, and action accuracy, where the highest-scoring verb and noun pair constitutes an action label. (5) Diving48<ref type="bibr" target="#b53">[54]</ref> contains 48 fine-grained categories of diving activities. Baselines. In the experiments, we compare ORViT to several models reported in previous work for the corresponding datasets. These include non-transformer approaches (e.g., I3D<ref type="bibr" target="#b10">[11]</ref> and SlowFast<ref type="bibr" target="#b20">[21]</ref>) as well as state-of-the-art transformers (TimeSformer, Mformer (MF), and MViT). We also cite results for two object-centric models: STIN [63] which uses boxes information, and the Space-Time Region Graph (STRG) model<ref type="bibr" target="#b82">[83]</ref> which extracts I3D features for objects and runs a graph neural network on those. Both STIN and STRG use the same input information as ORViT. Finally, we implement an object-centric transformer baseline combining STRG and STIN: we use the MF final patch tokens as input to the STRG model, resulting in STRG feature vector, and concatenate it to the STIN feature vector and the MF's CLS token. We refer to this as MF+STRG+STIN. Implementation Details. ORViT is implemented in Py-Torch, and the code will be released in our project page. Our training recipes and code are based on the MViT, MF, and TimeSformer code published by the authors. For all tasks and datasets, we use SORT</figDesc><table><row><cell>Model</cell><cell>Boxes</cell><cell cols="2">Compositional Top-1 Top-5 Top-1 Top-5 5-Shot 10-Shot Base Few-Shot</cell></row><row><cell>I3D [11]</cell><cell>?</cell><cell>42.8 71.3 73.6 92.2 21.8</cell><cell>26.7</cell></row><row><cell>SlowFast [21]</cell><cell>?</cell><cell>45.2 73.4 76.1 93.4 22.4</cell><cell>29.2</cell></row><row><cell>TimeSformer [7]</cell><cell>?</cell><cell>44.2 76.8 79.5 95.6 24.6</cell><cell>33.8</cell></row><row><cell>MF [65]</cell><cell>?</cell><cell>60.2 85.8 82.8 96.2 28.9</cell><cell>33.8</cell></row><row><cell cols="2">STRG (\w SF) [83] ?</cell><cell>52.3 78.3 75.4 92.7 24.8</cell><cell>29.9</cell></row><row><cell>STIN (\w SF) [63]</cell><cell>?</cell><cell>54.6 79.4 77.4 95.0 23.0</cell><cell>33.4</cell></row><row><cell>MF+STRG+STIN</cell><cell>?</cell><cell>62.3 86.0 83.7 96.8 29.8</cell><cell>36.5</cell></row><row><cell>ORViT MF(ours)</cell><cell>?</cell><cell>69.7 91.0 87.1 97.6 33.3</cell><cell>40.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>, where verbnoun combinations in the test data do not occur in the training data. The split contains 174 classes with 54,919/54,876 videos for training/validation. This setting is of particular Spatio-temporal Action Detection on AVA-V2.2.</figDesc><table><row><cell>Model</cell><cell cols="2">Boxes Pretrain</cell><cell>mAP</cell></row><row><cell>SlowFast [21], 4 ? 16, R50</cell><cell>?</cell><cell>K400</cell><cell>21.9</cell></row><row><cell>SlowFast [21], 8 ? 8, R50</cell><cell>?</cell><cell>K400</cell><cell>22.7</cell></row><row><cell>SlowFast [21], 8 ? 8, R101</cell><cell>?</cell><cell>K400</cell><cell>23.8</cell></row><row><cell>MViT-B [30], 16 ? 4</cell><cell>?</cell><cell>K400</cell><cell>25.5</cell></row><row><cell>MViT-B [30], 32 ? 3</cell><cell>?</cell><cell>K400</cell><cell>27.3</cell></row><row><cell cols="2">ORViT MViT-B, 16 ? 4 (Ours) ?</cell><cell cols="2">K400 26.6 (+1.1)</cell></row><row><cell cols="2">ORViT MViT-B, 32 ? 3 (Ours) ?</cell><cell cols="2">K400 28.0 (+0.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>shows that ORViT-MViT achieves +1.1, +0.7 MAP improvements over the MViT-B 16x4 and MViT-B 32x3, thereby showcasing the power of our proposed object-centric representation fusion scheme. Something-Something V2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Diving48</cell><cell></cell><cell></cell><cell cols="3">(c) Epic-Kitchens100</cell></row><row><cell>Model</cell><cell>Pretrain</cell><cell>Top-1</cell><cell>Top-5</cell><cell>Model</cell><cell cols="3">Pretrain Frames Top-1</cell><cell>Method</cell><cell>Pretrain</cell><cell>A</cell><cell>V</cell><cell>N</cell></row><row><cell>SlowFast, R101  ?</cell><cell>K400</cell><cell>63.1</cell><cell>87.6</cell><cell>SlowFast, R101  ?</cell><cell>K400</cell><cell>16</cell><cell>77.6</cell><cell>SlowFast, R50  ?</cell><cell>K400</cell><cell cols="2">38.5 65.6 50.0</cell></row><row><cell>ViViT-L  ?</cell><cell>IN+K400</cell><cell>65.4</cell><cell>89.8</cell><cell>TimeSformer  ?</cell><cell>IN</cell><cell>16</cell><cell>74.9</cell><cell>ViViT-L  ?</cell><cell cols="3">IN+K400 44.0 66.4 56.8</cell></row><row><cell>MViT-B, 64  ?</cell><cell>K600</cell><cell>68.7</cell><cell>91.5</cell><cell>TimeSformer-L  ?</cell><cell>IN</cell><cell>96</cell><cell>81.0</cell><cell>MF  ?</cell><cell cols="3">IN+K400 43.1 66.7 56.5</cell></row><row><cell>MF  ?</cell><cell>IN+K400</cell><cell>66.5</cell><cell>90.1</cell><cell>TQN  ?</cell><cell cols="3">K400 ALL 81.8</cell><cell>MF-L  ?</cell><cell cols="3">IN+K400 44.1 67.1 57.6</cell></row><row><cell>MF+STRG</cell><cell>IN+K400</cell><cell>66.1</cell><cell>90.0</cell><cell>TimeSformer  ?</cell><cell>IN</cell><cell>32</cell><cell>80.0</cell><cell>MF-HR  ?</cell><cell cols="3">IN+K400 44.5 67.0 58.5</cell></row><row><cell>MF+STIN</cell><cell>IN+K400</cell><cell>66.5</cell><cell>89.8</cell><cell>TimeSformer + STRG</cell><cell>IN</cell><cell>32</cell><cell>78.1</cell><cell>MF-HR + STRG</cell><cell cols="3">IN+K400 42.5 65.8 55.4</cell></row><row><cell>MF+STRG+STIN</cell><cell>IN+K400</cell><cell>66.6</cell><cell>90.0</cell><cell>TimeSformer + STIN</cell><cell>IN</cell><cell>32</cell><cell>81.0</cell><cell>MF-HR + STIN</cell><cell cols="3">IN+K400 44.2 67.0 57.9</cell></row><row><cell>MF-L  ?</cell><cell>IN+K400</cell><cell>68.1</cell><cell>91.2</cell><cell cols="2">TimeSformer + STRG + STIN IN</cell><cell>32</cell><cell>83.5</cell><cell cols="4">MF-HR + STRG + STIN IN+K400 44.1 66.9 57.8</cell></row><row><cell cols="4">ORViT MF(Ours) ORViT MF-L(Ours) IN+K400 69.5 (+1.4) 91.5 (+0.3) IN+K400 67.9 (+1.4) 90.5 (+0.4)</cell><cell>ORViT TimeSformer(Ours)</cell><cell>IN</cell><cell>32</cell><cell>88.0 (+8.0)</cell><cell cols="2">ORViT MF-HR(Ours) IN+K400</cell><cell cols="2">45.7 68.4 58.7 (+1.2) (+1.4) (+.2)</cell></row></table><note>(a)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison</figDesc><table /><note>to state-of-the-art on video action recognition. We report top-1 (%) and top-5 (%) accuracy on SSv2. On Epic- Kitchens100 (EK100), we report top-1 (%) action (A), verb (V), and noun (N) accuracy. On Diving48 we report top-1 (%). Difference between baselines and ORViT is denoted by (+X). IN refers to IN-21K. We denote methods that do not use bounding boxes with ? . For additional results and details, including the model size, see section D.2 in supplementary.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>?1.01 ?1.01 ORViT [L:2]+ODM 68.8 90.5 ?1.03 ?1.12 ORViT [L:2,7,11]+ODM 69.7 91.0 ?1.09 ?1.36 (b) Object-centric Baselines</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>(c) Boxes</cell><cell></cell><cell cols="2">(d) ODM Dimension</cell></row><row><cell>Layers MF ORViT [L:12] ORViT [L:2]</cell><cell cols="2">Top-1 Top-5 GFLOP Param 60.2 85.8 ?1(370) ?1(109) 63.9 87.6 ?1.01 ?1.01 66.7 89.2 Model MF MF + RoIAlign 59.6 Top-1 Top-5 60.2 85.8 84.5 MF + Boxes 63.7 86.9</cell><cell>Model Full boxes Null boxes Grid boxes</cell><cell>Top-1 Top-5 60.9 84.5 60.4 84.2 60.9 84.8</cell><cell cols="3">Dim. Top-1 Top-5 GFLOP MF 60.2 85.8 ?1 (370) ?1 (109) Param 0 67.4 89.8 ?1.03 ?1.02 128 68.7 90.3 ?1.03 ?1.03</cell></row><row><cell></cell><cell>ORViT (Ours) 69.7</cell><cell>91.0</cell><cell>Random boxes</cell><cell>60.7 85.0</cell><cell>256 68.9 90.5</cell><cell>?1.03</cell><cell>?1.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Object Regions (Ours) 69.7 91.0</cell><cell>768 69.7 91.0</cell><cell>?1.1</cell><cell>?1.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Ablations. Evaluation of different model ablations and baselines on the "SomethingElse" split (Tables (a-d) see text). We report pretrain, param (10 6 ), GFLOPS (10 9 ), and top-1 and top-5 video action recognition accuracy.Table (e) reports ablations on the Diving dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>A light-weight version of ORViT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The results on SomethingElse are 68.8, 68.7, and 69.7, respectively. In this case, our simple sum is superior to the other methods. T ? O learnable embeddings. We observe a small difference when experimenting with T ? O and separate T and O embeddings (69.6 Vs. 69.7) on the SomethingElse dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In what follows, we omit the count of the CLS feature for brevity."Moving something and something away from each other"</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">O represents the maximum number of objects in the training set. If a clip has less than O boxes, we pad the remaining embeddings with zeros.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Boxes are obtained by pre-training FasterRCNN with a ResNeXt101-FPN<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b58">59]</ref> on ImageNet and COCO human keypoint images as in<ref type="bibr" target="#b20">[21]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We refer to the "Object-Dynamics Module" as ODM stream.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Tete Xiao, Medhini Narasimhan, Rodolfo (Rudy) Corona, and Colorado Reed for helpful feedback and discussions. This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080). Prof. Darrell's group was supported in part by DoD including DARPA's XAI, and LwLL programs, as well as BAIR's industrial alliance programs. This work was completed in partial fulfillment for the Ph.D. degree of the first author.</p><p>Module". We observe that reducing the dimension of the coordinate embeddings ( B) from 768 to 256 has little impact on the action accuracy in SSv2 (67.9% Vs. 67.3%) and SomethingElse (69.7% Vs. 68.9%), although having only 114M model parameters (an addition of 5M parameters to the MF baseline that has 109M). Indeed this indicates that our main approach is the main reason for the observed improvements and not necessarily the addition of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Standard Action Recognition Results</head><p>We next report in <ref type="table">Table 7</ref> the full results table for the standard action recognition task, including extra models and details, which were not included in the main paper.</p><p>Additionally, we add a light version of ORViT for each dataset. This version use embedding dimension of 256 in the "Object-Dynamics Module", as stated in Section D.1. In SSv2, the ORViT-Light model improves the MF baseline by 0.8 at the cost of additional 5M parameters (5% more parameters), while the ORViT model (non-light version) improves by 1.4% at the cost of additional 39M parameters (36% more parameters). In Diving48, the ORViT-Light model improves the TimeSformer baseline by 6.8 at the cost of additional 5M parameters (3% more parameters), while the ORViT model (non-light version) improves by 8% at the cost of additional 39M parameters (32% more parameters). In EK100, the ORViT-Light model improves the MF-HR baseline by 1.6, 1.5, 0.2 (A, V, N) at the cost of additional 5M parameters (5% more parameters), while the ORViT model (non-light version) improves by 1.2, 1.4, 0.2 at the cost of additional 39M parameters (36% more parameters). We note that the ORViT-Light even outperforms the non-light version (1.6 Vs. 1.2), demonstrating the object movement is less significant in this data.</p><p>Last, we separately evaluated the accuracy on Kinetics-400 with ORViT MViT-B 16x4 and noticed that it improved by +2.0% over MViT-B 16x4.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ORViT-Mformer</head><p>Mformer "Tearing something into two pieces" "Stuffing something into something"</p><p>"Turning the camera left while filming something"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ORViT-Mformer</head><p>Mformer "Putting something that can't roll onto a slanted surface, so it stays where it is"  <ref type="figure">Figure 6</ref>. Object contribution to the patch tokens. For each object token, we plot the attention weight given by the patch tokens, normalized over the patch tokens.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action search: Spotting targets in videos and its application to temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unified graph structured models for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compositional video synthesis with action graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021-07-02" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking the faster R-CNN architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rescaling egocentric vision. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recognizing action at a distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="726" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Drg: Dual relation graph for human-object interaction detection. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Objects in action: An approach for combining action understanding and object perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Haoqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mangalam</forename><surname>Karttikeya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yanghao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Jitendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feichtenhofer</forename><surname>Christoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<title level="m">Multiscale vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning canonical representations for scene graph to image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Amir Globerson, and Trevor Darrell. Spatio-temporal action graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Brosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mapping images to scene graphs with permutation-invariant structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshiko</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Jonathan Berant, and Amir Globerson</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Actionbytes: Learning from trimmed videos to localize actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1168" to="1177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning object detection from captions via textual scene attributes. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achiya</forename><surname>Jerbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A new approach to linear filtering and prediction problems&quot; transaction of the asme journal of basic. In transaction of the asme journal of basic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>K?lm?n</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keizo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Referring relationships. ECCV</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1908.03557</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Oscar: Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Peeking into the future: Predicting future person activities and locations in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5718" to="5727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">BMN: boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3888" to="3897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno>2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1942" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Something-else: Compositional action recognition with spatial-temporal interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Activity graph transformer for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megha</forename><surname>Nawhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra Florian Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Jonathan Berant, and Amir Globerson. Differentiable scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshiko</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yeh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Bandla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06992</idno>
		<title level="m">Mid-level features improve recognition of interactive activities</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12424,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Video visual relation detection via multi-modal feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Hao Hao Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Longterm temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Long-Term Feature Banks for Detailed Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Towards Long-Form Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Chao-Yuan Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 13</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2019" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5794" to="5803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">What can neural networks reason about</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Marginalized average attentional network for weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01830</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Relational deep reinforcement learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Temporal query networks for fine-grained video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gputa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Temporal relational reasoning in videos. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Crosstask weakly supervised learning from instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramazan</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
