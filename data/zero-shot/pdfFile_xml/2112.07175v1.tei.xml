<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-training Transformer with Videos and Images Improves Action Recognition Ruoming Pang ? Apple AI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><forename type="middle">Yu</forename><surname>Google Brain</surname></persName>
							<email>jiahuiyu@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
							<email>cfifty@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Han</forename><surname>Google Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
							<email>adai@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><forename type="middle">Google</forename><surname>Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">USC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Co-training Transformer with Videos and Images Improves Action Recognition Ruoming Pang ? Apple AI</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In learning action recognition, models are typically pretrained on object recognition with images, such as Ima-geNet, and later fine-tuned on target action recognition with videos. This approach has achieved good empirical performance especially with recent transformer-based video architectures. While recently many works aim to design more advanced transformer architectures for action recognition, less effort has been made on how to train video transformers. In this work, we explore several training paradigms and present two findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition has received significant attention from the research community, as many applications can benefit * This work was conducted at Google Brain. <ref type="bibr">?</ref>  from improved action recognition modeling, such as video retrieval <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>, video captioning <ref type="bibr" target="#b16">[17]</ref>, video QA <ref type="bibr" target="#b17">[18]</ref>, etc. Datasets are one dimension of improvement. Video datasets have evolved from hundreds of videos in controlled environments <ref type="bibr" target="#b23">[24]</ref> to millions of videos crawled from the Internet <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>. In addition to quantity, the scope of videos has also broadened. For example, the topics covered by action recognition datasets have evolved from simple body motions like waving and handshaking to more complex activities present in our daily life. Simultaneously, with the increase of data and class distribution complexity, modeling architecture complexity has likewise increased <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. Among these architectures, Transformer based approaches have recently demonstrated state-of-the-art performance on several benchmarks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref>. However, since Transformer models are more data-hungry <ref type="bibr" target="#b26">[27]</ref> and action recognition datasets are relatively small in scale, large Transformer models are typically first trained on image datasets and later finetuned on the target action recognition dataset. While the current pre-training and fine-tuning action recognition paradigm is straightforward and manifests strong empirical results, it may be overly restrictive for building general-purpose action-recognition models. Compared to a dataset like ImageNet <ref type="bibr" target="#b6">[7]</ref> that covers a large range of object recognition classes, action recognition datasets like Kinetics <ref type="bibr" target="#b13">[14]</ref> and Something-Something-v2 (SSv2) <ref type="bibr" target="#b12">[13]</ref> pertain to limited topics. For example, Kinetics focuses on actions like "cliff diving" and "ice climbing" while SSv2 contains information related to object agnostic activities like "pretending to put something onto something else." As a result, adapting an action recognition that has been fine-tuned on Something-Something-v2 to a disparate dataset like Kinetics is likely to result in poor performance. Differences in objects and video backgrounds among datasets further exacerbate learning a general-purpose action recognition classification model, and even though video datasets may be increasing in size, prior work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref> suggests significant data augmentation and regularization is necessary to achieve strong performance. This latter finding may indicate the model quickly overfits on the target dataset, and as a result, hinders its capacity to generalize to other action recognition tasks.</p><p>In this work, we aim to build a training strategy for a general purpose action recognition model. Inspired by prior works in vision and language that demonstrate a single Transformer model can be extended to many downstream tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>, we propose to leverage both image and video data to jointly train a single action recognition model. This approach is buttressed by two main findings. First, disparate video datasets cover a diverse set of activities, and training them together in a single model could lead to a model that excels in a wide range of activities. Second, video is a perfect source for learning motion information, while images are great for exploiting appearance structure. Compared to ImageNet, action recognition datasets have relatively little data. Combining this with the high spatial redundancy among frames in a clip, and leveraging a diverse distribution of image examples may be beneficial in building robust spatial representations in video models.</p><p>With this background as motivation, we suggest a new training scheme: co-training video and image for action recognition (COVER). Similar to the typical pre-training and fine-tuning paradigm, COVER first pre-trains the model on an image dataset, but during fine-tuning, it simultaneously trains a single model on multiple action recognition and image datasets to build robust spatial and temporal representations of video data. More concretely, COVER adopts a multi-task learning setup to train multiple datasets within one model, with the total loss for a given batch equal to the weighted sum of the losses across each dataset in that batch. Our empirical findings suggest this approach is competitive across several action recognition benchmarks. Moreover, unlike the current paradigm of pre-train once and fine-tune on each downstream action recognition benchmark, COVER learns generalizable representations which can be used on downstream action recognition tasks without any additional finetuning. Our empirical findings in <ref type="table" target="#tab_0">Table 1</ref> indicate this simplified approach leads to improved performance on Kinetics, SomethingSomething-v2, and Moments-in-Time when compared with the typical pre-train and fine-tune paradigm.</p><p>Our contributions are three-fold: ? We analyze simultaneous training of image and video data for the purpose of modeling action recognition data.</p><p>? We propose COVER, an approach that learns robust spatial and temporal representations via simultaneous learning across multiple action recognition and image datasets. The learned representations can be immediately applied to several downstream tasks to perform competitively with prior pre-training and fine-tuning paradigms.</p><p>? COVER established new State-of-The-Art results on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action recognition modeling persists as a challenging problem in the field of computer vision. Recent work in this domain largely focus on two dimensions to improve performance: modeling changes and training paradigm.</p><p>Video Transformers Recently, and inspired by the Visual Transformer <ref type="bibr" target="#b8">[9]</ref> and BERT <ref type="bibr" target="#b7">[8]</ref> in the domain of Natural Language Processing, action recognition modeling has begun to adopt transformer-based architectures such as TimeSFormer <ref type="bibr" target="#b2">[3]</ref>, ViViT <ref type="bibr" target="#b1">[2]</ref>, and Multiscale Vision Transformer <ref type="bibr" target="#b10">[11]</ref>. In this class of models, TimeSFormer and ViViT directly extend the Visual Transformer into the video domain by adding temporal attention layers. Based on this architecture, TimeSFormer and ViViT find that large-scale image data pretraining is beneficial, and with this training policy, they can surpass the performance of ConvNet-based architectures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. Further exploring training policies, Multiscale Vision Transformer finds that, with carefully tuned parameters, strong performance can be achieved without pre-training on a large-scale image dataset. A separate approach, and inspired by the Swin Transformer <ref type="bibr" target="#b18">[19]</ref>, Video Swin-Transformer <ref type="bibr" target="#b19">[20]</ref> adopts 3D shifted windows to learn spatial and temporal aware video representations. While modeling approaches have undoubtedly driven much of the recent advancement of action recognition, we instead direct our focus to these models' training policy and how training on varying data distributions may learn general-purpose action recognition models.</p><p>Training Paradigms We define training policy as the technique(s) used to train a model. In this sense, and for the domain of action recognition, Two-Stream ConvNet <ref type="bibr" target="#b24">[25]</ref> and I3D <ref type="bibr" target="#b5">[6]</ref> were the first approaches to leverage image data to improve video modeling. More concretely, both models leveraged the feature extractors learned on image data during "pre-training" and would later "fine-tune" on a downstream video benchmark. The concept of using image data to learn spatial relationships which can be transferred to video understanding has also been applied to recent transformer-based approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20]</ref>. For instance, OmniSource <ref type="bibr" target="#b9">[10]</ref> extends 3D ConvNet by pre-training on both image and video datasets, and then later fine-tuning on a target dataset. Similarly, UniDual is also tailored to ConvNets and proposes fine-tuning on one image and one video dataset to improve video modeling performance. Unlike OmniSource and UniDual, COVER is tailored to transformer-based architectures. More specifically, OmniSource and UniDual require an additional layer to process image data, whereas COVER directly processes video and image data without any modification to the model architecture. Moreover, OmniSource fine-tunes the learned weights on each down-stream dataset during evaluation. Our approach simplifies this process by building representations that are generalizable across multiple datasets. UniDual fine-tunes the model on a single image and a single video dataset. This differs from COVER that explores learning multiple video datasets simultaneously. Finally, our empirical results suggest COVER outperforms OmniSource and UniDual on several action recognition datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>To motivate our suggested change, we first describe the typical spatio-temporal transformer framework, paying special attention to improving generalisation by pre-training on image data and fine-tuning on video data. Next, we build on this analysis to present co-training image and video for action recognition (COVER). COVER changes the typical training paradigm by leveraging multiple image and video datasets to train a spatio-temporal transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Transformer for Action Recognition</head><p>The action recognition framework is composed of two components, an action recognition model f and a training policy. In this paper, we describe a simple spatio-temporal attention factorized transformer (i.e., TimeSFormer <ref type="bibr" target="#b2">[3]</ref>) as a prototypical action recognition model.</p><p>TimeSFormer TimeSFormer is an extension of Visual Transformer <ref type="bibr" target="#b8">[9]</ref>. Similar to a ViT model, TimeSformer can be reduced to a sequence of self-attention blocks; however rather than using a single spatial self-attention mechanism, TimeSformer augments the self-attention block with a temporal attention mechanism. Analogous to the Transformer model introduced for natural language processing, both spatial and temporal attention mechanisms are formulated as Multi-Head Attention (MHA). MHA takes three sets of input elements, i.e., the key set K, the query set Q, and the value set V , and performs scaled dot-product attention as:</p><formula xml:id="formula_0">MHA(K, Q, V ) = FFN Softmax( Q K ? d ) ? V</formula><p>Here, FFN is a feed-forward neural network and d is the dimension of K and Q. With different choices of K and V , MHA can be categorized as either spatial attention or temporal attention. Spatial attention corresponds to K and V sampled from the same frame and temporal attention corresponds to K and V sampled across the frames of a video clip. Each TimeSFormer block contain one layer of temporal attention and one layer of spatial attention. TimeSFormer takes n video frames x video as input. The frames are first uniformly cropped into a sequence of image patches with size of (n, s, h, w), where s is the number of image patches within one frame. h and w represent the spatial resolution. The image patches are then fed into L TimeSFormer blocks through multiple spatial attention and temporal attention layers. An affine transformation is then applied to the learned representation t video = f (x video ) to attain a probability distribution across all label classes c video = MLP(t video ).</p><p>In this paper, we describe TimeSFormer using ? s and ? t , where ? s represents the parameters within spatial attention layers and ? t represents the parameters within the temporal attention layers. We describe the classification layer as ? MLP . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard Training Paradigm</head><p>where</p><formula xml:id="formula_2">{c image } = MLP(f ({x image }; ? s ))</formula><p>is the classification probability. {x image } and {y image } denote the videos and labels in a mini-batch, and are randomly sampled from the image dataset D image . is the cross-entropy loss function.</p><p>After pre-training, both spatial attention layers and temporal attention layers are finetuned on the target video datasets.</p><formula xml:id="formula_3">(? s , ? t ) = arg min ?s,?t ({y video }, {c video })</formula><p>where (x video , y video ) are sampled from the video dataset D video .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action Recognition Analysis</head><p>A robust learned representation of video data should be descriptive in both the spatial and temporal dimension. Our empirical findings suggest the typical pre-training and finetuning paradigm may limit the model's capacity to construct generalizable representations by fine-tuning on a single, and relatively small, downstream action recognition task.</p><p>Expanding on this last point, it is likely each video dataset incorporates a dataset-specific bias. Applying this hypothesis to two popular video datasets, K400 and SSv2, we find SSv2 focuses on object interaction and relies on complex temporal reasoning to achieve strong performance <ref type="bibr" target="#b1">[2]</ref>. On the other hand, K400 focuses on interactions among humans and objects. Given the strong performance of non-temporal action recognition models on this dataset, complex temporal reasoning may be significantly less important than learning robust representations comprised of spatial information.</p><p>To further analyze the inherent dataset bias hypothesis, we reverse the order of frames in a clip during test-time. We follow the standard paradigm by training a TimeSformer model <ref type="bibr" target="#b2">[3]</ref> using a 224?224 image resolution. We report our findings with both the normal and reversed ordering of frames along the temporal dimension in <ref type="table">Table 2</ref>. Our findings indicate the model may learn strong temporal-based representations when training on SSv2, while temporal information appears less important for K400. For example, the difference in test accuracy on K400 is small at -0.1%, but the difference in test accuracy on SSv2 is -36.2%.</p><p>Another facet of our analysis relates to the limited size and scope of publicly available video datasets. Given the difference in focus of each dataset, the representations learned on one dataset distribution may not generalize to that of other datasets. To analyze this dimension, we conduct an experiment by training a TimeSformer model to achieve strong performance on the Kinetics-400 dataset and then reinitialize the classification layer, freeze the feature extraction layers, and fine-tune the model on SSv2. Our findings indicate this approach would only yield an accuracy of 19.5% on SSv2, and we interpret this result to indicate the feature extraction mechanisms learned by one action recognition model may have difficulty generalizing to other datasets.</p><p>A possible solution to mitigate inherent dataset biases would be to collect a single large-scale video dataset covering a diverse range of actions and events. However, such a collection would be challenging to design and timeconsuming to create. An added layer of complexity relates to deciding the label classes of this dataset, and mapping a single video to one or more classes is non-trivial and would require careful design. A different approach is to learn representations applicable to many disparate action recognition datasets. Rather than learning video representations specific to a single dataset during fine-tuning, we instead suggest learning a single model across many action recognition dataset distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">COVER: Co-train Videos and Images for Action Recognition</head><p>COVER leverages different action recognition dataset distributions, as well as large-scale object recognition, to construct a general-purpose feature-extraction model. We first introduce the mechanism of learning representations suited for multiple video datasets and then describe the process of integrating image data into the fine-tuning paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Co-train Videos</head><p>To learn from N video datasets, we adopt a multi-task learning paradigm and equip the action recognition model f with N classification heads {MLP i } n i=1 . However, pre-training is unchanged. Notably, we adopt the typical pre-training policy of learning all non-temporal parameters on a large-scale object recognition dataset by minimizing the coss entropy loss over Eq. 1.</p><p>In the co-training policy, we follow the pretraining and finetuning strategy. Similar to the standard training policy, the spatial attention layers ? s is learnt by minimizing the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP Layer</head><p>Video Classifier <ref type="figure">Figure 2</ref>. COVER adopts multi-task learning strategy. Each dataset has its own classifier. For the image dataset, we consider images as single frame videos. Therefore, the temporal multi-head attention will not affect the image input. cross entropy loss over Eq. 1.</p><p>During fine-tuning, COVER learns both spatial and temporal attention layers across the samples (</p><formula xml:id="formula_4">x i video , y i video ) ? D i video from N datasets jointly, where (x i video , y i video )</formula><p>is the video and its label sampled from the dataset i.  where w i is the weight for the loss function of the dataset i.</p><p>Jointly learning action recognition feature extractors across multiple video datasets conveys two advantages. First, as the model is directly trained on multiple datasets, the learned video representations are more general and can be directly evaluated on those datasets without additional finetuning. Second, and as emphasized in prior work <ref type="bibr" target="#b2">[3]</ref>, there may be benefits from expanding the scope and quantity of action recognition examples. Attention-based models may easily overfit to a smaller video distribution, thus degrading the generalization of the learned representations. Training on multiple datasets mitigates this challenge by reducing the risk of overfitting. Finally, as indicated in Sect. 3.3.1, certain datasets may focus on different inductive biases of video modeling. For example, one dataset may emphasize the modeling of temporal information while others emphasize spatial representational learning. Jointly learning on both distributions may lead to more robust feature extractors that encode both appearance and motion information to improve performance on action recognition benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Co-train Video and Image Data</head><p>To maintain strong spatial representations, COVER trains a model f on both image and video datasets. Similar to the training policy of transformer-based video models, we first pre-train spatial attention layers ? s using a large object recognition dataset, and then fine-tune the entire model (? s , ? t ) using both video datasets D i video and image datasets D j image as <ref type="figure">Fig. 2</ref>.</p><p>We adapt an object recognition image task to an action recognition video paradigm with minimal modification by considering an image as a video with only one frame. In this context, we can directly create a batch of both image x j image ? D j image and video x i video ? D i video data as input to the TimeSformer model f . With regards to the object recognition task, we obtain classification outputs c j image = MLP(f (x j image ; ? s , ? t )), and for the video datasets, we denote dataset-specific classification outputs as c j video = MLP(f (x j video ; ? s , ? t )). The weighted loss for co-training over both images and videos is</p><formula xml:id="formula_5">image video = i w i video ? ({y i video }, {c i video }) + j w j image ? ({y j image }, {c j image })</formula><p>where w j image and w i video represents the loss weights for the image dataset j and the video dataset i. We minimize image video to optimize parameters ? s and ? t .</p><p>The comparison between the standard training policy and COVER are summarized in <ref type="figure" target="#fig_0">Fig. 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>An ideal video representation should capture both the appearance and motion information from a video. Although video datasets are informational sources for learning motion information, the spatial information contained within a video clip may be limited. This is due to redundancy among frames and the relatively "small" size of video datasets compared to classical image datasets. Therefore, even by simultaneously training a model on many video datasets, the model's capacity to learn appearance information may be hindered. Although image dataset pretraining may provide a good start for obtaining the appearance knowledge, it is possible the robust spatial representations are diluted during fine-tuning on highly spatially redundant video data. Reducing the robustness of learned spatial representations would likely decrease model performance; however, these representations may be maintained by continuing to train object recognition with action recognition during the fine-tuning stage of learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we first present the experiment setup and the implementation details. Then we study the performance of COVER on five large-scale video datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Datasets. We evaluate our approach on 5 challenging video datasets, Kinetics-400 (K400) <ref type="bibr" target="#b13">[14]</ref>, Kinetics-600 (K600) <ref type="bibr" target="#b3">[4]</ref>, Kinetics-700 (K700) <ref type="bibr" target="#b4">[5]</ref>, SomethingSomething-v2 (SSv2) <ref type="bibr" target="#b12">[13]</ref>, and Moments in Time (MiT) <ref type="bibr" target="#b21">[22]</ref>. The Kinetics dataset is a collection of datasets containing three variants, Kinetics-400, Kinetics-600, and Kinetics-700 with 400, 600, and 700 classes respectively. The Kinetics dataset focuses on daily human-object interaction. The SSv2 dataset contains 174 classes with 168K videos for training and 24K videos for evaluation. It contains videos with object-agnostic action. The Moments-in-Time dataset is one of the largest datasets for video understanding. The dataset contains 791K videos for training and 33K videos for evaluation, which cover human actions as well as animal actions. The Moments-in-Time dataset covers a wide range of videos from Youtube to cartoons. For all datasets, we use the standard training and testing splits. We report the results on the testing split for all datasets.</p><p>Implementation details. We conducted experiments using TimeSFormer <ref type="bibr" target="#b2">[3]</ref>, as it achieves the balance between efficiency and performance. The TimeSFormer model contains 24 TimeSFormer blocks. Within each block, there is one temporal multi-head attention layer followed by one spatial multi-head attention layer. Each multi-head attention layer has 16 attention heads with 1024 hidden dimensions in total. Note that our co-training policy could also be applied to other transformer-based approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> without further modification.</p><p>Given a video, we first uniformly sampled 16 frames across the entire video. Then image patches with the resolution of 448?448 are randomly cropped from the sampled frames to form the input. Similarly, when co-training with images, we randomly cropped a 448?448 image patch. We only applied random cropping and random horizontal flip-ping to augment the training data during training. Note that the horizontal flipping is not applied to SSv2 videos, as two of the SSv2 action categories, "Pushing something from left to right" and "Pushing something from right to left", are symmetric. Following TimeSFormer, we evaluate our model by averaging the predictions from 1?3 views of cropping. Specifically, during evaluation, we obtain three spatial crops from video. We didn't do temporal cropping during the evaluation.</p><p>We study the performance under three pre-training image datasets, ImageNet-21k <ref type="bibr" target="#b6">[7]</ref>, JFT-300M <ref type="bibr" target="#b14">[15]</ref>, and JFT-3B <ref type="bibr" target="#b29">[30]</ref>. We finetune the pre-trained model on video and image datasets using a mini-batch of 128. Within each batch, we sampled videos and images from all finetuning datasets. The sampling rate is proportional to the size of the datasets. The model is optimized using SGD with momentum set to 0.9. The model is trained for 20 epochs. The initial learning rate is set as 5e?3. It drops to 5e?4 and 5e?5 at epochs 11 and 15, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main results</head><p>We summarize the COVER performance in <ref type="table">Table 3</ref> and <ref type="table" target="#tab_2">Table 4</ref>. We reported our model performance under three pre-training datasets, ImageNet-21k, JFT-300M, and JFT-3B. For each pre-training setting, we co-train our model on SSv2, MiT, ImageNet, and different versions of Kinetics jointly.</p><p>We compare COVER with the TimeSFormer under the same ImageNet-21k pre-training setting in <ref type="table">Table 3</ref>. With the same architecture, COVER co-trained on multiple datasets achieves 2.4%, 2.3%, and 2.3% improvement on K400, K600, and SSv2, respectively. Compared with ViViT and VidTr, COVER improves K400, K600, K700, and MiT performance by a margin, which verifies the effectiveness of the proposed approach. COVER achieves lower performance than Video SwinTrans, due to its more advanced architecture.</p><p>When pretrained on a larger-scale image dataset, JFT-300M, COVER surpassed the previous best performance by 0.9%, 0.5%, 0.2%, and 3.9% on K400, K600, SSv2, and MiT in top-1 accuracy. Pretraining on an even larger image dataset, JFT-3B, further boosts the top-1 accuracy to 87.2% on K400, 87.9% on K600, 79.8% on K700, 70.9% on SSv2, and 46.1% on MiT. Our results indicate that training methodology for the transformer model is important. Orthogonal to improving model architecture, co-training a simple spatiotemporal transformer with multiple datasets could achieve superior performance.</p><p>COVER didn't alter the architecture or evaluation scheme. Thus, COVER has the same inference speed as TimeS-Former. <ref type="table" target="#tab_0">COVER improves the SoTA on Top-1 accuracy across   Model   Pretrain  Finetune  K400  K600  K700 SSv2</ref> MiT Views</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>Video SwinTrans <ref type="bibr" target="#b19">[20]</ref> ImageNet21k K400 84.9 all datasets. In this section, we conduct ablation studies on the COVER pretrained on JFT-3B to empirically analyze the performance gain of COVER.</p><formula xml:id="formula_6">- - - - 10?5 K600 - 86.1 - - - 10?5 ViViT [2] ImageNet21k K400 81.3 - - - - 4?3 K600 - 83.0 - - - 4?3 SSv2 - - - 65.9 - 4?3 MiT - - - - 38.5 4?3 VidTr [31] ImageNet21k K400 80.5 - - - - 1?3 K700 - - 70.8 - - 1?3 SSv2 - - - 63.0 - 1?3 TimeSFormer [3] ImageNet21k K400 80.7 - - - - 1?3 K600 - 82.2 - - - 1?3 SSv2 - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Co-training with multiple datasets</head><p>The main argument is that co-training over multiple image and video datasets can improve action recognition performance. To verify this argument, we summarize the ablation results in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>We first limit the scope to two datasets, K400 and SSv2. We compare the co-training performance with the results of training independently. Co-training improves the top-1 accuracy on K400 by 1.4% and SSv2 by 2.7%, which indicates that jointly learning K400 and SSv2 could enhance the performance of both tasks. We further include the MiT dataset into co-training. Compared with co-training on just  Loss weight for image and video classification Our cotraining paradigm involves two losses, the image classification loss and the video classification loss. We study the performance under different loss weights in <ref type="table">Table 6</ref>. By increasing the image classification loss weight, we observe that the model is encouraged to learn better appearance repre- sentations. Thus, ImageNet accuracy improves from 86.1% to 86.6%. However, as the model focuses on learning better appearance information, the model's ability to capture motion structure is reduced. We observed 0.2% top-1 accuracy drops on SSv2 and MiT datasets, which indicates it is vital to balance the image and video classification loss weight.</p><p>Transfer learning on other datasets We use transfer learning as a showcase to verify the video representation quality. Specifically, we trained on the source datasets, then finetuned and evaluated on the target dataset. The results are summarized in <ref type="figure" target="#fig_4">Fig. 3</ref>. We first consider K400 as the target dataset. COVER co-trained on SSv2 and MiT improves the top-1 accuracy on K400?K400 by 1.3%, SSv2?K400 by 1.7%, and MiT?K400 by 0.4%. Similarly, we observe that by transferring to SSv2, COVER achieves 2%, 1.8%, and 1.1% improvement over SSv2?SSv2, K400?SSv2, and MiT?SSv2, respectively. Improvement on transfer learning shows that COVER co-trained on multiple datasets could learn better visual representations than the standard training paradigm, which is useful for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present COVER, a training paradigm that jointly learns action recognition and object recognition tasks in a single model for the purpose of constructing a general-purpose action recognition framework. Our analysis indicates it may be beneficial to integrate many video datasets into one multi-task learning paradigm. We highlight the importance of continuing to learn on image data during fine-tuning to maintain robust spatial representations. Our empirical findings suggest COVER can learn a single model which achieves impressive performance across many action recognition datasets without an additional stage of fine-tuning on each downstream application. In particular, COVER sets a new state-of-the-art performance on Kinetics, SSv2 and MiT.</p><p>In the supplementary material, we provide details and experiments omitted from the main text due to the limited space, including:</p><p>1. ? A describes the implementation details for the cotraining on video and image data ( ?3.3.2 in the main text).</p><p>2. ? B provides additional ablation studies on COVER. Specifically, we study the performance under different input configurations and different model variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details of Co-train Video and Image Data</head><p>We provide the details of implementing the co-training on multiple video and image datasets. Without modifying the architecture, we calculate the image and video classification losses by feed-forwarding the images and videos separately.</p><p>We consider the images as single-frame videos. To calculate the image classification loss, we take images as input and obtain the loss ({y j image }, {c j image }). As images are considered as single-frame videos, the key set K, the query set Q, and the value set V in the temporal multi-head attention layer are the same. Temporal multi-head attention will be a feed-forward network.</p><p>After obtaining the image loss, we feed-forward the video to the model f and obtain the video classification loss ({y j video }, {c j video }). The co-training loss for video and image data image video is the weighted average of both image and video classification loss. We calculate the gradients and update the weights based on image video .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments on COVER</head><p>In this section, we reported the ablation studies that are omitted from the main paper due to space limitations. Specifically, we studied COVER performance under different model variants and different input configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model variants</head><p>We first compare the performance under the different sizes of architectures. We consider the TimeSFormer-Base model for comparison. The TimeSFormer-Base architecture is similar to the TimeSFormer-Large model, which contains 12 TimeS-Former blocks. Each block includes one temporal multihead attention layer and one spatial multi-head attention layer. The multi-head attention layer has 12 attention heads with 768 hidden dimensions. We follow the same learning rate scheduling and apply the same data augmentation to train the TimeSFormer-Base model as described in Sect. 4.1 in the main paper.</p><p>We summarize the comparison between TimeSFormer-Large and TimeSFormer-Base model in <ref type="figure" target="#fig_5">Fig. 4</ref>. We observe that with co-training, the large model improves the top-1   Input configuration Next, we analyze the model performance under different input configurations. Here, we follow TimeSFormer <ref type="bibr" target="#b2">[3]</ref> to consider two input setting, standard and high resolution. We sample 8 frames from the video for the standard setting, and the input patch resolution is 224 ? 224.</p><p>For the high-resolution setting, 16 frames are sampled from video. The resolution for the input patch is 448 ? 448. We summarize the results of the model pre-trained on JFT-3B in <ref type="table" target="#tab_5">Table 7</ref>. The model with high resolution constantly improves the performance by a large margin across all benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The comparison between the proposed COVER and the standard training paradigm. The difference is COVER co-trained on multiple image and video datasets during finetuning, while the standard training paradigm only finetuned on one dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>All video samples are processed by the model f via the shared parameters ? s and ? t . The sample representations are then distributed to the appropriate classification head to obtain the classification probability c i video = MLP(f (x i video ; ? s , ? t )). We calculate the training loss for samples in video dataset i as i video = ({y i video }, {c i video }) To optimize the parameters ? s and ? t , we minimize the weighted sum of the loss function across all N datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>? s , ? t ) = arg min ?s,?t i w i ? i video</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of transfer learning the representation learned by COVER and standard training paradigm. A?B means the model is trained on dataset A and then finetuned on dataset B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of base model and large model using COVER pretrained on JFT-3B. The large model constantly improves over the base model on all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Part of this work was conducted while at Google Brain Top-1 accuracy comparison between standard training policy and COVER using TimeSFormer pretrained on ImageNet-21k (I21K). COVER co-trained with multiple image and video datasets achieves better performance on all datasets.</figDesc><table><row><cell>Training Policy</cell><cell cols="2">Standard COVER</cell><cell>?</cell></row><row><cell>Kinetics-400</cell><cell>80.7</cell><cell>83.1</cell><cell>+2.4</cell></row><row><cell>Kinetics-600</cell><cell>82.2</cell><cell>84.5</cell><cell>+2.3</cell></row><row><cell>Kinetics-700</cell><cell>-</cell><cell>74.9</cell><cell>-</cell></row><row><cell>SSv2</cell><cell>62.4</cell><cell>64.7</cell><cell>+2.3</cell></row><row><cell>Moments-in-Time</cell><cell>-</cell><cell>41.5</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Due to the large amount of parameters and the limited size of the video dataset, the Frame order Normal ReversedTable 2. Performance of TimeSFormer trained using 224 ? 224 image resolution and evaluated on normal frame order and reversed frame order. Reversed frame order means the order of frames are reversed during test-time.standard training policy follows a classical pretraining and finetuning approach. The model is first pretrained on a large object recognition image dataset D image and then finetuned on the target downstream video dataset D video . Specifically, during the pre-training stage, the temporal attention layers are all removed. Only the parameters of the spatial attention layers ? s are optimized, by minimizing the training loss ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell>K400</cell><cell>78.1</cell><cell>78.0</cell><cell>-0.1</cell></row><row><cell>SSv2</cell><cell>58.8</cell><cell>22.6</cell><cell>-36.2</cell></row></table><note>s = arg min ?s ({y image }, {c image })</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison with the SoTA pretrained on larger-scale datasets. COVER pretrained on JFT-300M surpassed all SoTA by a margin. COVER pretrained on even larger dataset (JFT-3B) established a new set of SoTA for all datasets. Dash ("-") means the results are not applicable. We note our improvements compared with the previous state-of-the-arts.</figDesc><table><row><cell>62.4</cell><cell>-</cell><cell>1?3</cell></row></table><note>Table 3. Comparison with the SoTA pretrained on ImageNet-21k. Our results COVER is based on TimeSFormer architecture. Comparing with TimeSFormer, COVER achieves significant improvements across all datasets. Comparing with ViViT [2] and VidTr [31], COVER improves performance on K400, K600, K700, and MiT datasets. The views are denoted as # of temporal crops ? # of spatial crops. Dash ("-") means the results are not applicable. We note our improvements compared with TimeSFormer of same sizes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Detailed co-training Top-1 accuracy with model pretrained on JFT-3B. By co-training with more datasets, COVER keeps improve performance on K400, K600, K700, SSv2, and MiT datasets.</figDesc><table><row><cell cols="5">Models finetuned on K400+SSv2+MiT+ImNet</cell></row><row><cell cols="5">w image w video K400 SSv2 MiT ImageNet</cell></row><row><cell>0</cell><cell>1</cell><cell>87.2</cell><cell>70.6</cell><cell>45.9 -</cell></row><row><cell>0.5</cell><cell>1</cell><cell>87.2</cell><cell>70.8</cell><cell>46.1 86.1</cell></row><row><cell>0.75</cell><cell>1</cell><cell>87.2</cell><cell>70.6</cell><cell>45.8 86.6</cell></row><row><cell cols="5">Table 6. Top-1 accuracy of COVER under different image classi-</cell></row><row><cell cols="5">fication loss weights. With larger wimage, the model improves the</cell></row><row><cell cols="5">ImageNet performance however sacrifices video dataset results.</cell></row><row><cell cols="5">two datasets, co-training on all three improves K400 and</cell></row><row><cell cols="5">SSv2 performance by 0.8% and 0.7%, respectively. We</cell></row><row><cell cols="5">observe that co-training with MiT also improves MiT per-</cell></row><row><cell cols="5">formance by 1.0%. Finally, we co-train both image and</cell></row><row><cell cols="5">video datasets together. Adding ImageNet in the co-training</cell></row><row><cell cols="5">datasets further improves SSv2 and MiT by 0.2% and 0.2%,</cell></row><row><cell cols="5">establishing a new SoTA. We observe a similar improvement</cell></row><row><cell cols="5">on the model co-trained with K600+SSv2+MiT+ImageNet</cell></row><row><cell cols="4">and K700+SSv2+MiT+ImageNet.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Comparison of different input settings for co-training on Kinetics, SSv2, and MiT datasets. accuracy by 3.2%, 2.8%, 3.9%, 2.1%, 5.2% on K400, K600, K700, SSv2, and MiT. By enlarging the model capacity, we didn't experience overfitting. Here, we empirically show that COVER could unleash the model performance by leveraging a large amount of training data. With the co-training, the large model shows solid improvement over the base model by a large margin across all datasets.</figDesc><table><row><cell cols="4">a) Models trained on K400+SSv2+MiT+ImNet</cell></row><row><cell>Input setting</cell><cell cols="3">K400 SSv2 MiT</cell></row><row><cell>Standard</cell><cell>85.1</cell><cell>67.4</cell><cell>44.2</cell></row><row><cell>High Resolution</cell><cell>87.2</cell><cell>70.8</cell><cell>46.1</cell></row><row><cell cols="4">(b) Models trained on K600+SSv2+MiT+ImNet</cell></row><row><cell>Input setting</cell><cell cols="3">K600 SSv2 MiT</cell></row><row><cell>Standard</cell><cell>85.6</cell><cell>67.7</cell><cell>44.0</cell></row><row><cell>High Resolution</cell><cell>87.9</cell><cell>70.9</cell><cell>45.9</cell></row><row><cell cols="4">(b) Models trained on K700+SSv2+MiT+ImNet</cell></row><row><cell>Input setting</cell><cell cols="3">K700 SSv2 MiT</cell></row><row><cell>Standard</cell><cell>76.2</cell><cell>67.9</cell><cell>43.9</cell></row><row><cell>High Resolution</cell><cell>79.8</cell><cell>70.6</cell><cell>45.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linagzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11178</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding? NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Omni-sourced webly-supervised learning for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<title level="m">Multiscale vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coot: Cooperative hierarchical transformer for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ging</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Movinets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><forename type="middle">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04632</idno>
		<title level="m">A multi-task benchmark for video-and-language understanding evaluation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Video swin transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tokenlearner: What can 8 learned tokens do for images and videos? NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Scaling vision transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
