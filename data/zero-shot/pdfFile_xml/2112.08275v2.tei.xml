<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SeqFormer: Sequential Transformer for Video Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology 2 Bytedance</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology 2 Bytedance</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology 2 Bytedance</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology 2 Bytedance</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology 2 Bytedance</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SeqFormer: Sequential Transformer for Video Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Video instance segmentation, Video transformer</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present SeqFormer for video instance segmentation. SeqFormer follows the principle of vision transformer that models instance relationships among video frames. Nevertheless, we observe that a stand-alone instance query suffices for capturing a time sequence of instances in a video, but attention mechanisms shall be done with each frame independently. To achieve this, SeqFormer locates an instance in each frame and aggregates temporal information to learn a powerful representation of a video-level instance, which is used to predict the mask sequences on each frame dynamically. Instance tracking is achieved naturally without tracking branches or post-processing. On YouTube-VIS, SeqFormer achieves 47.4 AP with a ResNet-50 backbone and 49.0 AP with a ResNet-101 backbone without bells and whistles. Such achievement significantly exceeds the previous state-of-the-art performance by 4.6 and 4.4, respectively. In addition, integrated with the recently-proposed Swin transformer, SeqFormer achieves a much higher AP of 59.3. We hope SeqFormer could be a strong baseline that fosters future research in video instance segmentation, and in the meantime, advances this field with a more robust, accurate, neat model. The code is available at https://github.com/wjf5203/SeqFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video Instance Segmentation (VIS) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b29">30]</ref> is an emerging vision task that aims to simultaneously perform detection, classification, segmentation, and tracking of object instances in videos. Compared to image instance segmentation <ref type="bibr" target="#b10">[11]</ref>, video instance segmentation is much more challenging since it requires accurate tracking of objects across an entire video.</p><p>Previous VIS algorithms can be roughly divided into two categories. The first mainstream follows the tracking-by-detection paradigm, extending image instance segmentation models with a tracking branch <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref>. These methods first predict candidate detection and segmentation frame-by-frame, and then associate them by classification <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> or re-identification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref> to track the instance through a video. Nevertheless, the tracking process is sensitive to occlusions and motion blur that are common in videos. Another mainstream is to predict clip-level instance masks by taking a video clip <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> or the entire video <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref> as input. It divides a video into multiple overlapping clips and generates mask sequences with clip-by-clip matching on overlapping frames. More recently, VisTR <ref type="bibr" target="#b38">[39]</ref> first adapts transformer <ref type="bibr" target="#b36">[37]</ref> to VIS and uses instance queries to obtain instance sequence from video clips. After that, IFC <ref type="bibr" target="#b13">[14]</ref> improves the performance and efficiency of VisTR by building communications between frames in a transformer encoder.</p><p>In this paper, we present Sequential Transformer (SeqFormer), which follows the principle of vision transformer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref> and models instance relationships among video frames. As in <ref type="bibr" target="#b13">[14]</ref>, we observe that a stand-alone instance query suffices although an object may be of different positions, sizes, shapes, and various appearances. Nevertheless, it is witnessed that the attention process shall be done with each frame independently, so that the model will attend to locations following with the movement of instance through the video. This observation aligns with the conclusion drawn in action recognition <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b28">29]</ref>, where the 1D time domain and 2D space domain have different characteristics and should be handled in a different fashion.</p><p>Considering the movement of an instance in a video, a model is supposed to attend to different spatial locations following the motion of the instance. We decompose the shared instance query into frame-level box queries for the attention mechanism to guarantee that the attention focuses on the same instance on each frame. The box queries are kept on each frame and used to predict the bounding box sequences. Then the features within the bounding boxes are aggregated to refine the box queries on the current frame. By repeating this refinement through decoder layers, SeqFormer locates the instance in each frame in a coarse-to-fine manner, in a similar way to Deformable DETR <ref type="bibr" target="#b48">[49]</ref>.</p><p>However, to mitigate redundant information from non-instance frames, those box queries are aggregated in a weighted manner, where the weights are end to end learned upon the box embeddings. The generated representation, which retains richer object cues, is used to predict the category and generate dynamic convolution weights of mask head. Since the box sequences are predicted and refined in the decoder, SeqFormer naturally and succinctly establishes the association of instances across frames.</p><p>In summary, SeqFormer enjoys the following advantages:</p><p>-SeqFormer is a neat and efficient end-to-end framework. Given an arbitrary long video as input, SeqFormer predicts the classification results, box sequences, and mask sequences in one step without the need for additional tracking branches or hand-craft post-processing. -As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, SeqFormer sets the new state-of-the-art performance on YouTube-VIS 2019 benchmark <ref type="bibr" target="#b44">[45]</ref>. SeqFormer achieves 47.4 AP with a ResNet-50 backbone and 49.0 AP with a ResNet-101 backbone without bells and whistles. Such achievement significantly exceeds the previous state-ofthe-art performance by 4.6 and 4.4, respectively. With a ResNext-101 backbone, SeqFormer achieves 51.2 AP, which is the first time that an algorithm achieves an AP above 50. In addition, integrated with the recently-proposed Swin transformer, SeqFormer achieves a much higher AP of 59.3. -With the query decomposition mechanism, SeqFormer attends to locations following with the movement of instance through the video and learns a powerful representation for instance sequences. -The code and the pre-trained models are publicly available. We hope the SeqFormer, with the idea of making attention follow with the movement of object, could be a strong baseline that fosters future research in video instance segmentation, and in the meantime, advances this field with a more robust, accurate, neat model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image Instance Segmentation Instance Segmentation is the most fundamental and challenging task in computer vision, which aims to detect every instance and segment every pixel respectively in static images. Instance segmentation was dominated by Mask R-CNN architecture <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b12">13]</ref> for a long time, Mask R-CNN <ref type="bibr" target="#b10">[11]</ref> directly introduces fully convolutional mask head to Faster R-CNN <ref type="bibr" target="#b30">[31]</ref> in a multi-task learning manner. Recently, one stage models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35]</ref> emerged as excellent frameworks for instance segmentation. Solo <ref type="bibr" target="#b37">[38]</ref> and CondInst <ref type="bibr" target="#b34">[35]</ref> propose one stage instance segmentation pipeline and achieve comparable performance. CondInst <ref type="bibr" target="#b34">[35]</ref> proposes to dynamically generate the mask head parameters for each instance, which is used to predict the mask of the corresponding instance. QueryInst <ref type="bibr" target="#b7">[8]</ref> proposes a query based instance segmentation framework based on Sparse R-CNN <ref type="bibr" target="#b33">[34]</ref>, which also take advantage of the Dynamic mask head. Dynamic mask head can be efficiently adopted into video segmentation tasks because instances with the same identity on different frames can share the same mask head parameters.</p><p>Video Instance Segmentation. Video instance segmentation is extended from the traditional image instance segmentation, and aims to simultaneously segment and track all object instances in the video sequence. The baseline method Mask-Track R-CNN <ref type="bibr" target="#b44">[45]</ref> is built upon Mask R-CNN <ref type="bibr" target="#b10">[11]</ref> and introduces a tracking head to associate each instance in the video. SipMask <ref type="bibr" target="#b3">[4]</ref> proposes a spatial preservation module to generate spatial coefficients for mask predictions based on the one-stage FCOS <ref type="bibr" target="#b35">[36]</ref>. STMask <ref type="bibr" target="#b16">[17]</ref> proposes a spatial feature calibration to extract features for frame-level instance segmentation on each frame, and further introduces a temporal fusion module to aggregate temporal information from adjacent frames. STEm-Seg [2] models a video clip as a single 3D spatial-temporal volume and enables inference procedure based on clustering. CrossVIS <ref type="bibr" target="#b45">[46]</ref> proposes a learning scheme that uses the instance feature in the current frame to pixel-wisely localize the same instance in other frames. MaskProp <ref type="bibr" target="#b2">[3]</ref> and Propose-Reduce <ref type="bibr" target="#b17">[18]</ref> take advantage of mask propagation, which can achieve high performance, but it is very computationally intensive.</p><p>Transformers. Transformer <ref type="bibr" target="#b36">[37]</ref> was first proposed for the sequence-to-sequence machine translation task and became the basic component in most Natural Language Processing tasks. Recently, Transformer has been successfully applied in many visual tasks such as Object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34]</ref>, segmentation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, tracking <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44]</ref>, video recognition <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23]</ref>. VIT <ref type="bibr" target="#b6">[7]</ref> firstly applies transformer in image recognition and model an image as sequence of patches, which achieves comparable performance with traditional CNN architecture. DETR <ref type="bibr" target="#b4">[5]</ref> proposes a new detection paradigm upon transformers, which simplifies the traditional detection framework and abandons the hand-crafted post-processing module. Deformable DETR <ref type="bibr" target="#b48">[49]</ref> achieves better performance by using local attention and multi-scale feature maps. VisTR <ref type="bibr" target="#b38">[39]</ref> is the first method that adapts Transformer to the VIS task. However, VisTR has a fixed number of input queries hardcoded by video length and maximum number of instances. Each query corresponds to an object on every single frame. In our method, instances with the same identity share a same query, which aggregates information across the video and learn a global feature representation for efficient segmentation. IFC <ref type="bibr" target="#b13">[14]</ref> improves the performance and efficiency of VisTR by building communications between frames in the transformer encoder instead of flatting the space-time features into one dimension, but it still flatten the space-time features for the transformer decoder. Our model is designed to carry out the instance feature capturing independently on different frames, which makes the model attend to locations following with the movement of instance through the video. The overall architecture of SeqFormer. Given the feature maps of input frames, the initial instance query is decomposed into frame-level box queries at the first decoder layer. The box queries are kept on each frame and serve as anchors without interacting with each other. The features extracted by box queries from each frame are aggregated to the instance query after each decoder layer, which is used for predicting dynamic mask head parameters. Then the mask head convolves the encoded feature maps to generate the mask sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The network architecture is visualized in <ref type="figure">Fig. 2</ref>. SeqFormer has a CNN backbone and a transformer encoder for extracting feature maps from each frame independently. Next, a transformer decoder is adapted to locate the instance sequences and generate a video-level instance representation. Finally, three output heads are used for instance classification, instance sequences segmentation, and bounding box prediction, respectively. Backbone. Given an input video x v ? R T ?3?H?W with 3 color channels and T frames of resolution H ? W , the CNN backbone (e.g., ResNet <ref type="bibr" target="#b11">[12]</ref>) extracts feature maps for each frame independently. Transformer Encoder. First, a 1 ? 1 convolution is used to reduce the channel dimension of the all the feature maps to C = 256, creating new feature maps</p><formula xml:id="formula_0">{f ? t } T t=1 , f ? t ? R C?H ? ?W ? , t ? [1, T ].</formula><p>After adding fixed positional encodings <ref type="bibr" target="#b4">[5]</ref>, the transformer encoder performs deformable attention <ref type="bibr" target="#b48">[49]</ref> on the feature maps, resulting in the output feature maps {f t } T t=1 , with the same resolutions as the input. To perform attention mechanisms on each frame independently, we keep the spatial and temporal dimensions of feature maps rather than flattening them into one dimension.</p><p>Query Decompose Transformer Decoder. Given a video, humans can effortlessly identify every instance and associate them through the video, despite the various appearance and changing positions on different frames. If an instance is hard to recognize due to occlusion or motion blur in some frames, humans can still re-identify it through the context information from other frames. In other words, for the same instance on different frames, humans treat them as a whole instead of individuals. This is the crucial difference between video and image instance segmentation. Motivated by this, we propose Query Decompose Transformer Decoder, which aims to learn a more and robust video-level instance representation across frames.</p><p>We introduce a fixed number of learnable embeddings to query the features of the same instance from each frame, termed Instance Queries. Different from the instance queries corresponding to frame-level instances in VisTR <ref type="bibr" target="#b38">[39]</ref>, which has a fixed number of input queries hardcoded by video length and maximum number of instances, our instance queries correspond to video-level instances. Since the changing appearance and position of the instance, the model should attend to different exact spatial locations of each frame. To achieve this goal, we propose to decompose the instance query into T frame-specific box queries, each of which serves as an anchor for retrieving and locating features on the corresponding frame.</p><p>At the first decoder layer, an instance query I q ? R C is used to query the instance features on features maps of each frame independently:</p><formula xml:id="formula_1">B 1 t = DeformAttn(I q , f t ),<label>(1)</label></formula><p>where B 1 t ? R C is the box query on frame t from the 1-st decoder layer, and DeformAttn indicates deformable attention module in <ref type="bibr" target="#b48">[49]</ref> . Given a query element and the frame feature map f t , deformable attention only attends to a small set of key sampling points. At the l-th (l &gt; 1) layer, the box query B l?1 t from last layer is used as input:</p><formula xml:id="formula_2">B l t = DeformAttn(B l?1 t , f t ),<label>(2)</label></formula><p>and the instance query aggregates the temporal features by a weighted sum of all the box queries at the end of every decoder layers, where the weights are end to end learned upon the box embedding:</p><formula xml:id="formula_3">I l q = T t=1 B l t ? FC(B l t ) T t=1 FC(B l t ) + I l?1 q .<label>(3)</label></formula><p>After N d decoder layers, we get an instance query and T box queries for each instance. The instance query is a shared video-level instance representation, and the box query contains the position information for predicting the bound box on each frame. We define the instance query I N d q and box queries {B N d t } T t=1 from the last layer of decoder as output instance embedding and box embeddings {BE t } T t=1 , BE t ? R N ?d . Output Heads. As shown in <ref type="figure">Fig. 2</ref>, we add mask head, box head, class head on the top of the decoder outputs. A linear projection acts as the class head to produce the classification results. Given the instance embedding from the transformer decoder with index ?(i), class head output a class probability of class c i (which may be ?) asp ?(i) (c i ) .</p><p>The box head is a 3-layer feed forward network (FFN) with ReLU activation function and a linear projection layer. For BE t of each frame, the FFN predicts the normalized center coordinates, height and width of the box w.r.t. the frame. Thus, for the instance with index ?(i), we denote the predicted box sequence a?</p><formula xml:id="formula_4">b ?(i) = {b (?(i),1) ,b (?(i),2) , ...,b (?(i),T ) }.</formula><p>As for mask head, we leverage dynamic convolution <ref type="bibr" target="#b34">[35]</ref> as our mask head. The output instance embedding of decoder contains the information of instance on all frames, thus it can be regarded as a more robust instance representation. We can use instance embedding to efficiently generate the entire mask sequences. First, a 3-layer FFN encodes the instance embedding into parameters ? i of mask head with index ?(i), which has three 1 ? 1 convolution layers. The instances with the same identity on different frames share the same mask head parameters, which makes the segmentation very efficient. Each convolution layer has 8 channels and uses ReLU as the activation function except for the last one, following <ref type="bibr" target="#b34">[35]</ref>. As shown in <ref type="figure">Fig. 2</ref>, there is a mask branch that provides the feature maps for mask head to predict instance masks. We employ an FPN-like architecture as the mask branch to make use of multi-scale feature maps from transformer encoder and generate feature maps sequences {F </p><formula xml:id="formula_5">} T t=1 , F t mask ? R 10? H 8 ? W 8 .</formula><p>The mask head performs convolution on these high-resolution sequence feature maps F t mask to predict the mask sequences:</p><formula xml:id="formula_6">{m t i } T t=1 = {MaskHead(F t mask , ? i )} T t=1 ,<label>(4)</label></formula><p>where MaskHead performs three-layer 1 ? 1 convolution on given feature maps with the kernels reshaped from ?. By sharing the same mask head parameters for instances with the same identity on different frames, our method can efficiently perform instance segmentation on each frame. Similar to DETR <ref type="bibr" target="#b4">[5]</ref>, we add output heads and Hungarian loss after each decoder layer as an auxiliary loss to supervise the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance Sequences Matching and Loss</head><p>Our method predicts a fixed-size set of N predictions in a single pass through the decoder, and N is set to be significantly larger than the number of instances in a video. To train our network, we first need to find a bipartite graph matching between the prediction and the ground truth. Let y denotes the ground truth set of video-level instance, and? i = {? i } N i=1 denotes the predicted instance set. Each element i of the ground truth set can be seen as</p><formula xml:id="formula_7">y i = {c i , (b i,1 , b i,2 , ..., b i,T )},</formula><p>where c i is the target class label including ?, and b i,t ? [0, 1] 4 is a vector that defines ground truth bounding box center coordinates and its relative height and width in the frame t. For the predictions of instance with index ?(i), we take the output of class headp ?(i) (c i ) and predicted bounding boxb ?(i) . Then we define the pair-wise matching cost between ground truth y i and a prediction with index ?(i).</p><formula xml:id="formula_8">L match (y i ,? ?(i) ) = ?p ?(i) (c i ) + L box (b i ,b ?(i) ),<label>(5)</label></formula><p>where c i ? = ?. Note that Eq. 5 does not consider the similarity between mask prediction and mask ground truth, as such mask-level comparison is computationally expensive. To find the best assignment of a ground truth to a prediction, we search for a permutation of N elements ? ? S n with the lowest cost:</p><formula xml:id="formula_9">? = arg min ??Sn N i L match (y i ,? ?(i) ).<label>(6)</label></formula><p>Following prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>, the optimal assignment is computed with the Hungarian algorithm <ref type="bibr" target="#b14">[15]</ref>. Given the optimal assignment? , we use Hungarian loss for all matched pairs to train our network:</p><formula xml:id="formula_10">L Hung (y,?) = N i=1 ? logp? (i) (c i ) + 1 {ci? =?} L box (b i ,b?(i)) +1 {ci? =?} L mask (m i ,m?(i)) .<label>(7)</label></formula><p>For L box , we use a linear combination of the L 1 loss and the generalized IoU loss <ref type="bibr" target="#b31">[32]</ref>. The mask sequences {m t i } T t=1 from mask head with <ref type="bibr">1 8</ref> of the video resolution which may loss some details, thus we upsample the predicted mask to <ref type="bibr">1 4</ref> of the video resolution, and downsample the ground truth mask to the same resolution for mask loss, following <ref type="bibr" target="#b34">[35]</ref>. The mask loss L mask is defined as a combination of the Dice <ref type="bibr" target="#b26">[27]</ref> and Focal loss <ref type="bibr" target="#b18">[19]</ref>. We calculate box loss and mask loss on each frame and take the average for Hungarian loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>We evaluate our method on YouTube-VIS 2019 <ref type="bibr" target="#b44">[45]</ref> and YouTube-VIS 2021 <ref type="bibr" target="#b42">[43]</ref> datasets. YouTube-VIS 2019 is the first and largest dataset for video instance segmentation, which contains 2238 training, 302 validation, and 343 test highresolution YouTube video clips. It has a 40-category label set and 131k highquality instance masks. In each video, objects with bounding boxes and masks are labeled every five frames. YouTube-VIS 2021 is an improved and extended version of YouTube-VIS 2019 dataset, it contains 3,859 high-resolution videos and 232k instance annotations. The newly added videos in the dataset include more instances and frames.</p><p>Video instance segmentation is evaluated by the metrics of average precision (AP) and average recall (AR). Different from image instance segmentation, each instance in a video contains a sequence of masks. To evaluate the spatio-temporal consistency of the predicted mask sequences, the IoU computation is carried out in the spatial-temporal domain. This requires a model not only to obtain accurate segmentation and classification results at frame-level but also to track instance masks between frames accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Model settings. ResNet-50 <ref type="bibr" target="#b11">[12]</ref> is used as our backbone network unless otherwise specified. Similar to <ref type="bibr" target="#b48">[49]</ref>, we use the features from the last three stages as {C3, C4, C5} in ResNet, which correspond to the feature maps with strides {8, 16, 32}. And adding the lowest resolution feature map C6 obtained via a 3 ? 3 stride 2 convolution on the C5. We set sampled key numbers K=4 and eight attention heads for deformable attention modules. We use six encoder and six decoder layers of hidden dimension 256 for the transformer, and the number of instance queries is set to 300.</p><p>Training. We used AdamW <ref type="bibr" target="#b24">[25]</ref> optimizer with base learning rate of 2 ? 10 ?4 , ?1 = 0.9 , ?2 = 0.999, and weight decay of 10 ?4 . Learning rates of the backbone and linear projections used for deformable attention modules are multiplied by a factor of 0.1. We first pre-train the model on COCO <ref type="bibr" target="#b19">[20]</ref> by setting the number of input frames T = 1. Given the pretrained weights, we train our models on the YouTube-VIS dataset with input frames T = 5 sampled from the same video.</p><p>The training data of the YouTube-VIS dataset is not sufficient, which makes a model prone to overfitting. To address this problem, we adopt 80K training images in the COCO for compensation, following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>. We only use the images with 20 overlapping categories in COCO and augment them with ?10 ? rotation to generate a five-frame pseudo video. We train our model on the mixed dataset including COCO and the video dataset for 12 epochs, and the learning rate is decayed at the 6-th and 10-th epoch by a factor of 0.1. The input frame sizes are downsampled so that the longest side is at most 768 pixels. The model is implemented with PyTorch-1.7 and is trained on 8 V100 GPUs of 32G RAM, with 2 video clips per GPU. Inference. SeqFormer is able to model a video of arbitrary length without grouping frames into subsequences. We take the whole video as input during inference, which is downscaled to 360p, following MaskTrack R-CNN <ref type="bibr" target="#b44">[45]</ref>. Seq-Former learns a video-level instance representation used for dynamic segmentation on each frame and classification, and the box sequences are generated by the decoder. Thus, no post-processing is needed for associating instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>The comparison of SeqFormer with previous state-of-the-art methods on YouTube-VIS 2019 are listed in <ref type="table">Table 1</ref>. MaskProp <ref type="bibr" target="#b2">[3]</ref> and ProposeReduce <ref type="bibr" target="#b17">[18]</ref> are the state-of-the-art methods, which take a strong backbone to extract spatial <ref type="table">Table 1</ref>. Quantitative results of video instance segmentation on YouTube-VIS 2019 validation set. The result with superscript " ?" is obtained without coco joint training. The best results with the same backbone are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Method Params FPS AP AP50 AP75 AR1 AR10 features and use mask propagation to improve the segmentation and tracking, but suffer from low inference speed. We list the methods with different backbones for fair comparison. It can be observed that SeqFormer significantly surpasses all the previous best reported results by at least 4 AP with the same backbone. Training our model with coco pseudo videos improves the AP from 45.1 to 47.4. SeqFormer with a ResNet-50 backbone can even achieve competitive performance against state-of-the-art methods with a ResNeXt-101 backbone. By adopting Swin transformer <ref type="bibr" target="#b23">[24]</ref> as our backbone without further modifications, SeqFormer can first achieve 59.3 AP on this benchmark, outperforming the best previous results by a large margin of 11.7 AP. To understand the runtime efficiency, we measure FPS of SeqFormer excluding the data loading process of multiple images on NVIDIA Tesla V100. With an input size of 360p and a ResNet-50 backbone on YouTube-VIS, the inference FPS is 72.3. While surpassing the stateof-the-art AP by a large margin, SeqFormer is the second fast one following IFC. An example of qualitative comparison with previous meth- ods is given in <ref type="figure">Fig. 3</ref>, the mask predictions of SeqFormer are more stable over time. Please refer to the Sup. Mat. for more qualitative results. We also evaluate our approach on the recently introduced YouTube-VIS 2021 dataset, which is a more challenging dataset with more videos and a higher number of instances and frames. As shown in <ref type="table" target="#tab_1">Table 2</ref>, SeqFormer achieves 40.5 AP with a ResNet-50 backbone, surpassing previous methods by 3.9 AP. We believe that our effective method will serve as a strong baseline on these benchmarks and facilitate future research in video instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>This section conducts extensive ablation experiments to study the effects of different settings in our proposed method. All the ablation experiments are conducted with the ResNet-50 backbone and training on YouTube-VIS 2019 dataset rather than the mixed dataset. Instance query decomposition. Instance query decomposition plays an important role in our method. Since an instance may have different positions on each frame, the iterative refinement of the spatial sampling region should be performed independently on each frame. To keep the temporal consistency of instances, we use the temporal-shared instance query for deformable attention and get box queries for each frame. The box queries will be kept through all the decoder layers and serve as frame anchors for the same instance. Experiments of models without box queries and using the shared instance query for each decoder layer are presented in <ref type="table" target="#tab_2">Table 3</ref>. The model without query decomposition manages to achieve only 34.1 AP. It is because the query controls the sampling region of deformable attention. Using the same instance query for each frame will result in the same spatial sampling region on each frame, as shown in <ref type="figure" target="#fig_3">Fig. 4 (a)</ref>, which is inaccurate and insufficient for video-level instance representation. We further visualize the sampling points of the second and the last decoder layers in <ref type="figure" target="#fig_3">Fig. 4  (b)</ref> and (c). The box queries decoupled from instance query serve as anchors for locating features and iteratively refining the sampling region on the current frame. It can be seen that SeqFormer attends to locations following with the movement of instance through the video in a coarse-to-fine manner. Please refer to the Sup. Mat. for more visualization of sampling points.</p><p>Spatial temporal dimensions. Previous transformer-based methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b13">14]</ref> flatten the spatial and temporal dimensions of video features into one dimension for the transformer decoder. We argue that the temporal dimension should not be flattened with spatial dimensions, since it was recognized that the 2D space domain and 1D time domain have different characteristics and should be intuitively handled in a different way <ref type="bibr" target="#b46">[47]</ref>. Thus, we retain the complete 3D spatio-temporal dimensions and perform explicit region sampling and information aggregation on all frames. In this experiment, we study the effect of this architecture by replacing deformable transformer with vanilla transformer and flattening the spatial and temporal dimensions, termed as 'flatten' in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>For fair comparison, we use single-scale deformable attention as the baseline, termed as 'single-scale', which use the same scale feature map with 'flatten', the default setting termed as 'multi-scale'. By keeping spatial-temporal dimensions of video features, the AP increased from 35.1 to 42.5. The use of multi-scale feature maps can only improve 2.6 AP, which proves that the success of our method mainly comes from the preservation of the temporal dimension and the explicit spatial sampling.  Aggregation of temporal information. The frame-level box queries and the predicted boxes can align the instance features from all frames, there are several ways to aggregate the aligned features into the instance query. We conduct an experiment to evaluate the different aggregation ways for these features, as shown in <ref type="table">Table 5</ref>. In the 'sum' setting, the features from different frames are directly added together as the instance feature of this decoder layer. In the 'average' setting, the feature on each frame is averaged as the instance feature. In the 'weighted-sum' setting, we apply a softmax layer and a fully-connected layer on box embeddings to get the weights of each frame, and the features are aggregated in a weighted sum in Eq. 3. The result is 30.6 AP and 43.2 AP for 'sum' and 'average' settings respectively. Direct summation will cause the value to be unstable with different frame numbers. Since some instances only appear in a few frames, directly averaging features from all frames may cause the information to be diluted. Please refer to the Sup. Mat. for more details and visualization of different frame weights. Robust instance representation. Our decoder explicitly aligns and aggregates the information from each frame to learn a video-level instance representation. In this experiment, we try to generate instance representation with fewer frames. We use the instance representation to generate a mask head and apply the mask head on each frame to get the mask sequences, as shown in <ref type="table">Table 6</ref>. Surprisingly, with only one frame as input, the generated mask head can produce a competitive result of 38.1 AP. With five frames as input, the performance is only 0.5 AP worse than taking all frames as input. This result shows that the mask head learned by our method can generalize well to unseen frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed an effective transformer architecture for video instance segmentation, named SeqFormer, which performs attention mechanisms on each frame independently and learns a shared powerful instance query for each video-level instance. With the proposed instance query decomposition, our network can align the instance features and naturally tackle the instance tracking without additional tracking branches or post-processing. We demonstrated that our method surpasses all state-of-the-art methods by a large margin. We believe that our neat and efficient approach will serve as a strong baseline for future research in video instance segmentation. In <ref type="figure" target="#fig_4">Fig. 5</ref>, we visualize the results of SeqFormer with four challenging cases. It can be seen that SeqFormer can handle these situations well. In <ref type="figure" target="#fig_5">Fig. 6</ref>, we show more qualitative results of the intermediate attention of transformer decoder. Since the same initial instance query is used to predict sampling points for each frame in the first decoder layer (Eq.1), the distribution of sampling points on each frame is the same in <ref type="figure" target="#fig_5">Fig. 6 (a) and (d)</ref>. After that, the initial instance query is decomposed into frame-level box queries that are kept and maintained independently on each frame. Starting from the second layer of the SeqFormer decoder, the box query is used to predict the sampling points of the current frame, and the sampled features are used to refine the box query for the next decoder layer. By doing so, SeqFormer attends to different spatial locations following the motion of the instance in a coarse-to-fine manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Aggregation of Temporal Information</head><p>SeqFormer is able to attend to different spatial locations following the motion of the instance. The aligned features are aggregated into an instance query to generate a video-level instance representation. However, an instance may not appear in every frame due to occlusion and camera motion. The features from frames without instance are useless or even harmful. To address this, Se-qFormer aggregates temporal features in a weighted manner, where the weights are learned upon the box queries in Eq.3. We visualize the learned weights and the corresponding frames in <ref type="figure" target="#fig_6">Fig. 7</ref>. It can be seen that the features from frames without instance have lower weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Qualitative Comparisons</head><p>We provide some qualitative comparisons with other methods in <ref type="figure" target="#fig_7">Fig. 8</ref>, the mask predictions of SeqFormer are more stable over time. More video results and comparisons can be found in the rest of the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Clip Matching</head><p>Our model can be extended to per-clip model through clip matching algorithm to handle long videos. Specifically, we divide long videos into clips with overlapping frames, and match clip-level instance masks by calculate the matching scores which are space-time soft IoU of overlapping frames, following IFC. We evaluate our method by varying the length of clips in <ref type="table" target="#tab_5">Table 7</ref>. Our method still achieves competitive performance but slightly worse when evaluating in the clip-wise manner. This manner makes our method handle very long videos with limited computational resources and have a wider range of application scenarios.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Performance vs. Model Size. All results are reported with single model and single-scale inference. SeqFormer significantly outperforms the previous method with similar parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. The overall architecture of SeqFormer. Given the feature maps of input frames, the initial instance query is decomposed into frame-level box queries at the first decoder layer. The box queries are kept on each frame and serve as anchors without interacting with each other. The features extracted by box queries from each frame are aggregated to the instance query after each decoder layer, which is used for predicting dynamic mask head parameters. Then the mask head convolves the encoded feature maps to generate the mask sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 8</head><label>1</label><figDesc>of the input resolution and have 8 channels for each frame independently. Then the feature mapF t mask is concatenated with a map of the relative coordinates from center ofb (?(i),t) in corresponding frames to provide a location cue for predicting the instance mask. Thus we get the {F t mask</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The sampling points from the first decoder layer is shown in (a), which is coarse and inaccurate. The refined accurate sampling points from the second and last decoder layer are shown in (b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of SeqFormer on the YouTube-VIS 2019 validation dataset. The first row shows the instances with various poses. The second row shows the case of a lot of similar instances that are close together with overlapping. The third row shows the situation where an instance reappears after being occluded while in motion. The last row shows an instance severely occluded by the other instance. The same colors depict the mask sequences of the same instances A.1 Visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of attention. We draw the sampling points that the deformable attention attends to. The four frames in each row are from the same video. Each sampling point is marked as a filled circle whose color indicates its corresponding instance identity.(a) and (d) show the sampling points from the first decoder layer. (b) and (e) show the sampling points from the second decoder layer. (c) and (f) show the sampling points from the last decoder layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of the normalized softmax weights and the corresponding frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative comparisons with other methods on YouTube-VIS 2019. All methods use ResNet-50 backbone. The three frames in each row are from the same video. The mask predictions of SeqFormer are more stable over time. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of video instance segmentation on YouTube-VIS 2021 validation set. The best results with the same backbone are in bold.</figDesc><table><row><cell cols="2">Backbone Method</cell><cell cols="3">AP AP50 AP75 AR1 AR10</cell></row><row><cell></cell><cell cols="4">MaskTrack R-CNN [45] 28.6 48.9 29.6 26.5 33.8</cell></row><row><cell></cell><cell>SipMask [4]</cell><cell cols="3">31.7 52.5 34.0 30.8 37.8</cell></row><row><cell>ResNet-50</cell><cell>CrossVIS [46]</cell><cell cols="3">34.2 54.4 37.9 30.4 38.2</cell></row><row><cell></cell><cell>IFC [14]</cell><cell>36.6 57.9 39.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SeqFormer</cell><cell cols="3">40.5 62.4 43.7 36.1 48.1</cell></row><row><cell>Swin-L</cell><cell>SeqFormer</cell><cell cols="3">51.8 74.6 58.2 42.8 58.1</cell></row><row><cell>VisTR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>IFC</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SeqFormer</cell><cell></cell><cell></cell><cell></cell></row></table><note>Fig. 3. Qualitative comparisons with other methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Instance query decomposition. Decomposing instance query into frame-level box queries is critical for SeqFormer.</figDesc><table><row><cell cols="2">Decompose AP AP50 AP75 AR1 AR10</cell></row><row><cell>w/o</cell><cell>34.1 53.7 34.9 34.8 40.9</cell></row><row><cell>w</cell><cell>45.1 66.9 50.5 45.6 54.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Spatial and temporal dimensions. Keeping spatial-temporal feature dimensions and performing instance feature capture independently on different frames brings about 7.4 AP gains. Multi-scale feature maps can further bring 2.6 AP.</figDesc><table><row><cell>Feature</cell><cell>AP AP50 AP75 AR1 AR10</cell></row><row><cell>flatten</cell><cell>35.1 56.8 35.6 38.1 41.8</cell></row><row><cell cols="2">single-scale 42.5 64.6 46.5 41.5 50.9</cell></row><row><cell cols="2">multi-scale 45.1 66.9 50.5 45.6 54.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Temporal information aggregation. Weighted sum brings a performance gain of 1.9 in AP. Aggregation AP AP50 AP75 AR1 AR10 sum 30.6 44.5 34.3 37.2 45.0 average 43.2 65.2 48.5 43.4 52.8 weighted-sum 45.1 66.9 50.5 45.6 54.6 Fewer frames for instance representation.We evenly sample fewer frames from a video to generate the mask head.</figDesc><table><row><cell cols="2">Frames AP AP50 AP75 AR1 AR10</cell></row><row><cell>1</cell><cell>38.1 58.3 41.3 38.7 47.5</cell></row><row><cell>3</cell><cell>43.4 65.4 47.6 42.4 51.3</cell></row><row><cell>5</cell><cell>44.6 66.5 49.7 44.8 54.6</cell></row><row><cell>10</cell><cell>44.7 66.9 49.5 44.3 53.5</cell></row><row><cell>all</cell><cell>45.1 66.9 50.5 45.6 54.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Evaluating SeqFormer in a clip-wise manner.</figDesc><table><row><cell cols="2">Clip Length Whole 20</cell><cell>15</cell><cell>10</cell></row><row><cell>AP</cell><cell cols="3">45.1 44.8 43.6 42.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Xiaoding Yuan for the support and discussions about implementation details. We thank the anonymous reviewers for their efforts and valuable feedback to improve our work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatiotemporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020) 2, 4</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020) 2, 4</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020) 1, 4</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020) 2, 4, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Instances as queries</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03400</idno>
		<title level="m">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Msn: Efficient online mask selection network for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10452</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video instance segmentation using interframe communication transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS (2021) 2, 4</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Uniformer: Unified transformer for efficient spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04676</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Spatial feature calibration and temporal fusion for effective one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Video instance segmentation with a proposereduce paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13746</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sg-net: Spatial granularity network for onestage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end temporal action detection with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02702</idno>
		<title level="m">TrackFormer: Multiobject tracking with transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">fourth international conference on 3D vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06649</idno>
		<title level="m">1st place solution for youtubevos challenge 2021: Video instance segmentation</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05392</idno>
		<title level="m">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Occluded video instance segmentation: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15460</idno>
		<title level="m">Transtrack: Multiple object tracking with transformer</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2020) 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2021) 2, 4, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Language as queries for referring video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://youtube-vos.org/dataset/vis/8" />
		<title level="m">Youtubevis dataset 2021 version</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal transformer for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05970</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Trajectory convolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
