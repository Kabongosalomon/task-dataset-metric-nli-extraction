<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Fusion Network for RGB-D Semantic Segmentation Incorporating Unexpected Obstacle Detection for Road-driving Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Real-time Fusion Network for RGB-D Semantic Segmentation Incorporating Unexpected Obstacle Detection for Road-driving Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic scene understanding</term>
					<term>RGB-D fusion</term>
					<term>obstacle detection</term>
					<term>autonomous driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation has made striking progress due to the success of deep convolutional neural networks. Considering the demands of autonomous driving, real-time semantic segmentation has become a research hotspot these years. However, few real-time RGB-D fusion semantic segmentation studies are carried out despite readily accessible depth information nowadays. In this paper, we propose a real-time fusion semantic segmentation network termed RFNet that effectively exploits complementary cross-modal information. Building on an efficient network architecture, RFNet is capable of running swiftly, which satisfies autonomous vehicles applications. Multidataset training is leveraged to incorporate unexpected small obstacle detection, enriching the recognizable classes required to face unforeseen hazards in the real world. A comprehensive set of experiments demonstrates the effectiveness of our framework. On Cityscapes, Our method outperforms previous state-of-the-art semantic segmenters, with excellent accuracy and 22Hz inference speed at the full 2048?1024 resolution, outperforming most existing RGB-D networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E NVIRONMENT perception is a significant task for intelligent robots and systems in object classification, autonomous driving, and localization. In recent years, this field has witnessed remarkable progress thanks to deep Convolutional Neural Networks (CNNs) based semantic segmentation methods <ref type="bibr" target="#b0">[1]</ref> [2] <ref type="bibr" target="#b2">[3]</ref>. As an environment perception method to be applied in autonomous driving, safety, accuracy, and efficiency are the vital factors in semantic segmentation for upper-level navigational tasks. However, unexpected road hazards like debris, bricks, stones, and cargos become the most dangerous and difficult elements to detect in autonomous driving imagery. According to the AAA Foundation for Traffic Safety, debris on the road led to more than 200,000 crashes on U.S. roadways between 2011 and 2014, resulting in approximately 39,000 injuries and more than 500 deaths <ref type="bibr" target="#b3">[4]</ref>. These obstacles are generally small in size but not fixed in shape and type, making detecting them a challenging subject that *This work has been partially funded through the AccessibleMaps project. This work has been supported by Hangzhou SurImage Technology Co., Ltd. <ref type="bibr" target="#b0">1</ref>   has aroused interest among the robotics and computer vision community. For these reasons, it is desirable to develop a semantic segmentation based method incorporating pixel-wise unexpected obstacle detection.</p><p>Compared to expensive 3D sensors like LIDAR, RGB camera is a much lower cost solution with higher resolution. Based on RGB stereo camera, there have been some attempts to detect small obstacles with the help of geometry cues and CNNs <ref type="bibr" target="#b4">[5]</ref>, but only relying on apparent information in the RGB image alone is not sufficient for obstacle detection <ref type="bibr" target="#b5">[6]</ref>. For example, manhole covers, and small obstacles can both cause gradient changes in the image. The traversable areas and obstacles in the depth map vary vastly in depth maps. Depth maps contain more location and contour information that can be used as a critical indicator of objects in realworld driving scenarios. In this sense, appropriately combining of appearance and depth is promising to improving the performance <ref type="bibr" target="#b5">[6]</ref> [7] <ref type="bibr" target="#b8">[8]</ref>. But most accuracy-oriented RGB-D semantic segmentation works focus on indoor scenes <ref type="bibr" target="#b6">[7]</ref> [9] <ref type="bibr" target="#b11">[10]</ref>, without assuring a fast inference speed that is necessary for autonomous vehicles.</p><p>On the other hand, the outstanding capacity of CNNs is based on a large amount of annotated data, especially for semantic segmentation tasks <ref type="bibr" target="#b12">[11]</ref>. Current mainstream autonomous driving datasets generally assume only some fixed categories of objects in the scene, ignoring unforeseen hazards like unexpected small obstacles in the real world. For instance, Cityscapes <ref type="bibr" target="#b13">[12]</ref> only divides objects to 19 classes, without defining any unexpected class. Multi-source training has been proven to effectively increase recognizable semantics without having to relabel the dataset <ref type="bibr" target="#b14">[13]</ref>. However, previous multi-source training frameworks have only considered the heterogeneity in the label hierarchies of RGB data, missing the opportunity to leverage complementary depth information from different sources.</p><p>In this paper, we propose a framework that combines RGB-D semantic segmentation and obstacle detection. RFNet, a real-time fusion network for RGB-D semantic segmentation is elaborately designed. With our multi-dataset training strategy, our framework is able to classify 19 categories in Cityscapes incorporating pixel-wise unexpected small obstacle detection (see <ref type="figure" target="#fig_0">Figure 1</ref>). An extensive set of experiments shows the effectiveness and efficiency of the proposed framework for the semantic segmentation task. The main contributions of our work are threefold:</p><p>? We propose RFNet, a real-time fusion network for RGB-D semantic segmentation incorporating detection of unexpected obstacle, which achieves higher accuracy with fast inference compared to other state-of-the-art methods on the Cityscapes dataset. ? Depth complementary features are efficiently extracted in the proposed network, which improves the accuracy compared to the single RGB-stream architecture. ? Multi-dataset training and the depth stream in the architecture enable the network to work remarkably effective in detecting unexpected small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RGB-D Semantic Segmentation</head><p>High-quality dense depth maps from depth sensors like Kinect and RealSense boost the development of indoor semantic segmentation. Early attempt like <ref type="bibr" target="#b15">[14]</ref> simply concatenated RGB and depth channels as a four-channel input, and fed it into a conventional RGB modal network. However, such method can not exploit complementary information from depth maps in most times <ref type="bibr" target="#b16">[15]</ref>. Wang et al. <ref type="bibr" target="#b17">[16]</ref> introduced depthaware CNN which augmented conventional CNN with a depth similarity term, but it only works well with dense depth maps. Schneider et al. <ref type="bibr" target="#b18">[17]</ref> designed a lightweight depth branch with GoogLeNet <ref type="bibr" target="#b19">[18]</ref> and explored different points for merging the depth and RGB networks. In FuseNet <ref type="bibr" target="#b10">[9]</ref> and RedNet <ref type="bibr" target="#b11">[10]</ref>, RGB images and depth maps are fed into two separate neural network branches respectively, which are fused before the upsampling. In <ref type="bibr" target="#b20">[19]</ref>, depth maps are pre-processed as HHA features that encode horizontal disparity, height above ground and angle. Park et al. <ref type="bibr" target="#b21">[20]</ref> proposed a multilevel feature fusion scheme by introducing multi-modal feature fusion to the RefineNet blocks. ACNet <ref type="bibr" target="#b6">[7]</ref> achieved a breakthrough by proposing an attention complementary module to exploit complementary depth information efficiently. These studies prove that RGB-D semantic segmentation can achieve better segmentation results than single RGB-based methods. The major reason for this is that compared to the single RGB images, depth maps contain more location and contour information that benefit the context-critical semantic segmentation.</p><p>Compared to indoor scene depth maps from Kinect or RealSense, outdoor traffic scene depth maps are much more sparse. Li et al. <ref type="bibr" target="#b22">[21]</ref> simply stacked smoothed depth maps with RGB images as a 4-channel input. Based on VGG <ref type="bibr" target="#b23">[22]</ref>, Kreso et al. <ref type="bibr" target="#b24">[23]</ref> introduced a scale selection layer and used the depth maps as a guidance to produce a scale-invariant representation to free appearance from the scale. In <ref type="bibr" target="#b25">[24]</ref>, luminance information is used for depth map enhancement. Most recently, Deng et al. <ref type="bibr" target="#b26">[25]</ref> proposed a Residual Fusion Block (RFB) to formulate the interdependencies of the encoders to extract cross-modal features based on ERFNet <ref type="bibr" target="#b27">[26]</ref>. Low latency is crucial in autonomous driving applications, but most of these methods cannot meet the real-time constraint. In this paper, we propose a real-time fusion network to achieve swift inference while retaining a highly competitive performance among the state of the art for RGB-D segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unexpected Obstacle Detection for Self-driving Cars</head><p>Detecting unexpected small but potentially hazardous obstacles on the road is a vital task for autonomous driving, and this subject has always been a research hotspot. Generally these methods for detecting and localizing generic obstacles are based on stereo cameras integrated on self-driving cars. Among these methods, most are based on the generic geometric criteria. The Stixel algorithm <ref type="bibr" target="#b28">[27]</ref> represents obstacles with a set of rectangular vertical obstacle segments, providing a robust representation of the 3D scene. Geometric point cluster methods like <ref type="bibr" target="#b29">[28]</ref> and <ref type="bibr" target="#b30">[29]</ref> exploit geometric relation between 3D points to detect and cluster obstacle points.</p><p>Because of the superiority in making use of visual appearance and context of images, CNNs are adopted in contemporary researches. Ramos et al. <ref type="bibr" target="#b8">[8]</ref> presented a principled Bayesian framework to fuse the semantic segmentation predicted from a convolutional neural network and stereo-based detection results from the Fast Direct Planar Hypothesis Testing (FPHT) method. MergeNet <ref type="bibr" target="#b5">[6]</ref> was proposed with a multistage training procedure involving weight sharing, separating learning of low and high level features from the RGB-D input and a refining stage which learns to fuse the obtained complementary features. But all these methods can only predict three main classes: free-space, obstacle, and background. To meet the demands of autonomous driving, we need a more universal approach that can enrich the detectable semantics beyond simple roads/obstacles separation. In this work, we address unexpected obstacle detection by incorporating it in a multi-source semantic segmentation framework to provide a unified pixel-wise scene understanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>The entire network architecture of RFNet is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In the encoder part of the architecture, we design two independent branches to extract features for RGB and depth images separately-RGB branch as the main branch and Depth branch as the subordinate branch. In both branches, we choose ResNet-18 <ref type="bibr" target="#b31">[30]</ref> as the backbone to extract features from inputs because ResNet-18 has moderate depth and residual structure, and its small operation footprint is compatible with real-time operation. After each layer of ResNet-18, the output features from Depth branch are fused to RGB branch after the Attention Feature Complementary (AFC) module. The spatial pyramid pooling (SPP) block gathers the fused RGB-D features from two branches and produces feature maps with multi-scale information. Finally, referred to SwiftNet <ref type="bibr" target="#b32">[31]</ref>, we design the efficient upsampling modules to restore the resolution of these feature maps with skip connections from the RGB branch.</p><p>RGB-D fusion module. As discussed in the last part, the depth maps contain more contour and location information that benefit RGB semantic segmentation. In order to fuse RGB and depth information effectively, we design an RGB-D fusion module termed Attention Feature Complementary (AFC) module (shown in <ref type="figure" target="#fig_2">Figure 3</ref>) to make the network focus on learning more complementary informative features from RGB and Depth branches. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, in the AFC module, we leverage a SE block <ref type="bibr" target="#b34">[32]</ref> as the channel attention method. SE block can learn to use global information to emphasize informative channels and suppress less useful channels, which helps the AFC module exploit informative features from both branches effectively.</p><p>With the multi-branch architecture, we have the RGB input feature maps X = [x 1 , . . . , x C ] ? R C?H?W and depth input feature maps Y = [y 1 , . . . , y C ] ? R C?H?W . First we use global average pooling as a channel descriptor based on channel attention mechanism, then we add a 1?1 convolution layer with the same channels as input. This 1?1 convolution layer is able to excavate correlations between channels. The followed sigmoid function is applied to activate the convolution result and constrain the value of the weight vector between 0 and 1. Next, we do outer product for the weight vector and input feature maps in both branches. Finally, by adding results from RGB branch and Depth branch, we have the resulted feature map Z ? R C?H?W , expressed as:</p><formula xml:id="formula_0">Z = X ? ? 1 [? 1 (X)] + Y ? ? 2 [? 2 (Y )]<label>(1)</label></formula><p>Here, ? denotes global pooling and 1?1 convolution. ? and ? denote outer product and sigmoid function respectively. By applying such attention mechanism in RGB-D fusion, more informative features obtain higher values of weights, which helps us exploit complementary information from depth maps more effectively.</p><p>After four ResNet blocks and AFC module, the fused feature maps contain rich high-level semantic information. In order to increase the receptive field to cover pixels of large objects while maintaining a real-time speed, referred to <ref type="bibr" target="#b32">[31]</ref> [33] <ref type="bibr" target="#b36">[34]</ref>, we adopt Spatial Pyramid Pooling (SPP) to average features over aligned grids with different granularities before the upsampling.</p><p>Efficient upsampling module. The purpose of the decoder is to upsample semantically rich visual features in coarse spatial resolution to the input resolution. We adopt a simple decoder that contains three simple upsampling modules with skip connections from the encoder. In the first two upsampling modules, low-resolution feature maps from the former block are upsampled with bilinear interpolation to the same resolution as feature maps from skip connection, then these two streams of feature maps are element-wisely added and finally mixed with a 3?3 convolution. The third upsampling module is slightly different because we add a convolution layer and a 4-times bilinear interpolation at last to restore to the same resolution as the input. More precisely, the skip connection is routed before the second ReLU of the residual block because the current study shows that skip connection from any other stage impairs the accuracy <ref type="bibr" target="#b32">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Dataset Learning</head><p>As a data-driven technology, annotated labels are essential for semantic segmentation, but we can not annotate all classes in the real world. In order to utilize as much and diverse training data as possible and increase the number of recognizable classes from a few dozens to virtually anything that a scene can contain, multi-source learning is an effective method. However, simply mixing two or more datasets for training may cause some problems. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the heterogeneity in the annotation type and sample amount may cause overfitting to one of the data sources, leading to incomplete segmentation when simply mixing the datasets. This is because classes in different datasets may conflict with each other. For example, the annotation type of these classes is different, or a certain class is a subclass of a class in another dataset. To facilitate multi-source learning with such heterogeneity, we design some training strategies. Formally, we have datasets D 1 , . . . , D c , . . . , D n . Class set A contains classes that do not conflict with each other, and class set B contains the rest. For these conflicted classes, we refer to dataset D c as a standard annotation. Let us denote an image by x, and the corresponding human annotation for x is provided and denoted by y, where y(m, n) ? 1, . . . , C is the label of pixel x(m, n), and C is the total number of classes. l denotes the total number of images in all datasets and ? denotes the segmentation model. We train on the joint multi-datasets with the loss function shown below:</p><formula xml:id="formula_1">loss = 1 l l i=1 [L A (? (x i ) , y i ) + ?L B (? (x i ) , y i )] (2)</formula><p>where L A (., .) and L B (., .) denotes cross entropy loss function for class set A and B respectively. ? is a hyperparameter balancing the weights of different classes, and in our work we set ? as following:</p><formula xml:id="formula_2">? = 1, x i ? D c 0, x i / ? D c<label>(3)</label></formula><p>For instance, in this work we leverage Cityscapes <ref type="bibr" target="#b13">[12]</ref> and Lost and Found <ref type="bibr" target="#b4">[5]</ref>. Note that Cityscapes has annotations on 19 classes except for those unexpected small obstacles. We make some modifications to the loss function while training. The road class in Cityscapes and small obstacle class in Lost and Found do not conflict with classes in other datasets, which belong to class set A. The rest classes in Cityscapes are conflicted with the background class in Lost and Found, so they are divided into class B. In this situation, we assume Cityscapes as standard dataset D c . In the training stage, background class and free-space in Lost and Found should not be counted in the loss function. In our situation, the ignorance of background class makes coarse-annotated freespace class in Lost and Found helpful for improving the training data amount, so we also include free-space in the final loss. With the presented multi-dataset training strategy, our RFNet learns to predict 19 classes from Cityscapes and the critical unexpected small obstacle class from the Lost and Found dataset.</p><p>Although unexpected small obstacle is a generalized conception, which is not limited to obstacle types in Lost and Found, the definition of this particular set of classes allows us to meet the demand by exploiting the power of deep learning methods. For example, learning that all kinds of obstacles have some common contextual property, being of small dimensions and surrounded at least partly by free-space. Thereby, the network is able to generalize far beyond its training data with the multi-source learning strategy when facing innumerable possible corner cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Datasets</head><p>In this work, two RGB-D semantic segmentation datasets: Cityscapes and Lost and Found are exploited.</p><p>Cityscapes <ref type="bibr" target="#b13">[12]</ref> is a large-scale RGB-D dataset that focuses on semantic understanding of urban street scenes. It contains 2975/500/1525 images in the training/validation/testing subsets, both with finely annotated labels on 19 classes. The images cover 50 different cities with a full resolution of 2048?1024.</p><p>The Lost and Found <ref type="bibr" target="#b4">[5]</ref> dataset consists of 2014 annotated frames from 112 stereo video sequences, along with coarse annotations of free-space areas and fine-grained annotations of the small obstacles on the road. Among them, training set and validation set contain 814 and 1200 images with a resolution of 2048?1024, covering different small obstacles present at long distance with non-uniform road textures/appearances and pathways with many non-obstacle class objects acting as distractors.</p><p>Both disparity images from Cityscapes and Lost and Found are obtained by using the semi-global matching algorithm <ref type="bibr" target="#b37">[35]</ref>, which is a sophisticated method for the estimation of a dense disparity map from a rectified stereo image pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The models were implemented on a single 2080Ti GPU with CUDA 10.0, CUDNN 7.6.0, and PyTorch 1.1. Adam <ref type="bibr" target="#b38">[36]</ref> is used for optimization with the learning rate set to 4?10 ?4 , where cosine annealing learning rate scheduling policy <ref type="bibr" target="#b39">[37]</ref> is adopted to adjust learning rate with a minimum value of 1?10 ?6 in the last epoch. The weight decay is set to 1?10 ?4 . We initialize the ResNet-18 in both RGB branch and Depth branch with pre-trained weights from ImageNet <ref type="bibr" target="#b40">[38]</ref>, and initialize the rest part of the model with kaiming initialization <ref type="bibr" target="#b41">[39]</ref>. More precisely, we average the weights for RGB inputs to match the shape of one-channel depth image in the Depth branch, as research works <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b18">[17]</ref> show that RGB pre-trained weights also boost depth image feature extraction. For pre-trained parameters, we update them with a 4 times smaller learning rate and apply 4 times smaller weight decay. Because the left and bottom part of the disparity images are not applicable due to the restrictions of semi-global matching algorithm, we crop these pixels and resize images back to the original resolution with bilinear upsampling. The rest of the data augmentation operations consist of scaling with random factors between 0.5 and 2, random horizontal flipping, and random cropping with an output resolution of 768?768. We train all the models for 200 epochs with a batch size of 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Analysis</head><p>Ablation Study. We perform the ablation study on our RFNet to explore the influence of different architecture variants and fusion schemes on the network accuracy where the results are shown in <ref type="table" target="#tab_1">Table I</ref>. Results in this section are obtained by evaluating on the blended validation set of Cityscapes and Lost and Found, which includes all images from both validation datasets. All backbones in these models are initialized with ImageNet pre-trained weights.</p><p>In the table, the single RGB method only exploits the RGB branch of RFNet. Here, compared to SwiftNet <ref type="bibr" target="#b32">[31]</ref>, the only difference is the SE block after each block of the ResNet-18. It is a control group to determine if the depth information helps improve the accuracy, which achieves a mean Intersection over Union (mIoU) of 69.20%. In the RGB-D-Stack method, we stack depth maps with respective RGB images to form a 4channel input to the single branch of RFNet. The low accuracy of the method (65.20% in mIoU) proves that depth information is not exploited effectively in this way. We also design RGB-D-Fusion (concatenation), where the only difference of this method to the RGB-D-Fusion (element-wise add) in our RFNet is that RGB feature maps and depth feature maps are concatenated to a higher dimension feature maps and restore to the original dimension after a 1?1 convolution. Results show that this method (68.67%) performs clearly worse than RFNet (72.22%). This is because in a compact network like RFNet, concatenation is a more inefficient way to make use of the depth information.</p><p>To eliminate the cause that more parameters in two-branch RFNet make it perform better, we design and train the RGB-RGB-Fusion method. The difference of the RGB-RGB-Fusion method to RFNet is that inputs are duplicate RGB images instead of RGB-D images, and after each AFC module, the element-wise added feature maps are divided by 2. The accuracy (69.37%) is much lower than RFNet and approximately the same as the single RGB method, proving the benefit of fusion in RFNet is not simply owing to the increased parameters. We also perform an experiment to explore the influence of the proposed multi-dataset training strategy. It turns out that without multi-dataset training strategy, our RFNet gets nearly 20% lower IoU because of the class conflictions in two datasets. Finally the proposed RFNet with the proposed multi-dataset training strategy achieves a mIoU of 72.22%, which is significantly better than the baseline (single RGB architecture) and other fusion-based variants, demonstrating the effectiveness of our fusion scheme bridged by the designed attention complementary modules.</p><p>Numerical Performance Comparison. Based on our multidataset training, we create a benchmark to compare our RFNet with the other two real-time networks: ERF-PSPNet <ref type="bibr" target="#b35">[33]</ref> (a light-weight network), SwiftNet <ref type="bibr" target="#b32">[31]</ref> (whose network architecture is very similar to our RFNet). The first two networks only take RGB input. <ref type="table" target="#tab_1">Table II</ref> shows IoU of all 20 classes in the new multi-source setting. Our RFNet achieves higher accuracy in most of the classes. Compared to SwiftNet, RFNet improves accuracy remarkably in certain classes like fence, traffic light, terrain, truck, bus, train, and small obstacle, which is benefited from the depth complementary information. <ref type="figure" target="#fig_4">Figure 5</ref> shows some examples from the validation set of Cityscpaes and Lost and Found, which demonstrates the excellent segmentation accuracy of our RFNet in various scenarios with or without small obstacles.</p><p>To explore how the proposed RFNet improves precision in different depth ranges, we perform analysis on mean IoU and the IoU of small obstacle in different depth ranges for RFNet and SwiftNet. We calculate the depth value of each pixel from disparity value. The maximum depth value is set to 100 and limited by the quality of disparity image, while all the unmatched pixels are set to 100. Bar graph 6 shows that RFNet performs better in all depth ranges in the case of mean IoU of 20 classes. Specifically, RFNet boosts the accuracy of unexpected small obstacle recognition in close and middle ranges remarkably. This is reasonable because disparity images derived from semi-global matching algorithm have higher accuracy at close range, and contribute more to the prediction than pixels with greater depth values.</p><p>In <ref type="table" target="#tab_1">Table III</ref> we also compare our RFNet with other state-ofthe-art networks on the Cityscapes validation set. The column of speed reports the inference speed of a full resolution image (2048?1024) on a single RTX 2080Ti. Specifically, ERF-PSPNet and SwiftNet are implemented on the same hardware. Compared to mainstream RGB semantic segmentation networks, our RFNet achieves better results while maintaining a real-time performance, which proves that exploiting depth information helps improving accuracy. In the table, we also list some other RGB-D fusion networks: LDFNet <ref type="bibr" target="#b25">[24]</ref> and RFBNet based on ERFNet <ref type="bibr" target="#b26">[25]</ref>. Our RFNet is both more accurate and faster than these multimodal networks. Overall, rare multi-modal semantic segmentation methods meet the real-time prediction speed, while our method achieves the highest accuracy on the validation set of Cityscapes to the best of our knowledge that meets both demands including real-time inference, highly qualified accuracy, and capacity to leverage complementary features in cross-modal imagery.</p><p>Qualitative Performance Study. We present the qualitative examples in <ref type="figure">Figure 8</ref>. In this paper, the main purpose of exploiting depth information is to enhance the segmentation accuracy in classes which are difficult for the RGB method, including small obstacles. The appearance and surface texture of the small obstacles are not fixed, and it is easy to be confused with graffiti, manhole covers, zebra crossings on the road. Comparatively, in the depth map where texture is ignored, the contour of small obstacles is clear. Graffiti, manhole covers are flat, making it part of the road surface in the depth map. All these features of depth maps enable to   <ref type="bibr" target="#b1">[2]</ref> 70.4% n/a ENet <ref type="bibr" target="#b42">[40]</ref> 58.3% 76.9 * ERFNet <ref type="bibr" target="#b27">[26]</ref> 65.8% 20.8 ERF-PSPNet <ref type="bibr" target="#b35">[33]</ref> 64.1% 20.4 SwiftNet <ref type="bibr" target="#b32">[31]</ref> 72.0% 41.0 VGG-D (ScaleInvariant) <ref type="bibr" target="#b24">[23]</ref> 64.4% n/a LDFNet <ref type="bibr" target="#b25">[24]</ref> 68.5% 18.4 GoogLeNet (NiN-2) <ref type="bibr" target="#b18">[17]</ref> 69.1% n/a RFBNet (ERFNetEnc) <ref type="bibr" target="#b26">[25]</ref> 72.0% n/a RFNet (Ours) 72.5% 22.2 * Speed on half resolution images. reduce the chance of false alarm in detecting small obstacles. We compare our RFNet with SwiftNet <ref type="bibr" target="#b32">[31]</ref>, which has a similar network architecture with RFNet, where <ref type="figure">Figure 8</ref> shows representative contrast results from the two networks. As a purely RGB-based method, SwiftNet fails to predict some small obstacles on the road and predicts manhole cover as small obstacle. RFNet correctly detect small obstacles and classifies the manhole as part of the road, which demonstrates the superiority of our method for safety-critical road sensing. RFNet also performs better in large-scale objects like bus and truck because contours of these classes are much clearer in depth maps compared to RGB images. Furthermore, for the input image from <ref type="figure" target="#fig_0">Figure 1, Figure 7</ref> shows the feature maps after the second block from RFNet, in which the first two are feature maps from RGB and depth branch respectively, and the merged feature maps are from the output part of the AFC module. As it can be clearly seen, compared to RGB feature maps, small obstacle is much more clear and manhole cover disappears in depth feature maps, while the feature map after AFC module takes the advantages of both branches. In summary, the AFC module enables RFNet to effectively exploit the depth features in a complementary way, improving the accuracy of obstacle detection evidenced by both numerical and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this study, we propose RFNet, a real-time fusion network for RGB-D semantic segmentation on road-driving images. With the designed AFC module, RFNet exploits complementary depth information effectively and significantly improves the accuracy over purely RGB-based methods. With the presented multi-source training strategy, RFNet can also detect unexpected small obstacles, enriching the recognizable classes required to face the real world with unforeseen hazards. More importantly, RFNet operates at 22Hz with full resolution Cityscapes images and 41.6Hz with half resolution on a single Nvidia GTX2080Ti GPU, which makes it ideally suitable for autonomous driving applications. Our RFNet outperforms state-of-the-art RGB-D fusion methods in terms of accuracy and speed. In the future, we plan to further streamline RFNet and deploy it to portable TPU devices with robustness augmented. The source code of our RFNet is available at https://github.com/AHupuJR/RFNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Examples from the Lost and Found dataset and corresponding results of the methods: (a) SwiftNet (unexpected obstacle wrongly classified as car), (b) The proposed RFNet (clear and consistent segmentation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of RFNet: the proposed network architecture for real-time fusion-based RGB-D semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>AFC: Attention Feature Complementary module to exploit crossmodel information from RGB and Depth inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>RGB images and ground truth from Cityscapes (first row) and Lost and Found (second row) respectively. The last column shows the inference result if simply training on two datasets without consideration of the heterogeneity in the annotation style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Predictions with additional unexpected obstacle class from RFNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Mean IoU of 20 classes and IoU of Small Obstacle from SwiftNet and RFNet respectively. RFNet improves precision in all depth ranges, especially in close and middle ranges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>(a) RGB feature maps (b) Depth feature maps (c) Merged feature maps Visualization of feature maps from the second block of RFNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>L. Sun, X. Hu and W. Hu are with State Key Laboratory of Modern Optical Instrumentation, Zhejiang University, China {leo_sun, hxx_zju, huweijian}@zju.edu.cn 2 K. Yang is with Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany kailun.yang@kit.edu</figDesc><table /><note>3 K. Wang is with National Optical Instrumentation Engineering Technology Research Center, Zhejiang University, China wangkaiwei@zju.edu.cn</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>OF RFNET ON THE CITYSCAPES AND LOST AND FOUND VALIDATION SET WITH DIFFERENT DESIGN CHOICES. CLASS IOU(%) RESULTS OF THREE NETWORKS ON THE BLENDED VALIDATION SET OF CITYSCAPES AND LOST AND FOUND DATASET. LIST OF CLASSES(FROM LEFT TO RIGHT): ROAD, SIDEWALK, BUILDING, WALL, FENCE, POLE, TRAFFIC LIGHT, TRAFFIC SIGN, VEGETATION, TERRAIN, SKY, PEDESTRIAN, RIDER, CAR, TRUCK, BUS, TRAIN, MOTORBIKE, BICYCLE AND SMALL OBSTACLE.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="4">RGB-D Fusion Dual-branch Concatenation Element-wise add</cell><cell cols="2">mIoU(%) Params</cell></row><row><cell>Single RGB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.20%</cell><cell>12.17M</cell></row><row><cell>RGB-D-Stack</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>65.20%</cell><cell>12.17M</cell></row><row><cell cols="3">RGB-D-Fusion (concatenation)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.67%</cell><cell>25.08M</cell></row><row><cell cols="4">RGB-RGB-Fusion (element-wise add)</cell><cell></cell><cell></cell><cell></cell><cell>69.37%</cell><cell>23.69M</cell></row><row><cell cols="4">RFNet (without multi-dataset training strategy)</cell><cell></cell><cell></cell><cell></cell><cell>53.83%</cell><cell>23.69M</cell></row><row><cell>RFNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72.22%</cell><cell>23.69M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell></row><row><cell>PER-Network</cell><cell>Roa Sid</cell><cell>Bui</cell><cell>Wal Fen Pol</cell><cell>TLi TSi</cell><cell>Veg Ter</cell><cell cols="2">Sky Ped Rid Car Tru Bus Tra Mot Bic</cell><cell>SOb mIoU</cell></row><row><cell>ERF-PSPNet</cell><cell cols="8">89.3 65.1 82.1 43.4 39.8 46.9 48.2 46.1 86.4 45.2 88.4 68.5 57.2 90.1 55.6 62.1 65.6 56.9 64.8 60.3</cell><cell>63.1</cell></row><row><cell>SwiftNet</cell><cell cols="8">95.7 60.6 89.1 50.9 53.6 56.9 61.1 71.4 90.7 55.0 92.2 75.2 58.5 92.7 65.3 81.3 70.0 56.2 72.1 62.8</cell><cell>70.6</cell></row><row><cell>Our RFNet</cell><cell cols="8">96.0 60.6 90.8 50.2 59.9 60.0 62.6 72.8 91.1 57.3 92.5 76.1 57.9 93.3 73.8 82.3 73.2 54.0 72.7 67.9</cell><cell>72.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF SEMANTIC SEGMENTATION METHODS ON THE VALIDATION SET OF CITYSCAPES.</figDesc><table><row><cell>Network</cell><cell>Multimodal</cell><cell>mIoU(%)</cell><cell>Speed (FPS)</cell></row><row><cell>FCN8s [1]</cell><cell></cell><cell>65.3%</cell><cell>2.0</cell></row></table><note>* DeepLabV2-CRF</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The prevalence of motor vehicle crashes involving road debris, united states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Tefft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Age (years)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="10" to="11" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lost and found: detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mergenet: A deep net architecture for small obstacle discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1440" to="1444" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting unexpected obstacles for self-driving cars: Fusing deep learning and geometric modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1025" to="1032" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rednet: Residual encoderdecoder network for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01054</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">See clearer at night: towards robust nighttime semantic segmentation through day-night image conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning in Defense Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11169</biblScope>
			<biblScope unit="page">111690</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training of convolutional networks on multiple heterogeneous datasets for street scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1045" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3572</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth-aware CNN for RGB-D segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<idno>abs/1803.06791</idno>
		<ptr target="http://arxiv.org/abs/1803.06791" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal neural networks: Rgb-d for semantic segmentation and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fr?hlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?tsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="98" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4980" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Traffic scene segmentation based on rgb-d image and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1664" to="1669" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional scale invariance for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>?au?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Incorporating luminance, depth and color information by a fusion-based network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2374" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rfbnet: deep multimodal networks with residual fusion blocks for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00135</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards a global optimal multi-layer stixel representation of dense 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="51" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Obstacle detection and terrain classification for autonomous off-road navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Castano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talukder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous robots</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo obstacle detection in challenging environments: the viac experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buzzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1599" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-(a) RGB (b) Disparity (c) GT (d)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>SwiftNet (e) RFNet</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Qualitative semantic segmentation results from SwiftNet and the proposed RFNet that exploits both RGB and depth information. driving images</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">616</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unifying terrain awareness for the visually impaired through real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1506</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
