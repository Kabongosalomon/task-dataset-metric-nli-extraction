<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sheaf Neural Networks with Connection Laplacians</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Barbero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitz</forename><surname>S?ez De Oc?riz Borde</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
						</author>
						<title level="a" type="main">Sheaf Neural Networks with Connection Laplacians</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Sheaf Neural Network (SNN) is a type of Graph Neural Network (GNN) that operates on a sheaf, an object that equips a graph with vector spaces over its nodes and edges and linear maps between these spaces. SNNs have been shown to have useful theoretical properties that help tackle issues arising from heterophily and over-smoothing. One complication intrinsic to these models is finding a good sheaf for the task to be solved. Previous works proposed two diametrically opposed approaches: manually constructing the sheaf based on domain knowledge and learning the sheaf end-to-end using gradientbased methods. However, domain knowledge is often insufficient, while learning a sheaf could lead to overfitting and significant computational overhead. In this work, we propose a novel way of computing sheaves drawing inspiration from Riemannian geometry: we leverage the manifold assumption to compute manifold-and-graph-aware orthogonal maps, which optimally align the tangent spaces of neighbouring data points. We show that this approach achieves promising results with less computational overhead when compared to previous SNN models. Overall, this work provides an interesting connection between algebraic topology and differential geometry, and we hope that it will spark future research in this direction. Correspondence to: Federico Barbero &lt;fb548@cam.ac.uk&gt;.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b19">(Scarselli et al., 2008)</ref> have shown encouraging results in a wide range of applications, ranging from drug design <ref type="bibr" target="#b21">(Stokes et al., 2020)</ref> to guiding discoveries in pure mathematics <ref type="bibr" target="#b7">(Davies et al., 2021)</ref>. One advantage over traditional neural networks is that they can leverage the extra structure in graph data, such as edge connections.</p><p>GNNs, however, do not come without issues. Traditional GNN models, such as Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016)</ref> have been shown to work poorly on heterophilic data. In fact, GCNs use homophily as an inductive bias by design, that is, they assume that connected nodes will likely belong to the same class and have similar feature vectors, which is not true in many real-world applications <ref type="bibr" target="#b25">(Zhu et al., 2020a)</ref>. Moreover, GNNs also suffer from over-smoothing <ref type="bibr" target="#b16">(Oono &amp; Suzuki, 2019)</ref>, which prevents these models from improving, and may actually even worsen their performance when stacking several layers. These two problems are, from a geometric point of view, intimately connected <ref type="bibr" target="#b3">(Chen et al., 2020a;</ref><ref type="bibr" target="#b2">Bodnar et al., 2022)</ref>. <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref> showed that when the underlying "geometry" of the graph is too simple, the issues discussed above arise. More precisely, they analysed the geometry of the graph through cellular sheaf theory <ref type="bibr" target="#b6">(Curry, 2014;</ref><ref type="bibr" target="#b9">Hansen, 2020;</ref><ref type="bibr" target="#b11">Hansen &amp; Ghrist, 2019)</ref>, a subfield of algebraic topology <ref type="bibr" target="#b13">(Hatcher, 2005)</ref>. A cellular sheaf associates a vector space to each node and edge of a graph, and linear maps between these spaces. A GNN which operates over a cellular sheaf is known as a Sheaf Neural Network <ref type="bibr">(SNN)</ref> arXiv:2206.08702v1 <ref type="bibr">[cs.</ref>LG] 17 Jun 2022 <ref type="bibr" target="#b10">(Hansen &amp; Gebhart, 2020;</ref><ref type="bibr" target="#b2">Bodnar et al., 2022)</ref>.</p><p>SNNs work by computing a sheaf Laplacian, which recovers the well-known graph Laplacian when the underlying sheaf is trivial, that is, when vector spaces are 1-dimensional and we apply identity maps between them. <ref type="bibr" target="#b11">Hansen &amp; Ghrist (2019)</ref> have first shown the utility of SNNs in a toy experimental setting, where they used a manually-constructed sheaf Laplacian based on full knowledge of the data generation process. <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref> proposed to learn this sheaf Laplacian from data using stochastic gradient descent, making these types of models applicable to any graph dataset. However, this can also lead to computational complexity problems, overfitting and optimisation issues.</p><p>This work proposes a novel technique that aims to precompute a sheaf Laplacian from data in a deterministic manner, removing the need to learn it with gradient-based approaches. We do this through the lens of differential geometry, by assuming that the data is sampled from a low-dimensional manifold and optimally aligning the neighbouring tangent spaces via orthogonal transformations ( see <ref type="figure" target="#fig_0">Figure 1</ref>). This idea was first introduced as groundwork for vector diffusion maps by <ref type="bibr" target="#b20">Singer &amp; Wu (2012)</ref>. However, it only assumed a point-cloud structure. Instead, one of our contributions involves the computation of these optimal alignments over a graph structure. We find that our proposed technique performs well, while reducing the computational overhead involved in learning the sheaf.</p><p>In Section 2, we present a brief overview of cellular sheaf theory and neural sheaf diffusion <ref type="bibr" target="#b2">(Bodnar et al., 2022)</ref>. Next, in Section 3, we give details of our new procedure used to pre-compute the sheaf Laplacian before the model-training phase, which we refer to as Neural Sheaf Diffusion with Connection Laplacians (Conn-NSD). We then, in Section 4, evaluate this technique on various datasets with varying homophily levels. We believe that this work is a promising attempt at connecting ideas from algebraic topology and differential geometry with machine learning, and hope that it will spark further research at their intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We briefly overview the necessary background, starting with GNNs and cellular sheaf theory and concluding with neural sheaf diffusion. The curious reader may refer to Curry (2014); Hansen (2020); <ref type="bibr" target="#b11">Hansen &amp; Ghrist (2019)</ref> for a more in-depth insight into cellular sheaf theory, and to <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref> for the full theoretical results of neural sheaf diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Neural Networks</head><p>GNNs are a family of neural network architectures that generalise neural networks to arbitrarily structured graphs.</p><p>A graph G = (V, E) is a tuple consisting of a set of nodes V and a set of edges E. We can represent each node in the graph with a d-dimensional feature vector x v and group all the n = |V | feature vectors into a n ? d matrix X. We represent the set of edges E with an adjacency matrix A. A GNN layer then takes these two matrices as input to produce a new set of (latent) feature vectors for each node:</p><formula xml:id="formula_0">H (l) = f H (l?1) , A .</formula><p>(1)</p><p>In the case of a multi-layer GNN, the first layer l = 1, takes as input H (0) = X, whereas subsequent layers, l, take as input H (l?1) , the latent features produced by the GNN layer immediately before it. There are numerous architectures which take this form, with one of the most popular being the Graph Convolutional Network (GCN) <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016)</ref> which implements Equation (1) the following way:</p><formula xml:id="formula_1">H (l) = ? D ? 1 2?D ? 1 2 H (l?1) W (l) ,<label>(2)</label></formula><p>where ? is a non-linear activation function (e.g. ReLU), A = A + I,D is the diagonal node degree matrix of? and W (l) is a weight matrix. This update propagation is local (due to the adjacency matrix), meaning that each latent feature vector is updated as a function of its local neighbourhood, weighted by a weight matrix and then symmetrically normalised. This kind of model has proven to be extremely powerful in a myriad of tasks. The weight matrix W (l) at each layer is learnt from the data through back-propagation, by minimising some loss function (e.g. cross-entropy loss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cellular Sheaf Theory</head><p>Definition 2.1. A cellular sheaf (G, F) on an undirected graph G = (V, E) consists of:</p><formula xml:id="formula_2">? A vector space F(v) for each v ? V , ? A vector space F(e) for each e ? E, ? A linear map F v e : F(v) ? F(e) for each incident node-edge pair v e.</formula><p>The vector spaces of the node and edges are called stalks, while the linear maps are called restriction maps. It is then natural to group the various spaces. The space which is formed by the node stalks is called the space of 0-cochains, while the space formed by edge stalks is called the space of 1-cochains.</p><p>Definition 2.2. Given a sheaf (G, F), we define the space of 0-cochains C 0 (G, F) as the direct sum over the vertex stalks C 0 (G, F) := v?V F(v). Similarly, the space of 1-cochains C 1 (G, F) as the direct sum over the edge stalks C 1 (G, F) := e?E F(e).</p><p>Defining the spaces C 0 (G, F) and C 1 (G, F) allows us to construct a linear co-boundary map ? : C 0 (G, F) ? C 1 (G, F). From an opinion dynamics perspective <ref type="bibr" target="#b12">(Hansen &amp; Ghrist, 2021)</ref>, the node stalks may be thought of as the private space of opinions and the edge stalks as the space in which these opinions are shared in a public discourse space. The co-boundary map ? then measures the disagreement between all the nodes. Definition 2.3. Given some arbitrary orientation for each edge e = u ? v ? E, we define the co-boundary map</p><formula xml:id="formula_3">? : C 0 (G, F) ? C 1 (G, F) as ?(x) e = F v e x v ? F u e x u . Here x ? C 0 (G, F) is a 0-cochain and x v ? F(v) is the vector of x at the node stalk F(v).</formula><p>The co-boundary map ? allows us to construct the sheaf Laplacian operator over a sheaf.</p><p>Definition 2.4. The sheaf Laplacian of a sheaf is a map</p><formula xml:id="formula_4">L F : C 0 (G, F) ? C 0 (G, F) defined as L F = ? ?.</formula><p>The sheaf Laplacian is a symmetric positive semi-definite (by construction) block matrix. The diagonal blocks are</p><formula xml:id="formula_5">L Fv,v = v e F v e F v e , while the off-diagonal blocks are L Fv,u = ?F v e F u e . Definition 2.5. The normalised sheaf Laplacian ? F is defined as ? F = D ? 1 2 L F D ? 1 2 where D is the block- diagonal of L F .</formula><p>Although stalk dimensions are arbitrary, we work with node and edge stalks which are all d-dimensional for simplicity. This means that each restriction map is d ? d, and therefore so is each block in the sheaf Laplacian. With n we denote the number of nodes in the underlying graph G, which results in our sheaf Laplacian having dimensions nd ? nd.</p><p>If we construct a trivial sheaf where each stalk is isomorphic to R and the restriction maps are identity maps, then we recover the well-known n ? n graph Laplacian from the sheaf Laplacian. This effectively means that the sheaf Laplacian generalises the graph Laplacian by considering a non-trivial sheaf on G. Definition 2.6. The orthogonal (Lie) group of dimension d, denoted O(d), is the group of d ? d orthogonal matrices together with matrix multiplication.</p><p>If we constrain the restriction maps in the sheaf to belong to the orthogonal group (i.e., F v e ? O(d)), the sheaf becomes a discrete O(d)-bundle and can be thought of as a discretised version of a tangent bundle on a manifold. The sheaf Laplacian of the O(d)-bundle is equivalent to a connection Laplacian used by <ref type="bibr" target="#b20">Singer &amp; Wu (2012)</ref>. The orthogonal restriction maps describe how vectors are rotated when transported between stalks, in a way analogous to the transportation of tangent vectors on a manifold.</p><p>Orthogonal restriction maps are advantageous because orthogonal matrices have fewer free parameters, making them more efficient to work with. The Lie group O(d) has a d(d ? 1)/2-dimensional manifold structure (compared to the d 2 -dimensional general linear group describing all invertible matrices). In d = 2, for instance, 2 ? 2 rotation matrices have only one free parameter (the rotation angle).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neural Sheaf Diffusion</head><p>We now discuss the existing sheaf-based machine learning models and their theoretical properties. Consider a graph</p><formula xml:id="formula_6">G = (V, E) where each node v ? V has a d-dimensional feature vector x v ? F(v).</formula><p>We construct an nd-dimensional vector x ? C 0 (G, F) by column-stacking the individual vectors x v . Allowing for f feature channels, we produce the feature matrix X ? R (nd)?f . The columns of X are vectors in C 0 (G, F), one for each of the f channels.</p><p>Sheaf diffusion is a process on (G, F) governed by the following differential equation:</p><formula xml:id="formula_7">X(0) = X,?(t) = ?? F X(t),<label>(3)</label></formula><p>which is discretised via the explicit Euler scheme with unit step-size:</p><formula xml:id="formula_8">X(t + 1) = X(t) ? ? F X(t) = (I nd ? ? F ) X(t)</formula><p>The model used by <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref> for experimental validation was of the for?</p><formula xml:id="formula_9">X = ?? ? F (t) (I n ? W 1 ) X(t)W 2<label>(4)</label></formula><p>where W 1 and W 2 are weight matrices, the restriction maps defining ? F (t) are computed by a learnable parametric matrix-valued function F v e:=(v,u) = ?(x v , x u ), on which additional constraints (e.g., diagonal or orthogonal structure) can be imposed. Equation (4) was discretised as</p><formula xml:id="formula_10">X t+1 = X t ? ? ? F (t) I n ? W t 1 X t W t 2<label>(5)</label></formula><p>It is important to note that the sheaf F(t) and the weights W t 1 , W t 2 in equation <ref type="formula" target="#formula_10">(5)</ref> are time-dependent, meaning that the underlying "geometry" evolves over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Connection Sheaf Laplacians</head><p>The sheaf Laplacian ? F (t) arises from the sheaf F(t) built upon the graph G, which in turn is determined by constructing the individual restriction maps F v e . Instead of learning a parametric function F v e:=(v,u) = ?(x v , x u ) as done by <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref>, we compute the restriction maps in a non-parametric manner at pre-processing time. In doing so, we avoid learning the maps by backpropagation. In particular the restriction maps we compute are orthogonal. We work with this class because it was shown to be more efficient when using the same stalk width as compared to other models in <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref>, and due to the geometric analogy to parallel transport on manifolds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Local PCA &amp; Alignment for Point Clouds</head><p>We adapt a procedure to learn orthogonal transformations on point clouds, presented by <ref type="bibr" target="#b20">Singer &amp; Wu (2012)</ref>. Their construction relies on the so-called "manifold assumption", positing that even though data lives in a high-dimensional space R p , the correlation between dimensions suggests that in reality, the data points lie on a d-dimensional Riemannian manifold M d embedded in R p (with significantly lower dimension, d p).</p><p>Assume the manifold M d is sampled at points {x 1 , . . . , x n } ? R p . At every point x i , M d has a tangent space T xi M (which is analogous to our F(v)) that intuitively contains all the vectors at x i that are tangent to the manifold. A mechanism allowing to transport vectors between two T xi M and T xj M at nearby points is a connection (or parallel transport, which would correspond to our transport maps F v e F u e between F(u) and F(v)).</p><p>Computing a connection on the discretised manifold is a two step procedure. First, orthonormal bases of the tangent spaces for each data point are constructed via local PCA. Next, the tangent spaces are optimally aligned via orthogonal transformations, which can be thought of as mappings from one tangent space to a neighbouring one. Singer &amp; Wu (2012) computed a ? P CA -neighbourhood ball of points for each point x i denoted N xi, P CA . This forms a set of neighbouring points x i1 , . . . , x i N i . Then the p ? N i matrix X i = [x i1 ?x i , . . . , x i N i ?x i ] is obtained, which centres all of the neighbours at x i . Next, an N i ? N i weighting matrix D i is constructed, giving more importance to neighbours closer to x i . This allows us to compute the p ? N i matrix B i =X i D i . Then Singular Value Decomposition (SVD) is used on B i such that B i = U i ? i V i . Assuming that the singular values are in decreasing order, the first d left singular vectors are kept (the first d vectors of U i ), forming the matrix O i . Note that the columns of O i are orthonormal by construction and they form a d-dimensional subspace of R p . This basis constitutes our approximation to the basis of the tangent space T xi M.</p><p>To compute the orthogonal matrix O ij , which represents our orthogonal transformation from T xi M to T xj M, it is sufficient to first of all compute the SVD of O i O j = U?V and then O ij = UV . O ij is the orthogonal transformation which optimally aligns the tangent spaces T xi M and T xj M based on their bases O i and O j . Whenever x i and x j are "nearby", <ref type="bibr" target="#b20">Singer &amp; Wu (2012)</ref> show that O ij is an approximation to the parallel transport operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local PCA &amp; Alignment for Graphs</head><p>The technique has many valuable theoretical properties, but was originally designed for point clouds. In our case, we also wish to leverage the valuable edge information at our disposal. To do this, instead of computing the neighbourhood N xi, P CA , we take the 1-hop neighbourhood N 1 xi of x i . A problem is encountered when computing the weighting matrix D i , which gives different weightings dependent on the distance to the centroid of the neighbourhood. We make the assumption that D i is an identity matrix, giving the same weighting to each node in the neighbourhood, as they are all at a 1-hop distance from the reference feature vector. This means that in our approach B i =X i D i =X i .</p><p>Following this modification, the technique matches the procedure proposed by <ref type="bibr" target="#b20">Singer &amp; Wu (2012)</ref>. We compute the SVD of B i to extract O i from the left singular vectors. We finally compute the orthogonal transport maps O ij from the SVD of O i O j . This gives a modified version of the alignment procedure, that is now graph-aware. To the best of our knowledge, this a novel technique to operate over graphs. A diagram of the newly proposed approach is displayed in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Estimating d is non-trivial, that is, the dimension of the tangent space (in our case, the stalks). In fact, we are assuming that every neighbourhood is larger than d or else B i would have less than d singular vectors, and our construction would be ill-defined. This is clearly not always the case for all d. While <ref type="bibr" target="#b20">Singer &amp; Wu (2012)</ref> proposed to estimate d directly from the data, we leave d as a tunable hyper-parameter.</p><p>To solve the problem for nodes which have less than d neighbours, we take the closest neighbours in terms of the Euclidean distance which are not in the 1-hop neighbourhood. In other words, when there are less than d neighbours, we pick the remaining neighbours following the original procedure by <ref type="bibr" target="#b20">Singer &amp; Wu (2012)</ref>. We note that one could try to consider an n-hop neighbourhood instead, in a similar fashion to P CA in the original technique. Still, this comes with a larger computational overhead and complications related to the weightings. Furthermore, if a graph has a disconnected node, this would still be an issue. In practice, d is kept small such that most nodes have at least d edge-neighbours.</p><p>Algorithm 1 shows the pseudo-code for our technique. In principle, the LocalNeighbourhood function selects the neighbours based on the 1-hop neighbourhood. If the number of these neighbours is less than the stalk dimension, we pick the closest neighbours based on the Euclidean distance, which are not in the 1-hop neighbourhood. Assuming unit cost for SVD, the run-time increases linearly with the number of data-points. Also, given that the approach here described is performed at pre-processing time, we are able to compute the sheaf Laplacian in a deterministic way in constant time during training. This removes the overhead required whilst backpropagating through the sheaf Laplacian to learn the parametric function ?. It also helps counter</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Local PCA &amp; Alignment for Graphs</head><p>Input: feature matrix X, EdgeIndex, stalk dimension d // Graph Local PCA for i = 0 to len(X) do // 1-hop neighbourhood and closest vectors // (Euclidean distance) if needed, centred at x ? X i = LocalNeighbourhood(X, EdgeIndex, i)</p><formula xml:id="formula_11">U i , ? i , V i = SVD(X i ) // Choose first d left singular vectors O i = U i [:, : d] end for // Alignment for i, j in EdgeIndex do U, ?, V = SVD(O i O j ) O ij = UV end for</formula><p>issues related to overfitting, especially when the dimension of the stalks increases as we are removing the additional parameters which come with ?, reducing model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We evaluate our model on several datasets, and compare its performance to a variety of models recorded in the literature, as well as to some especially designed baselines. For consistency, we use the same datasets as the ones discussed by <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref>. These are real-world datasets which aim at evaluating heterophilic learning <ref type="bibr" target="#b18">(Rozemberczki et al., 2021;</ref><ref type="bibr" target="#b17">Pei et al., 2020)</ref>. They are ordered based on their homophily coefficient 0 ? h ? 1, which is higher for more homophilic datasets. Effectively, h is the fraction of edges which connect nodes of the same class label. The results are collected over 10 fixed splits, where 48%, 32%, and 20% of nodes per class are used for training, validation, and testing, respectively. The reported results are chosen from the highest validation score. <ref type="table" target="#tab_0">Table 1</ref> contains accuracy results for a wide range of models, along with ours, Conn-NSD, for node classification tasks. An important baseline is the Multi-Layer Perceptron (MLP), whose result we report in the last row of <ref type="table" target="#tab_0">Table 1</ref>. The MLP has access only to the node features and it provides an idea of how much useful information GNNs can extract from the graph structure. The GNN models in <ref type="table" target="#tab_0">Table 1</ref> can be classiffied in 3 main categories: 1. Classical: GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016)</ref>, <ref type="bibr">GAT (Velickovic et al., 2017)</ref>, GraphSAGE <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref>, 2. Models for heterophilic settings: GGCN <ref type="bibr" target="#b23">(Yan et al., 2021)</ref>, Geom-GCN <ref type="bibr" target="#b17">(Pei et al., 2020)</ref>, H2GCN <ref type="bibr">(Zhu et al., 2020b)</ref>, GPRGNN <ref type="bibr" target="#b5">(Chien et al., 2020)</ref>, FAGCN <ref type="bibr" target="#b1">(Bo et al., 2021</ref><ref type="bibr">), MixHop (Abu-El-Haija et al., 2019</ref>, 3. Models which address over-smoothing: GCNII <ref type="bibr">(Chen et al., 2020b)</ref>, PairNorm <ref type="bibr" target="#b24">(Zhao &amp; Akoglu, 2019)</ref>, Additionally, we also include the results presented by <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref> using sheaf diffusion models, and the two random baselines: RandEdge-NSD and RandNode-NSD. RandEdge-NSD generates the sheaf by sampling a Haarrandom matrix <ref type="bibr" target="#b15">(Meckes, 2019)</ref> for each edge. RandNode-NSD instead generates the sheaf by sampling a Haar-random matrix for each node O i and then by computing the transport maps O ij from O i and O j . These last two baselines help us determine how our sheaf structure performs against a randomly sampled one.</p><p>As we can see from the results, sheaf diffusion models tend to perform best for the heterophilic datasets such as Texas, Wisconsin, and Film. On the other hand, their relative performance drops as homophily increases. This is expected since, for example, classical models such as GCN and GAT exploit homophily by construction, whereas sheaf diffusion models are more general, adaptable, and versatile, but at the same time lose the inductive bias provided by classical models for homophilic data.</p><p>Conn-NSD, alongside the other original discrete sheaf diffusion methods, consistently beats the random orthogonal sheaf baselines, which shows that our model incorporates meaningful geometric structure. The proposed Conn-NSD model achieves excellent results on the Texas and Film datasets, outperforming Diag-NSD, O(d)-NSD, and Gen-NSD, using fewer learnable parameters. Furthermore, Conn-NSD also obtains competitive results for Wisconsin, Cornell and Pubmed and remains close-behind on Citeseer and Cora.</p><p>It is only in the case of the Squirrel dataset, and to a lesser extent Chameleon, that Conn-NSD is not able to perform as well as the models discussed by <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref>. The Squirrel dataset contains a large amount of nodes and a substantially greater number of edges than all the other datasets. Importantly, the underlying MLP used for classification scores poorly. It may be that the extra flexibility provided by learning the sheaf is specially beneficial in cases in which the underlying MLP achieves low accuracy. Nevertheless, Conn-NSD still convincingly outperforms the random baselines, especially on these last two datasets.</p><p>Overall, Conn-NSD performs comparably well to learning the sheaf via gradient-based approaches in most cases. It also seems most well-suited on graphs with a very low amount of nodes. This may be explained by the fact that Conn-NSD aims to mitigate overfitting, acting as a form of regularisation which allows for faster training and fewer parameters.</p><p>Runtime performance Finally, we measure the speedup achieved by moving the computation of the sheaf Laplacian  at pre-processing time. <ref type="table" target="#tab_1">Table 2</ref> displays the mean wall-clock time for an epoch measured in seconds, obtained with a NVIDIA TITAN X GPU and an Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz. Conn-NSD achieves significantly faster inference times when compared to its direct counter-part O(d)-NSD from <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref>. The larger datasets see the most benefit, with Squirrel showing a 45.8% speed up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed and evaluated a novel technique to compute the sheaf Laplacian of a graph deterministically, obtaining promising results. This was done by leveraging existing differential geometry work that constructs orthogonal maps that optimally align tangent spaces between points, relying on the manifold assumption. We crucially adapted this intuition to be graph-aware, leveraging the valuable edge connection information in the graph structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The orthonormal bases of Tx i M and Tx j M are determined by local PCA using nodes in the 1-hop neighbourhood of xi and xj respectively. The orthogonal mapping Oij is a map from Tx i M to Tx j M which optimally aligns their bases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Accuracy ? variance for various node classification datasets and models. The datasets are sorted by increasing order of homophily. Our technique is denoted Conn-NSD, while the other Sheaf Diffusion models are Diag-NSD, O(d)-NSD and Gen-NSD. The top three models are coloured by First, Second and Third, respectively. The first section includes sheaf-based models, while the second includes other GNN models.</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Film</cell><cell>Squirrel</cell><cell>Chameleon</cell><cell>Cornell</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora</cell></row><row><cell>Homophily level</cell><cell>0.11</cell><cell>0.21</cell><cell>0.22</cell><cell>0.22</cell><cell>0.23</cell><cell>0.30</cell><cell>0.74</cell><cell>0.80</cell><cell>0.81</cell></row><row><cell>#Nodes</cell><cell>183</cell><cell>251</cell><cell>7,600</cell><cell>5,201</cell><cell>2,277</cell><cell>183</cell><cell>3,327</cell><cell>18,717</cell><cell>2,708</cell></row><row><cell>#Edges</cell><cell>295</cell><cell>466</cell><cell>26,752</cell><cell>198,493</cell><cell>31,421</cell><cell>280</cell><cell>4,676</cell><cell>44,327</cell><cell>5,278</cell></row><row><cell>#Classes</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>7</cell><cell>3</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Mean seconds per epoch for each of the datasets. The proposed model achieves faster inference times because it does not need to learn and build a Laplacian at each layer.</figDesc><table><row><cell></cell><cell cols="2">Texas Wisconsin</cell><cell>Film</cell><cell cols="6">Squirrel Chameleon Cornell Citeseer Pubmed Cora</cell></row><row><cell>#Nodes</cell><cell>183</cell><cell>251</cell><cell>7,600</cell><cell>5,201</cell><cell>2,277</cell><cell>183</cell><cell>3,327</cell><cell>18,717</cell><cell>2,708</cell></row><row><cell>#Edges</cell><cell>295</cell><cell>466</cell><cell cols="2">26,752 198,493</cell><cell>31,421</cell><cell>280</cell><cell>4,676</cell><cell>44,327</cell><cell>5,278</cell></row><row><cell cols="2">Conn-NSD (ours) 0.010</cell><cell>0.013</cell><cell>0.017</cell><cell>0.310</cell><cell>0.169</cell><cell>0.013</cell><cell>0.011</cell><cell>0.147</cell><cell>0.015</cell></row><row><cell>O(d)-NSD</cell><cell>0.017</cell><cell>0.018</cell><cell>0.022</cell><cell>0.572</cell><cell>0.296</cell><cell>0.019</cell><cell>0.017</cell><cell>0.263</cell><cell>0.022</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">University of Cambridge 2 University of Oxford &amp; Twitter 3 DeepMind.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We showed that this technique achieves competitive empiri-cal results and it is able to beat or match the performance of the original models by <ref type="bibr" target="#b2">Bodnar et al. (2022)</ref> on most datasets, as well as to consistently outperform the random sheaf baselines. This suggests that in some cases it may not be necessary to learn the sheaf through a parametric function, but instead the sheaf can be computed as a pre-processing step. This work may be regarded as a regularisation technique for SNNs, which also reduces the training time as it removes the need to backpropagate through the sheaf.</p><p>We believe we have uncovered an exciting research direction which aims to find a way to compute sheaves nonparametrically with an objective that is independent of the downstream task. Furthermore, we are excited by the prospect of further research tying intuition stemming from the fields of algebraic topology and differential geometry to machine learning. We believe that this work forms a promising first step in this direction.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mixhop</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
	<note>In international conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Beyond lowfrequency information in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00797</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Di Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 Workshop on Geometrical and Topological Representation Learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Joint adaptive feature smoothing and topology extraction via generalized pagerank gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheaves</surname></persName>
		</author>
		<title level="m">cosheaves and applications. University of Pennsylvania</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Advancing mathematics by guiding human intuition with ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Toma?ev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tanburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Juh?sz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">600</biblScope>
			<biblScope unit="issue">7887</biblScope>
			<biblScope unit="page" from="70" to="74" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Laplacians of Cellular Sheaves: Theory and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebhart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06333</idno>
		<title level="m">Sheaf neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward a spectral theory of cellular sheaves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied and Computational Topology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="315" to="358" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opinion dynamics on discourse sheaves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2033" to="2060" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Algebraic topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hatcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The random matrix theory of the classical compact groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Meckes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">218</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geom-Gcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vector diffusion maps and the connection laplacian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on pure and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1067" to="1144" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Macnair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bloom-Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chiappino-Pepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Badran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Chory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="688" to="702" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. stat</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pairnorm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
