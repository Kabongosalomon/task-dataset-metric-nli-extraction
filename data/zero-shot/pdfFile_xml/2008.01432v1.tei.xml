<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boundary Content Graph Neural Network for Temporal Action Proposal Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueran</forename><surname>Bai</surname></persName>
							<email>baiyueran@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Wang</surname></persName>
							<email>wangyingying02@qiyi.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">iQIYI</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
							<email>yhtong@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">iQIYI</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyue</forename><surname>Liu</surname></persName>
							<email>liuqiyue@qiyi.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">iQIYI</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Liu</surname></persName>
							<email>liujunhui@qiyi.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">iQIYI</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boundary Content Graph Neural Network for Temporal Action Proposal Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Temporal action proposal generation ? Graph Neural Net- work ? Temporal action detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action proposal generation plays an important role in video action understanding, which requires localizing high-quality action content precisely. However, generating temporal proposals with both precise boundaries and high-quality action content is extremely challenging. To address this issue, we propose a novel Boundary Content Graph Neural Network (BC-GNN) to model the insightful relations between the boundary and action content of temporal proposals by the graph neural networks. In BC-GNN, the boundaries and content of temporal proposals are taken as the nodes and edges of the graph neural network, respectively, where they are spontaneously linked. Then a novel graph computation operation is proposed to update features of edges and nodes. After that, one updated edge and two nodes it connects are used to predict boundary probabilities and content confidence score, which will be combined to generate a final high-quality proposal. Experiments are conducted on two mainstream datasets: ActivityNet-1.3 and THUMOS14. Without the bells and whistles, BC-GNN outperforms previous state-ofthe-art methods in both temporal action proposal and temporal action detection tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal action proposal generation becomes an active research topic in recent years, as it is a fundamental step for untrimmed video understanding tasks, such as temporal action detection and video analysis. A useful action proposal method could distinguish the activities we are interested in, so that only intervals containing visual information indicating activity categories can be retrieved.</p><p>Although extensive studies have been carried out in the past, generating temporal proposals with both precise boundaries and rich action content remains a challenge <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>. Some existing methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref> are proposed to generate candidate proposals by sliding multi-scale temporal windows in videos with regular interval or designing multiple temporal anchor instances for temporal feature maps. Since the lengths of windows and anchors are fixed and set previously, these methods cannot generate proposals with precise boundaries and lack flexibility to retrieve action instances of varies temporal durations.</p><p>Recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref> aim to generate higher quality proposals. <ref type="bibr" target="#b12">[13]</ref> adopts a "local to global" fashion to retrieve proposals. In the first, temporal boundaries are achieved by evaluating boundary confidence of every location of the video feature sequence. Then, content feature between boundaries of each proposal is used to generate content confidence score of proposal. <ref type="bibr" target="#b10">[11]</ref> proposes an endto-end pipeline, in which confidence score of boundaries and content of densely distributed proposals are generated simultaneously. Although these works can generate proposals with higher quality, they ignore to make explicit use of interaction between boundaries and content. <ref type="figure">Fig. 1</ref>. Schematic depiction of the proposed approach. The red box denotes an action instance in a video. We regard temporal locations with regular interval as start locations and end locations for video segments. Start locations S and end locations E are regarded as nodes. Only when the location of S is before E, we define the content between them as an edge to connect them. Then, a novel graph reasoning operation is applied to enable the relationship between nodes and edges. Finally, two nodes and the edge connected them form a temporal proposal.</p><p>To address this drawback, we propose Boundary Content Graph Neural Network (BC-GNN), which uses a graph neural network to model interaction between boundaries and content of proposals. As shown in <ref type="figure">Fig 1,</ref> a graph neural network links boundaries and content into a whole. For the graph of each video, the nodes denote temporal locations, while the edges between nodes are defined based on content between these locations. This graph enables information exchanging between nodes and edges to generate more dependable boundary probabilities and content confidence scores. In our proposed framework, a graph neural network is constructed to link boundaries and content of temporal proposals firstly. Then a novel graph computation operation is proposed to update features of edges and nodes. After that, one updated edge and two nodes it connects are used to product boundary probabilities and content confidence score, which are combined to generate a candidate proposal.</p><p>In summary, the main contributions of our work are three folds:</p><p>(1) We propose a new approach named Boundary Content Graph Neural Network (BC-GNN) based on the graph neural network to enable the relationship between boundary probability predictions and confidence evaluation procedures.</p><p>(2) We introduce a novel graph reasoning operation in BC-GNN to update attributes of the edges and nodes in the boundary content graph.</p><p>(3) Experiments in different datasets demonstrate that our method outperforms other existing state-of-the-art methods in both temporal action proposal generation task and temporal action detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Action Recognition. Recognizing action classes in trimmed videos is a both basic and significant task for the purpose of video understanding. Traditional approaches are mostly based on hand-crafted feature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25]</ref>. As the progress of Convolutional Neural Networks (CNN) in recent years, CNN based methods are widely adopted in action recognition and achieve superior performance. One type of these methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6]</ref> focus on combining multiple data modalities. Furthermore, other methods attempt to exploit the spatial-temporal feature by using 3D convolution operation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b2">3]</ref>. The feature sequence extracted by action recognition models can be used as the input feature sequence of our network framework to analyze long and untrimmed video. Graph Neural Network. Graph Neural Networks(GNNs) are proposed to handle graph-structured data with deep learning. With the development of deep learning, different kinds of GNNs appear one after another. <ref type="bibr" target="#b16">[17]</ref> proposes the Graph Convolutional Networks(GCNs), which defines convolutions on the nongrid structures. <ref type="bibr" target="#b23">[24]</ref> adopts attention mechanism in GNNs. <ref type="bibr" target="#b8">[9]</ref> proposes an effective way to exploit features of edges in GNNs. Methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref> based on GNNs are also applied to many areas in computer vision, since the effectiveness of these GNNs. In this paper, we adopt a variation of convolution operation in <ref type="bibr" target="#b8">[9]</ref> to compute feature of nodes in our graph nueral network.</p><p>Temporal Action Proposal Generation. The goal of temporal action proposal generation task is to retrieve temporal segments that contain action instance with high recall and precision. Previous methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15]</ref> use temporal sliding window to generate candidate proposals. However, durations of ground truth action instances are various, the duration flexibility are neglected in these methods. Some methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref> adopt multi-scale anchors to generate proposals, and these methods are similar with the idea in anchor-based object detection. <ref type="bibr" target="#b31">[32]</ref> proposes Temporal Actionness Grouping (TAG) to output actionness probability for each temporal location over the video sequence using a binary actionness classifier. Then, continuous temporal regions with high actionness score are combined to obtain proposals. This method is effective and simple, but the proposal it generates lacks the confidence for ranking. Recently, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref> generate proposals in a bottom-up and top-down fashion. As bottom-up, boundaries of temporal proposals are predicted at first. As top-down, content between boundaries is evaluated as a confidence score. While the relations between boundaries and content is not utilized explicitly, which is quite important we believe. In this paper, we combine boundary probability predictions and confidence evaluation procedures into a whole by graph neural network. It facilitates information exchanging through these two branches, and brings strong quality improvement in temporal action proposal generation and temporal action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>In this section, we will introduce the details of our approach illustrated in <ref type="figure" target="#fig_0">Fig.2</ref>. In Feature Encoding, visual contents of input video are encoded into feature sequence by a spatial and temporal action recognition network, then this sequence of features is fed into our proposed Boundary Content Graph Neural Network (BC-GNN) framework. There are four modules in BC-GNN: Base Module, Graph Construction Module (GCM), Graph Reasoning Module (GRM) and Output Module. The Base Module is the backbone which is used to exploit local semantic information of input feature sequence. GCM takes feature sequences from Base Module as input and construct a graph neural network. In the GRM module, a new graph computation operation is proposed to update attributes of edges and nodes. Output Module takes the updated edges and nodes as input to predict boundary and content confidence scores. At last, proposals are generated by score fusion and Soft-NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>One untrimmed video consists of a sequence of l v frames, and this sequence can be denoted as X = {x n } lv n=1 . Action instances in the video content compose a set named ? g = {? n = (t n s , t n e )} Ng n=1 , where t n s and t n e denote the start and end temporal points of the n th action instance respectively, and N g denotes the total number of action instances in this video. Classes of these action instances are not considered in temporal action proposal generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Encoding</head><p>Two-stream network <ref type="bibr" target="#b21">[22]</ref> is adopted in our framework as visual encoder, since this encoder shows good performance in video action recognition task. This twostream network consists of spatial and temporal branches. Spatial one is used to encode RGB frames and temporal one is adopted for encoding flow frames. They are designed to capture information from appearance and motion seperately.</p><p>More specifically, an input video X with l v frames is downsampled to a sequence of l s snippets S = {s n } ls n=1 in a regular temporal interval ? . Thus, the length of snippet sequence l s is calculated as l s = l v /? . Every snippet s n in sequence S is composed of a RGB frame x n and several optical frames o n . After feeding S into two-stream network, two sequences of action class scores are predicted from top layers of both branches. Then, these two sequences of scores are concatenated together at feature dimension to generate a feature sequence F = {f n } ls n=1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Boundary Content Graph Network</head><p>Base Module. On one hand, Base Module expands the receptive field, thus it serves as the backbone of whole network. On the other hand, because of the uncertainty of untrimmed videos' length, Base Module applies temporal observation window with fixed length l w to normalize length of input sequences for the whole framework. The length of observation windows depends on type of datasets. We denote input feature sequence in one window as F i ? R Di?lw , where D i is the input feature dimension size. We use two stacked 1D convolution to design our Base Module since local features are needed in sequential parts, written by F b = conv1d 2 (conv1d 1 (f i )). After feeding feature sequence F i into convolutional layers,</p><formula xml:id="formula_0">F b ? R D b ?lw is generated. Graph Construction Module(GCM).</formula><p>The goal of GCM is to construct a boundary content graph network. <ref type="figure" target="#fig_1">Fig.3</ref>(a) shows the simplified structure of undirected graph generated by GCM.</p><p>Three convolutional layers conv1d s , conv1d e and conv1d c will be adopted for</p><formula xml:id="formula_1">F b ? R D b ?lw separately to generate three feature sequence F s ? R Dg?lw , F e ? R Dg?lw and F c ? R Dc?lw .</formula><p>It should be noted that feature dimension size of F s and F e are equal to D g .</p><p>We regard feature sequences F s and F e as two sets of feature elements, denoted as F s = {f s,i } lw i=1 and F e = {f e,j } lw j=1 , where f s,j and f e,j are the i th start feature in F s and the j th end feature in F e . Then we conduct the Cartesian product between sets F s and F e , denoted as</p><formula xml:id="formula_2">F s ? F e = {(f s,i , f s,j )|f s,i ? F s ? f e,j ? F e }.</formula><p>To clear out the illegals, we remove every tuple whose start location i is greater than or equal to the end feature location j from the F s ? F e and name the start-end pair set to</p><formula xml:id="formula_3">M SE = {(f s,i , f s,j )|(f s,i ? F s ) ? (f e,j ? F e ) ? (i &lt; j)}.</formula><p>The pairs of start and end feature form a start-end pair set M SE .</p><p>To achieve content representation, we select feature elements between the i th temporal location and the j th location from F c as a sequence {f c,n } j n=i . We adopt linear interpolation to achieve constant N vectors at temporal dimension from {f c,n } j n=i , and denote it as f c,(i,j) ? R Dc?N . After generating f c,(i,j) , we reshape its dimension size from D c ? N to (D c ? N ) ? 1, and apply a fully connected layer f c 1 to make dimension size of f c,(i,j) same with f s,i and f e,j , denoted as f c,(i,j) ? R Dg . Thus, we achieve a content set M C = {f c,(i,j) |i &lt; j}. Content between the i th temporal location and the j th temporal location composes content set M C .</p><p>Then, the start-end pair set M SE and content set M C make up a undirected graph. Since the tuple (f s,i , f e,j ) ? M SE corresponds to the video segment that starts at the i th temporal location and ends at the j th temporal location. If elements in F s and F e are regarded as the nodes of a graph, tuples in M SE identify the connection relationship between these nodes. Meanwhile the tuples in M SE and elements in M C are mapped one by one. Therefore, elements in M C can be regarded as the edges of this graph. Formally, graphs can be denoted as G = (V, E, I), where V , E and I are their nodes, edges and incidence functions respectively. In our graph, we define nodes as V = F s ? F e , edges as E = M C and incidence function as</p><formula xml:id="formula_4">I = M c ? M SE , where M SE ? V ? V .</formula><p>We call f s,i start node, and call f e,i end node.</p><p>In summary, we build a restricted undirected bipartite graph in which start nodes are only connected to end nodes whose temporal locations are behind them. It should be noted edge feature in our boundary content graph is not scalars but multi-dimensional feature vectors. For convenience of description, this digraph only contains one end node and three start nodes. Red curves denote the start to end edge which point from start node to end node, and the grey curves denote the end to start edge which point from end node to start node.</p><formula xml:id="formula_5">(a) Undirected Graph in GCM (b) Directed Graph in GRM</formula><p>Graph Reasoning Module(GRM). In order to enable information exchanging between nodes and edges, we propose a new graph computation operation. One time of graph reasoning operation is applied in a block named Graph Reasoning Block (GRB). GRM consists of two stacked GRBs. Our graph computation operation is divided into edge update and node update step. Edge update step is intended to aggregate the attributes of the two nodes connected by the edge. As mentioned above, we construct an undirected bipartite graph, in which edges are not directed and start nodes only connect with end nodes. Since the feature required from start nodes to end nodes is different from information from end nodes to start nodes. We converse the undirected graph into a directed graph or a bi-directed edge. This conversion is shown in <ref type="figure" target="#fig_1">Fig.3(b)</ref>, every undirected edge is split into two opposite directed edges. In detail, we divide an undirected edge in this graph into two directed edges with the same nodes connection and opposite direction. In other words, one undirected edge turns into two directed edges, which are start to end directed edge and end to start directed edge. We define one directed edge from the i th start feature f s,i ? F s to the j th end feature f e,j ? F e as d (i,j) , and define directed edge from end feature f e,j to start feature f s,i as d (j,i) , where subscript i is only used for start node, j is only used for end node, and (i, j) identifies the direction of the directed edge which points from the i th start node to the j th end node.</p><p>Features of directed edges d (i,j) and d (j,i) are same before the edge updating, denoted as d (i,j) = d (j,i) = f c,(i,j) , where f c,(i,j) is feature of the undirected edge in undirected graph. The edge updating can be described as</p><formula xml:id="formula_6">d (i,j) = ?(? s2e ? (d (i,j) * f s,i * f e,j )) + d (i,j) ) d (j,i) = ?(? e2s ? (d (j,i) * f s,i * f e,j ) + d (j,i) ) ,<label>(1)</label></formula><p>where " * " and "?" denote element-wise product and matrix product separately. ? s2e ? R Dg?Dg and ? e2s ? R Dg?Dg are different trainable parameter matrices, and "?" denotes activation function ReLU. Node update step aims to aggregate attributes of the edges and their adjacent nodes. We adopt the variation of graph convolution in <ref type="bibr" target="#b8">[9]</ref>. For the convenience of description, we denote start node and end node as general node n k ? R Dg , where k denotes the k th node in the graph. The total number of these nodes is l N = l w ?2, and these general nodes form a set as N = {n k } l N k=1 . Meanwhile, we treat updated start to end edged (i,j) and updated end to start edged (j,i) as general edge e (h,t) ? R Dg . These general edges form a set as E = {e (h,t) |n h ? N ? n t ? N }. As usual, the node pointed by the directed edge is called the tail node, and the node where the edge starts is called the head node. It is defined that e (h,t) is from head node n h to tail node n t . Considering that the number of nodes connected to each other is different, and to avoid increasing the scale of output features through multiplication, we first normalize the features of edges before the graph convolution operation. This normalization operation is described as</p><formula xml:id="formula_7">e p (h,t) = e p (h,t) K k=1 e p (h,k) ,<label>(2)</label></formula><p>where p is the p th feature in feature vectors e (h,t) and? (h,t) , and K is the number of tail nodes. Note that all elements in e (h,t) are nonnegative. Then the convolution process of node features is described as</p><formula xml:id="formula_8">n t = ?(? node ? ( H h=1 (? (h,t) * n h )) + n t ),<label>(3)</label></formula><p>where trainable matrix ? node ? R Dg?Dg is divided into ? start and ? end depending on type of node n t , and H is the number of head nodes. This convolution operation gathers the information of head nodes to the tail nodes through the directed edges. After performing the above two steps, there are a new node feature set? = {? k } l N k=1 and an edge feature set? = {? (h,t) |? h ?? ?? t ?? } generated in one GRB. These two sets become input of the second GRB.</p><p>Output Module. As shown in <ref type="figure">Fig.1</ref>, a candidate proposal is generated using a pair of opposite directed edges and their connected nodes. Boundaries and content confidence scores of the candidate proposals are generated based on their nodes and edges, respectively. The details are described as following.</p><p>Before fed into Output Module, directed edge feature set? is divided into a start to end edge feature set and an end to start edge feature set, which are denoted as? s2e = {? s2e,(i,j) |i &lt; j ?? s2e ??} and? e2s = {? e2s,(j,i) , |i &lt; j ?? e2s,(j,i) ??}. Meanwhile, node feature set? is divided into a start node feature set? s = {? s,i } lw i=1 and an end node feature set? e = {? e,j } lw j=1 . Based on this divided feature sets, we build a candidate proposal feature set M SCE = {(? s,i ,? e,j ,? s2e,(i,j) ,? e2s,(j,i) )|i &lt; j}, where? s,i ?? s is the i th start node feature ,? e,j ?? e is the j th end node feature,? s2e,(i,j) ?? s2e is directed edge feature from the i th start node to the j th end node and? e2s,(j,i) ?? e2s is directed edge feature from the j th end node to the i th start node. The elements in M SCE are mapped to M SE one by one.</p><p>Output Module generates one proposal set ? p = {? n } l ? n=1 , where ? n = (t s , p s , t e , p e , p c ). t s and t e are start and end temporal locations of ? n separately. p s , p e and p c are the confidence scores of boundary locations t s , t e and confidence score of content between boundaries t s and t e .</p><p>Each element in M SCE is computed to get a ? n , and the computation operation is described as</p><formula xml:id="formula_9">? n = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? t s = i, t e = j, p s = ?(? SO ?? s,i ), p e = ?(? EO ?? e,j ), p c = ?(? CO ? (? s2e,(i,j) ? s2e,(j,i) )) ,<label>(4)</label></formula><p>where "?" denotes activation function sigmoid, "?" denotes matrix multiplication, and " " denotes concatenating operation at feature dimension between vectors. ? SO , ? EO and ? CO denote trainable vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training of BC-GNN</head><p>Label Assignment. Given a video, we first extract feature sequence by twostream network <ref type="bibr" target="#b21">[22]</ref>. Then, we use sliding observation windows with length l w in feature sequence to get a series of feature sequences with length of l w . The ground-truth action instances in this window compose an instance set ? g = {? n g = (t n g,s , t n g,e )} lg n=1 , where l g is the size of ? g . ? n g starts at the temporal position t n g,s and ends at t n g,e . For each ground truth action instance ? g n , we define its start interval r n s = [t n g,s ? d n g /10, t n g,s + d n g /10] and end interval r n g,e = [t n g,s ? d n g /10, t n g,s + d n g /10] separately, where d n g = t n g,e ? t n g,s . After that, the start region and end region are defined as following </p><p>Extracted features in observation window are denoted as F i . Taking F i as the input, BC-GNN outputs a set ? p = {? n = (t s , p s , t e , p e , p c )} lp n=1 , where l p is the size of ? p . Because a plenty of temporal proposals share boundaries, boundary locations t s and t e are duplicated in ? p . We select a start set S = {s n = (t s , p s , b s )|} ls n=1 , an end set E = {e n = (t e , p e , b e )} le n=1 and a content set C = {c n = (t s , t e , p c , b c )} lc n=1 from ? P . In these three sets, b s , b e and b c are assigned labels for s n , e n and c n based on ? g . If t s locates in the scope of r g,s , label b s in start tuple s n is set to constant 1, otherwise it is set to 0. In the same way we can get the label of e n . If b c of content tuple c n is set to 1, two conditions need to be satisfied. One is that t s and t e of content tuple c n located in the regions of r g,s and r g,e respectively. The other is that IoU between [t s , t e ] and any ground-truth action instances ? g = (t g,s , t g,e ) is larger than 0.5. Training Objective. We train BC-GNN in the form of a multi-task loss function. It can be denoted as</p><formula xml:id="formula_11">L objective = L bl (S) + L bl (E) + L bl (C).<label>(6)</label></formula><p>We adopt weighted binary logistic regression loss function L bl for start, end and content losses, where L bi is denoted as</p><formula xml:id="formula_12">L bl (X) = N n=1 (? + ? bi ? log p n + ? ? ? (1 ? bi)) ? log(1 ? p n )),<label>(7)</label></formula><p>where ? + = N (bi) , ? ? = N (1?bi) and N is the size of set X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference of BC-GNN</head><p>During inference, we conduct BC-GNN with same procedures described in training to generation proposals set ? p = {? n = (t s , t e , p s , p e , p c )} lp n=l . To get final results, BC-GNN undergos score fusion and redundant proposals suppression steps. Score Fusion. To generate a confidence score for each proposal ? n , we fuse its boundary probabilities and content confidence score by multiplication. This procedure can be described as</p><formula xml:id="formula_13">p f = p s * p e * p c .<label>(8)</label></formula><p>Thus, the proposals set can be denoted as ? p = {? n = (t s , t e , p f )} lp n=l . Redundant Proposals Suppression. After generating a confidence score for each proposal, it is necessary to remove redundant proposals which highly overlap with each other. In BC-GNN, we adopt Soft-NMS algorithm to remove redundant proposals. Candidate proposal set ? P turns to be ? P = ? n = (ts, te, p f ) l P n=1 , where l P is the number of final proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We present details of experimental settings and evaluation metrics in this section. Then we compare the performance of our proposed method with previous stateof-the-art methods on benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Setup</head><p>ActivityNet-1.3. This dataset is a large-scale dataset for temporal action proposal generation and temporal action detection tasks. ActivityNet-1.3 contains 19,994 annotated videos with 200 action classes, and it is divided into three sets by ratio of 2:1:1 for training, validation and testing separately. Mean Average Precision (mAP) is used to evaluate the results of action detector. Average Precision (AP) of each class is calculated individually. On ActivityNet-1.3 dataset, a set of tIoU thresholds [0.5 : 0.05 : 0.95] is used for calculating average mAP and tIoU thresholds {0.5, 0.75, 0.95} for mAP. On THUMOS-14, mAP with tIoU thresholds {0.3, 0.4, 0.5, 0.6, 0.7} is used. Implement Details. We adopt two-stream network <ref type="bibr" target="#b21">[22]</ref> for feature encoding, which pre-trained on training set of ActivityNet-1.3. The frame interval ? is set to 5 in THUMOS-14 and 16 in ActivityNet-1.3. In Base Module, we set the length of observation window l w to 128 on THUMOS-14. And in GCM, we get rid of the segments more than 64 snippets, which can cover 98% of all action instances. We linearly interpolate feature sequence of each video to 100 at the temporal dimension in ActivityNet-1.3, which means lw = 100 in this dataset. The learning rate of training BC-GNN is set to 0.0001, and weight decay is set to 0.005 on both datasets. We conduct 20 epoch of model training with the strategy of early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Temporal Action Proposal Generation</head><p>Temporal action proposal generation method aims to find segments in videos which highly overlap with ground-truth action instances. We compare BC-GNN with state-of-the-art methods to verify the effectiveness of our method in this section.</p><p>Comparison with state-of-the-art methods. Comparative experiments are conducted on two widely used benchmarks ActivityNet-1.3 and THUMOS-14.</p><p>The results of comparison on validation of ActivityNet-1.3 dataset between our method and other state-of-the-art temporal action proposal generation approaches are shown in <ref type="table">Table 1</ref>. Our method BC-GNN outperforms other leading methods by a large margin, and our method performs particularly well in aspect of AR@100. <ref type="table">Table 1</ref>. Comparison between our approach and other state-of-the-art methods on validation set of ActivityNet-1.3 dataset in terms of AR@AN and AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Prop-SSAD <ref type="bibr" target="#b11">[12]</ref>  Comparison between our method and other state-of-the-art proposal generation methods on testing set of THUMOS-14 dataset in terms of AR@AN is demonstrate in <ref type="table">Table 2</ref>. Flow feature, 2Stream feature and C3D feature are adopt as the input of these methods for ensuring a fair comparison. In this experiment, BC-GNN outperforms other state-of-the-art methods in a large margin.</p><p>These experiments verify the effectiveness of our BC-GNN. BC-GNN achieves the significant performance improvement since it makes explicit use of interaction between boundaries and content. Ablation Study. In GRM module, we convert an undirected graph into a directed graph and propose an edge feature updating operation. To evaluate the effectiveness of these strategies, we study ablation experiments in two control groups. We study the models in two control groups. In the first group, we study three types of the graphs: model with Graph Convolutional Network (GCN) manner in which edges are formed by cosine distance between nodes features, and model with directed or undirected edges. Since GCNs does not update edges, the models in the first group do not apply edge updating for the fair. In the second group, we study the effectiveness of directed edge in BC-GNN. The experimental results are listed in <ref type="table" target="#tab_1">Table 3</ref> and the average recall against average number of proposals at different tIoU thresholds are shown in <ref type="figure" target="#fig_4">Fig.4</ref> . The comparison results show that both of strategies are effective and essential. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Temporal Action Detection with Our Proposals</head><p>Temporal action detection is another aspect of evaluating the quality of proposals. On ActivityNet-1.3, we adopt a two-stage framework that detects action instances by classifying proposals. Proposals are generated by our proposal generator firstly and the top-100 temporal proposals per video are retained by ranking. Then, for each video in validation set, its top-1 video-level classification result will be obtained by using two-stream network <ref type="bibr" target="#b32">[33]</ref> and all the proposals of this video share the classification result as their action classes. On THUMOS-14, we use the top-2 video-level classification scores generated by UntrmmedNet <ref type="bibr" target="#b27">[28]</ref> and proposal-level classification score generated by SCNN-cls to classify first 200 temporal proposals for one video. The results of multiplying the confidence scores of proposals with classification are used for retrieving detection results. Comparison results between our method and other approaches on validation set of ActivityNet-1.3 in terms of mAP and average mAP are shown in <ref type="table">Table  4</ref>. Our method reaches state-of-the-art on this dataset which validates our approach. We compare our method with other existing approaches on testing set of THUMOS-14 in <ref type="table">Table 5</ref>. Our approach is superior to the other existing twostage methods on the evaluation metrics mAP, which confirms the effectiveness of our proposed proposal generator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, a new method of temporal action proposal generation named Boundary Content Graph Network (BC-GNN) is proposed. A boundary content graph is proposed to exploit the interaction between boundary probability generation and confidence evaluation. A new graph reasoning operation is also introduced to update the features of nodes and edges in the boundary content graph. In the meantime, an output module is designed to generate proposals using the strengthened features. The experimental results on popular datasets</p><p>show that our proposed BC-GNN method achieves promising performance in both temporal proposal generation and temporal action detection tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The framework of BC-GNN. Feature Encoding encodes the video into sequence of feature. Base Module expands the receptive field. GCM constructs boundary content graph network in which start nodes and end nodes are denoted as green circles and yellow circles separately. GRM updates edges and nodes, to relate information between edges and nodes. Finally, Output Module generates every candidate proposal with each edge and its connected nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Construction of undirected graph in GCM. Yellow circle denotes the start node fs,i sampled from feature Fs, green circle denotes the end node fe,i sampled from feature Fe, and blue line denotes the undirected edge which is generated from feature vectors between temporal locations Pi and Pj in Fc. The translucent circles denote the nodes without edge connection. (b) Structure of directed graph in GRM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>THUMOS- 14 .</head><label>14</label><figDesc>This dataset includes 1,010 videos and 1,574 videos in the validation and testing sets with 20 classes. And it contains action recognition, temporal action proposal generation and temporal action detection tasks. For the action proposal generation and detection tasks, there are 200 and 212 videos with temporal annotations in the validation and testing sets. Evaluation Metrics. Average Recall (AR) with Average Number (AN) of proposals per video calculated under different temporal intersection over union (tIoU) is used to evaluate the quality of proposals. AR calculated at different AN is donated as AR@AN. tIoU thresholds [0.5 : 0.05 : 0.95] is used for ActivityNet-1.3 and tIoU thresholds [0.5 : 0.05 : 1.0] is used for THUMOS-14. Specially, the area under the AR vs. AN curve named AUC is also used as an evaluation metric in ActivityNet-1.3 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Ablation study for our BC-GNN is verified the effectiveness of its modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for model with GCN, edge update step and directed edge.</figDesc><table><row><cell>Method</cell><cell cols="4">Directed Edge updating AR@100 AUC(val)</cell></row><row><cell>GCN</cell><cell>-</cell><cell>-</cell><cell>75.57</cell><cell>66.88</cell></row><row><cell>BC-GNN</cell><cell>?</cell><cell>?</cell><cell>76.18</cell><cell>67.36</cell></row><row><cell>BC-GNN</cell><cell></cell><cell>?</cell><cell>76.15</cell><cell>67.53</cell></row><row><cell>BC-GNN</cell><cell>?</cell><cell></cell><cell>76.40</cell><cell>67.79</cell></row><row><cell>BC-GNN</cell><cell></cell><cell></cell><cell>76.73</cell><cell>68.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Action detection results on validation set of ActivityNet-1.3 dataset in terms of mAP and average mAP. Comparison between our approach and other temporal action detection methods on THUMOS-14.</figDesc><table><row><cell>Method</cell><cell></cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell><cell></cell><cell>Average</cell></row><row><cell>CDC [20]</cell><cell></cell><cell>43.83</cell><cell>25.88</cell><cell>0.21</cell><cell></cell><cell>22.77</cell></row><row><cell>SSN [30]</cell><cell></cell><cell>39.12</cell><cell>23.48</cell><cell>5.49</cell><cell></cell><cell>23.98</cell></row><row><cell cols="2">BSN [13] + [33]</cell><cell>46.45</cell><cell>29.96</cell><cell>8.02</cell><cell></cell><cell>30.03</cell></row><row><cell cols="2">BMN [11] + [33]</cell><cell>50.07</cell><cell>34.78</cell><cell>8.29</cell><cell></cell><cell>33.85</cell></row><row><cell cols="2">BC-GNN + [33]</cell><cell>50.56</cell><cell>34.75</cell><cell>9.37</cell><cell></cell><cell>34.26</cell></row><row><cell>Method</cell><cell>Classifier</cell><cell>0.7</cell><cell>0.6</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell></row><row><cell>TURN [8]</cell><cell>SCNN-cls</cell><cell>7.7</cell><cell>14.6</cell><cell>25.6</cell><cell>33.2</cell><cell>44.1</cell></row><row><cell>BSN [13]</cell><cell>SCNN-cls</cell><cell>15.0</cell><cell>22.4</cell><cell>29.4</cell><cell>36.6</cell><cell>43.1</cell></row><row><cell>MGG [14]</cell><cell>SCNN-cls</cell><cell>15.8</cell><cell>23.6</cell><cell>29.9</cell><cell>37.8</cell><cell>44.9</cell></row><row><cell>BMN [11]</cell><cell>SCNN-cls</cell><cell>17.0</cell><cell>24.5</cell><cell>32.2</cell><cell>40.2</cell><cell>45.7</cell></row><row><cell>BC-GNN</cell><cell>SCNN-cls</cell><cell>19.1</cell><cell>26.3</cell><cell>34.2</cell><cell>41.2</cell><cell>46.3</cell></row><row><cell>TURN [8]</cell><cell>UNet</cell><cell>6.3</cell><cell>14.1</cell><cell>24.5</cell><cell>35.3</cell><cell>46.3</cell></row><row><cell>BSN [13]</cell><cell>UNet</cell><cell>20.0</cell><cell>28.4</cell><cell>36.9</cell><cell>45.0</cell><cell>53.5</cell></row><row><cell>MGG [14]</cell><cell>UNet</cell><cell>21.3</cell><cell>29.5</cell><cell>37.4</cell><cell>46.8</cell><cell>53.9</cell></row><row><cell>BMN [11]</cell><cell>UNet</cell><cell>20.5</cell><cell>29.7</cell><cell>38.8</cell><cell>47.4</cell><cell>56.0</cell></row><row><cell>BC-GNN</cell><cell>UNet</cell><cell>23.1</cell><cell>31.2</cell><cell>40.4</cell><cell>49.1</cell><cell>57.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2911" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting edge features for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9211" to="9219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A spatio-temporal descriptor based on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsza Lek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06750</idno>
		<title level="m">Temporal convolution based action proposal: Submission to activitynet 2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>The lear submission at thumos 2014</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Multimedia</title>
		<meeting>the 15th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cdc: Convolutionalde-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">THUMOS14 Action Recognition Challenge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<title level="m">Towards good practices for very deep two-stream convnets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02716</idno>
		<title level="m">A pursuit of temporal accuracy in general activity detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08011</idno>
		<title level="m">Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
