<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
							<email>lhoyer@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
							<email>ddai@mpi-inf.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="department">MPI for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Domain Adaptation</term>
					<term>Semantic Segmentation</term>
					<term>Multi-Resolution</term>
					<term>High-Resolution</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) aims to adapt a model trained on the source domain (e.g. synthetic data) to the target domain (e.g. real-world data) without requiring further annotations on the target domain. This work focuses on UDA for semantic segmentation as real-world pixel-wise annotations are particularly expensive to acquire. As UDA methods for semantic segmentation are usually GPU memory intensive, most previous methods operate only on downscaled images. We question this design as low-resolution predictions often fail to preserve fine details. The alternative of training with random crops of high-resolution images alleviates this problem but falls short in capturing long-range, domain-robust context information. Therefore, we propose HRDA, a multi-resolution training approach for UDA, that combines the strengths of small high-resolution crops to preserve fine segmentation details and large low-resolution crops to capture long-range context dependencies with a learned scale attention, while maintaining a manageable GPU memory footprint. HRDA enables adapting small objects and preserving fine segmentation details. It significantly improves the state-ofthe-art performance by 5.5 mIoU for GTA?Cityscapes and 4.9 mIoU for Synthia?Cityscapes, resulting in unprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available at github.com/lhoyer/HRDA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Even though neural networks currently are the unchallenged approach to solve many computer vision problems, their training often requires a large amount of annotated data. For certain tasks, such as semantic segmentation, providing the annotations is particularly labor-intensive as pixel-wise labels of the entire image are necessary, which can take more than one hour per image <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b55">56]</ref>. Therefore, several methods aim to reduce the annotation burden such as weakly-supervised learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr">101]</ref>, semi-supervised learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b57">58]</ref>, and unsupervised domain adaption (UDA) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr">99]</ref>. In this work, we focus on UDA. To avoid the annotation effort for the target dataset, the network is trained on a source dataset with existing or cheaper annotations such as automatically labeled arXiv:2204.13132v2 [cs.CV] 26 Jul 2022 synthetic data <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref>. However, neural networks are usually sensitive to domain shifts. This problem is approached in UDA by adapting the network, which is trained with source data, to unlabeled target images.</p><p>UDA methods are usually more GPU memory intensive than regular supervised learning as UDA training often requires images from multiple domains, additional networks (e.g. teacher model or domain discriminator), and additional losses, which consume significant additional GPU memory. Therefore, most UDA semantic segmentation methods so far (e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr">97]</ref>) follow the convention of downscaling images due to GPU memory constraints (see <ref type="figure" target="#fig_0">Fig. 1 a)</ref>. Taking Cityscapes as an example, current UDA methods use half the full resolution (i.e. 1024?512 pixels), while most supervised methods use the full resolution (i.e. 2048?1024 pixels). This is one of the key differences in the training setting of UDA and supervised semantic segmentation, possibly contributing to the gap between the state-of-the-art performance of UDA and supervised learning.</p><p>We question this design choice as predictions from low-resolution (LR) inputs often fail to recognize small objects such as distant traffic lights and to preserve fine segmentation details such as limbs of distant pedestrians. However, naively learning UDA with full high-resolution (HR) images is often difficult as the resolution quadratically affects the GPU memory consumption. A common remedy is training with random crops of the image. While training with HR crops allows to adapt small objects and preserve segmentation details, it limits the learned long-range context dependencies to the crop size. This is a crucial disadvantage for UDA as context information and scene layout are often domain-robust (e.g. rider on bicycle or sidewalk at the side of road) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr">96]</ref>. Further, while HR inputs are necessary to adapt small objects, they can be disadvantageous compared to LR inputs when adapting large stuff-regions such as close sidewalks (see <ref type="bibr">Sec. 5.4)</ref>. At HR, these regions often contain too detailed, domain-specific features (e.g. detailed sidewalk texture), which are detrimental to UDA. LR inputs 'hide away' these features, while still providing sufficient details to recognize large regions across domains.</p><p>To effectively combine the strength of these two approaches while maintaining a manageable GPU memory footprint, we propose HRDA, a novel multiresolution framework for UDA semantic segmentation (see <ref type="figure" target="#fig_0">Fig. 1 b)</ref>. First, HRDA uses a large LR context crop to adapt large objects without confusion from domain-specific HR textures and to learn long-range context dependencies as we assume that HR details are not crucial for long-range dependencies. Second, HRDA uses a small HR detail crop from the region within the context crop to adapt small objects and to preserve segmentation details as we assume that long-range context information play only a subordinate role in learning segmentation details. In that way, the GPU memory consumption is significantly reduced while still preserving the main advantages of a large crop size and a high resolution. Given that the importance of the LR context crop vs. the HR detail crop depends on the content of the image, HRDA fuses both using an input-dependent scale attention. During UDA, the attention learns to decide how trustworthy the LR and the HR predictions are in every image region. Previous multi-resolution frameworks for supervised learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b78">79]</ref> cannot naively be applied to state-of-the-art UDA due to GPU memory constraints as they operate on full LR and HR images. To adapt HRDA to the target domain, it can be trained with pseudo-labels fused from multiple resolutions. To further increase the robustness of the detail pseudo-labels with respect to different contexts, they are generated using an overlapping sliding window mechanism.</p><p>This work makes four contributions. To the best of our knowledge, it is the first work on UDA semantic segmentation (1) systematically studying the influence of resolution and crop size, (2) exploiting HR inputs for adapting small objects and fine segmentation details, (3) applying multi-resolution fusion with a learned scale attention for object-scale-dependent adaptation, and (4) fusing a nested large LR crop to capture long-range context and small HR crop to capture details for memory-efficient UDA training. HRDA provides significant performance gains when applied to various UDA strategies <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b66">67]</ref>. When used with the SOTA method DAFormer <ref type="bibr" target="#b28">[29]</ref>, HRDA gains +5.5 mIoU for GTA?Cityscapes and +4.9 mIoU for Synthia?Cityscapes, resulting in unprecedented 73.8 and 65.8 mIoU, respectively (see <ref type="figure" target="#fig_0">Fig. 1</ref> c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic Segmentation: Most semantic segmentation approaches are based on (convolutional) neural networks, which can be effectively trained in an end-to-end manner to perform pixel-wise classification as first shown by Long et al. <ref type="bibr" target="#b42">[43]</ref>. This concept was further improved in different aspects including increasing the receptive field while preserving spatial details <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b89">90]</ref>, integrating context information <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr">98]</ref>, attention mechanisms <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b90">91]</ref>, refining boundaries <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b83">84]</ref>, and Transformer-based architectures <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b91">92]</ref>.</p><p>Several architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b89">90]</ref> utilize intermediate features with different scales, which are generated from a single scale input, to aggregate context information. Furthermore, multi-scale input inference, where predictions from scaled versions of an image are combined via average or max pooling, is often used to obtain better results <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b81">82]</ref>. However, the naive pooling is independent of the image content, which can lead to suboptimal results. Therefore, Chen et al. <ref type="bibr" target="#b4">[5]</ref> and Yang et al. <ref type="bibr" target="#b78">[79]</ref> segment multi-scale inputs and learn an attentionweighted sum of the predictions. Tao et al. <ref type="bibr" target="#b60">[61]</ref> further propose a hierarchical attention that is agnostic to the number of scales during inference. Unsupervised Domain Adaptation (UDA): To adapt a semantic segmentation network to the target domain, several strategies have been proposed, while most can be grouped into adversarial training and self-training. In adversarial training <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>, a domain discriminator is trained in order to provide supervision to align the domain distributions based on style-transferred inputs <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref> or network features/outputs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b68">69]</ref>. In self-training, the network is adapted to the target domain using high-confidence pseudo-labels. In order to regularize the training and to avoid pseudo-label drift, approaches such as confidence thresholding <ref type="bibr" target="#b47">[48,</ref><ref type="bibr">99]</ref>, pseudo-label prototypes <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b87">88]</ref>, and consistency regularization <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b61">62]</ref> based on data augmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>, different context <ref type="bibr" target="#b35">[36,</ref><ref type="bibr">96]</ref>, domain-mixup <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr">96]</ref>, or multiple models <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b94">95]</ref> have been used. Several works also combine self-training and adversarial training <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b92">93]</ref>, minimize the entropy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref>, refine boundaries <ref type="bibr" target="#b40">[41]</ref>, use a curriculum <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b88">89]</ref>, or learn auxiliary tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b70">71]</ref>. The use of semantic segmentation networks with multi-scale features is quite common in UDA as many methods evaluate their approach with DeepLabV2 <ref type="bibr" target="#b2">[3]</ref>. However, these features are generated from a single-scale input. While some works apply multiscale average pooling for inference <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b68">69]</ref> or enforce scale consistency <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b59">60]</ref> of low-resolution inputs, they fall short in learning an input-adaptive multi-scale fusion. To the best of our knowledge, HRDA is the first work to learn a multiresolution input fusion for UDA semantic segmentation. For that purpose, we newly extend scale attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b60">61]</ref> to UDA and reveal its significance for UDA by improving the adaptation process across different object scales. Further, we propose fusing nested crops with different scales and sizes, which successfully overcomes the pressing issue of limited GPU memory for multi-resolution UDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head><p>In UDA, a neural network f ? is trained using source domain images</p><formula xml:id="formula_0">X S = {x S,m HR } N S m=1 with x S,m HR ? R H S ?W S ?3 and target domain images X T = {x T,m HR } N T m=1 with x T,m HR ? R H T ?W T ?3</formula><p>to achieve a good performance on the target domain.</p><p>However, labels are only available for the source domain Y S = {y S,m HR } N S m=1 with y S,m HR ? {0, 1} H S ?W S ?C . As the following definitions refer to the same source/target sample, we will drop index m to avoid convolution. Most previous UDA methods bilinearly downsample ?(?, ?) the images and labels x S HR , x T HR , and y S HR by a dataset-specific factor s S , s T ? 1 to satisfy GPU memory constraints, e.g.</p><formula xml:id="formula_1">x T LR = ?(x T HR , 1/s T ) ? R H T s T ? W T s T ?3</formula><p>. Some methods such as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b86">87]</ref> additionally crop the LR image but for simplicity, we consider full LR images in this section.</p><p>As only source labels are available, the supervised categorical cross-entropy loss can only be calculated for the source predictions? S</p><formula xml:id="formula_2">LR = f ? (x S LR ) L S = L ce (? S LR , y S LR , 1) ,<label>(1)</label></formula><formula xml:id="formula_3">L ce (?, y, q) = ? H(y) i=1 W (y) j=1 C c=1 q ij y ijc log ?(?, H(y) H(?) ) ijc .<label>(2)</label></formula><p>As the predictions are usually smaller than the input due to the output stride of the segmentation network, they are upsampled to the label size H(y) ? W (y). However, a model trained only with L S usually does not generalize well to the target domain. In order to adapt the model to the target domain, UDA methods incorporate an additional loss for the target domain L T , which is added to the overall loss L = L S + ?L T . The target loss can be defined according to the used training strategies such as adversarial training <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b68">69]</ref> or self-training <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr">99]</ref>. In this work, we mainly evaluate HRDA with the self-training method DAFormer <ref type="bibr" target="#b28">[29]</ref>, as it is the current state-of-the-art method for UDA semantic segmentation. In self-training, the model is iteratively adapted to the target domain by training it with pseudo-labels for target images, predicted by a teacher network g ? :</p><formula xml:id="formula_4">p T LR,ijc = [c = arg max c ? g ? (x T LR ) ijc ? ] ,<label>(3)</label></formula><p>where [?] denotes the Iverson bracket. The pseudo-labels are used to additionally train the network f ? on the target domain</p><formula xml:id="formula_5">L T = L ce (? T LR , p T LR , q T LR ) .<label>(4)</label></formula><p>As the pseudo-labels are not necessarily correct, their quality is weighted by a confidence estimate q T LR <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr">99]</ref>. After each training step t, the teacher model g ? is updated with the exponentially moving average of the weights of f ? , implementing a temporal ensemble to stabilize pseudo-labels, which is a common strategy in semi-supervised learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b61">62]</ref> and UDA <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b62">63]</ref> </p><formula xml:id="formula_6">? t+1 ? ?? t + (1 ? ?)? t .<label>(5)</label></formula><p>Further, DAFormer <ref type="bibr" target="#b28">[29]</ref> uses consistency training <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b61">62]</ref>, i.e. the network f ? is trained on augmented target data following DACS <ref type="bibr" target="#b62">[63]</ref>, while the teacher model g ? generates the pseudo-labels using non-augmented target images. Besides self-training, DAFormer <ref type="bibr" target="#b28">[29]</ref> uses a domain-robust Transformer network, rare class sampling, and feature regularization based on ImageNet features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>In this work, we propose a multi-resolution framework for UDA as small objects and segmentation details are easier to adapt with high-resolution (HR) inputs, while large stuff regions are easier to adapt with low-resolution (LR) inputs. As UDA methods require more GPU memory than regular supervised training, we design a training strategy based on a large LR context crop to learn long-range context dependencies and a small HR detail crop to preserve segmentation details while maintaining a manageable GPU memory footprint (Sec. 4.1). The strengths of both LR context and HR detail crop are combined by fusing their predictions with a learned scale attention (Sec. 4.2). For a robust pseudo-label generation, we further utilize overlapping slide inference to fuse predictions with different contexts (Sec. 4.3). The proposed method is designed to be applicable to common network architectures and can be combined with existing UDA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Context and Detail Crop</head><p>Due to GPU memory constraints, it is not feasible to train state-of-the-art UDA methods with full-sized high-/multi-resolution inputs as images from multiple domains, additional networks, and additional losses are required for UDA training. Therefore, most previous works only use LR inputs. However, HR inputs are important to recognize small objects and produce fine segmentation borders. In order to still utilize HR inputs, random cropping is a possible solution. However, random cropping restricts learning context-aware semantic segmentation, especially for long-range dependencies and scene layout, which might be critical for UDA as context relations are often domain-invariant (e.g. car on road, rider on bicycle) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr">96]</ref>. In order to both train with long-range context as well as high resolution, we propose to combine different crop sizes for different resolutions, i.e. a large LR context crop and a small HR detail crop (see <ref type="figure" target="#fig_2">Fig. 2 a)</ref>. The purpose of the context crop is to provide a large crop to learn long-range context relations. The purpose of detail crop is to focus on HR to recognize small objects and produce fine segmentation details, which does not necessarily require faraway context information. In order to segment the entire image during model validation, overlapping sliding window inference is used (see Sec. 4.3).</p><p>The context crop x c ? R hc?wc?3 is obtained by cropping from the original HR image x HR ? R H?W ?3 and bilinear downsampling by the factor s ? 1</p><formula xml:id="formula_7">x c,HR = x HR [b c,1 : b c,2 , b c,3 : b c,4 ] , x c = ?(x c,HR , 1/s)<label>(6)</label></formula><p>The crop bounding box b c is randomly sampled from a discrete uniform distribution within the image size while ensuring that the coordinates can be divided by k = s ? o with o ? 1 denoting the output stride of the segmentation network to ensure exact alignment in the later fusion process:  The detail crop x d ? R h d ?w d ?3 is randomly cropped from within the context crop region, which is necessary to enable the fusion of context and detail predictions in the later process:</p><formula xml:id="formula_8">b c,1 ? U{0, (H ? sh c )/k} ? k , b c,2 = b c,1 + sh c ,<label>(7)</label></formula><formula xml:id="formula_9">b c,3 ? U{0, (W ? sw c )/k} ? k , b c,4 = b c,3 + sw c .</formula><formula xml:id="formula_10">x d = x c,HR [b d,1 : b d,2 , b d,3 : b d,4 ] ,<label>(8)</label></formula><formula xml:id="formula_11">b d,1 ? U{0, (sh c ? h d )/k} ? k , b d,2 = b d,1 + h d ,<label>(9)</label></formula><formula xml:id="formula_12">b d,3 ? U{0, (sw c ? w d )/k} ? k , b d,4 = b d,3 + w d .</formula><p>In this work, we use context and detail crops of the same dimension, i.e. h c = h d and w c = w d , to balance the required resources for both crops and provide a good trade-off between context-aware and detailed predictions. The context downscale factor is s = 2 following the LR design of previous UDA methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b62">63]</ref>, which means that the context crop covers 4 times more content at half the resolution compared to the detail crop.</p><p>Using a feature encoder f E and a semantic decoder f S , the context semantic</p><formula xml:id="formula_13">segmentation? c = f S (f E (x c )) ? R hc o ? wc o ?C and the detail semantic segmenta- tion? d = f S (f E (x d )) ? R h d o ? w d</formula><p>o ?C are predicted. The networks f E and f S are shared for both HR and LR inputs. This not only saves memory usage but also increases the robustness of the network against different resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Resolution Fusion</head><p>While the HR detail crop is well-suited to segment small objects such as poles or distant pedestrians, it lacks the ability to capture long-range dependencies, which is disadvantageous in segmenting large stuff regions such as large regions of sidewalk. The opposite is the case for the LR context crop. Therefore, we fuse the predictions from both crops using a learned scale attention <ref type="bibr" target="#b4">[5]</ref> to predict in which image regions to trust predictions from context and detail crop. Additionally, the scale attention provides the advantage that it enables adapting objects at the better-suited scale. For example, small objects are easier to adapt at HR while large objects are easier to adapt at LR as the appearance of an object should have a resolution high enough to be discriminative but not too high to avoid that the network overfits to domain-specific detailed textures.</p><p>The scale attention decoder f A learns to predict the scale attention</p><formula xml:id="formula_14">a c = ?(f A (f E (x c ))) ? [0, 1] hc o ? wc</formula><p>o ?C to weigh the trustworthiness of LR context and HR detail predictions. The sigmoid function ? ensures a weight in [0, 1], where 1 means a focus on the HR detail crop. The attention is predicted from the context crop as it has a better grasp on the scene layout (larger context). As the predictions are smaller than the inputs due to the output stride o, the crop coordinates are scaled accordingly in the following steps. Outside of the detail crop c d , the attention is set to 0 as there is no detail prediction available</p><formula xml:id="formula_15">a ? c ? R hc o ? wc o , a ? c (i, j) = a c (i, j) if b d,1 s?o ? i &lt; b d,2 s?o ? b d,3 s?o ? j &lt; b d,4 s?o 0 otherwise .<label>(10)</label></formula><p>The detail crop is aligned with the (upsampled) context crop by padding it with zeros to a size of shc o ? swc</p><formula xml:id="formula_16">? y ? d (i, j) = ? d (i ? b d,1 o , j ? b d,3 o ) if b d,1 o ? i &lt; b d,2 o ? b d,3 o ? j &lt; b d,4 o 0 otherwise .<label>(11)</label></formula><p>The predictions from multiple scales are fused using the attention-weighted sum</p><formula xml:id="formula_17">y c,F = ?((1 ? a ? c ) ?? c , s) + ?(a ? c , s) ?? ? d .<label>(12)</label></formula><p>The encoder, segmentation head, and attention head are trained with the fused multi-scale prediction and the detail crop prediction</p><formula xml:id="formula_18">L S HRDA = (1 ? ? d )L ce (? S c,F , y S c,HR , 1) + ? d L ce (? S d , y S d , 1) ,<label>(13)</label></formula><p>where the ground truth y S c,HR /y S d is cropped according to Eq. 6/8. Additionally supervising the detail crop is helpful to learn more robust features for HR inputs even though the attention might favor the context crop in that region. An additional loss for? c is not necessary as it is already directly supervised in regions without detail crop (see Eq. 10). The target loss L T HRDA is adapted accordingly</p><formula xml:id="formula_19">L T HRDA = (1 ? ? d )L ce (? T c,F , p T c,F , q T c,F ) + ? d L ce (? T d , p T d , q T d ) .<label>(14)</label></formula><p>For pseudo-label prediction, we also utilize multi-resolution fusion (see Sec. 4.3). Therefore, when predicting pseudo-labels the scale attention focuses on the better-suited resolution (e.g. HR for small objects). As the pseudo-labels are further used to train the model also with the worse-suited resolution (e.g. LR for small objects), it improves the robustness for both small and large objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pseudo-Label Generation with Overlapping Sliding Window</head><p>For self-training with Eq. 14, it is necessary to generate a high-quality HRDA pseudo-label p T c,F for the context crop x T c,HR . The underlying HRDA prediction y T c,F is fused from the LR prediction? T c and HR prediction? T c,HR using the full scale attention a T c similar to Eq. 12</p><formula xml:id="formula_20">y T c,F = ?((1 ? a T c ) ?? T c , s) + ?(a T c , s) ?? T c,HR .<label>(15)</label></formula><p>Therefore, the HR prediction? T c,HR is necessary for the entire context crop x T c,HR instead of just the detail crop x d . Note that for pseudo-label generation g ? instead of f ? is used for predictions. Even though large HR network inputs are problematic during training, they are not an issue during pseudo-label inference as no data for the backpropagation has to be stored. However, the used DAFormer <ref type="bibr" target="#b28">[29]</ref>, as well as other Vision Transformers <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b91">92]</ref>, have a learned (implicit) positional embedding that works best if training and inference input size are the same. Therefore, we infer the HR prediction? T c,HR using a sliding window of size h d ? w d over the HR context crop x T c,HR (see <ref type="figure" target="#fig_2">Fig 2 b</ref>). The window is shifted with a stride of h d /2 ? w d /2 to generate overlapping predictions with different contexts, which are averaged to increase robustness. The crops of the sliding window can be processed in parallel as the images in a batch, which allows for efficient computation on the GPU.</p><p>For model validation or deployment, the full-scale HRDA semantic segmentation? F ,HR of the entire image x HR is necessary. As the context crop is usually smaller than the entire image,? F ,HR is generated using an overlapping sliding window over the entire image x HR with a size of sh c ? sw c and a stride of sh c /2 ? sw c /2. Within the sliding window, the HRDA prediction is generated in the same way as? T c,F for the pseudo-label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Datasets: As target data, the real-world Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref> of European street scenes with 2975 training and 500 validation images of 2048?1024 pixels is used. As source data, the synthetic datasets GTA <ref type="bibr" target="#b51">[52]</ref> with 24,966 images of 1914?1052 pixels and Synthia <ref type="bibr" target="#b53">[54]</ref> with 9,400 images of 1280?760 pixels are used. Previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b79">80]</ref> downsample Cityscapes to 1024?512 and GTA to 1280?720. Instead, we maintain the full resolution for Cityscapes. To train with the same scale ratio of source and target images as previous works, we resize GTA to 2560?1440 and Synthia to 2560?1520 pixels. Network Architecture: Our default network is based on DAFormer <ref type="bibr" target="#b28">[29]</ref>. It consists of an MiT-B5 encoder <ref type="bibr" target="#b75">[76]</ref> and a context-aware feature fusion decoder <ref type="bibr" target="#b28">[29]</ref>. For the scale attention decoder, we use the lightweight SegFormer MLP decoder <ref type="bibr" target="#b75">[76]</ref> with an embedding dimension of 256. When evaluating other  <ref type="bibr" target="#b62">[63]</ref> data augmentation. For adversarial training and entropy minimization, we use SGD with a learning rate of 0.0025 and ? adv =? ent =0.001. The context and detail crop are generated using h c =w c =h d =w d =512 with s=2 to balance the required resources for both crops in the default case. The detail loss weight is chosen empirically ? d =0.1. The experiments are conducted on a Titan RTX GPU with 24 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with State-of-the-Art UDA Methods</head><p>First, we compare the proposed HRDA with previous UDA methods in Tab. 1. It can be seen that HRDA outperforms the previously best state-of-the-art method by a significant margin of +5.5 mIoU on GTA?Cityscapes and +4.9 mIoU on Synthia? Cityscapes. HRDA improves the IoU of almost all classes across both datasets. The highest performance gains are achieved for classes with fine segmentation details such as pole, traffic light, traffic sign, person, rider, motorbike, and bike. But also large classes such as truck, bus, and train benefit from HRDA. This is also reflected in the visual examples in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">HRDA for Different UDA Methods</head><p>HRDA is designed to be applicable to most UDA methods. In Tab. 2, we compare the performance without and with HRDA of three further representative UDA methods. It can be seen that HRDA consistently improves the performance by at least +2.4 mIoU, demonstrating the importance of high-and multi-resolution inputs for UDA in general. Also, it shows that HRDA can be applied to different network architectures. The highest improvement is achieved for self-training Image ProDA <ref type="bibr" target="#b86">[87]</ref> DAFormer <ref type="bibr" target="#b28">[29]</ref> HRDA (Ours) Ground Truth  methods (row 3-5) with +5.5 mIoU and more, which shows that the HRDA pseudo-labels positively reinforce the UDA process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Influence of Resolution and Crop Size on UDA</head><p>In the following, we analyze the underlying principles of HRDA on GTA? Cityscapes, starting with the influence of the resolution and crop size on UDA.</p><p>For the comparison, we use the relative crop size h/ H T s , which is normalized by the image height at the corresponding resolution, to disentangle the crop size from the used image resolution. <ref type="figure">Fig. 4</ref> shows that both an increased resolution and crop size improve the performance for both UDA and supervised learning. A large crop size is even more important for UDA than for supervised learning, i.e. a 4 times smaller LR crop reduces the performance by 39% for UDA and by 14% for supervised training. The larger crop provides more context clues and improves the performance of all classes, especially the ones that are difficult to adapt such as wall, fence, truck, bus, and train (cf. row 1 and 3 in <ref type="figure">Fig. 5</ref>), probably, as the relevant context clues are more domain-invariant <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr">96]</ref>. A higher input resolution improves the UDA performance by a similar amount as it improves supervised learning. The improvement originates from a higher IoU for small classes such as pole, traffic light, traffic sign, person, motorbike, and bicycle, while some large classes such as road, sidewalk, and terrain have a decreased performance (cf. row 1 and 2 in <ref type="figure">Fig. 5</ref>). This supports that large objects are easier to adapt at LR while small objects are easier to adapt at HR, which can be exploited by the multi-resolution fusion of HRDA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Combining Crops from Multiple Resolutions with HRDA</head><p>Multi-Resolution UDA: Next, we combine crops from LR and HR using the proposed multi-resolution training for UDA. Tab. 3 shows that training with multiple resolutions improves the performance over both LR-only and HR-only training by +3.4 mIoU (cf. row 2 and 3), which demonstrates that multi-resolution fusion with scale attention results in better domain adaptation. Context Crop Size: Based on the observation that large crops are important for UDA (Sec. 5.4), we increase the context crop size while keeping the detail crop size fixed, which further improves the performance by +5.3 mIoU (cf. row 3 and 5), demonstrating the effectiveness of the proposed small HR detail and large LR context crops. <ref type="figure">Fig. 5</ref> shows that the multi-resolution training combines the strength of the single-scale training with HR 0.5 and LR 1.0 as the multi-resolution IoU of each class is better than the best single-scale IoU (cf. row 2, 3, and 4). Detail Crop Size: Already the combination of the context crop LR 1.0 with an even smaller detail crop HR 0.25 outperforms training with solely the context crop by +2.1 mIoU (cf. row 1 and 3 in Tab. 4), even though HR 0.25 alone performs ?20.8 mIoU worse (cf. row 1 and 2). This shows that the multi-resolution fusion effectively exploits the strength of the small detail crop while compensating for its lacking long-range dependencies with the context crop. Further increasing the detail crop size, results in additional performance gains (cf. row 2 and 5). This shows that even though context information is not crucial for the detail    crop, it is still helpful to some extent while being limited due to GPU memory constraints. Detail Crop Variants: It is crucial for HRDA to use context/detail crops with different resolutions. Using LR instead of HR for the detail crop gives only a marginal gain of +0.6 over the baseline (cf. row 1 and 2 in Tab. 6). However, using an LR crop that is bilinearly upsampled to HR as detail crop does improve the performance by +3.4 mIoU (cf. row 1 and 3 in Tab. 6) but is still -1.9 mIoU worse than using a real HR detail crop (cf. row 3 and 4 in Tab. 6). This shows that the improved performance of HRDA comes from both the additional zoomed-in context information as well as the additional details in the HR image. Comparison with Naive HR: We compare HRDA with naive large HR crops (HR 0.75 ), which have a comparable GPU memory footprint as HRDA. This is a very strong baseline, which is already +1.7 mIoU better than DAFormer <ref type="bibr" target="#b28">[29]</ref>. Tab. 5 shows that HRDA still outperforms HR 0.75 crops by +3.8 mIoU (cf. row 1 and 3). Even when reducing the crop size of HRDA to match the size of HR 0.75 , HRDA is still +1.3 mIoU better while requiring 40% less GPU memory. This demonstrates that combining LR context crop and HR detail crop performs better than naively increasing the resolution, due to HRDA's capability of capturing large context information and multi-resolution fusion. HRDA Component Ablations: The components of HRDA are ablated in Tab. 7. The most crucial component is the learned scale attention. While naively averaging the predictions from both scales gives no improvement over just using the context crop (cf. row 2 and 3), the learned scale attention improves the performance by +3.0 mIoU (cf. row 2 and 4). This shows that it is crucial to learn which scale is best-suited to adapt certain image regions. Generating pseudolabels with different context views by overlapping slide detail crops results in a <ref type="table">Table 7</ref>. Component ablation of HRDA.</p><p>Context Detail Scale Attention Overlapping Detail Detail Loss mIoU</p><formula xml:id="formula_21">1 - ? - - - 65.1 ?1.9 2 ? - - - - 68.5 ?0.9 3 ? ? Average - - 67.5 ?0.8 4 ? ? Learned - - 71.5 ?0.5 5 ? ? Learned ? - 72.4 ?0.1 6 ? ? Learned ? ? 73.8 ?0.3</formula><p>Image LR Pred. HR Pred. Scale Attent. Fused Pred. G. Truth further gain of +0.9 mIoU (cf. row 4 and 5). Finally, additional supervision of the detail crop (? d = 0.1) further provides +1.4 mIoU (cf. row 5 and 6). Qualitative Analysis: <ref type="figure" target="#fig_4">Fig. 6</ref> provides representative visual examples demonstrating that LR predictions work better for large objects such as a bus or sidewalk (row 1/2) while HR predictions work better for small objects and fine details (row 3). The scale attention focuses on LR for large objects and on HR for small objects and segmentation borders, combining the strength of both. Supplement: The supplement provides further parameter studies, additional baseline comparisons, a runtime analysis, and an extended qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we presented HRDA, a multi-resolution approach for UDA that combines the advantages of small HR detail crops and large LR context crops using a learned scale attention, while maintaining a manageable GPU memory footprint. It can be combined with various UDA methods and achieves a consistent, significant improvement. Overall, HRDA achieves an unprecedented performance of 73.8 mIoU on GTA?Cityscapes and 65.8 mIoU on Synthia?Cityscapes, which is a respective gain of +5.5 mIoU and +4.9 mIoU over the previous SOTA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Source Code</head><p>The source code to reproduce HRDA and all ablation studies is provided at https://github.com/lhoyer/HRDA. Please, refer to the contained README.md for further instructions to set up the environment and run the experiments. Our implementation is based on the DAFormer framework <ref type="bibr" target="#b28">[29]</ref> and the mmsegmentation framework <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Influence of Detail Loss Weight</head><p>In <ref type="figure" target="#fig_0">Fig. S1</ref>, the sensitivity of the UDA performance of HRDA with respect to the detail loss weight ? d is studied. It is shown that values in the range between 0.1 and 0.3 give a consistently good UDA performance, which is a reasonably broad range for a robust hyperparameter choice. If ? d is either too small or too large, HRDA focuses too much on LR or HR, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Influence of Context Scale</head><p>We further study the influence of the downscale factor of the context crop s c in Tab. S1. It can be seen that the default downscale factor s c = 2 provides the best performance. A context crop with a higher downscale factor s c = 4 performs worse by -2.6 mIoU than the default choice (cf. row 2 and 3) but is still slightly better than just using the detail crop by +0.8 mIoU (cf. row 1 and 2). We assume that with s c = 4 the resolution of the context crop is too low to be useful for UDA. A context crop with a small downscale factor s c = 1.33 performs better than the high downscale factor s c = 4 by +1.6 mIoU (cf. row 2 and 4) but still does not achieve the performance of the default s c = 2 with a difference of -1.0 mIoU. Possibly, the resolution of s c = 1.33 is too similar to the detail crop resolution s d = 1 and, therefore, it does not provide a sufficiently different perspective on the data, which is important for multi-resolution UDA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Further Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Scale-Invariance Loss</head><p>As another baseline for HRDA, we integrate the scale-invariance loss of Guan et al. <ref type="bibr" target="#b23">[24]</ref> into DAFormer <ref type="bibr" target="#b28">[29]</ref>. The optimal loss weight when combined with DAFormer is determined with a grid search as 0.    <ref type="bibr" target="#b28">[29]</ref>, which is the basis of HRDA. This effect might be caused by the shape bias of the used Transformer encoder as discussed in DAFormer <ref type="bibr" target="#b28">[29]</ref>. Possibly, the performance for the mentioned stuff classes could be improved for HRDA by integrating the depth-clues as done in CorDA <ref type="bibr" target="#b70">[71]</ref> or pseudo-label prototypes as used in ProDA <ref type="bibr" target="#b86">[87]</ref>. Further, Tab. S5 and Tab. S6 show the performance of HRDA when used with a DeepLabV2 network instead of a DAFormer network. It can be seen that HRDA DeepLabV2 outperforms all DeepLabV2-based UDA methods (all methods except DAFormer) on GTA?Cityscapes and that it even outperforms DAFormer on Synthia?Cityscapes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Further Qualitative Examples</head><p>In <ref type="figure" target="#fig_2">Fig. S2-S6</ref>, we compare the predicted semantic segmentation of HRDA to the two strongest previous UDA methods from Tab. S5, namely ProDA <ref type="bibr" target="#b86">[87]</ref> and DAFormer <ref type="bibr" target="#b28">[29]</ref>. Further, we visualize the scale attention of HRDA as the weighted sum over the scale attention channels for each class with weight being the softmax of the segmentation prediction. White regions mean that HRDA focuses on the prediction from the HR input.</p><p>Overall, HRDA better recognizes small classes and segments finer details. This is especially the case for distant poles, traffic lights, and traffic signs (see <ref type="figure" target="#fig_2">Fig. S2</ref>) as well as distant pedestrians, riders, bicycles, and motorcycles (see <ref type="figure" target="#fig_3">Fig. S3</ref>). For these regions, HRDA uses the prediction from the HR input as can be seen in the HRDA scale attention (white encodes a focus on HR). Further, HRDA is able to better recognize difficult stuff classes such as sidewalk and wall (see <ref type="figure">Fig. S4</ref>) as well as to better distinguish different vehicle classes (see <ref type="figure">Fig. S5</ref>). HRDA uses LR input for that purpose as can be seen in the HRDA scale attention (black encodes a focus on LR).</p><p>Even though HRDA sets new standards, UDA is still a challenging task. This can be observed for classes that are easy to confuse with others and that have a considerable domain gap such as sidewalk, terrain, or fence, which results in adaptation errors (see <ref type="figure" target="#fig_4">Fig. S6</ref>).</p><p>HRDA Att. HRDA (Ours) DAFormer <ref type="bibr" target="#b28">[29]</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Most previous UDA methods were only trained with downscaled inputs to account for their high GPU memory footprint. (b) Our HRDA incorporates fine segmentation details from a random high-resolution (HR) detail crop and context information from a random low-resolution (LR) context crop. Their predictions are fused using a learned scale attention (best viewed zoomed-in). In that way, HRDA can utilize HR details and long-range context information while keeping a manageable memory footprint. (c) Compared to previous works, HRDA provides a major performance gain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>o?w d /o sh c /o?sw c /o h c /o?w c /o h c /o?w c /o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Multi-resolution training with low-resolution (LR) context and highresolution (HR) detail crop. The prediction of the detail crop is fused into the context prediction within the region where it was cropped from by a learned scale attention. (b) For pseudo-label generation, multiple detail crops are generated using overlapping slide inference to cover the entire context crop. The pseudo-label is fused from HR pred.? T c,HR and LR pred.? T c with the full attention a T c similar to (a) (see Sec. 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative comparison of HRDA with previous methods on GTA?Cityscapes. HRDA improves the segmentation of small classes such as pole, traffic sign, traffic light, and rider as well as large and difficult classes such as bus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Visual examples of the different predictions and the scale attention of HRDA. Large objects are better segmented from LR while small objects are better segmented from HR. The scale attention learns to utilize this pattern for fusing LR and HR predictions. The examples are zoomed in (2x or 6x) for better visibility of the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Acknowledgements: This work is supported by the European Lighthouse on Secure and Safe AI (ELSA) and a Facebook Academic Gift on Robust Perception (INFO224). 96. Zhou, Q., Feng, Z., Gu, Q., Pang, J., Cheng, G., Lu, X., Shi, J.,Ma, L.: Contextaware mixup for domain adaptive semantic segmentation. In: WACV. pp. 514-524 (2021) 3, 4, 6, 11 97. Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised learning of depth and ego-motion from video. In: CVPR. pp. 1851-1858 (2017) 2 98. Zhou, Y., Sun, X., Zha, Z.J., Zeng, W.: Context-reinforced semantic segmentation. In: CVPR. pp. 4046-4055 (2019) 4 99. Zou, Y., Yu, Z., Kumar, B., Wang, J.: Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In: ECCV. pp. 289-305 (2018) 1, 2, 4, 5, 10, 25 100. Zou, Y., Yu, Z., Liu, X., Kumar, B., Wang, J.: Confidence regularized self-training. In: ICCV. pp. 5982-5991 (2019) 25 101. Zou, Y., Zhang, Z., Zhang, H., Li, C.L., Bian, X., Huang, J.B., Pfister, T.: Pseudoseg: Designing pseudo labels for semantic segmentation. In: ICLR (2021) 1Supplementary Material A OverviewIn the supplementary material for HRDA, we provide the source code (Sec. B), additional experimental analysis (Sec. C and D), comparisons with further baselines (Sec. E), an analysis of the runtime (Sec. F), an extended comparison with previous UDA methods (Sec. G), and a comprehensive qualitative analysis of the predictions from HRDA (Sec. H).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Study of the UDA performance of HRDA with respect to the detail loss weight ? d on GTA?Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. S6 .</head><label>S6</label><figDesc>Failure cases of classes with a low UDA performance such as sidewalk, terrain, and fence on GTA?Cityscapes. Some examples are zoomed in for better visibility of the details. The zoom factor is provided in the bottom left corner of each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with previous UDA methods. The results of HRDA are averaged over 3 random seeds. Further methods are shown in the supplement. Road S.walk Build. Wall Fence Pole Tr.Light Sign Veget. Terrain Sky Person Rider Car Truck Bus Train M.bike Bike mIoU 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4 57.5 DAFormer [29] 95.7 70.2 89.4 53.5 48.1 49.6 55.8 59.4 89.9 47.9 92.5 72.2 44.7 92.3 74.5 78.2 65.1 55.9 61.8 68.3 HRDA (Ours) 96.4 74.4 91.0 61.6 51.5 57.1 63.9 69.3 91.3 48.4 94.2 79.0 52.9 93.9 84.1 85.7 75.9 63.9 67.5 73.8</figDesc><table><row><cell></cell><cell cols="2">GTA5 ? Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CBST [99]</cell><cell>91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9</cell><cell cols="5">34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 45.9</cell></row><row><cell>DACS [63]</cell><cell>89.9 39.7 87.9 30.7 39.5 38.5 46.4 52.8 88.0</cell><cell cols="4">44.0 88.8 67.2 35.8 84.5 45.7 50.2 0.0</cell><cell>27.3 34.0 52.1</cell></row><row><cell>CorDA [71]</cell><cell>94.7 63.1 87.6 30.7 40.6 40.2 47.8 51.6 87.6</cell><cell cols="4">47.0 89.7 66.7 35.9 90.2 48.9 57.5 0.0</cell><cell>39.8 56.0 56.6</cell></row><row><cell>BAPA [41]</cell><cell>94.4 61.0 88.0 26.8 39.9 38.3 46.1 55.3 87.8</cell><cell cols="4">46.1 89.4 68.8 40.0 90.2 60.4 59.0 0.0</cell><cell>45.1 54.2 57.4</cell></row><row><cell>ProDA [87]</cell><cell cols="3">87.8 Synthia ? Cityscapes</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CBST [99]</cell><cell>68.0 29.9 76.3 10.8 1.4 33.9 22.8 29.5 77.6</cell><cell>-</cell><cell>78.3 60.6 28.3 81.6</cell><cell>-</cell><cell>23.5 -</cell><cell>18.8 39.8 42.6</cell></row><row><cell>DACS [63]</cell><cell>80.6 25.1 81.9 21.5 2.9 37.2 22.7 24.0 83.7</cell><cell>-</cell><cell>90.8 67.6 38.3 82.9</cell><cell>-</cell><cell>38.9 -</cell><cell>28.5 47.6 48.3</cell></row><row><cell>BAPA [41]</cell><cell>91.7 53.8 83.9 22.4 0.8 34.9 30.5 42.8 86.6</cell><cell>-</cell><cell>88.2 66.0 34.1 86.6</cell><cell>-</cell><cell>51.3 -</cell><cell>29.4 50.5 53.3</cell></row><row><cell>CorDA [71]</cell><cell>93.3 61.6 85.3 19.6 5.1 37.8 36.6 42.8 84.9</cell><cell>-</cell><cell>90.4 69.7 41.8 85.6</cell><cell>-</cell><cell>38.4 -</cell><cell>32.6 53.9 55.0</cell></row><row><cell>ProDA [87]</cell><cell>87.8 45.7 84.6 37.1 0.6 44.0 54.6 37.0 88.1</cell><cell>-</cell><cell>84.4 74.2 24.3 88.2</cell><cell>-</cell><cell>51.1 -</cell><cell>40.5 45.6 55.5</cell></row><row><cell cols="2">DAFormer [29] 84.5 40.7 88.4 41.5 6.5 50.0 55.0 54.6 86.0</cell><cell>-</cell><cell>89.8 73.2 48.2 87.2</cell><cell>-</cell><cell>53.2 -</cell><cell>53.9 61.7 60.9</cell></row><row><cell cols="2">HRDA (Ours) 85.2 47.7 88.8 49.5 4.8 57.2 65.7 60.9 85.3</cell><cell>-</cell><cell cols="3">92.9 79.4 52.8 89.0 -64.7 -</cell><cell>63.9 64.9 65.8</cell></row></table><note>UDA methods in Tab. 2, we use a ResNet101 [25] backbone with a DeepLabV2 [3] decoder both as segmentation and scale attention head. Training: By default, we follow the DAFormer [29] self-training strategy (see Sec. 3) and training parameters, i.e. AdamW [44] with a learning rate of 6?10 ?5 for the encoder and 6?10 ?4 for the decoder, a batch size of 2, linear learning rate warmup, ? st =1, ?=0.999, and DACS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>HRDA applied to different UDA methods on GTA?Cityscapes. Mean and standard deviation are provided over 3 random seeds.</figDesc><table><row><cell>UDA Method</cell><cell>Network</cell><cell cols="3">w/o HRDA w/ HRDA Improvement</cell></row><row><cell cols="3">1 Entropy Min. [67] DeepLabV2 [3] 44.3 ?0.4</cell><cell>46.7 ?1.2</cell><cell>+2.4</cell></row><row><cell>2 Adversarial [64]</cell><cell cols="2">DeepLabV2 [3] 44.2 ?0.1</cell><cell>47.1 ?1.0</cell><cell>+2.9</cell></row><row><cell>3 DACS [63]</cell><cell cols="2">DeepLabV2 [3] 53.9 ?0.6</cell><cell>59.4 ?1.2</cell><cell>+5.5</cell></row><row><cell>4 DAFormer [29]</cell><cell cols="2">DeepLabV2 [3] 56.0 ?0.5</cell><cell>63.0 ?0.4</cell><cell>+7.0</cell></row><row><cell>5 DAFormer [29]</cell><cell cols="2">DAFormer [29] 68.3 ?0.5</cell><cell>73.8 ?0.3</cell><cell>+5.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Segmentation performance over the relative crop size (h/ H T s ) for different resolutions and for both the UDA method DAFormer<ref type="bibr" target="#b28">[29]</ref> and the target-supervised oracle. There is no value for HR1.0 due to GPU memory constraints.Fig. 5. Class-wise IoU for the UDA method DAFormer [29] for different crop resolutions XR (sLR=2, sHR=1) and relative crop sizes a=h/ H T s XR . Crops are denoted as XRa. The colors indicate the difference to the first row.</figDesc><table><row><cell></cell><cell></cell><cell>UDA (DAFormer)</cell><cell></cell><cell></cell><cell>Supervised Learning (Oracle DAFormer)</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mIoU</cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Resolution LR (s=2)</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HR (s=1)</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell>0.50 Relative Crop Size 0.75</cell><cell>1.00</cell><cell>0.25</cell><cell>0.50 Relative Crop Size 0.75</cell><cell>1.00</cell></row><row><cell cols="7">S.walk 96 71 88 46 27 44 50 56 89 45 91 70 40 91 60 66 39 50 61 62 Build. Wall Fence Pole T.Light T.Sign Veget. Terrain Sky Person Rider Car Truck Bus Train M.bike Bike mIoU Fig. 4. Road LR0.5 HR0.5 93 63 89 47 27 52 56 62 90 40 91 76 41 92 61 66 67 56 67 65 LR1.0 96 71 89 54 49 49 56 60 90 50 92 72 46 92 69 80 67 57 62 68 LR1.0+HR0.5 96 74 91 62 51 57 64 69 91 48 94 79 53 94 84 86 76 64 68 74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>HRDA context size. XRa denotes crops with resolution XR (sLR=2, sHR=1) and relative crop size a=h/ H T s XR .</figDesc><table><row><cell cols="3">Context Crop Detail Crop mIoU</cell></row><row><cell>1 LR0.5</cell><cell>-</cell><cell>62.1 ?2.1</cell></row><row><cell>2 -</cell><cell>HR0.5</cell><cell>65.1 ?1.9</cell></row><row><cell>3 LR0.5</cell><cell>HR0.5</cell><cell>68.5 ?0.6</cell></row><row><cell>4 LR0.75</cell><cell>HR0.5</cell><cell>71.1 ?1.7</cell></row><row><cell>5 LR1.0</cell><cell>HR0.5</cell><cell>73.8 ?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>HRDA detail size. XRa denotes crops with resolution XR (sLR=2, sHR=1) and relative crop size a=h/ H T s XR .</figDesc><table><row><cell cols="3">Context Crop Detail Crop mIoU</cell></row><row><cell>1 LR1.0</cell><cell>-</cell><cell>68.5 ?0.9</cell></row><row><cell>2 -</cell><cell>HR0.25</cell><cell>47.7 ?2.4</cell></row><row><cell>3 LR1.0</cell><cell>HR0.25</cell><cell>70.6 ?0.7</cell></row><row><cell>4 LR1.0</cell><cell>HR0.375</cell><cell>71.7 ?0.4</cell></row><row><cell>5 LR1.0</cell><cell>HR0.5</cell><cell>73.8 ?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison of HRDA with naive HR crops that have a comparable GPU memory footprint (HR0.75).</figDesc><table><row><cell cols="2">Context Detail</cell><cell>Mem.</cell><cell>mIoU</cell></row><row><cell>1 -</cell><cell cols="3">HR0.75 22.0 GB 70.0 ?1.2</cell></row><row><cell>2 LR0.75</cell><cell cols="3">HR0.375 13.5 GB 71.3 ?0.3</cell></row><row><cell>3 LR1.0</cell><cell>HR0.5</cell><cell cols="2">22.5 GB 73.8 ?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>HRDA detail crop variants. Up-LR: LR crop upsampled to HR resolution.</figDesc><table><row><cell cols="3">Context Crop Detail Crop mIoU</cell></row><row><cell>1 LR1.0</cell><cell>-</cell><cell>68.5 ?0.9</cell></row><row><cell>2 LR1.0</cell><cell>LR0.5</cell><cell>69.1 ?0.4</cell></row><row><cell>3 LR1.0</cell><cell>Up-LR 0.5</cell><cell>71.9 ?1.5</cell></row><row><cell>4 LR1.0</cell><cell>HR0.5</cell><cell>73.8 ?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S1 .</head><label>S1</label><figDesc>Influence of the context downscale factor s on HRDA performance. A larger downscale factor s results in a lower crop resolution. The relative crop size is a=h/ H T s .</figDesc><table><row><cell>Context sc</cell><cell cols="2">Context Rel. Size ac Detail s d</cell><cell cols="2">Detail Rel. Size a d mIoU</cell></row><row><cell>1 -</cell><cell>-</cell><cell>1 (HR)</cell><cell>0.5</cell><cell>65.1 ?1.9</cell></row><row><cell>2 4</cell><cell>0.5</cell><cell>1 (HR)</cell><cell>0.5</cell><cell>65.9 ?1.2</cell></row><row><cell>3 2 (LR)</cell><cell>0.5</cell><cell>1 (HR)</cell><cell>0.5</cell><cell>68.5 ?0.6</cell></row><row><cell>4 1.33</cell><cell>0.5</cell><cell>1 (HR)</cell><cell>0.5</cell><cell>67.5 ?0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>In Sec. 5.5 of the main paper, we compared HRDA with the naive HR crops (HR 0.75 ) for DAFormer<ref type="bibr" target="#b28">[29]</ref>. Here, we extend this comparison also to DACS<ref type="bibr" target="#b62">[63]</ref>, which uses another UDA method and network architecture. Tab. S2 col. 5 shows that naive HR training improves the performance of DACS similarly to DAFormer. HRDA outperforms naive HR training by +3.6 mIoU for DACS and +3.8 mIoU for DAFormer.</figDesc><table><row><cell>E.1 Overlapping Sliding Window Inference (OSW)</cell></row><row><cell>Prior works in the field of UDA (including DAFormer) use whole image inference</cell></row><row><cell>while HRDA utilizes overlapping sliding window inference (OSW). To show that</cell></row><row><cell>the improvement of HRDA is not mainly caused by the OSW inference, we also</cell></row><row><cell>evaluate prior arts with OSW (see Tab. S2 col. 4). It can be seen that OSW only</cell></row><row><cell>slightly benefits DAFormer by +0.3 mIoU. Still, HRDA outperforms DAFormer</cell></row><row><cell>with OSW by +5.2 mIoU.</cell></row><row><cell>E.2 Naive High-Resolution UDA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S2 .</head><label>S2</label><figDesc>Overlapping sliding window inference (OSW) and naive HR training for different UDA methods and network architectures on GTA?Cityscapes. ?0.6 53.9 ?0.6 55.8 ?1.6 59.4 ?1.2 DAFormer [29] DAFormer [29] 68.3 ?0.5 68.6 ?0.3 70.0 ?1.2 73.8 ?0.3</figDesc><table><row><cell>UDA Method</cell><cell>Network</cell><cell>Baseline</cell><cell>OSW</cell><cell>Naive HR+OSW HRDA</cell></row><row><cell>DACS [63]</cell><cell cols="2">DeepLabV2 [3] 53.9</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>1. As shown in Tab. S3, DAFormer with scale-invariance loss<ref type="bibr" target="#b23">[24]</ref> achieves 68.8 mIoU on GTA?Cityscapes, which is only a small gain of +0.5 mIoU over DAFormer, while HRDA achieves +5.5 mIoU. We assume that the effect of the scale-invariance loss is not so pronounced as DAFormer is much stronger (68.3 mIoU) than the original baseline(43.8  mIoU). This further emphasizes that the contribution of HRDA goes beyond scale consistency training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S3 .</head><label>S3</label><figDesc>Comparison of scale consistency training and HRDA on GTA?Cityscapes.The training of HRDA takes 32h on a Titan RTX. To put this into context, the training of other UDA methods<ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b86">87]</ref> can take several days. The runtime and memory consumption of HRDA during inference is shown in Tab. S4. The inference runs with 0.8 img/s, 3.3 TFLOPs, and 9.4 GB GPU memory. The focus of HRDA is UDA performance and not fast inference as the latter is not an inherent constraint of UDA. Still, if efficient inference is important, nonoverlapping sliding window inference (stride equal to window size) can be used at test-time resulting in 2.2 img/s, 1.8 TFLOPs, and 3.2 GB with still 73.4 mIoU. Alternatively, the knowledge of HRDA can be distilled into a faster single-scale DeepLabV2<ref type="bibr" target="#b2">[3]</ref> model, which is commonly used for UDA. For that purpose, the multi-resolution HRDA is utilized to generate high-quality pseudo-labels for the target domain and the single-scale DeepLabV2 model is trained on the target domain with a pixel-wise cross-entropy loss using the pseudo-labels. The distilled DeepLabV2 model achieves 70.4 mIoU at 3.4 img/s, 1.4 TFLOPs, and 1.3 GB. Both results are significantly better than the previous SOTA performance of DAFormer<ref type="bibr" target="#b28">[29]</ref>, which is 68.3 mIoU.</figDesc><table><row><cell></cell><cell cols="3">Baseline w/ Scale-Invariance [24] w/ HRDA</cell></row><row><cell>Original [24]</cell><cell>43.8</cell><cell>48.1</cell><cell>-</cell></row><row><cell cols="2">DAFormer [29] 63.3</cell><cell>63.8</cell><cell>73.8</cell></row><row><cell cols="3">F Training and Inference Time</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S4 .</head><label>S4</label><figDesc>Runtime and memory consumption of HRDA variants during inference on an Nvidia Titan RTX.We extend the comparison of HRDA with previous UDA methods from the main paper by a large selection of further methods for GTA?Cityscapes in Tab. S5 and for Synthia?Cityscapes in Tab. S6. It can be seen that HRDA DAFormer (the default HRDA based on DAFormer) still outperforms previous UDA methods by a large margin both for the class-wise IoU as well as the overall mIoU. The highest performance gains are achieved for classes with fine segmentation details such as pole, traffic light, traffic sign, person, rider, motorbike, and bike. But also large classes such as truck, bus, and train benefit from HRDA. Only a few classes such as road, sidewalk, fence, and vegetation on Synthia?Cityscapes have a lower performance than the respective best comparison method. The comparably low performance is probably inherited from DAFormer</figDesc><table><row><cell></cell><cell cols="4">Throughput (img/s) TFLOPs GPU Mem. (GB) mIoU</cell></row><row><cell>HRDA</cell><cell>0.8</cell><cell>3.3</cell><cell>9.4</cell><cell>73.8</cell></row><row><cell cols="2">HRDA w/ Non-Overlapping SW 2.2</cell><cell>1.8</cell><cell>3.2</cell><cell>73.4</cell></row><row><cell cols="2">HRDA w/ Distilled DeepLabV2 3.4</cell><cell>1.4</cell><cell>1.3</cell><cell>70.4</cell></row><row><cell cols="5">G Extended Comparison with Previous UDA Methods</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S5 .</head><label>S5</label><figDesc>Comparison with previous UDA methods on GTA?Cityscapes. The results of HRDA are averaged over 3 random seeds.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Fig. S2. Example predictions showing a better recognition and finer segmentation details of small classes such as pole, traffic light, and traffic sign on GTA?Cityscapes. Some examples are zoomed in for better visibility of the details. The zoom factor is provided in the bottom left corner of each image.Fig. S3. Example predictions showing a better recognition and finer segmentation of small distant classes such as pedestrian, rider, motorcycle and bicycle on GTA?Cityscapes. Some examples are zoomed in for better visibility of the details. The zoom factor is provided in the bottom left corner of each image.Fig. S4. Example predictions showing a better recognition of difficult stuff classes such as sidewalk and wall on GTA?Cityscapes. Some examples are zoomed in for better visibility of the details. The zoom factor is provided in the bottom left corner of each image.Fig. S5. Example predictions showing a better differentiation of vehicle classes such as car, truck, bus, and train on GTA?Cityscapes. Some examples are zoomed in for better visibility of the details. The zoom factor is provided in the bottom left corner of each image.</figDesc><table><row><cell>Image Image Image Image Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G. Truth G. Truth G. Truth G. Truth G. Truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ProDA [87] HRDA Att. HRDA (Ours) DAFormer [29] ProDA [87] HRDA Att. HRDA (Ours) DAFormer [29] ProDA [87] HRDA Att. HRDA (Ours) DAFormer [29] ProDA [87] HRDA Att. HRDA (Ours) DAFormer [29] ProDA [87]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>road sky road sky road sky road sky road sky</cell><cell>sidew. person sidew. person sidew. person sidew. person sidew. person</cell><cell>build. rider build. rider build. rider build. rider build. rider</cell><cell>wall car wall car wall car wall car wall car</cell><cell>fence truck fence truck fence truck fence truck fence truck</cell><cell>pole bus pole bus pole bus pole bus pole bus</cell><cell>tr. light tr. sign veget. train m.bike bike tr. light tr. sign veget. train m.bike bike tr. light tr. sign veget. train m.bike bike tr. light tr. sign veget. train m.bike bike tr. light tr. sign veget. train m.bike bike</cell><cell>terrain n/a. terrain n/a. terrain n/a. terrain terrain n/a. n/a.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention to scale: Scaleaware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<ptr target="https://www.cityscapes-dataset.com/license/1,9" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITSC. pp</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC (2020)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2107.096004" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>ACMMM</publisher>
			<biblScope unit="page" from="2825" to="2833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dlow: Domain flow and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scale variance minimization for unsupervised domain adaptation in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Three ways to improve semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DAFormer: Improving network architectures and training strategies for domain-adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2022) 1</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving semi-supervised and domain-adaptive semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12545</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grid saliency for context explanations of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Contextual-relation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mlsl: Multi-level self-supervised learning for domain adaptation with spatially independent and semantically consistent labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-Supervised Semantic Segmentation With Directional Context-Aware Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Spigan: Privileged adversarial learning from simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving semantic segmentation via decoupled body and edge supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zigzagnet: Fusing top-down and bottom-up context for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Bapa-net: Boundary adaptation and prototype alignment for cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cross-domain semantic segmentation via domain-invariant interactive relation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Coarse-to-fine domain adaptive semantic segmentation with photometric alignment and category-center regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pixmatch: Unsupervised domain adaptation via pixelwise consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Manrai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Domain bridge for unpaired image-to-image translation and unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pizzati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaccaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cerri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WACV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Augco: Augmentation consistencyguided self-training for source-free domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kartik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10140</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<ptr target="https://download.visinf.tu-darmstadt.de/data/from_games/2,9" />
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Medical Image Computing and Computerassisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<ptr target="http://synthia-dataset.net/" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>dataset license: CC BY-NC-SA 3.0</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning from scale-invariant examples for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Subhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">DACS: Domain Adaptation via Cross-domain Mixed Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV. pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Scribble-supervised lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Unal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Dada: Depth-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Classes matter: A finegrained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Continual test-time domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS (2021)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">An adversarial perturbation oriented domain adaptation approach for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Context-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>WACV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Attention to refine through multi scales for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Ocnet: Object context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Segfix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Multiple fusion adaptation: A strong framework for unsupervised semantic segmentation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>BMVC</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A curriculum domain adaptation approach to the semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequenceto-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Unsupervised scene adaptation with memory regularization in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Uncertaintyaware consistency regularization for cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08878</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Wall Fence Pole Tr.Light Sign Veget. Terrain Sky Person Rider Car Truck Bus Train M.bike Bike mIoU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Road</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Build</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Wall Fence Pole Tr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Road</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Build</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Light Sign Veget. Sky Person Rider Car Bus M.bike Bike mIoU16 mIoU13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gio-Ada</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>8] 78.3 29.2 76.9 11.4 0.3 26.5 10.8 17.2 81.7 81.9 45.8 15.4 68.0 15.9 7.5 30.4</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seg-Uncert</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
