<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Loss Landscape in Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Abacus.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Nolen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Abacus.AI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Savani</surname></persName>
							<email>ysavani@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Abacus.AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring the Loss Landscape in Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) has seen a steep rise in interest over the last few years. Many algorithms for NAS consist of searching through a space of architectures by iteratively choosing an architecture, evaluating its performance by training it, and using all prior evaluations to come up with the next choice. The evaluation step is noisy -the final accuracy varies based on the random initialization of the weights. Prior work has focused on devising new search algorithms to handle this noise, rather than quantifying or understanding the level of noise in architecture evaluations. In this work, we show that (1) the simplest hill-climbing algorithm is a powerful baseline for NAS, and (2), when the noise in popular NAS benchmark datasets is reduced to a minimum, hill-climbing to outperforms many popular state-of-the-art algorithms. We further back up this observation by showing that the number of local minima is substantially reduced as the noise decreases, and by giving a theoretical characterization of the performance of local search in NAS. Based on our findings, for NAS research we suggest (1) using local search as a baseline, and (2) denoising the training pipeline when possible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Neural architecture search (NAS) is a widely popular area of machine learning which seeks to automate the development of the best neural network for a given dataset. Many methods for NAS have been proposed, including reinforcement learning, gradient descent, and Bayesian optimization <ref type="bibr">[Elsken et al., 2018</ref><ref type="bibr" target="#b31">, Zoph and Le, 2017</ref><ref type="bibr" target="#b4">, Liu et al., 2018b</ref>. Many popular NAS algorithms can be instantiated by the optimization problem min a?A f (a), where A denotes a set of architectures (the search space) and f (a) denotes the objec-tive function for a, often set to the validation accuracy of a after training using a fixed set of hyperparameters. With the release of three NAS benchmark datasets <ref type="bibr" target="#b29">[Ying et al., 2019</ref><ref type="bibr">, Dong and Yang, 2020</ref><ref type="bibr" target="#b17">, Siems et al., 2020</ref>, the extreme computational cost for NAS is no longer a barrier, and it is easier to fairly compare different algorithms. However, recent work has noticed that the training step used in these benchmarks is quite stochastic <ref type="bibr" target="#b29">[Ying et al., 2019</ref><ref type="bibr" target="#b17">, Siems et al., 2020</ref>. In NAS, where the goal is to search over complex neural architectures, a noisy reward function makes the problem even more challenging. In fact, recent innovations are designed specifically to handle the noisy objective function <ref type="bibr" target="#b30">, Zaidi et al., 2020</ref>. A natural question is therefore, how much of the complexity of NAS can be attributed to the noise in the training pipeline?</p><p>In this work, 1 we answer this question by showing that the difficulty of NAS is highly correlated with the noise in the training pipeline. We show that (1) the simplest local search algorithm (hill-climbing) is already a strong baseline in NAS, and (2), when the noise in the training pipeline is reduced to a minimum, local search is sufficient to outperform many state-of-the-art techniques.</p><p>Local search is a simple and canonical greedy algorithm in combinatorial optimization and has led to famous results in the study of approximation algorithms <ref type="bibr" target="#b7">[Michiels et al., 2007</ref><ref type="bibr">, Cohen-Addad et al., 2016</ref><ref type="bibr">, Friggstad et al., 2019</ref>. However, local search has been neglected in the field of NAS; a recent paper even suggests that it performs poorly due to the number of local minima throughout the search space <ref type="bibr" target="#b20">[Wang et al., 2018]</ref>. The most basic form of local search, often called the hill-climbing algorithm, consists of starting with a random architecture and then iteratively training all architectures in its neighborhood, choosing the best one for the next iteration. The neighborhood is typically defined as all architectures which differ by one operation or edge. Local search finishes when it reaches a (local or global) optimum, or when it exhausts its runtime budget.</p><p>We show that on NAS-Bench-101, 201, and 301/ DARTS <ref type="bibr" target="#b29">[Ying et al., 2019</ref><ref type="bibr">, Dong and Yang, 2020</ref><ref type="bibr" target="#b17">, Siems et al., 2020</ref><ref type="bibr" target="#b4">, Liu et al., 2018b</ref>, if the noise is reduced to a minimum, then local search is competitive with all popular state-of-the-art NAS algorithms. This result is especially surprising because local search, which can be implemented in five lines of code (Algorithm 1), is in stark contrast to most state-of-the-art NAS algorithms, which have many moving parts and even use neural networks as subroutines <ref type="bibr" target="#b22">[Wen et al., 2019</ref><ref type="bibr" target="#b16">, Shi et al., 2019</ref>. This suggests that the complexity in prior methods may have been developed to deal with the noisy reward function. We also experimentally show that as the noise in the training pipeline increases, the number of local minima increases, and the basin of attraction of the global minimum decreases. These results further suggest that NAS becomes easier when the noise is reduced.</p><p>Motivated by these findings, we also present a theoretical study to better understand the performance of local search under different levels of noise. The underlying optimization problem in NAS is a hybrid between discrete optimization, on a graph topology, and continuous optimization, on the distribution of architecture accuracies. We formally define a NAS problem instance by the graph topology, a global probability density function (PDF) on the architecture accuracies, and a local PDF on the accuracies between neighboring architectures, and we derive a set of equations which calculate the probability that a randomly drawn architecture will converge to within of the global optimum, for all &gt; 0. As a corollary, we give equations for the expected number of local minima, and the expected size of the preimage of a local minimum. These results completely characterize the performance of local search. To the best of our knowledge, this is the first result which theoretically predicts the performance of a NAS algorithm, and may be of independent interest within discrete optimization. We run simulations which suggest that our theoretical results predict the performance on real datasets reasonably well.</p><p>Our findings raise a few points for the field of NAS. Since much of the difficulty is in the stochasticity of the training pipeline, denoising the training pipeline as much as possible is worthwhile for future work. Second, our work suggests that local methods for NAS may be promising. That is, methods which explore the search space by iteratively making small edits to the best architectures found so far. Furthermore, we suggest using local search as a baseline for future work. We release our code, and we discuss our adherence to the NAS research checklist <ref type="bibr" target="#b2">[Lindauer and Hutter, 2019]</ref>.</p><p>Our contributions. We summarize our contributions.</p><p>? We show that local search is a strong baseline in its own right, and outperforms many state-of-the-art NAS algorithms across three popular benchmark datasets when the noise in the training pipeline is reduced to a minimum. We also show that the number of local minima increases as the noise in the training pipeline increases. Our results suggest that making the training pipeline in NAS more consistent, is just as worthwhile as coming up with novel search algorithms. ? We give a theoretical characterization of the properties of a dataset necessary for local search to give strong performance. We experimentally validate these results on real neural architecture search datasets. Our results improve the theoretical understanding of local search and lay the groundwork for future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Local search has been studied since at least the 1950s in the context of the traveling salesman problem <ref type="bibr">[Bock, 1958</ref><ref type="bibr">, Croes, 1958</ref>, machine scheduling <ref type="bibr" target="#b10">[Page, 1961]</ref>, and graph partitioning <ref type="bibr">[Kernighan and Lin, 1970]</ref>. Local search has consistently seen significant attention in theory <ref type="bibr">[Aarts and Lenstra, 1997</ref><ref type="bibr">, Balcan et al., 2020</ref><ref type="bibr">, Johnson et al., 1988</ref> and practice <ref type="bibr">[Bentley, 1992, Johnson and</ref><ref type="bibr">McGeoch, 1997]</ref>. There is also a large variety of work in local optimization of noisy functions, handling the noise by averaging the objective function over multiple evaluations <ref type="bibr" target="#b13">[Rakshit et al., 2017</ref><ref type="bibr">, Akimoto et al., 2015</ref>, using surrogate models <ref type="bibr">[Booker et al., 1998, Caballero and</ref><ref type="bibr">Grossmann, 2008]</ref>, or using regularization .</p><p>NAS has gained popularity in recent years <ref type="bibr" target="#b0">[Kitano, 1990</ref><ref type="bibr" target="#b19">, Stanley and Miikkulainen, 2002</ref><ref type="bibr" target="#b31">, Zoph and Le, 2017</ref>, although the first few techniques have been around since at least the 1990s <ref type="bibr" target="#b28">[Yao, 1999</ref><ref type="bibr" target="#b15">, Shah et al., 2018</ref>. Common techniques include Bayesian optimization <ref type="bibr">[Kandasamy et al., 2018</ref><ref type="bibr" target="#b15">, Jin et al., 2018</ref>, reinforcement learning <ref type="bibr" target="#b31">[Zoph and Le, 2017</ref><ref type="bibr" target="#b11">, Pham et al., 2018</ref><ref type="bibr" target="#b3">, Liu et al., 2018a</ref>, gradient descent <ref type="bibr">[Liu et al., 2018b, Dong and</ref><ref type="bibr">Yang, 2019]</ref>, predictionbased <ref type="bibr" target="#b24">[White et al., 2021a</ref><ref type="bibr" target="#b16">, Shi et al., 2019</ref><ref type="bibr" target="#b25">, White et al., 2021b</ref>, evolution <ref type="bibr" target="#b6">[Maziarz et al., 2018</ref>, and using novel encodings to improve the search <ref type="bibr" target="#b23">[White et al., 2020</ref><ref type="bibr" target="#b26">, Yan et al., 2020</ref><ref type="bibr">, 2021</ref>.</p><p>Recent papers have highlighted the need for fair and reproducible NAS comparisons <ref type="bibr">Talwalkar, 2019, Lindauer and</ref>, spurring the release of three NAS benchmark datasets <ref type="bibr" target="#b29">[Ying et al., 2019</ref><ref type="bibr">, Dong and Yang, 2020</ref><ref type="bibr" target="#b17">, Siems et al., 2020</ref>, each of which utilize tens of thousands of pretrained neural networks. See the recent survey <ref type="bibr">[Elsken et al., 2018]</ref> for a more comprehensive overview on NAS.</p><p>There has been some prior work using local search for NAS. Elsken et al. <ref type="bibr">[2017]</ref> use local search with network morphisms guided by cosine annealing, which is a more complex variant. <ref type="bibr" target="#b20">Wang et al. [2018]</ref> use local search as a baseline, but kill the run after encountering a local minimum rather than using the remaining runtime budget to start a new run. Concurrent work has also shown that simple local search is a strong baseline on NASBench-101 <ref type="bibr" target="#b9">[Ottelander et al., 2020]</ref> for multi-objective NAS (where the objective is a function of accuracy and network complexity). This work focuses on macro search rather than cell-based search, and does not investigate the effect of noise on the performance. The existence of this work strengthens one of our conclusions (that local search is a strong NAS algorithm) because it is now independently verified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we formally define the local search algorithm and notation that will be used for the rest of the paper. Given a set A, denote an objective function : A ? [0, ?). We refer to A as a search space of neural architectures, and (v) as the expected validation loss of v ? A over a fixed dataset and training pipeline. When running a NAS algorithm, we have access to a noisy version of , i.e., when we train an architecture a, we receive loss= (v) + x for noise x drawn from a distribution D v (we explore different families of distributions in Sections 4 and 5). The goal is to find v * = argmin v?A (v), the neural architecture with the minimum validation loss, or an architecture whose validation loss is within of the minimum, for some small &gt; 0. We define a neighborhood function N : A ? 2 A . For instance, N (v) might represent the set of all neural architectures which differ from v by one operation or edge.</p><p>Local search in its simplest form (also called the hillclimbing algorithm) is defined as follows. Start with a random architecture v and evaluate (v) by training v. Iteratively train all architectures in N (v), and then replace v with the architecture u such that u = argmin w?N (v) (w). Continue until we reach an architecture v such that ?u ? N (v), (v) ? (u), i.e., we reach a local minimum. See Algorithm 1. We often place a runtime bound on the algorithm, in which case the algorithm returns the architecture v with the lowest value of (v) when it exhausts the runtime budget.</p><p>In Section 4, we also consider two simple variants. In the query_until_lower variant, instead of evaluating every architecture in the neighborhood N (v) and picking the best one, we draw architectures u ? N (v) at random without replacement and move to the next iteration as soon as (u) &lt; (v).</p><p>In the continue_at_min variant, we do not stop at a local minimum, instead moving to the second-best architecture found so far and continuing until we exhaust the runtime budget. One final variant, which we explore in Appendix A, is choosing k initial architectures at random, and setting v 1 to be the architecture with the lowest objective value.</p><p>Notation. Now we define the notation used in Sections 4 and 5. Given a search space A and a neighborhood function N , we define the neighborhood graph G N = (A, E N ) such that for u, v ? A, the edge (u, v) is in E N if and only if </p><formula xml:id="formula_0">v 0 ) = ?; set i = 1 3. While (v i ) &lt; (v i?1 ) : i. Evaluate (u) for all u ? N (v i ) ii. Set v i+1 = argmin u?N (vi) (u); set i = i + 1 Output: Architecture v i v ? N (u). We assume that v ? N (u) implies u ? N (v)</formula><p>, therefore, the neighborhood graph is undirected. We only consider symmetric neighborhood functions, that is, v ? N (u) implies u ? N (v). Therefore, we may assume that the neighborhood graph is undirected. Given G, N , and a loss function , define LS :</p><formula xml:id="formula_1">A ? A such that ?v ? A, LS(v) = argmin u?N (v) (u) if min u?N (v) (u) &lt; (v),</formula><p>and LS(v) = ? otherwise. In other words, LS(v) denotes the architecture after performing one iteration of local search starting from v. See <ref type="figure" target="#fig_2">Figure 4</ref>.1 for an example. For integers</p><formula xml:id="formula_2">k ? 1, recursively define LS k (v) = LS(LS k?1 (v)). We set LS 0 (v) = v and denote LS * (v) = min k|LS k (v) =? LS k (v)</formula><p>, that is, the output when running local search to convergence, starting at v. Similarly, define the preimage</p><formula xml:id="formula_3">LS ?k (v) = {u | LS k (u) = v} for integers k ? 1 and LS ? * (v) = {u | ?k ? 0 s.t. LS ?k (u) = v}. That is, LS ? * (v)</formula><p>is a multifunction which defines the set of all points u which reach v at some point during local search. We refer to LS ? * as the full preimage of v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we discuss our experimental setup and results. To promote reproduciblity, we discuss how our experiments follow the best practices checklist <ref type="bibr" target="#b2">[Lindauer and Hutter, 2019]</ref> in Appendix A, and we release our code at https://github.com/naszilla/naszilla. We start by describing the benchmark datasets used in our experiments.</p><p>NAS benchmarks. To conduct our experiments, we use three of the most popular NAS benchmarks: NASBench-101, 201, and 301/DARTS. NASBench-101 consists of over 423,000 unique neural architectures with precomputed validation and test accuracies for 108 epochs on CIFAR-10. The cell-based search space consists of five nodes which can take on any DAG structure, and each node can be one of three operations. Each architecture was trained a total of three times using different random seeds. The NASBench-201 dataset consists of 5 6 = 15, 625 unique neural architectures, with precomputed validation and test accuracies for 200 epochs on CIFAR-10, CIFAR-100, and ImageNet-16-120.</p><p>The search space consists of a cell which is a complete directed acyclic graph over 4 nodes. Therefore, there are 4 2 = 6 edges. Each edge can be one of five operations. As in NASBench-101, on each dataset, each architecture was trained three times using different random seeds.</p><p>The DARTS <ref type="bibr" target="#b4">[Liu et al., 2018b]</ref> search space is a popular search space for large-scale cell-based NAS experiments on CIFAR-10. The search space contains roughly 10 18 architectures, consisting of two cells: a convolutional cell and a reduction cell, each with six nodes. The first two nodes are input from previous layers, and the last four nodes can take on any DAG structure such that each node has degree two. Each edge can take one of eight operations. Recently, a surrogate benchmark, dubbed NASBench-301 <ref type="bibr" target="#b17">[Siems et al., 2020]</ref>, has been created for the DARTS search space. The surrogate benchmark is created using an ensemble of XG-Boost models <ref type="bibr">[Chen and Guestrin, 2016]</ref>, each initialized with different random weights.</p><p>Local search performance. We evaluate the relative performance of local search for NAS in settings with and without noise. In a real-world NAS experiment, noise reduction can be achieved by modifying the training hyperparameters, which we discuss later in this section. However, modifying the hyperparameters of the NASBench architectures is impractical due to the extreme computational cost needed to create the benchmarks <ref type="bibr">[Ying et al., 2019, Dong and</ref><ref type="bibr">Yang, 2020]</ref>. Instead, we artificially remove much of the noise in these benchmarks using two different techniques. On NASBench-101 and 201, where each architecture was independently trained three times, the standard way to use the benchmark is to draw validation accuracies at random. However, for each architecture, we can average all three validation accuracies to obtain a less noisy estimate. On NASBench-301, where the standard way to evaluate architectures is by using one estimate from the ensemble uniformly at random, we can take the mean of all of the ensemble estimates. This is shown to be less noisy even than the data used to train the ensemble itself <ref type="bibr" target="#b17">[Siems et al., 2020]</ref>. In general, we can easily control the noise of NASBench-301 by returning the mean of the ensemble estimates plus a random normal variable with mean 0 and standard deviation equal to the standard deviation of the ensemble estimates, multiplied by a constant x. x = 0 corresponds to the denoised setting, while x = 1 corresponds to the standard setting.</p><p>We compare local search to random search, regularized evolution , Bayesian optimization, and BANANAS <ref type="bibr" target="#b24">[White et al., 2021a]</ref>. For every algorithm, we used the code directly from the corresponding open source repositories. For more details on the implementations, see Appendix A. We gave each algorithm a budget of 300 evaluations. For each algorithm, we recorded the test loss of the architecture with the best validation loss Local minima statistics. Now we further show that denoised NAS is a simpler optimization problem by computing statistics about the loss landscape of the noisy and denoised search spaces. We start by running experiments on NASBench-201. Since this benchmark is only size 15625, the global minimum is known, which allows us to compute the percent of architectures that converge to the global minimum when running local search. We also compute the number of local minima and average number of iterations of local search before convergence. We run experiments using the standard and denoised versions of NASBench-201 (defined earlier in this section), and we also use a fully randomized version by replacing the validation error for each architecture with a number drawn uniformly from [0, 1]. For each experiment, we started local search from all 15625 initial seeds for local search, and averaged the results. See <ref type="table" target="#tab_1">Table 1</ref>. On the denoised search space, almost half of the 15625 architectures converge to the global minimum, but under 7% reach the global minimum on the standard search space. In <ref type="figure" target="#fig_2">Figure 4</ref>.1, we give a visualization of the topologies of the denoised and fully random search spaces.</p><p>Finally, we run experiments on NASBench-301. Due to the extreme size (10 18 ), the global minimum is not known. However, as described above, the surrogate nature of NASBench-301 allows for a more fine-grained control of the noise. In <ref type="figure" target="#fig_2">Figure 4</ref>.3, we plot the performance of NAS algorithms with respect to the level of noise in the search space. We also show that the average number of local search iterations needed for convergence decreases with noise.    Discussion. The simple local search algorithm achieves competitive performance on all NAS benchmarks, beating out many popular algorithms. Furthermore, we see a distinct trend across different benchmarks showing that local search performs best (relative to other algorithms) when the noise in the training pipeline is reduced to a minimum. Further experimentation shows that with less noise, there are fewer local minima and local search takes more iterations to converge. These results suggest that NAS becomes substantially easier when the noise is reduced -enough for a very simple algorithm to achieve strong performance. Since local search can be implemented in five lines of code, we encourage local search to be used as a benchmark in future work. We also suggest denoising the noise in the training pipeline. This can be achieved by techniques such as cosine annealing the learning rate <ref type="bibr" target="#b5">[Loshchilov and Hutter, 2016]</ref>, batch normalization <ref type="bibr">[Ioffe and Szegedy, 2015]</ref>, and regularization techniques such as dropout <ref type="bibr">[Baldi and Sadowski, 2013]</ref> and early-stopping <ref type="bibr" target="#b12">[Prechelt, 1998]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THEORETICAL CHARACTERIZATION</head><p>In this section, we give a theoretical analysis of local search for NAS, including a complete characterization of its performance. We present a general result which can be applied to any NAS search space. We also give an experimental validation of our results at the end of the section, which suggests that our theoretical results predict the performance of real datasets reasonably well.</p><p>In a NAS application, the topology of the search space is fixed and discrete, while the distribution of validation losses is randomized and continuous, due to the non-deterministic nature of training a neural network. Therefore, we assume that the validation loss for a trained architecture is sampled from a global probability distribution, and for each architecture, the validation losses of its neighbors are sampled from a local probability distribution. Recall the definitions of G N and LS from the end of Section 3. Given a graph G N = (A, E N ), each node v ? A has a loss (v) ? R sampled from a PDF which we denote by pdf n . For any two neighbors (v, u) ? E N , the PDF for the validation loss x of architecture u is given by pdf e ( (v), x). Choices for the distribution pdf e are constrained by the fixed topology of the search space, as well as the distribution pdf n . In Appendix B, we discuss this further by formally defining measurable spaces for all random variables in our framework.</p><p>Our main result is a formula for the fraction of nodes in the search space which are local minima, as well as a formula for the fraction of nodes v such that the loss of LS * (v) is within of the loss of the global optimum, for all ? 0. In other words, we give a formula for the probability that the local search algorithm outputs a solution that is close to opti-mal. Note that such a formula characterizes the performance of local search. We give the full proofs for all of our results in Appendix B. For the rest of this section, we assume for all v ? A, |N (v)| = s, and we assume G N is vertex transitive (given u, v ? A, there exists an automorphism of G N which maps u to v). Let v * denote the architecture with the global minimum loss, therefore the support of the distribution of validation losses is a subset of</p><formula xml:id="formula_4">[ (v * ), ?). That is, ? (v) pdf n (v)dv = 1.</formula><p>Technically, the integrals in this section are Lebesgue integrals. However, we use the more standard Riemann-Stieltjes notation for clarity. We also slightly abuse notation and define LS ? * (v) = LS ? * (x) when (v) = x. In the following statements, we assume there is a fixed graph G N , and the validation accuracies are randomly assigned from a distribution defined by pdf n and pdf e . Therefore, the expectations are over the random draws from pdf n and pdf e . 2 Theorem 5.1. Given |A| = n, , s, , pdf n , and pdf e ,</p><formula xml:id="formula_5">E[|{v ? A | LS * (v) = v}|] = n ? (v * ) pdf n (x) ? x pdf e (x, y)dy s dx, and E[|{v ? A | (LS * (v)) ? (v * ) ? }|] = n (v * )+ (v * ) pdf n (x) ? x pdf e (x, y)dy s ? E[|LS ? * (x)|]dx.</formula><p>Proof sketch.. To prove the first statement, we introduce an indicator random variable to test if the architecture is a local minimum:</p><formula xml:id="formula_6">I(v) = I{LS * (v) = v}. Then E[|{v ? A | LS * (v) = v}|] = n ? P ({v ? A | I(v) = 1}) = n ? (v * ) pdf n (x) ? x pdf e (x, y)dy s dx.</formula><p>Intuitively, in the proof of the second statement, we follow similar reasoning but multiply the probability in the outer integral by the expected size of v's full preimage to weight the integral by the probability a random point will converge to v. Formally, we introduce an indicator random variable on the architecture space that tests if a node will terminate on a local minimum that is within of the global minimum:</p><formula xml:id="formula_7">I (v) = I{LS * (v) = u ? l(u) ? l(v * ) ? }</formula><p>We use this random variable along with the first statement of the theorem, to prove the second statement.</p><formula xml:id="formula_8">E[|{v ? A | (LS * (v)) ? (v * ) ? }|] = n ? P ({I = 1}) = n (v * )+ (v * ) pdf n (x) ? (v) pdf e (x, y)dy s ? E[|LS ? * (x)|]dx</formula><p>In Appendix B, we use Theorem 5.1 along with Chebyshev's Inequality <ref type="bibr">[Chebyshev, 1867]</ref> to show that, in the case where the validation accuracy of each architecture has Gaussian noise, the expected number of local minima can be bounded in terms of the standard deviation of the noise. In the next lemma, we derive a recursive equation for |LS ? * (v)|. We define the branching fraction of graph</p><formula xml:id="formula_9">G N as b k = |N k (v)|/ (|N k?1 (v)| ? |N (v)|), where N k (v)</formula><p>denotes the set of nodes which are distance k to v in G N . For example, the branching fraction of a tree with degree d is 1 for all k, and the branching fraction of a clique is b 1 = 1 and b k = 0 for all k &gt; 1. One more example is as follows. In Appendix A, we show that the neighborhood graph of the NASBench-201 search space is (K 5 ) 6 and therefore its branching factor is b k = 6?k+1 6k . Lemma 5.2. Given A, , s, pdf n , and pdf e , then for all v ? A, we have the following equations. </p><formula xml:id="formula_10">E[|LS ?1 (v)|] = s ? (v) pdf e ( (v), y) (5.1) ? ? (v) pdf e (y, z)dz s?1 dy, and E[|LS ?k (v)|] = b k?1 ? E[|LS ?1 (v)|] (5.2) ? ? (v) pdf e ( (v), y)E[|LS ?(k?1) (y)|]dy ? (v) pdf e ( (v),</formula><formula xml:id="formula_11">+ s ? G( (v)) s ? e G( (v)) s , where G(x) = ?</formula><p>x g(y)dy. Now we use a similar technique to give a closed-form expression for Theorem 5.1 when the local and global distributions are uniform. We stress that this lemma is simply an application of Lemma 5.2, and our main results (Theorem 5.1 and Lemma 5.2) hold without any assumptions on the local and global distributions.</p><formula xml:id="formula_12">Lemma 5.3. If pdf n (x) = pdf e (x, y) = U ([0, 1]) ?x ? A, then E[|{v | v = LS * (v)}|] = n s+1 and E[|{v | (LS * (v)) ? (v * ) ? }|] = n ? i=0 ? ? s i 1 ? (1 ? ) (i+1)s+1 (i + 1)s + 1 ? i?1 j=0 b j js + 1 ? ? .</formula><p>Proof sketch.. The probability density function of U ([0, 1]) is equal to 1 on [0, 1] and 0 otherwise. Then ? x pdf e (x, y)dy = 1 x dy = (1 ? x). We use this in combination with Theorem 5.1 to prove the first statement:</p><formula xml:id="formula_13">E[|{v | v = LS * (v)}|] = n ? (v * ) 1 ? (1 ? x) s dx = n s + 1 .</formula><p>To prove the second statement, first we use induction on the expression in Lemma 5.2 to show that for all v ? A,</p><formula xml:id="formula_14">E[|LS ? * (v)|] = ? k=0 E[|LS ?k (v)|] = ? k=0 s k (1 ? (v)) sk ? k?1 i=0 b i is + 1 .</formula><p>We plug this into the second part of Theorem 5.1:</p><formula xml:id="formula_15">E[|{v | (LS * (v)) ? (v * ) ? }|] = n (v * )+ (v * ) 1 ? (1 ? x) s ? k=0 E[|LS ?k (x)|]dx = n (v * )+ (v * ) (1 ? x) s ? k=0 s k (1 ? x) sk k?1 i=0 b j is + 1 dx = n ? i=0 ? ? s i 1 ? (1 ? ) (i+1)s+1 (i + 1)s + 1 ? i?1 j=0 b j js + 1 ? ? .</formula><p>In the next section, we show that our theoretical results can be used to predict the performance of local search.</p><p>Simulation Results. We run a local search simulation using the equations in the previous section as a means of experimentally validating our theoretical results with real data (we use NASBench-201). In order to use these equations, first we must approximate the local and global probability density functions of the three datasets in NASBench-201. We note that approximating these distributions are not feasible for large search spaces; the purpose of our theoretical results are meant only to provide a deeper understanding of local search and lay the groundwork for future studies. We start by visualizing the probability density functions of the three datasets. See <ref type="figure">Figure 5</ref>.2. We see the most density  along the diagonal, meaning that architectures with similar accuracy are more likely to be neighbors. Therefore, we can approximate the PDFs by using the following equation:</p><formula xml:id="formula_16">pdf(u) = 1 ? ? 2? ? e ? 1 2 ( u?v ? ) 2 1 0 1 ? ? 2? ? e ? 1 2 ( w?v ? ) 2 dw (5.3)</formula><p>This is a normal distribution with mean u ? v and standard deviation ?, truncated so that it is a valid PDF in [0, 1]. We note that prior work has also modeled architecture accuracies in NAS with a normal distribution . To model the global PDF for each dataset, we plot a histogram of the validation losses and match them to the closest-fitting values of ? and v. See <ref type="figure">Figure 5</ref>.1. The best values of ? are 0.18, 0.1, and 0.22 for CIFAR-10, CIFAR-100, and ImageNet16-120, respectively, and the best values for v are all 0.25. To model the local PDF for each dataset, we compute the random walk autocorrelation (RWA) on each dataset. RWA is defined as the autocorrelation of the accuracies of points visited during a random walk on the neighborhood graph <ref type="bibr" target="#b21">[Weinberger, 1990</ref><ref type="bibr" target="#b18">, Stadler, 1996</ref>, and was used to measure locality in NASBench-101 in prior work <ref type="bibr" target="#b29">[Ying et al., 2019]</ref>. For the full details of the steps taken to model the datasets in NASBench-201, see Appendix A. Now we use Theorem 5.1 to compute the probability that a randomly drawn architecture will converge to within of the global minimum when running local search. Since there is no closed-form solution for the expression in Lemma 5.2, we compute Theorem 5.1 up to the 5th preimage. We compare this to the experimental results on NASBench-201. We also compare the performance of the NASBench-201 search space with validation losses drawn uniformly at random, to the performance predicted by Lemma 5.3. Finally, we compare the preimage sizes of the architectures in NASBench-201 with randomly drawn validation losses to the sizes predicted in Lemma 5.2. See <ref type="figure" target="#fig_2">Figure 4</ref>.2. Our theory exactly predicts the performance and the preimage sizes of the uniform random NASBench-201 dataset. On the three image datasets, our theory predicts the performance fairly accurately, but is not perfect due to our assumption that the distribution of accuracies is unimodal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We show that the difficulty of NAS scales dramatically with the level of noise in the architecture evaluation pipeline, on popular NAS benchmarks (NASBench-101, 201, and 301). In particular, the simplest local search algorithm is sufficient to outperform popular state-of-the-art NAS algorithms when the noise in the evaluation pipeline is reduced to a minimum. We further show that as the noise increases, the number of local minima increases, and the basin of attraction to the global minimum shrinks. This suggests that when the noise in popular NAS benchmarks is reduced to a minimum, the number of local minima decreases, making the loss landscape easy to traverse. Since local search is a simple technique that often gives competitive performance, we encourage local search to be used as a benchmark for NAS in the future. We also suggest denoising the training pipeline whenever possible in future NAS applications.</p><p>Motivated by our findings, we give a theoretical study which explains the performance of local search for NAS on different search spaces. We define a probabilistic graph optimization framework to study NAS problems, and we give a characterization of the performance of local search for NAS in our framework. Our results improve the theoretical understanding of local search and lay the groundwork for future studies. We validate this theory with experimental results. Investigating more sophisticated variants of local search for NAS such as Tabu search, simulated annealing, or multi-fidelity local search, are interesting next steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work done while all authors were employed at Abacus.AI. We thank Willie Neiswanger for his help with this project.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILS FROM SECTION 4</head><p>In this section, we give details and supplementary results for Section 4.</p><p>For every benchmark NAS algorithm, we used the code directly from its corresponding open-source repository. For regularized evolution, we changed the population size from 50 to 30 to account for fewer queries. We did not change any hyperparameters for the other baseline algorithms. For vanilla Bayesian optimization, we used the ProBO implementation <ref type="bibr" target="#b8">[Neiswanger et al., 2019]</ref>. Our experimental setup is the same as prior work (e.g., <ref type="bibr" target="#b29">[Ying et al., 2019]</ref>). At each timestep t, we report the test error of the architecture with the best validation error found so far, and we run 200 trials of each algorithm and average the result.</p><p>Now we give the local search experimental results for all three datasets of NASBench-201. This is a similar experimental setup to the plots in <ref type="figure" target="#fig_2">Figure 4</ref>.2 (left and middle), but for NASBench-201. See <ref type="figure">Figure A.</ref>1. We tested the simplest local search algorithm (hill-climbing), as well as the continue_at_min variant described in Section 3, which we denote by Local++. Note that for the case of ImageNet16-120, the initial level of noise is so high that all NAS algorithms actually perform worse in the reduced noise version of the problem. Now we evaluate the performance of local search as a function of the number of initial random architectures drawn at the beginning. We run local search with the number of initial random architectures set to 1, and 10 to 100 in increments of 10. For each number of initial random architectures, we ran 2000 trials and averaged the results. See <ref type="figure">Figure A.</ref>2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DETAILS FROM SIMULATION EXPERIMENTS</head><p>In this section, we give more details for our simulation experiment described in Section 4.</p><p>For convenience, we restate Equation 5.3, the function used to approximate the datasets in NASBench-201.</p><formula xml:id="formula_17">pdf(u) = 1 ? ? 2? ? e ? 1 2 ( u?v ? ) 2 1 0 1 ? ? 2? ? e ? 1 2 ( w?v ? ) 2 dw</formula><p>This is a normal distribution with mean u ? v and standard deviation of ?, truncated so that it is a valid PDF in [0, 1]. For a visualization, see <ref type="figure">Figure A</ref>.3. In order to choose an appropriate probability density function for modelling the datasets in NASBench-201, we approximate the ? values for both the local and global PDFs.</p><p>To model the global PDF for each dataset, we plot a histogram of the validation losses and match them to the closestfitting values of ? and v. See <ref type="figure">Figure 5</ref>.1. The best values are ? = 0.18, 0.1, and 0.22 for CIFAR-10, CIFAR-100, and ImageNet16-120, respectively. Now we plot the random-walk autocorrelation (RWA) described in Section 4. Recall that RWA is defined as the autocorrelation of the accuracies of points visited during a walk of random single changes through the search space <ref type="bibr" target="#b21">[Weinberger, 1990</ref><ref type="bibr" target="#b18">, Stadler, 1996</ref>, and was used to measure locality in NASBench-101 in prior work <ref type="bibr" target="#b29">[Ying et al., 2019]</ref>. We compute the RWA for all three datasets in NASBench-201, by performing a random walk of length 100,000. See <ref type="figure" target="#fig_2">Figure A.4</ref>. We see that all three datasets in NASBench-201, as evidenced because there is a high correlation at distances close to 0. As the diameter of NASBench-201 is 6, the correlation approaches zero at distances beyond about 3.5. In order to model the local pdfs of each dataset, we also compute the RWA for Equation 5.3, and match each dataset with the closest value of ?. We see that a value of ? = 0.35 is the closest match for all three datasets.</p><p>Now for each of the three NASBench-201 datasets, we have estimates for the pdf e and pdf n distributions. We plug each (pdf e , pdf n ) pair into Theorem 5.1, which gives a plot of vs. percent of architectures that converge to within of the global optimum after running local search. We compare these to the true plot in <ref type="figure" target="#fig_2">Figure 4</ref>.2. For the random simulation, we are modeling the case where pdf e = pdf n = U ([0, 1]), so we can use Lemma 5.3 directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 BEST PRACTICES FOR NAS RESEARCH</head><p>The area of NAS research has had issues with reproducibility and fairness in empirical comparisons <ref type="bibr">Talwalkar, 2019, Ying et al., 2019]</ref>, and there is now a checklist for best practices <ref type="bibr" target="#b2">[Lindauer and Hutter, 2019]</ref>. In order to promote best practices, we discuss each point on the list, and encourage all NAS research papers to do the same.</p><p>? Releasing code. Our code is publicly available at https://github.com/naszilla/ naszilla. The training pipelines and search spaces are from popular existing NAS work: NASBench-101, NASBench-201, and NASBench-301.</p><p>? Comparing NAS methods. We made fair comparisons due to our use of NASBench-101, 201, and 301. For baseline comparisons, we used open-source code, a few times adjusting hyperparameters to be more appropriate for the search space. We ran ablation studies, compared to random search, and compared performance over time. We performed 200 trials on tabular benchmarks.</p><p>? Reporting important details. Local search only has two boolean hyperparameters, so we did not need to tune hyperparameters. We reported the times for the full NAS method and all details for our experimental setup.    N after N steps, we plot the x-axis as the square root as the autocorrelation shift, similar to prior work <ref type="bibr" target="#b29">[Ying et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DETAILS FROM SECTION 5</head><p>In this section, we give details from Section 5. For convenience, we restate all theorems and lemmas here.</p><p>We start by formally defining all measurable spaces in our theoretical framework. Recall that the topology of the search space is fixed and discrete, while the distribution of validation losses for architectures is randomized and continuous. This is because training a neural network is not deterministic; in fact, both NASBench-101 and NASBench-201 include validation and test accuracies for three different random seeds for each architecture, to better simulate real NAS experiments. Therefore, we assume that the validation loss for a trained architecture is sampled from a global probability distribution, and for each architecture, the validation losses of its neighbors are sampled from a local probability distribution.</p><p>Let (R, B(R)) denote a measurable space for the global validation losses induced by the dataset on the architectures, where B(R) is the Borel ?-algebra on R. The distribution for the validation loss of any architecture in the search space is given by pdf n (x)?x ? R.</p><p>Let (R 2 , B(R 2 )) denote a measurable space for validation losses in a neighborhood of an architecture. Let E : R 2 ? R denote a random variable mapping the validation losses of two neighboring architectures to the loss of the second architecture, E(x, y) ? y. E has a distribution that is characterized by probability density function pdf e (x, y)?x, y ? R. This gives us a probability over the validation loss for a neighboring architecture.</p><p>Every architecture v ? A has a loss (v) ? R that is sampled from pdf n . For any two neighbors (v, u) ? E N , the PDF for the validation loss x of architecture u is given by pdf e ( (v), x). Note that choices for the distribution pdf e are constrained by the fixed topology of the search space, as well as the selected distribution pdf n . Let (A, 2 A ) denote a measurable space over the nodes of the graph.</p><p>For the rest of this section, we fix an arbitrary neighborhood graph G N with vertex set A such that for all v ? A, |N (v)| = s, i.e., G N has regular degree s, and we assume that G N is vertex transitive. Each vertex in A is assigned a validation loss according to pdf n and pdf e defined above. The expectations in the following theorem and lemmas are over the random draws from pdf n and pdf e .</p><p>Theorem 5.1. Given |A| = n, , s, , pdf n , and pdf e ,</p><formula xml:id="formula_18">E[|{v ? A | LS * (v) = v}|] = n ? (v * ) pdf n (x) ? x pdf e (x, y)dy s dx, and E[|{v ? A | (LS * (v)) ? (v * ) ? }|] = n (v * )+ (v * ) pdf n (x) ? x pdf e (x, y)dy s ? E[|LS ? * (x)|]dx.</formula><p>Proof.. To prove the first statement, we introduce an indicator random variable on the architecture space to test if the architecture is a local minimum I : A ? R, where</p><formula xml:id="formula_19">I(v) = I{LS * (v) = v} = I{ (v) &lt; (u) ?u s.t. (u, v) ? E N }.</formula><p>The expected number of local minima in |A| is equal to |A| times the fraction of nodes in A which are local minima. Therefore, we have</p><formula xml:id="formula_20">E[|{v ? A | LS * (v) = v}|] = n ? P({I = 1}) = n ? ?? pdf n (x) ? P({x &lt; (u)?u s.t. (u, v) ? E N , x = (v)})dx = n ? ?? pdf n (x) ? x pdf e (x, y)dy s dx</formula><p>In line one we use the notation P({I = 1}) ? P({v ? A | I(v) = 1}).</p><p>To prove the second statement, we introduce an indicator random variable on the architecture space that tests if a node will terminate on a local minimum that is within of the global minimum, I : A ? R, where</p><formula xml:id="formula_21">I (v) = I{LS * (v) = u ? l(u) ? l(v * ) ? } = I{?S ? {LS ? * (u) : LS * (u) = u ? l(u) ? l(v * ) ? }, v ? S}</formula><p>We use this random variable to prove the second statement of the theorem.</p><formula xml:id="formula_22">E[|{v ? A | (LS * (v)) ? (v * ) ? }|] = n ? P({I = 1}) = n (v * )+ (v * ) P({v ? A | I(v) = 1, (v) = x}) ? E[|LS ? * (x)|]dx = n (v * )+ (v * ) pdf n (x) ? (v) pdf e (x, y)dy s ? E[|LS ? * (x)|]dx</formula><p>where the last equality follows from the first half of this theorem. This concludes the proof.</p><p>Recall that we defined the branching fraction of graph G N as</p><formula xml:id="formula_23">b k = |N k (v)|/ (|N k?1 (v)| ? |N (v)|),</formula><p>where N k (v) denotes the set of nodes which are distance k to v in G N . For example, the branching fraction of a tree with degree d is 1 for all k, and the branching fraction of a clique is b 1 = 1 and b k = 0 for all k &gt; 1. Also, for any graph, b 1 = 1. The neighborhood graph of the NASBench-201 search space is (K 5 ) 6 and therefore its branching factor is b k = 6?k+1 6k . Now we use Theorem 5.1 along with Chebyshev's Inequality <ref type="bibr">[Chebyshev, 1867]</ref> to show that, in the case when the validation error of each architecture has Gaussian noise ? N (0, ? 2 ), then we can bound the expected number of local minima by O ? 2s .</p><p>Corollary B.1. Given |A| = n, , s, pdf n , and pdf e , assume that for each architecture x, the accuracy is (x) + , where ? N (0, ? 2 ). Then E(# local minima) ? O ? 2s .</p><p>Proof. Recall Chebyshev's inequality: for a Gaussian random variable ? N (?, ? 2 ), we have P (| | ? L) ? ? 2 L 2 . If ? N (0, ? 2 ), then is a symmetric random variable, so we have P ( ? L) ? ? 2 2L 2 . We prove the statement using a modification to the first part of the proof of Theorem 5.1.</p><formula xml:id="formula_24">E[|{v ? A | LS * (v) = v}|] = n ? ? ?? pdf n (x) ? P(x &lt; (u) + ?u s.t. (u, v) ? E N , x = (v))dx = n ? ? ?? pdf n (x) ? ?? pdf e (x, y) ? (P( ? x ? y)) s dydx ? n ? ? ?? pdf n (x) ? ?? pdf e (x, y) ? 2 2(x ? y) 2 s dydx = ? 2s ? n ? ? ?? pdf n (x) ? ? ??</formula><p>pdf e (x, y) 1 2(x ? y) 2 s dydx Next, we restate and prove Lemma 5.2, which gives a formula for the k'th preimage of the local search function.</p><p>Lemma 5.2. Given A, , s, pdf n , and pdf e , then for all v ? A, we have the following equations. Proof.. The function LS ?1 (v) ? 2 A returns a set of nodes which form the preimage of node v ? A, namely, the set of all neighbors u ? N (v) with higher validation loss than v, and whose neighbors w ? N (u) excluding v have higher validation loss than (v). Formally,</p><formula xml:id="formula_25">LS ?1 (v) = {u ? A | LS(u) = v} = {u ? A | (v, u) ? E N , (v) &lt; (u), {v ? A\{v} | (v , u) ? E N , (v ) &lt; (v)} = ?}. Let LS ?1 v : A ? R denote a random variable where LS ?1 v (u) = I{u ? LS ?1 (v)}.</formula><p>The probability distribution for LS ?1 v gives the probability that a neighbor of v is in the preimage of v. We can multiply this probability by |N (v)| = s to express the expected number of nodes in the preimage of v.</p><formula xml:id="formula_26">E[|LS ?1 (v)|] = s ? P({LS ?1 v = 1}) = s ? (v) pdf e ( (v), y) ? l(v)</formula><p>pdf e (y, z)dz s?1 dy.</p><p>Note that the inner integral is raised to the power of s ? 1, not s, so as not to double count node v. We can use this result to find the preimage of node v after m steps. Let Following a similar argument as above, we compute the expected size of the m'th preimage set. Closed-form solution for single-variate PDFs. Now we give the details for Lemma 5.3. We start with a lemma that will help us prove Lemma 5.3. This lemma uses induction to derive a closed-form solution to Lemma 5.2 in the case where pdf e (x, y) is independent of x. In the first equality, we used Lemma 5.2, and in the fourth equality, we used the fact that This concludes the proof.</p><formula xml:id="formula_27">E[|LS ?m (v)|] = b k?1 ? E[|LS ?1 (v)|] ? E[|{?w ? A | ?u ? LS ?1 (v), LS ?(m?1) u (w) = 1}|] = b k?1 ? E[|LS ?1 (v)|]</formula><p>Next, we prove a lemma which gives strong approximation guarantees on the size of the full preimage of an architecture, again assuming that pdf e (x, y) is independent if x. For this lemma , we need to assume that n is large compared to s. However, this is the only lemma that assumes n is large. In particular, Lemma 5.3 will not need this assumption. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 . 1 :</head><label>41</label><figDesc>The local search tree for the architectures with the six lowest test losses (colored red) on CIFAR-10 on NASBench-201, denoised (left) or fully random (right</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 sim. -Unif. Random Thm 5.1 sim. -ImageNet16-120 Thm 5.1 sim. -CIFAR-10 Thm 5.1 sim. -CIFAR-100 Thm 5.1 sim. -Unif. Random Thm 5.1 sim. -ImageNet16-120 Thm 5.1 sim. -CIFAR-10 Thm 5.1 sim. -CIFAR-0.025 0.050 0.075 0.100 0.125 0.150 0.175 0with losses in U([0,1]) Figure 4.2: Performance of NAS algorithms on standard and denoised versions of NASBench-101 (top left/middle) and NASBench-301 (bottom left/middle). Probability that local search will converge to within of the global optimum, compared to Theorem 5.1 (top right). Validation loss vs. size of preimages, compared to Lemma 5.2 (bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>.3: Amount of noise present in the architecture evaluation step of NASBench-301 vs. performance of NAS algorithms (left), iterations (middle), and validation error (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 . 1 :</head><label>51</label><figDesc>Histogram of validation losses for CIFAR-100 (top) and ImageNet16-120 (bottom) in NASBench-201, fitted with the best values of ? and v in Equation 5.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 . 2 :</head><label>52</label><figDesc>Probability density function for CIFAR-10, CIFAR-100, and ImageNet16-120 on NASBench-201. For each coordinate (u, v), a darker color indicates that architectures with accuracy u and v are more likely to be neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A. 1 :</head><label>1</label><figDesc>Performance of NAS algorithms on denoised (top) and standard (bottom) versions of NASBench-201 CIFAR-10 (left), CIFAR-100 (middle), and ImageNet16-120 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A. 2 :FigureFigure</head><label>2</label><figDesc>Results for local search performance vs. number of inital randomly drawn architectures on NASBench-201 for CIFAR-10 (left), CIFAR-100 (middle), and ImageNet-16-120 (right). A.3: Normal PDF from Equation 5.3 plotted with three values of v. A.4: RWA vs. distance for three datasets in NASBench-201, as well as three values of ? in Equation 5.3. Since a random walk reaches a mean distance of ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>?k (v)|] = b k?1 ? E[|LS ?1 (v)|] pdf e ( (v), y)E[|LS ?(k?1) (y)|]dy ? (v) pdf e ( (v), y)dy .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>:</head><label></label><figDesc>A ? R denote a random variable where LS ?m v (u) = I{u ? LS ?m (v)} = I{?w ? LS ?1 (v), u ? LS ?(m?1) (w)}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>pdf e ( (v), y)E[|LS ?(m?1) (y)|]dy ? (v) pdf e ( (v), y)dy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Lemma B. 2 .??</head><label>2</label><figDesc>Assume there exists a function g such that pdf e (x, y) = g(y) for all x. Given v ? A, for k ? 1,E[|LS ?k (v)|] =s k equality follows from Lemma 5.2. Now we give a proof by induction for the closed-form equation. The base case, m = 1, is proven above. Given an integer m ? 1, assume that E[|LS ?m (v)|] = s m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Lemma B. 3 .</head><label>3</label><figDesc>Assume there exists g such that pdf e (x, y) = g(y) for all x. Denote G(x) = ? x g(y)dy. Given s, there exists N such that for all n &gt; N , for all v, we have1 + s ? G( (v)) s e s s+1 G( (v)) s ? E[|LS ? * (v)|] ? 1 + s ? G( (v)) s ? e G( (v)) s . Proof.. From Lemma B.2, we have E[|LS ? * (vsj = 1 j because 0 ? b j ? 1 for all 1 ? j.Therefore for all i,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Local searchInput: Search space A, objective function , neighborhood function N 1. Pick an architecture v 1 ? A uniformly at random 2. Evaluate (v 1 ); denote a dummy variable (</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Avg. num. of iterations until convergence, num. of local minima, and percent of initial architectures to reach the global minimum, for CIFAR-10 on NASBench-201.Version # iters # local min. % reached global min.</figDesc><table><row><cell>Denoised 5.36</cell><cell>21</cell><cell>47.4</cell></row><row><cell>Standard 4.97</cell><cell>55</cell><cell>6.71</cell></row><row><cell>Random 2.56</cell><cell>616</cell><cell>0.717</cell></row></table><note>that has been queried so far. We ran 200 trials of each al- gorithm and averaged the results. For local search, we set N (v) to denote all architectures which differ by one op- eration or edge. If local search converged before its bud- get, it started a new run. On NASBench-101 and 301, we used the query_until_lower variant of local search, and on NASBench-201, we used the continue_at_min variant. See Figure 4.2. On both NASBench-101 and 301, local search outperforms all other algorithms when the noise is minimal, amd performs similarly to Bayesian optimization in the stan- dard setting. We include the results for all three datasets of NASBench-201 in Appendix A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Each edge represents an iteration of local search, from the colder-colored node to the warmer-colored node. While 47.4% of architectures reach the global minimum in the denoised version, only 0.71% of architectures reach the global minimum in the random version.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">NASBench-301, standard</cell><cell></cell><cell></cell><cell></cell><cell cols="3">NASBench-301, reduced noise</cell></row><row><cell></cell><cell>5.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.8</cell><cell></cell><cell></cell></row><row><cell>test error of best neural net</cell><cell>5.0 5.2 5.4 5.6</cell><cell></cell><cell></cell><cell cols="2">Random Reg. Evolution BANANAS Local Search BayesOpt with GP</cell><cell>test error of best neural net</cell><cell>5.0 5.2 5.4 5.6</cell><cell></cell><cell></cell><cell>Random Reg. Evolution BANANAS Local Search BayesOpt with GP</cell></row><row><cell></cell><cell>100</cell><cell>150</cell><cell>200 time in GPU hours 250</cell><cell>300</cell><cell>350</cell><cell></cell><cell>100</cell><cell>150</cell><cell>200 time in GPU hours 250</cell><cell>300</cell><cell>350</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Qingquan Song, and Xia Hu. Auto-keras: Efficient neural architecture search with network morphism. arXiv preprint arXiv:1806.10282, 2018. David S Johnson and Lyle A McGeoch. The traveling salesman problem: A case study in local optimization.</figDesc><table><row><cell>Haifeng Jin, Local search in combinatorial optimization, 1(1):215-</cell><cell></cell></row><row><cell>310, 1997.</cell><cell></cell></row><row><cell>David S Johnson, Christos H Papadimitriou, and Mihalis</cell><cell></cell></row><row><cell>Yannakakis. How easy is local search? Journal of com-</cell><cell></cell></row><row><cell>puter and system sciences, 37(1):79-100, 1988.</cell><cell></cell></row><row><cell>Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider,</cell><cell></cell></row><row><cell>Barnabas Poczos, and Eric P Xing. Neural architecture</cell><cell></cell></row><row><cell>search with bayesian optimisation and optimal transport.</cell><cell></cell></row><row><cell>In Advances in Neural Information Processing Systems,</cell><cell></cell></row><row><cell>pages 2016-2025, 2018.</cell><cell></cell></row><row><cell>References</cell><cell>Tianqi Chen and Carlos Guestrin. Xgboost: A scalable</cell></row><row><cell>Brian W Kernighan and Shen Lin. An efficient heuristic pro-</cell><cell>tree boosting system. In Proceedings of the 22nd acm</cell></row><row><cell>E Aarts and JK Lenstra. Local search in combinatorial cedure for partitioning graphs. The Bell system technical</cell><cell>sigkdd international conference on knowledge discovery</cell></row><row><cell>optimization. John Wiley &amp; Sons, Inc., 1997. journal, 49(2):291-307, 1970.</cell><cell>and data mining, pages 785-794, 2016.</cell></row><row><cell>Milton Abramowitz and Irene A Stegun. Handbook of</cell><cell>Vincent Cohen-Addad, Philip N Klein, and Claire Mathieu.</cell></row><row><cell>mathematical functions with formulas, graphs, and math-</cell><cell>Local search yields approximation schemes for k-means</cell></row><row><cell>ematical tables, volume 55. 1948.</cell><cell>and k-median in euclidean and minor-free metrics. In</cell></row><row><cell>Youhei Akimoto, Sandra Astete-Morales, and Olivier Tey-taud. Analysis of runtime of optimization algorithms</cell><cell>Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS), pages 353-364, 2016.</cell></row><row><cell>for noisy functions over discrete codomains. Theoretical</cell><cell>Georges A Croes. A method for solving traveling-salesman</cell></row><row><cell>Computer Science, 605:42-50, 2015.</cell><cell>problems. Operations research, 6(6):791-812, 1958.</cell></row><row><cell>Maria-Florina Balcan, Nika Haghtalab, and Colin White.</cell><cell>Xuanyi Dong and Yi Yang. Searching for a robust neural</cell></row><row><cell>K-center clustering under perturbation resilience. ACM</cell><cell>architecture in four gpu hours. In Proceedings of the IEEE</cell></row><row><cell>Trans. Algorithms, 16(2), 2020. ISSN 1549-6325.</cell><cell>Conference on computer vision and pattern recognition,</cell></row><row><cell>Pierre Baldi and Peter J Sadowski. Understanding dropout.</cell><cell>pages 1761-1770, 2019.</cell></row><row><cell>Advances in neural information processing systems, 26: 2814-2822, 2013.</cell><cell>Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In</cell></row><row><cell>Jon Jouis Bentley. Fast algorithms for geometric traveling salesman problems. ORSA Journal on computing, 4(4):</cell><cell>Proceedings of the International Conference on Learning Representations (ICLR), 2020.</cell></row><row><cell>387-411, 1992.</cell><cell>Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter.</cell></row><row><cell>Frederick Bock. An algorithm for solving travelling-</cell><cell>Simple and efficient architecture search for convolutional</cell></row><row><cell>salesman and related network optimization problems. In</cell><cell>neural networks. arXiv preprint arXiv:1711.04528, 2017.</cell></row><row><cell>Operations Research, volume 6, pages 897-897, 1958.</cell><cell>Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.</cell></row><row><cell>Andrew J Booker, JE Dennis, Paul D Frank, David B Ser-</cell><cell>Neural architecture search: A survey. arXiv preprint</cell></row><row><cell>afini, and Virginia Torczon. Optimization using surrogate</cell><cell>arXiv:1808.05377, 2018.</cell></row><row><cell>objectives on a helicopter test example. In Computational Methods for Optimal Design and Control, pages 49-58. Springer, 1998.</cell><cell>Zachary Friggstad, Mohsen Rezapour, and Mohammad R Salavatipour. Local search yields a ptas for k-means in doubling metrics. SIAM Journal on Computing, 48(2):</cell></row><row><cell>Jos? A Caballero and Ignacio E Grossmann. An algorithm</cell><cell>452-480, 2019.</cell></row><row><cell>for the use of surrogate models in modular flowsheet optimization. AIChE journal, 54(10):2633-2650, 2008.</cell><cell>Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal</cell></row><row><cell>Pafnutii Lvovich Chebyshev. Des valeurs moyennes. J.</cell><cell>covariate shift. In International conference on machine</cell></row><row><cell>Math. Pures Appl, 12(2):177-184, 1867.</cell><cell>learning, pages 448-456. PMLR, 2015.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/ naszilla/naszilla. Accepted for the 37 th Conference on Uncertainty in Artificial Intelligence (UAI 2021).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In particular, given a node v with validation loss (v) the probability distribution for the validation loss of a neighbor depends only on (v) and pdf e , which makes the local search procedure similar to a Markov process. Our experiments inFigure 4.2 suggest this is a reasonable assumption in practice.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It follows that</head><p>The final equality comes from the well-known Taylor series e x = ? n=0</p><p>x n n! (e.g. <ref type="bibr">[Abramowitz and Stegun, 1948]</ref>) evaluated at x = G( (v)) s . Now we prove the lower bound. b 1 = 1 by definition for all graphs, and for 1 &lt; j ? D, 0 ? b j ? 1, where D denotes the diameter of the graph. (Since N D (v) = n for all v, b j is meaningless for j ? D.) Recall that all of our arguments assume vertex transitivity. It follows that b D?1 ? b D?2 ? ? ? ? ? b 1 . Now, for a fixed s, b D?1 approaches 1 as n approaches infinity. Therefore, given s, there exists N such that for all n &gt; N , b D?1 &gt; 2s+1 2(s+1) . Then for all i,</p><p>Therefore,</p><p>It follows that</p><p>The final equality again comes from the Taylor series e x , this time evaluated at x = s s+1 ? G( (v)) s . </p><p>Proof.. The probability density function of U ([0, 1]) is equal to 1 on [0, 1] and 0 otherwise. Let (v) = x. Then ? x pdf e (x, y)dy = 1 x dy = (1 ? x). Using Theorem 5.1, we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we plug in Equation B</head><p>.1 to the second part of Theorem 5.1.</p><p>This concludes the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms with graph generation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Kitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="476" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Best practices for scientific research on neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02453</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Quentin de Laroussilhe, and Andrea Gesmundo. Evolutionary-neural hybrid agents for architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Khorlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09828</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Theoretical aspects of local search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wil</forename><surname>Michiels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Korst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Probo: a framework for using probabilistic programming in bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11515</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Local search is a remarkably strong baseline for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>T Den Ottelander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dushatskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Virgolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bosman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08996</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An approach to the scheduling of jobs on machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Es Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="484" to="492" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Early stopping-but when?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="55" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Noisy evolutionary optimization algorithms-a comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyusha</forename><surname>Rakshit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagatam</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm and Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18" to="45" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Amoebanet: An sdn-enabled network service for big data science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raza</forename><surname>Syed Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenji</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajith</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Sasidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Demar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Guok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Macauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Pouyoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="70" to="82" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-objective neural architecture search via predictive network performance optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09336</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Nas-bench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovita</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09777</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Landscapes and their correlation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical chemistry</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuu</forename><surname>Jinnai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07440</idno>
		<title level="m">Alphax: exploring neural architectures with deep neural networks and monte carlo tree search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Correlated and uncorrelated fitness landscapes and how to tell the difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00848</idno>
		<title level="m">Neural predictor for neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A study on encodings for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Nolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Savani</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bananas: Bayesian optimization with neural architectures for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Savani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">How powerful are performance predictors in neural architecture search?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binxin</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01177</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Does unsupervised architecture representation learning help neural architecture search? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cate: Computation-aware neural architecture encoding with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evolving artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1423" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09635</idno>
	</analytic>
	<monogr>
		<title level="m">Towards reproducible neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheheryar</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arber</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08573</idno>
		<title level="m">Neural ensemble search for performant and calibrated predictions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
