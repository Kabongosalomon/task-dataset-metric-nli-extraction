<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Metrical Reconstruction of Human Faces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zielonka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>T?bingen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Metrical Reconstruction of Human Faces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Project website: https://zielon.github.io/mica/ arXiv:2204.06607v2 [cs.CV] 19 Oct 2022 2 W. Zielonka et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: An RGB image of a subject serves as input to MICA, which predicts a metrical reconstruction of the human face. Images from NoW <ref type="bibr" target="#b65">[63]</ref>, StyleGan2 <ref type="bibr" target="#b43">[42]</ref>.</p><p>Abstract. Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small-and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the stateof-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning to reconstruct 3D content from 2D imagery is an ill-posed inverse problem <ref type="bibr" target="#b3">[4]</ref>. State-of-the-art RGB-based monocular facial reconstruction and tracking methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">25]</ref> are based on self-supervised training, exploiting an underlying metrical face model which is constructed using a large-scale dataset of registered 3D scans (e.g., 33000 scans for the FLAME [51] model). However, when assuming a perspective camera, the scale of the face is ambiguous since a large face can be modeled by a small face that is close to the camera or a gigantic face that is far away. Formally, a point x ? R 3 of the face is projected to a point p ? R 2 on the image plane with the projective function ?(?) and a rigid transformation composed of a rotation R ? R 3?3 and a translation t ? R 3 : p = ?(R ? x + t) = ?(s ? (R ? x + t)) = ?(R ? (s ? x) + (s ? t))).</p><p>The perspective projection is invariant to the scaling factor s ? R, and thus, if x is scaled by s, the rigid transformation can be adapted such that the point still projects onto the same pixel position p by scaling the translation t by s. In consequence, face reconstruction methods might result in a good 2D alignment but can fail to reconstruct the metrical 3D surface and the meaningful metrical location in space. However, a metric 3D reconstruction is needed in any scenario where the face is put into a metric context. E.g., when the reconstructed human is inserted into a virtual reality (VR) application or when the reconstructed geometry is used for augmented reality (AR) applications (teleconferencing in AR/VR, virtual try-on, etc.). In these scenarios, the methods mentioned above fail since they do not reproduce the correct scale and shape of the human face. In the current literature <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b65">63,</ref><ref type="bibr" target="#b94">89]</ref>, we also observe that methods use evaluation measurements not done in a metrical space. Specifically, to compare a reconstructed face to a reference scan, the estimation is aligned to the scan via Procrustes analysis, including an optimal scaling factor. This scaling factor favors the estimation methods that are not metrical, and the reported numbers in the publications are misleading for real-world applications (relative vs. absolute/metrical error). In contrast, we aim for a metrically correct reconstruction and evaluation that directly compares the predicted geometry to the reference data without any scaling applied in a post-processing step which is fundamentally different. As discussed above, the self-supervised methods in the literature do not aim and cannot reconstruct a metrically correct geometry. However, training these methods in a supervised fashion is not possible because of the lack of data (no large-scale 3D dataset is available). Training on a small-or medium-scale 3D dataset will lead to overfitting of the networks (see study in the supplemental document). To this end, we propose a hybrid method that can be trained on a medium-scale 3D dataset, reusing powerful descriptors from a pretrained face recognition network (trained on a large-scale 2D dataset). Specifically, we propose the usage of existing 3D datasets like LYHM <ref type="bibr" target="#b17">[18]</ref>, FaceWarehouse <ref type="bibr" target="#b10">[11]</ref>, Stirling <ref type="bibr" target="#b29">[28]</ref>, etc., that contain RGB imagery and corresponding 3D reconstructions to learn a metrical reconstruction of the human head. To use these 3D datasets, significant work has been invested to unify the 3D data (i.e., to annotate and non-rigidly fit the FLAME model to the different datasets). This unification provides us with meshes that all share the FLAME topology. Our method predicts the head geometry in a neutral expression, only given a single RGB image of a human subject in any pose or expression. To generalize to unseen in the wild images, we use a state-of-the-art face recognition network <ref type="bibr" target="#b18">[19]</ref> that provides a feature descriptor for our geometry-estimating network. This recognition network is robust to head poses, different facial expressions, occlusions, illumination changes, and different focal lengths, thus, being ideal for our task (see <ref type="figure" target="#fig_0">Figure 3</ref>). Based on this feature, we predict the geometry of the face with neutral expression within the face space spanned by FLAME <ref type="bibr" target="#b52">[51]</ref>, effectively disentangling shape and expression. As an application, we demonstrate that our metrical face reconstruction estimator can be integrated in a new analysis-by-synthesis face tracking framework which removes the requirement of an identity initialization phase <ref type="bibr" target="#b77">[75]</ref>. Given the metrical face shape estimation, the face tracker is able to predict the face motion in a metrical space.</p><p>In summary, we have the following contributions:</p><p>a dataset of 3D face reference data for about 2300 subjects, built by unifying existing small-and medium-scale datasets under common FLAME topology. a metrical face shape predictor -MICA-which is invariant to expression, pose and illumination, by exploiting generalized identity features from a face recognition network and supervised learning. a hybrid face tracker that is based on our (learned) metrical reconstruction of the face shape and an optimization-based facial expression tracking. a metrical evaluation protocol and benchmark, including a discussion on the current evaluation practise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Reconstructing human faces and heads from monocular RGB, RGB-D, or multiview data is a well-explored field at the intersection of computer vision and computer graphics. Zollh?fer et al. <ref type="bibr" target="#b96">[91]</ref> provide an extensive review of reconstruction methods, focusing on optimization-based techniques that follow the principle of analysis-by-synthesis. Primarily, the approaches that are based on monocular inputs are based on a prior of face shape and appearance <ref type="bibr">[6, 7, 29, 30, 44, 71-76, 83, 84]</ref>. The seminal work of Blanz et al. <ref type="bibr" target="#b7">[8]</ref> introduced such a 3D morphable model (3DMM), which represents the shape and appearance of a human in a compressed, low-dimensional, PCA-based space (which can be interpreted as a decoder with a single linear layer). There is a large corpus of different morphable models <ref type="bibr" target="#b22">[23]</ref>, but the majority of reconstruction methods use either the Basel Face Model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b57">56]</ref> or the Flame head model <ref type="bibr" target="#b52">[51]</ref>. Besides using these models for an analysis-by-synthesis approach, there is a series of learned regression-based methods. An overview of these methods is given by Morales et al. <ref type="bibr" target="#b55">[54]</ref>. In the following, we will discuss the most relevant related work for monocular RGB-based reconstruction methods.</p><p>Optimization-based Reconstruction of Human Faces. Along with the introduction of a 3D morphable model for faces, Blanz et al. <ref type="bibr" target="#b7">[8]</ref> proposed an optimizationbased reconstruction method that is based on the principle of analysis-bysynthesis. While they used a sparse sampling scheme to optimize the color reproduction, Thies et al. <ref type="bibr" target="#b76">[74,</ref><ref type="bibr" target="#b77">75]</ref> introduced a dense color term considering the entire face region that is represented by a morphable model using differentiable rendering. This method has been adapted for avatar digitization from a single image <ref type="bibr" target="#b41">[40]</ref> including hair, is used to reconstruct high-fidelity facial reflectance and geometry from a single images <ref type="bibr" target="#b89">[85]</ref>, for reconstruction and animation of entire upper bodies <ref type="bibr" target="#b78">[76]</ref>, or avatars with dynamic textures <ref type="bibr" target="#b56">[55]</ref>. Recently, these optimization-based methods are combined with learnable components such as surface offsets or view-dependent surface radiance fields <ref type="bibr" target="#b36">[35]</ref>. In addition to a photometric reconstruction objective, additional terms based on dense correspondence <ref type="bibr" target="#b39">[38]</ref> or normal <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">35]</ref> estimations of neural network can be employed.</p><p>Optimization-based methods are also used as a building block for neural rendering methods such as deep video portraits <ref type="bibr" target="#b45">[44]</ref>, deferred neural rendering <ref type="bibr" target="#b75">[73]</ref>, or neural voice puppetry <ref type="bibr" target="#b74">[72]</ref>. Note that differentiable rendering is not only used in neural rendering frameworks but is also a key component for self-supervised learning of regression-based reconstruction methods covered in the following.</p><p>Regression-based Reconstruction of Human Faces. Learning-based face reconstruction methods can be categorized into supervised and self-supervised approaches. A series of methods are based on synthetic renderings of human faces to perform a supervised training of a regressor that predicts the parameters of a 3D morphable model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b62">60,</ref><ref type="bibr" target="#b63">61]</ref>. Genova et al. <ref type="bibr" target="#b35">[34]</ref> propose a 3DMM parameter regression technique that is based on synthetic renderings (where ground truth parameters are available) and real images (where multi-view identity losses are applied). It uses FaceNet <ref type="bibr" target="#b66">[64]</ref> to extract features for the 3DMM regression task. Tran et al. <ref type="bibr" target="#b80">[77]</ref> and Chang et al. <ref type="bibr" target="#b12">[13]</ref> (ExpNet) directly regress 3DMM parameters using a CNN trained on fitted 3DMM data. Tu et al. <ref type="bibr" target="#b83">[80]</ref> propose a dual training pass for images with and without 3DMM fittings. Jackson et al. <ref type="bibr" target="#b42">[41]</ref> propose a model-free approach that reconstructs a voxel-based representation of the human face and is trained on paired 2D image and 3D scan data. PRN <ref type="bibr" target="#b26">[26]</ref> is trained on 'in-the-wild' images with fitted 3DMM reconstructions <ref type="bibr" target="#b95">[90]</ref>. It is not restricted to a 3DMM model space and predicts a position map in the UV-space of a template mesh. Instead of working in UV-space, Wei et al. <ref type="bibr" target="#b86">[82]</ref> propose to use graph convolutions to regress the coordinates of the vertices. MoFA <ref type="bibr" target="#b72">[70]</ref> is a network trained to regress the 3DMM parameters in a self-supervised fashion. As a supervision signal, it uses the dense photometric losses of Face2Face <ref type="bibr" target="#b77">[75]</ref>. Within this framework, Tewari et al. proposed to refine the identity shape and appearance <ref type="bibr" target="#b71">[69]</ref> as well as the expression basis <ref type="bibr" target="#b70">[68]</ref> of a linear 3DMM. In a similar setup, one can also train a non-linear 3DMM <ref type="bibr" target="#b82">[79]</ref> or personalized models <ref type="bibr" target="#b13">[14]</ref>. RingNet <ref type="bibr" target="#b65">[63]</ref> regresses 3DMM parameters and is trained on 2D images using losses on the reproduction of 2D landmarks and shape consistency (different images of the same subject) and shape inconsistency (images of different subjects) losses. DECA <ref type="bibr" target="#b25">[25]</ref> extends RingNet with expression dependent offset predictions in UV space. It uses dense photometric losses to train the 3DMM parameter regression and the offset prediction network. This separation of a coarse 3DMM model and a detailed bump map has been introduced by Tran et al. <ref type="bibr" target="#b81">[78]</ref>. Chen et al. <ref type="bibr" target="#b14">[15]</ref> use a hybrid training composed of self-supervised and supervised training based on renderings to predict texture and displacement maps. Deng et al. <ref type="bibr" target="#b19">[20]</ref> train a 3DMM parameter regressor based on multi-image consistency losses and 'hybrid-level' losses (photometric reconstruction loss with skin attention masks, and a perceptionlevel loss based on FaceNet <ref type="bibr" target="#b66">[64]</ref>). On the NoW challenge <ref type="bibr" target="#b65">[63]</ref>, DECA <ref type="bibr" target="#b25">[25]</ref> and the method of Deng et al. <ref type="bibr" target="#b19">[20]</ref> show on-par state-of-the-art results. Similar to DECA's offset prediction, there are GAN-based methods that predict detailed color maps <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref> or skin properties <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b89">85</ref>] (e.g., albedo, reflectance, normals) in UV-space of a 3DMM-based face reconstruction. In contrast to these methods, we are interested in reconstructing a metrical 3D representation of a human face and not fine-scale details. Self-supervised methods suffer from the depth-scale ambiguity (the face scale, translation away from the camera, and the perspective projection are ambiguous) and, thus, predict a wrongly scaled face, even though 3DMM models are by construction in a metrical space. We rely on a strong supervision signal to learn the metrical reconstruction of a face using high-quality 3D scan datasets which we unified. In combination with an identity encoder <ref type="bibr" target="#b18">[19]</ref> trained on in-the-wild 2D data, including occlusions, different illumination, poses, and expressions, we achieve robust geometry estimations that significantly outperform state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Metrical Face Shape Prediction</head><p>Based on a single input RGB image I, MICA aims to predict a metrical shape of a human face in a neutral expression. To this end, we leverage both 'in-the-wild' 2D data as well as metric 3D data to train a deep neural network, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. We employ a state-of-the-art face recognition network <ref type="bibr" target="#b18">[19]</ref> which is trained on 'in-the-wild' data to achieve a robust prediction of an identity code, which is interpreted by a geometry decoder.</p><p>Identity Encoder. As an identity encoder, we leverage the ArcFace <ref type="bibr" target="#b18">[19]</ref> architecture which is pretrained on Glint360K <ref type="bibr" target="#b1">[2]</ref>. This ResNet100-based network is trained on 2D image data using an additive angular margin loss to obtain highly discriminative features for face recognition. It is invariant to illumination, expression, rotation, occlusion, and camera parameters which is ideal for a robust shape prediction. We extend the ArcFace architecture by a small mapping network M that maps the ArcFace features to our latent space, which can then be interpreted by our geometry decoder:</p><formula xml:id="formula_0">z = M(ArcF ace(I)),</formula><p>where z ? R 300 . Our mapping network M consists of three fully-connected linear hidden layers with ReLU activation and the final linear output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>We propose a method for metrical human face shape estimation from a single image which exploits a supervised training scheme based on a mixture of different 2D,2D/3D and 3D datasets. This estimation can be used for facial expression tracking using analysis-by-synthesis which optimizes for the camera intrinsics, as well as the per-frame illumination, facial expression and pose.</p><p>Geometry Decoder. There are essentially two types of geometry decoders used in the literature, model-free and model-based. Throughout the project of this paper, we conducted experiments on both types and found that both perform similarly on the evaluation benchmarks. Since a 3DMM model efficiently represents the face space, we focus on a model-based decoder. Specifically, we use FLAME <ref type="bibr" target="#b52">[51]</ref> as a geometry decoder, which consists of a single linear layer:</p><formula xml:id="formula_1">G 3DM M (z) = B ? z + A,</formula><p>where A ? R 3N is the geometry of the average human face and B ? R 3N ?300 contains the principal components of the 3DMM and N = 5023.</p><p>Supervised Learning. The networks described above are trained using paired 2D/3D data from existing, unified datasets D (see <ref type="bibr">Section 5)</ref>. We fix large portions of the pre-trained ArcFace network during the training and refine the last 3 ResNet blocks. Note that ArcFace is trained on a much larger amount of identities, therefore, refining more hidden layers results in worse predictions due to overfitting. We found that using the last 3 ResNet blocks gives the best generalization (see supplemental document). The training loss is:</p><formula xml:id="formula_2">L = (I,G)?D |? mask (G 3DM M (M(ArcF ace(I))) ? G)|,<label>(1)</label></formula><p>where G is the ground truth mesh and ? mask is a region dependent weight (the face region has weight 150.0, the back of the head 1.0, and eyes with ears 0.01). We use AdamW <ref type="bibr" target="#b54">[53]</ref> for optimization with fixed learning rate ? = 1e?5 and weight decay ? = 2e?4. We select the best performing model based on the validation set loss using the Florence dataset <ref type="bibr" target="#b2">[3]</ref>. The model was trained for 160k steps on Nvidia Tesla V100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Face Tracking</head><p>Based on our shape estimate, we demonstrate optimization-based face tracking on monocular RGB input sequences. To model the non-rigid deformations of the face, we use the linear expression basis vectors and the linear blendskinning of the FLAME [51] model, and use a linear albedo model <ref type="bibr" target="#b24">[24]</ref> to reproduce the appearance of a subject in conjunction with a Lambertian material assumption and a light model based on spherical harmonics. We adapt the analysis-bysynthesis scheme of Thies et al. <ref type="bibr" target="#b77">[75]</ref>. Instead of using a multi-frame model-based bundling technique to estimate the identity of a subject, we use our one-shot shape identity predictor. We initialize the albedo and spherical harmonics based on the same first frame using the energy:</p><formula xml:id="formula_3">E(?) = w dense E dense (?) + w lmk E lmk (?) + w reg E reg (?),<label>(2)</label></formula><p>where ? is the vector of unknown parameters we are optimizing for. The energy terms E dense (?) and E reg (?) measure the dense color reproduction of the face (? 1 -norm) and the deviation from the neutral pose respectively. The sparse landmark term E lmk (?) measures the reproduction of 2D landmark positions (based on Google's mediapipe <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b44">43]</ref> and Face Alignment <ref type="bibr" target="#b8">[9]</ref>). The weights w dense , w lmk and w reg balance the influence of each sub-objectives on the final loss. In the first frame vector ? contains the 3DMM parameters for albedo, expression, and rigid pose, as well as the spherical harmonic coefficients (3 bands) that are used to represent the environmental illumination <ref type="bibr" target="#b59">[58]</ref>. After initialization, the albedo parameters are fixed and unchanged throughout the sequence tracking.</p><p>Optimization. We optimize the objective function Equation <ref type="formula" target="#formula_3">(2)</ref> using Adam <ref type="bibr" target="#b47">[46]</ref> in PyTorch. While recent soft-rasterizers <ref type="bibr" target="#b53">[52,</ref><ref type="bibr" target="#b61">59]</ref> are popular, we rely on a sampling based scheme as introduced by Thies et al. <ref type="bibr" target="#b77">[75]</ref> to implement the differentiable rendering for the photo-metric reproduction error E dense (?). Specifically, we use a classical rasterizer to render the surface of the current estimation. The rasterized surface points that survive the depth test are considered as the set of visible surface points V for which we compute the energy term</p><formula xml:id="formula_4">E dense (?) = i?V |I(?(R ? p i (?) + t)) ? c i (?)|</formula><p>where p i and c i being the i-th vertex and color of the reconstructed model, and I the RGB input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dataset Unification</head><p>In the past, methods and their training scheme were limited by the availability of 3D scan datasets of human faces. While several small and medium-scale datasets are available, they are in different formats and do not share the same topology.</p><p>To this end, we unified the available datasets such that they can be used as a supervision signal for face reconstruction from 2D images. Specifically, we register the FLAME <ref type="bibr" target="#b52">[51]</ref> head model to the provided scan data. In an initial step, we fit the model to landmarks and optimize for the FLAME parameters based on an iterative closest point (ICP) scheme <ref type="bibr" target="#b4">[5]</ref>. We further jointly optimize FLAME's model parameters, and refine the fitting with a non-rigid deformation regularized by FLAME, similar to Li and Bolkart et al. <ref type="bibr" target="#b52">[51]</ref>. In <ref type="table" target="#tab_0">Table 1</ref>, we list the datasets that we unified for this project. We note that the datasets vary in the capturing modality and capturing script (with and without facial expressions, with and without hair caps, indoor and outdoor imagery, still images, and videos), which is suitable for generalization. The datasets are recorded in different regions of the world and are often biased towards ethnicity. Thus, combining other datasets results in a more diverse data pool. In the supplemental document, we show an ablation on the different datasets. Upon agreement of the different dataset owners, we will share our unified dataset, i.e., for each subject one registered mesh with neutral expression in FLAME topology. Note that in addition to the datasets listed in <ref type="table" target="#tab_0">Table 1</ref>, we analyzed the FaceScape dataset <ref type="bibr" target="#b90">[86]</ref>. While it provides a large set of 3D reconstructions (? 17k), which would be ideal for our training, the reconstructions are not done in a metrical space. Specifically, the data has been captured in an uncalibrated setup and faces are normalized by the eye distance, which has not been detailed in their paper (instead, they mention sub-millimeter reconstruction accuracy which is not valid). This is a fundamental flaw of this dataset, and also questions their reconstruction benchmark <ref type="bibr" target="#b94">[89]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Our experiments mainly focus on the metrical reconstruction of a human face from in the wild images. In the supplemental document, we show results for the sequential tracking of facial motions using our metrical reconstruction as initialization. The following experiments are conducted with the original models of the respective publications including their reconstructions submitted to the given benchmarks. Note that these models are trained on their large-scale datasets, training them on our medium-scale 3D dataset would lead to overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Face Shape Estimation</head><p>In recent publications, face shape estimation is evaluated on datasets where reference scans of the subjects are available. The NoW Challenge <ref type="bibr" target="#b65">[63]</ref> and the benchmark of Feng et al. <ref type="bibr" target="#b27">[27]</ref> which is based on Stirling meshes <ref type="bibr" target="#b29">[28]</ref> are used in the state-of-the-art methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b65">63]</ref>. We conduct several studies on these benchmarks and propose different evaluation protocols.</p><p>Non-Metrical Benchmark. The established evaluation methods on these datasets are based on an optimal scaling step, i.e., to align the estimation to the reference scan, they optimize for a rigid alignment and an additional scaling factor which results in a non-metric/relative error. This scaling compensates for shape mispredictions, e.g., the mean error evaluated on the NoW Challenge for the average FLAME mesh ( <ref type="table" target="#tab_1">Table 2</ref>) drops from 1.92mm to 1.53mm because of the applied scale optimization. This is an improvement of around 20% which has nothing to do with the reconstruction quality and, thus, creates a misleading benchmark score where methods appear better than they are. Nevertheless, we evaluate our method on these benchmarks and significantly outperform all stateof-the-art methods as can be seen in Tables 2 and 4 ('Non-Metrical' column).</p><p>Metrical Benchmark. Since for a variety of applications, actual metrical reconstructions are required, we argue for a new evaluation scheme that uses a purely rigid alignment, i.e., without scale optimization (see <ref type="figure" target="#fig_1">Figure 5</ref>). The error is calculated using an Euclidean distance between each scan vertex and the closest point on the mesh surface. This new evaluation scheme enables a comparison of methods based on metrical quantities (see <ref type="table" target="#tab_1">Tables 2 and 4</ref>) and, thus, <ref type="table">Table 3</ref>: Quantitative evaluation of the face shape estimation on the Stirling Reconstruction Benchmark <ref type="bibr" target="#b27">[27]</ref> using the NoW protocol <ref type="bibr" target="#b65">[63]</ref>. We list two different evaluations: the non-metric evaluation from the original benchmark and the metric evaluation. Note that for this experiment, we exclude the Stirling dataset from our training set.  <ref type="table" target="#tab_8">Table 4</ref>: Quantitative evaluation of the face shape estimation on the Stirling Reconstruction Benchmark <ref type="bibr" target="#b27">[27]</ref>. We list two different evaluations: the non-metric evaluation from the original benchmark and the metric evaluation. This benchmark is based on an alignment protocol that only relies on reference landmarks and, thus, is very noisy and dependent on the landmark reference selection (in our evaluation, we use the landmark correspondences provided by the FLAME [51] model). We use the image file list from <ref type="bibr" target="#b65">[63]</ref> to compute the scores (i.e., excluding images where a face is not detectable). Note that for this experiment, we exclude the Stirling dataset from our training set. is fundamentally different from the previous evaluation schemes. In addition, the benchmark of Feng et al. <ref type="bibr" target="#b27">[27]</ref> is based on the alignment using sparse facial (hand-selected) landmarks. Our experiments showed that this scheme is highly dependent on the selection of these markers and results in inconsistent evaluation results. In our listed results, we use the marker correspondences that come with the FLAME model <ref type="bibr" target="#b52">[51]</ref>. To get a more reliable evaluation scheme, we evaluate the benchmark of Feng et al. using the dense iterative closest point (ICP) technique from the NoW challenge, see <ref type="table">Table 3</ref>. On all metrics, our proposed method significantly improves the reconstruction accuracy. Note that some methods are even performing worse than the mean face <ref type="bibr" target="#b52">[51]</ref>. Qualitative Results. In <ref type="figure" target="#fig_0">Figure 3</ref>, we show qualitative results to analyze the stability of the face shape prediction of a subject across different expressions, head rotation, occlusions, or perspective distortion. As can be seen, our method is more persistent compared to others, especially, in comparison to Deng et al. <ref type="bibr" target="#b19">[20]</ref> where shape predictions vary the most. <ref type="figure">Figure 4</ref> depicts the challenging scenario of reconstructing toddlers from single images. Instead of predicting a small face for a child, the state of the art methods are predicting faces of adults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stirling/ESRC Benchmark</head><p>In contrast, MICA predicts the shape of a child with a correct scale.</p><p>In <ref type="figure" target="#fig_3">Figure 2</ref> reconstructions for randomly sampled identities from the Vox-Celeb2 <ref type="bibr" target="#b15">[16]</ref> dataset are shown. Some of the baselines, especially, RingNet <ref type="bibr" target="#b65">[63]</ref>, exhibits strong bias towards the mean human face. In contrast, our method is able to not only predict better overall shape but also to reconstruct challenging regions like nose or chin, even though the training dataset contains a much smaller identity and ethnicity pool. Note that while the reconstructions of the baseline methods look good under the projection, they are not metric as shown in <ref type="table" target="#tab_1">Tables 2 and 4</ref>. <ref type="figure">Fig. 4</ref>: Current methods are not predicting metrical faces, which becomes visible when displaying them in a metrical space and not in their image spaces. To illustrate we render the prediction of the faces of toddlers in a common metrical space using the same projection. State-of-the-art approaches trained in a selfsupervised fashion like DECA <ref type="bibr" target="#b25">[25]</ref> or weakly-supervised like FOCUS <ref type="bibr" target="#b51">[50]</ref> scale the face of an adult to fit the observation in the image space, thus, the prediction in 3D is non-metrical. In contrast, our reconstruction method is able to recover the physiognomy of the toddlers. Input images are generated by StyleGan2 <ref type="bibr" target="#b43">[42]</ref>.  <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b65">63]</ref> are based on a nonmetrical error metric (top-row). We propose a new evaluation protocol which measures reconstruction errors in a metrical space (bottom row) (c.f. <ref type="table" target="#tab_1">Table 2</ref>). Image from the NoW <ref type="bibr" target="#b65">[63]</ref> validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Limitations</head><p>Our method is not designed to predict shape and expressions in one forward pass, instead, we reconstruct the expression separately using an optimization-based tracking method. However, this optimization-based tracking leads to temporally coherent results, as can be seen in the suppl. video. In contrast to DECA <ref type="bibr" target="#b25">[25]</ref> or Deng et al. <ref type="bibr" target="#b19">[20]</ref>, the focus of our method is the reconstruction of a metrical 3D model, reconstructing high-frequent detail on top of our prediction is an interesting future direction. Our method fails, when the used face detector <ref type="bibr" target="#b67">[65]</ref> does not recognize a face in the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion &amp; Conclusion</head><p>A metrical reconstruction is key for any application that requires the measurement of distances and dimensions. It is essential for the composition of reconstructed humans and scenes where objects of known size are in, thus, it is especially important for virtual reality and augmented reality applications. However, we show that recent methods and evaluation schemes are not designed for this task. While the established benchmarks report numbers in millimeters, they are computed with an optimal scale to align the prediction and the reference. We strongly argue against this practice, since it is misleading and the errors are not absolute metrical measurements. To this end, we propose a simple, yet fundamental adjustment of the benchmarks to enable metrical evaluations. Specifically, we remove the optimal scaling, and only allow rigid alignment of the prediction with the reference shape. As a stepping stone towards metrical reconstructions, we unified existing small-and medium-scale datasets of paired 2D/3D data. This allows us to establish 3D supervised losses in our novel shape prediction framework. While our data collection is still comparably small (around 2k identities), we designed MICA that uses features from a face recognition network pretrained on a large-scale 2D image dataset to generalize to in-the-wild image data. We validated our approach in several experiments and show state-of-the-art results on our newly introduced metrical benchmarks as well as on the established scaleinvariant benchmarks. We hope that this work inspires researchers to concentrate on metrical face reconstruction. <ref type="figure">Fig. 6</ref>: Qualitative comparison on randomly sampled images from the VoxCeleb2 <ref type="bibr" target="#b15">[16]</ref> dataset. Our method is able to capture face shape with intricate details like nose and chin, while being metrical plausible (c.f., <ref type="table" target="#tab_1">Tables 2 and 4</ref>).  challenges. We refer to the main paper for the detailed statistics.</p><p>Abstract. In this supplemental document, we demonstrate the robustness of our proposed method in additional qualitative and quantitative experiments. The cumulative error plots from the NoW challenge presented in the main paper are also included in this document. Moreover, we present a justification of our architecture selection which is tailored for our unified dataset. Further, we discuss an alternative model-free estimation approach that does not rely on a 3DMM decoder and can be learned solely on our unified data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Additional Results</head><p>Our 3DMM-based shape estimation method presented in the main paper has two key components, (1) the encoder based on a face recognition network with a mapping network and (2) the 3DMM-based geometry decoder. The difference between our and the state-of-the-art methods w.r.t. their reconstruction quality gets well visible in the cumulative error plots in <ref type="figure" target="#fig_2">Figure 1</ref>. Moreover, <ref type="figure" target="#fig_3">Figure 2</ref> depicts side views of the reconstructions, which gives a better look at the shape quality. In this section, we present several ablation studies w.r.t. those modules and the used training data. All experiments were done with the same optimizer and hyper-parameter configuration as the main method except where stated otherwise. The Stirling dataset was excluded from all the ablation experiments. Encoder Ablation Studies. Exploiting generalized facial features from a face recognition network is a key component of our method to predict geometry from in-the-wild 2D data. However, completely refining the latent space of the face recognition network is not possible with our medium-size dataset, thus, we can only retrain selected layers to maintain generalizability. In <ref type="table" target="#tab_0">Table 1</ref>, we compare the performance of the two face recognition methods ArcFace <ref type="bibr" target="#b18">[19]</ref> and FaceNet <ref type="bibr" target="#b66">[64]</ref>. Overall, the pretrained ArcFace outperforms the pretrained FaceNet in terms of reconstruction quality in our shape estimation architecture. To further improve the results of ArcFace, we refine the last ResNet layer of ArcFace. Similarly, we conducted experiments on fine-tuning DECA <ref type="bibr" target="#b25">[25]</ref> using our mediumscale dataset and our reconstruction loss based on an ? 1 error metric. We trained the network on the same datasets like ArcFace for around 500 epochs. The finetuning of partial layers or entire pipeline leads to huge overfitting of the training data with significantly worse reconstructions on the test dataset (see <ref type="table" target="#tab_0">Table 1</ref>). In contrast, the partial fine-tuning of ArcFace in our approach gives the lowest mean reconstruction error of 1.35mm. It shows that we can effectively use the generalized features from the ArcFace network for the task of metrical face reconstruction. Decoder Ablation Studies. The decoder is defined by the 3DMM FLAME <ref type="bibr" target="#b52">[51]</ref>.</p><p>For our experiments in the main paper, we used 300 eigenvectors of the PCA basis. In <ref type="table" target="#tab_1">Table 2</ref>, we present an ablation study on the number of used eigenvectors (i.e., the size of the latent geometry code z). As can be seen, exploiting the full linear space of FLAME leads to the best performance. Dataset Ablation Studies. As described in the main paper, we used several datasets to construct our training set. We perform a leave-one-out analysis in <ref type="table">Table 3</ref> on the Stirling dataset. The LYHM dataset contains 1211 subjects and, thus, has the most significant influence on the training.</p><p>In addition to the datasets listed in the main paper, we also processed FaceScape <ref type="bibr" target="#b90">[86,</ref><ref type="bibr" target="#b94">89]</ref>. While FaceScape is a large-scale dataset, it has been recorded within an uncalibrated setup, thus, being in a none metrical scale which is introducing a bias in our prediction. <ref type="table">Table 3</ref>: To analyze the contribution of a single dataset, we perform a leaveone-out analysis. We report the reconstruction quality for images from Stirling dataset <ref type="bibr" target="#b29">[28]</ref> (where we exclude Stirling from training). As can be seen, LYHM <ref type="bibr" target="#b17">[18]</ref> has the highest influence on the reconstruction quality, leaving it out leads to an increase of the mean error for HQ images from 1.35mm to 1.43mm and for LQ images from 1.46mm to 1.51mm on the Stirling dataset <ref type="bibr" target="#b29">[28]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Studies on the Facial Expression Tracking</head><p>Our metrical face shape prediction can be used to initialize facial expression tracking. In contrast to methods like <ref type="bibr" target="#b25">[25]</ref>, our method uses a perspective camera model, which allows us to predict a depth. In <ref type="figure">Figure 4</ref>, we show a sample sequence from <ref type="bibr" target="#b84">[81]</ref> with an according depth and photo-metric error plot. <ref type="figure" target="#fig_0">Fig. 3</ref>: Evaluation of the tracking error on the 'Volker' sequence of <ref type="bibr" target="#b84">[81]</ref>. The RMSE depth error is computed based on the reference depth maps which have been reconstructed using a stereo system. The photo-metric error is computed based on an RMSE error metric assuming RGB in [0, 255] 3 .</p><p>As can be seen, our method results in the lowest photo-metric error in terms of a masked RMSE metric on the colors. The error plots shown in <ref type="figure" target="#fig_0">Figure 3</ref> contain the metric reconstruction error of the depth (RMSE). It is based on the reference depth information of the sequence, which has been reconstructed from a passive stereo system. We also evaluate the dense photometric error (RMSE), which can be computed for <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">25]</ref> too. In comparison to the method Face2Face <ref type="bibr" target="#b77">[75]</ref> which also uses a perspective camera model (11.0mm mean RMSE depth error), our metrical face shape estimation improves the tracking quality significantly (5.7mm mean RMSE). In the supplemental video, we show several tracking results which demonstrate that our proposed technique is temporally stable. <ref type="figure">Fig. 4</ref>: Photo-metric error on the three sequences from Garrido et al. <ref type="bibr" target="#b32">[31]</ref> shown in the supplemental video.The photo-metric error is computed based on an RMSE error metric assuming RGB in [0, 255] 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model-free Decoder</head><p>Inspired by pi-GAN <ref type="bibr" target="#b11">[12]</ref> and Dynamic Surface Function Networks <ref type="bibr" target="#b9">[10]</ref>, we also evaluated a coordinated-based multi layer perceptron (MLP) with sinusoidal activation functions (SIREN <ref type="bibr" target="#b69">[67]</ref>) to represent the geometry of a face (see <ref type="figure" target="#fig_1">Figure 5</ref>). This architecture can be trained solely on the data of our unified dataset without requiring any 3DMM model. The network and its sinusoidal activation functions are controlled by a mapping network M ? to represent different faces. The mapping network takes the identity code z as input and predicts the frequencies and phase shifts of the sinusoidal activation layers. The SIREN network S is evaluated at the FLAME <ref type="bibr" target="#b52">[51]</ref> template mesh vertices A ? R 3N and N = 5023 to leverage the correspondences of the 3D training data.</p><p>Since this model does not rely on the PCA basis of the FLAME model, it can predict meshes outside the FLAME face space. In comparison to the 3DMMbased model presented in the main paper, this model-free approach performs on par on the different benchmarks (see <ref type="table" target="#tab_8">Table 4</ref>). A benefit of the model-free decoder is that it can be trained solely on our dataset of paired 2D/3D data which is significantly smaller than the dataset of 3D scans used for the construction of the FLAME model (2.3k (our dataset) versus 4k subjects used for FLAME). <ref type="figure" target="#fig_1">Fig. 5</ref>: Overview of a model-free decoder. The model-free decoder is based on a Siren architecture <ref type="bibr" target="#b69">[67]</ref> using FiLM conditionings <ref type="bibr" target="#b11">[12]</ref>. In contrast to the FLAMEbased decoder, this model-free decoder can be trained in conjunction to the encoder only based on the dataset with the paired 2D/3D data which is smaller than the dataset of 3D scans used for constructing the FLAME model.</p><p>A drawback of this Siren-based approach is its runtime and complexity (3DMM only has a single linear layer for representing shape variations). The used SIREN network is a more compact representation using 8 hidden layers and 256 feature size with total 1976327 parameters, while the 3DMM has a linear layer with (300 + 1) * 5023 * 3 = 4535769 parameters. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative results on NoW Challenge<ref type="bibr" target="#b65">[63]</ref> to show the invariance of our method to changes in illumination, expression, occlusion, rotation, and perspective distortion in comparison to other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Established evaluation benchmarks like</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Cumulative plots for the (a) NoW [63] and (b) NoW-Metric (w/o scale)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Qualitative comparison on randomly sampled images from the VoxCeleb2<ref type="bibr" target="#b15">[16]</ref> dataset for side views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of our unified datasets. The used datasets vary in the capture modality and the capture protocol. Here, we list the number of subject, the minimum number of images per subjects, and whether the dataset includes facial expressions. In total our dataset contains 2315 subjects with FLAME topology.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Subj. #Min. Img. Expr.</cell></row><row><cell>Stirling [28]</cell><cell>133</cell><cell>8</cell><cell>?</cell></row><row><cell>D3DFACS [17]</cell><cell>10</cell><cell>videos</cell><cell>?</cell></row><row><cell>Florence 2D/3D [3]</cell><cell>53</cell><cell>videos</cell><cell>?</cell></row><row><cell>BU-3DFE [87]</cell><cell>100</cell><cell>83</cell><cell>?</cell></row><row><cell>LYHM [18]</cell><cell>1211</cell><cell>2</cell><cell>?</cell></row><row><cell>FaceWarehouse [11]</cell><cell>150</cell><cell>119</cell><cell>?</cell></row><row><cell>FRGC [57]</cell><cell>531</cell><cell>7</cell><cell>?</cell></row><row><cell>BP4D+ [88]</cell><cell>127</cell><cell>videos</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation of the face shape estimation on the NoW Challenge<ref type="bibr" target="#b65">[63]</ref>. Note that we list two different evaluations: the non-metrical evaluation from the original NoW challenge and our new metrical evaluation (including a cumulative error plot on the left). The original NoW challenge cannot be considered metrical since Procrustes analysis is used to align the reconstructions to the corresponding reference meshes, including scaling. We list all methods from the original benchmark and additionally show the performance of the average human face of FLAME<ref type="bibr" target="#b52">[51]</ref> as a reference (first row).</figDesc><table><row><cell>NoW-Metric Challenge</cell><cell cols="2">Non-Metrical [63]</cell><cell cols="2">Metrical (mm)</cell><cell></cell></row><row><cell>Method</cell><cell cols="5">Median Mean Std Median Mean Std</cell></row><row><cell>Average Face (FLAME [51])</cell><cell>1.21</cell><cell>1.53 1.31</cell><cell>1.49</cell><cell cols="2">1.92 1.68</cell></row><row><cell>3DMM-CNN [77]</cell><cell>1.84</cell><cell>2.33 2.05</cell><cell>3.91</cell><cell cols="2">4.84 4.02</cell></row><row><cell>PRNet [26]</cell><cell>1.50</cell><cell>1.98 1.88</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Deng et al [20] (TensorFlow)</cell><cell>1.23</cell><cell>1.54 1.29</cell><cell>2.26</cell><cell cols="2">2.90 2.51</cell></row><row><cell>Deng et al [20] (PyTorch)</cell><cell>1.11</cell><cell>1.41 1.21</cell><cell>1.62</cell><cell cols="2">2.21 2.08</cell></row><row><cell>RingNet [63]</cell><cell>1.21</cell><cell>1.53 1.31</cell><cell>1.50</cell><cell cols="2">1.98 1.77</cell></row><row><cell>3DDFA-V2 [37]</cell><cell>1.23</cell><cell>1.57 1.39</cell><cell>1.53</cell><cell cols="2">2.06 1.95</cell></row><row><cell>MGCNet [66]</cell><cell>1.31</cell><cell>1.87 2.63</cell><cell>1.70</cell><cell cols="2">2.47 3.02</cell></row><row><cell>UMDFA [47]</cell><cell>1.52</cell><cell>1.89 1.57</cell><cell>2.31</cell><cell cols="2">2.97 2.57</cell></row><row><cell>Dib et al. [21]</cell><cell>1.26</cell><cell>1.57 1.31</cell><cell>1.59</cell><cell cols="2">2.12 1.93</cell></row><row><cell>DECA [25]</cell><cell>1.09</cell><cell>1.38 1.18</cell><cell>1.35</cell><cell cols="2">1.80 1.64</cell></row><row><cell>FOCUS [50]</cell><cell>1.04</cell><cell>1.30 1.10</cell><cell>1.41</cell><cell cols="2">1.85 1.70</cell></row><row><cell>Ours</cell><cell>0.90</cell><cell>1.11 0.92</cell><cell>1.08</cell><cell cols="2">1.37 1.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Ablation study w.r.t. our face encoding network based on Stirling dataset<ref type="bibr" target="#b29">[28]</ref> using our metrical evaluation scheme. As a comparison, we also show the results for DECA<ref type="bibr" target="#b25">[25]</ref> fine-tuned on our dataset. The respective ResNet<ref type="bibr" target="#b40">[39]</ref> networks were refined in different configurations; {L3, L4} denotes the set of selected trainable layers from {L1, ..., L4}. Each layer is composed of several ResNet blocks, specifically, ArcFace uses {3, 13, 30, 3} and DECA {3, 4, 6, 3} ResNet blocks for the respective layers.</figDesc><table><row><cell>Encoder</cell><cell>Median Mean (mm) LQ HQ LQ HQ</cell><cell>Std LQ HQ</cell></row><row><cell>DECA [25] (frozen)</cell><cell cols="2">1.32 1.22 1.71 1.58 1.54 1.42</cell></row><row><cell>DECA [25] (fully trainable)</cell><cell cols="2">1.54 1.42 1.96 1.82 1.71 1.61</cell></row><row><cell>DECA [25] (L3 ? L4 trainable)</cell><cell cols="2">1.55 1.43 1.97 1.83 1.71 1.62</cell></row><row><cell>DECA [25] (L4 trainable)</cell><cell cols="2">1.55 1.49 1.97 1.83 1.71 1.61</cell></row><row><cell>Ours -FaceNet [64] (frozen)</cell><cell cols="2">1.37 1.29 1.75 1.65 1.56 1.47</cell></row><row><cell>Ours -ArcFace [19] (frozen)</cell><cell cols="2">1.25 1.18 1.60 1.52 1.43 1.37</cell></row><row><cell>Ours -ArcFace [19] (fully trainable)</cell><cell cols="2">1.18 1.11 1.52 1.42 1.38 1.27</cell></row><row><cell cols="3">Ours -ArcFace [19] (L2 ? L3 ? L4 trainable) 1.22 1.12 1.56 1.43 1.39 1.27</cell></row><row><cell>Ours -ArcFace [19] (L3 ? L4 trainable)</cell><cell cols="2">1.17 1.10 1.51 1.40 1.37 1.25</cell></row><row><cell>Ours -ArcFace [19] (L4 trainable)</cell><cell cols="2">1.15 1.06 1.46 1.35 1.30 1.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of the influence of the number of principle components (PCs) used for the shape decoder (Stirling dataset<ref type="bibr" target="#b29">[28]</ref> with NoW protocol (metrical)).</figDesc><table><row><cell>Decoder -#PC</cell><cell>Median Mean (mm) LQ HQ LQ HQ</cell><cell>Std LQ HQ</cell></row><row><cell>50</cell><cell cols="2">1.19 1.12 1.50 1.41 1.33 1.23</cell></row><row><cell>100</cell><cell cols="2">1.15 1.10 1.45 1.39 1.28 1.22</cell></row><row><cell>200</cell><cell cols="2">1.15 1.06 1.47 1.36 1.31 1.20</cell></row><row><cell>300</cell><cell cols="2">1.15 1.06 1.46 1.35 1.30 1.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Face Warehouse [17] 1.13 1.07 1.43 1.36 1.27 1.20</figDesc><table><row><cell>Dataset</cell><cell>Median Mean (mm) LQ HQ LQ HQ</cell><cell>Std LQ HQ</cell></row><row><cell>w/o LYHM [18]</cell><cell cols="2">1.18 1.12 1.51 1.43 1.35 1.27</cell></row><row><cell>w/o FRGC [87]</cell><cell cols="2">1.15 1.06 1.47 1.36 1.33 1.23</cell></row><row><cell>w/o BP4D+ [87]</cell><cell cols="2">1.14 1.09 1.46 1.38 1.30 1.21</cell></row><row><cell>w/o BU3DFE [87]</cell><cell cols="2">1.14 1.08 1.45 1.37 1.29 1.21</cell></row><row><cell>w/o D3DFACS [17]</cell><cell cols="2">1.13 1.06 1.43 1.35 1.28 1.19</cell></row><row><cell>w/o</cell><cell></cell><cell></cell></row></table><note>All 1.15 1.06 1.46 1.35 1.30 1.20</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Quantitative evaluation of the face shape estimation using Stirling dataset [28] and NoW protocol (metrical). LQ HQ LQ HQ LQ HQ LQ HQ LQ HQ Deng et al. [20] (PyTorch) 1.12 0.99 1.44 1.27 1.31 1.15 1.47 1.31 1.93 1.71 1.77 1.57 DECA [25] 1.09 1.03 1.39 1.32 1.26 1.18 1.32 1.22 1.71 1.58 1.54 1.42 Ours (SIREN) 1.01 0.94 1.28 1.19 1.15 1.06 1.20 1.09 1.53 1.39 1.35 1.23 Ours (FLAME) 0.96 0.92 1.22 1.16 1.11 1.04 1.15 1.06 1.46 1.35 1.30 1.20</figDesc><table><row><cell></cell><cell></cell><cell>Non-Metrical</cell><cell></cell><cell></cell><cell>Metrical (mm)</cell><cell></cell></row><row><cell>Stirling (NoW Protocol)</cell><cell>Median</cell><cell>Mean</cell><cell>Std</cell><cell>Median</cell><cell>Mean</cell><cell>Std</cell></row><row><cell></cell><cell>LQ HQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">G SIREN (z) = S(A | M ? (z)).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We thank Haiwen Feng for support with NoW and Stirling evalua- Disclosure. While TB is part-time employee of Amazon, his research was performed solely at, and funded solely by MPI. JT is supported by Microsoft Research gift funds.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-modal deep face normals with deactivable skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Abrevaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boukhayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Partial fc: Training 10 million identities on a single machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ying</surname></persName>
		</author>
		<idno>Arxiv 2010.05222</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The florence 2D/3D hybrid face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2072572.2072597</idno>
		<ptr target="https://doi.org/10.1145/2072572.2072597" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding</title>
		<meeting>the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="79" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What does 2D geometric information really tell us about 3D face shape?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1455" to="1473" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reanimating faces in images and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EUROGRAPHICS (EG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exchanging faces in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH. pp</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dynamic surface function networks for clothed human bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Burov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FaceWarehouse: A 3D facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">pi-GAN: Periodic implicit generative adversarial networks for 3D-aware image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Expnet: Landmark-free, deep, 3d facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Personalized face modeling for improved face reconstruction and motion retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vesdapunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photo-realistic facial details synthesis from single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH&quot;</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A facs valid 3d dynamic action unit database with applications to 3d dynamic morphable facial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cosker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krumhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.6126510</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2011.6126510" />
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical modeling of craniofacial shape and texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01260-7</idno>
		<ptr target="https://doi.org/10.1007/s11263-019-01260-72" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sub-center arcface: Boosting face recognition by large-scale noisy web faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W) (2019) 2, 5, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards high fidelity monocular face reconstruction with rich reflectance using self-supervised learning and ray tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thebault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chevallier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">End-to-end 3d face reconstruction with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3D morphable face models -past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3395208</idno>
		<ptr target="https://doi.org/10.1145/33952083" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<ptr target="https://github.com/HavenFeng/photometric_optimization7" />
		<title level="m">Photometric FLAME fitting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning an animatable detailed 3D face model from in-the-wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>2021) 2, 4, 5, 9</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint 3D face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluation of dense 3D reconstruction from 2D face images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J B</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?tsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="780" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/FG.2018.00123</idno>
		<ptr target="https://doi.org/10.1109/FG.2018.001232,9" />
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Evaluation of dense 3d reconstruction from 2d face images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J B</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?tsch</surname></persName>
		</author>
		<idno>abs/1803.05536</idno>
		<ptr target="http://arxiv.org/abs/1803.055362" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic face reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rehmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormaehlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">VDub -modifying face video of actors for plausible visual alignment to a dubbed audio track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sarmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUROGRAPHICS (EG)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reconstructing detailed dynamic face geometry from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2508363.2508380</idno>
		<ptr target="http://doi.acm.org/10.1145/2508363.25083806" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH Asia</title>
		<meeting>SIGGRAPH Asia</meeting>
		<imprint>
			<date type="published" when="2013-11" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ganfit: Generative adversarial network fitting for high fidelity 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast-ganfit: Generative adversarial network for high fidelity 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Unsupervised training for 3d morphable model regression</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Grassal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prinzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<title level="m">Neural head avatars from monocular rgb videos</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attention mesh: High-fidelity face mesh prediction in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Grishchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ablavatski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kartynnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Raveendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3d dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) (2020) 9</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<ptr target="http://arxiv.org/abs/1512.033853" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Avatar digitization from a single image for real-time rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fursund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3130800.31310887</idno>
		<ptr target="https://doi.org/10.1145/3130800.313108874" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Real-time facial surface geometry from monocular video on mobile gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kartynnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ablavatski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Grishchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep video portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">InverseFaceNet: Deep monocular inverse face rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">look ma, no landmarks!&quot; -unsupervised, model-based dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12347</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">AvatarMe: Realistically renderable 3D facial reconstruction&quot; in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lattas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Triantafyllou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">AvatarMe++: Facial shape and BRDF inference with photorealistic rendering-aware GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lattas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">To fit or not to fit: Model-based face reconstruction and occlusion segmentation from weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<idno>abs/2106.09614</idno>
		<ptr target="https://arxiv.org/abs/2106.096149" />
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning a model of facial shape and expression from 4D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<idno type="DOI">10.1145/3130800.3130813</idno>
		<ptr target="https://doi.org/10.1145/3130800.31308132" />
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Soft rasterizer: A differentiable renderer for image-based 3d reasoning. International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<ptr target="http://arxiv.org/abs/1711.051016" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Sukno</surname></persName>
		</author>
		<title level="m">Survey on 3d face reconstruction from uncalibrated images</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pagan: Real-time avatars using dynamic textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fursund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3272127.3275075</idno>
		<ptr target="https://doi.org/10.1145/3272127.32750754" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on advanced video and signal based surveillance</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2005.268</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2005.2688" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An efficient representation for irradiance environment maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<biblScope unit="page" from="497" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<idno type="DOI">10.1145/383259.383317</idno>
		<ptr target="https://doi.org/10.1145/383259.3833177" />
		<title level="m">SIGGRAPH &apos;01</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08501</idno>
		<title level="m">Accelerating 3d deep learning with pytorch3d</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<title level="m">3d face reconstruction by learning from synthetic data</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Photorealistic facial texture inference using deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning to regress 3d face shape and expression from an image without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298682</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2015.72986824" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hyperextended lightface: A facial attribute analysis framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Serengil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozpinar</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICEET53442.2021.9659697</idno>
		<ptr target="https://doi.org/10.1109/ICEET53442.2021.965969713" />
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Engineering and Emerging Technologies (ICEET). pp. 1-4. IEEE</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Selfsupervised monocular 3D face reconstruction by occlusion-aware multiview geometry consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12360</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) (2020)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fml: Face model learning from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Z?llhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoll?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Christian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Facevr: Real-time gaze-aware facial reenactment in virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>TOG)</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<title level="m">Neural voice puppetry: Audio-driven facial reenactment. European Conference on Computer Vision (ECCV) (2020)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deferred neural rendering: Image synthesis using neural textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Real-time expression transfer for facial reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Face2Face: Real-time face capture and reenactment of RGB videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Headon: Real-time reenactment of human portrait videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3197517.3201350</idno>
		<ptr target="http://dx.doi.org/10.1145/3197517.3201350" />
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3D morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Extreme 3D face reconstruction: Seeing through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Towards high-fidelity nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>eeding of IEEE Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Joint 3D face reconstruction and dense face alignment from a single image with 2D-assisted self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09359</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Lightweight binocular facial performance capture under uncontrolled lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="187" />
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<idno type="DOI">http:/doi.acm.org/10.1145/2366145.2366206</idno>
		<ptr target="http://doi.acm.org/10.1145/2366145.23662065" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>3d dense face alignment via graph convolution networks</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Realtime performance-based facial animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Graphics (TOG). vol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Face/Off: live facial puppetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">High-fidelity facial reflectance and geometry inference from an unconstrained image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3197517.3201364</idno>
		<ptr target="https://doi.org/10.1145/3197517.3201364" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Facescape: A large-scale high quality 3d face dataset and detailed riggable 3d face prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Automatic Face and Gesture Recognition (FGR06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/FGR.2006.6</idno>
		<ptr target="https://doi.org/10.1109/FGR.2006.6" />
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.374</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.3748" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.01082</idno>
		<title level="m">Facescape: 3d facial dataset and benchmark for singleview 3d face reconstruction</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">https:/doi.ieeecomputersociety.org/10.1109/CVPR.2016.23</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.234" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">State of the art on monocular 3D face reconstruction, tracking, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics State of the Art Reports)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Computer Graphics Forum</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
