<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Two-Stream Network for Violence Detection Using Separable Convolutional LSTM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahidul</forename><surname>Islam</surname></persName>
							<email>zahidulislam@iut-dhaka.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rukonuzzaman</surname></persName>
							<email>rukonuzzaman@iut-dhaka.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raiyan</forename><surname>Ahmed</surname></persName>
							<email>raiyanahmed@iut-dhaka.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md. Hasanul</roleName><surname>Kabir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshiur</forename><surname>Farazi</surname></persName>
							<email>moshiur.farazi@data61.csiro.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Islamic University of Technology Dhaka</orgName>
								<address>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>Data61-CSIRO</addrLine>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Two-Stream Network for Violence Detection Using Separable Convolutional LSTM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatically detecting violence from surveillance footage is a subset of activity recognition that deserves special attention because of its wide applicability in unmanned security monitoring systems, internet video filtration, etc. In this work, we propose an efficient two-stream deep learning architecture leveraging Separable Convolutional LSTM (SepConvLSTM) and pre-trained MobileNet where one stream takes in background suppressed frames as inputs and other stream processes difference of adjacent frames. We employed simple and fast input pre-processing techniques that highlight the moving objects in the frames by suppressing non-moving backgrounds and capture the motion in-between frames. As violent actions are mostly characterized by body movements these inputs help produce discriminative features. SepConvLSTM is constructed by replacing convolution operation at each gate of ConvLSTM with a depthwise separable convolution that enables producing robust long-range Spatio-temporal features while using substantially fewer parameters. We experimented with three fusion methods to combine the output feature maps of the two streams. Evaluation of the proposed methods was done on three standard public datasets. Our model outperforms the accuracy on the larger and more challenging RWF-2000 dataset by more than a 2% margin while matching state-of-the-art results on the smaller datasets. Our experiments lead us to conclude, the proposed models are superior in terms of both computational efficiency and detection accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Automatically detecting violence from surveillance footage is a subset of activity recognition that deserves special attention because of its wide applicability in unmanned security monitoring systems, internet video filtration, etc. In this work, we propose an efficient two-stream deep learning architecture leveraging Separable Convolutional LSTM (SepConvLSTM) and pre-trained MobileNet where one stream takes in background suppressed frames as inputs and other stream processes difference of adjacent frames. We employed simple and fast input pre-processing techniques that highlight the moving objects in the frames by suppressing non-moving backgrounds and capture the motion in-between frames. As violent actions are mostly characterized by body movements these inputs help produce discriminative features. SepConvLSTM is constructed by replacing convolution operation at each gate of ConvLSTM with a depthwise separable convolution that enables producing robust long-range Spatio-temporal features while using substantially fewer parameters. We experimented with three fusion methods to combine the output feature maps of the two streams. Evaluation of the proposed methods was done on three standard public datasets. Our model outperforms the accuracy on the larger and more challenging RWF-2000 dataset by more than a 2% margin while matching state-of-the-art results on the smaller datasets. Our experiments lead us to conclude, the proposed models are superior in terms of both computational efficiency and detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human activity recognition is a widely investigated problem in the field of computer vision that has diverse applications in human-computer interaction, robotics, surveillance, etc. In recent years, large-scale video action recognition has gained impressive improvements mostly due to the availability of large datasets, deep neural network architectures, video representation techniques, etc. Many works, on the other hand, focused on specific sub-tasks of action recognition such as spatial-temporal localization of activity, anomaly detection, action quality analysis (AQA), egocentric activity recognition, etc. One such important subset is violence detection which is widely applicable in public monitoring, surveillance systems, internet video filtering, etc. As digital media technologies like surveillance cameras are getting more and more ubiquitous, detecting violence from captured footage using manual inspection seems increasingly difficult. To address this issue, researchers have suggested different approaches that can detect violence from surveillance footage automatically without requiring any human interaction. Violence detection is a section of general action recognition task which specifically focuses on detecting aggressive human behaviors such as fighting, robbery, rioting, etc. Earlier works on violence detection mostly focused on engineering various descriptors that could effectively capture violent motion present in the video [1]- <ref type="bibr" target="#b2">[3]</ref>. Later on, the performance of these handcrafted features was surpassed by several end-to-end trainable deep learning methods which require little to no pre-processing <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b7">[6]</ref>. To validate the effectiveness of these methods, commonly three standard benchmark datasets were used called Hockey, Movies, and Violent-Flows. Recently, a new dataset called RWF-2000 has been proposed which is substantially bigger and more diverse. For applying these deep learning models in real-life practical scenarios both computational efficiency and accuracy need to be considered. In this respect, we present a novel two-stream CNN-LSTM based network that can produce discriminative Spatio-temporal features while requiring fewer parameters. In general action recognition tasks, surroundings or background information may serve as discriminative clues. For example, to identify the action playing golf, a background with green grass might be a good indicator. On the other hand, violent activities are mostly characterized by the body position, movements, and interactions whereas appearance-based features like color, texture, and background information play a minor role. Considering these factors, we used background suppressed frames and frame difference as the inputs to our network both of which help generate discriminative features to recognize violence.</p><p>We can encapsulate our significant contributions in the following points:</p><p>? We propose a novel two-stream deep learning architecture that leverages Separable Convolutional LSTM (SepCon-vLSTM) and pre-trained truncated MobileNet. ? We utilized simple and fast input pre-processing techniques that highlight the moving objects in the frames by suppressing non-moving backgrounds and capture the motion in-between frames. ? We leveraged SepConvLSTM which is constructed by replacing convolution operation at each gate of ConvLSTM with a depthwise separable convolution enabling us to use substantially fewer parameters. We experimented on three fusion strategies for combining the output features of two streams. ? We validate the performance of our models on three standard benchmark datasets. The proposed model outperforms the previous best result on the RWF-2000 dataset and matches state-of-the-art performance on the other datasets. Our model is also efficient in terms of the required number of parameters and FLOPs. The rest of the paper is laid out as follows: Section 2 provides an overview of related works on violence detection. Section 3 demonstrates the proposed method in detail. Section 4 explains training methods and experiments. Finally, Section 5 concludes our work and discusses possible future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Classical methods for violence detection were mostly focused on designing hand-crafted features that explicitly represent motion trajectory, the orientation of limbs, local appearance, inter-frame changes, etc. Using two such features, Motion Scale Invariant Feature Transform (MoSIFT), and Spatiotemporal Interest Points (STIP), Nievas et al. <ref type="bibr" target="#b8">[7]</ref> proposed leveraging Bag-of-Words framework. They also introduced two well-known violence detection datasets. Hassner et al. <ref type="bibr" target="#b9">[8]</ref> developed the Violent Flows (ViF) feature using changes of optical flow magnitudes. Improving upon this work, Gao et al. <ref type="bibr" target="#b10">[9]</ref> incorporated motion orientations and proposed Oriented Violent Flows (OViF). Deniz et al. <ref type="bibr" target="#b11">[10]</ref> proposed estimating extreme acceleration using Radon Transform on adjacent frames. Senst et al. <ref type="bibr" target="#b0">[1]</ref> proposed using Lagrangian directional fields for background motion compensation. Seranno et al. <ref type="bibr" target="#b12">[11]</ref> leveraged Hough Forests and 2D CNN to create a hybrid framework combining both handcrafted and learned features. However, handcrafted feature-based methods are mostly unsuitable for deploying in real-world applications due to their restricted generalization ability in diverse situations.</p><p>The popularity of deep learning methodologies lead to many works on violence detection focusing on building endto-end trainable neural networks that perform well with little to no pre-processing. Ding et al. <ref type="bibr" target="#b13">[12]</ref> employed a 3D Convolutional Network to recognize violence directly from raw inputs. Following the success of two-stream networks <ref type="bibr" target="#b14">[13]</ref> on general activity recognition tasks, Dong et al. <ref type="bibr" target="#b15">[14]</ref> added acceleration stream with spatial and temporal ones for detecting person to person violence. Optical flow, acceleration, or frame difference on separate streams boost temporal feature learning. Dai et al. <ref type="bibr" target="#b16">[15]</ref> proposed an LSTM that works over two streams to enhance the capture of temporal dynamics and a final SVM classifier for classification. The initial works on CNN-LSTM models used a fully connected regular LSTM layer that takes in 1-dimensional feature vectors as inputs and does not retain the spatial properties of the features learned by CNNs <ref type="bibr" target="#b15">[14]</ref>. On the other hand, using fully connected 2D LSTM layers is not feasible as they need a huge number of parameters. Sudhakaran et al. <ref type="bibr" target="#b5">[5]</ref> proposed using ConvLSTM <ref type="bibr" target="#b17">[16]</ref> as the recurrent unit to aggregate frame-level features which implements gate operations inside LSTM cell using convolutions reducing parameter count to a great extent. ConvLSTM can preserve spatial information and are capable of working on 2D features without flattening them to 1D vectors. They also showed that training on the difference of adjacent frames enhanced performance. Later, Hanson et al. <ref type="bibr" target="#b7">[6]</ref> extended this work to allow bidirectional temporal encodings in the feature vectors by using BiConvL-STM that leverages long-range information in both temporal directions. Li et al. <ref type="bibr" target="#b3">[4]</ref> proposed an efficient 3D CNN based on DenseNet <ref type="bibr" target="#b18">[17]</ref> architecture which requires significantly fewer parameters. Pixoto et al. employed two deep neural nets to extract Spatio-temporal features representing specific concepts and aggregated them using a shallow network. Some works <ref type="bibr" target="#b19">[18]</ref>, <ref type="bibr" target="#b20">[19]</ref> focused on multimodal detection of violence by combining visual and auditory cues. However, as the audio signal is generally unavailable in surveillance footage, most works concentrated on visual information. In our work, we leveraged MobileNet <ref type="bibr" target="#b22">[20]</ref> which is a lightweight 2D CNN that uses depthwise separable convolutions and clever design choices to develop a fast and efficient model geared towards mobile and embedded vision applications. We also employed Separable Convolutional LSTM (SepConvLSTM) which is constructed by replacing the convolution operations in the LSTM gates with depthwise separable convolutions. In a recent study, Separable Convolutional LSTM has been used for speeding up video segmentation task <ref type="bibr" target="#b23">[21]</ref>. However, we did not find any work in the field of activity recognition that focuses on utilizing SepConvLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>The objective of our proposed approach is to develop an end-to-end trainable neural network that can effectively capture long-range Spatio-temporal features to recognize violent actions while being computationally efficient. To this end, we developed a novel and efficient two-stream network for violence detection. We also developed a simple technique to highlight the body movements in the frames and suppress nonmoving background information that promulgates the capture of discriminative features. In this section, we first describe Separable Convolutional LSTM which is an integral component of our model. Then, we discuss the input pre-processing steps that are utilized in our pipeline. Finally, a description of the architecture of the proposed network and the fusion strategies are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Separable Convolutional LSTM</head><p>Depthwise separable convolution is an efficient modification of standard convolution operation where each input channel is convolved with one filter separately to produce an output with the same number of channels. Then, a 1 ? 1 convolution is applied to recombine the information across the channels. This results in a reduction of computation by a factor of 1 N + 1 K 2 where, K is kernel size and N is the number of output channels <ref type="bibr" target="#b22">[20]</ref>. Convolutional LSTM is a good choice to encode temporal changes in a sequence of spatial feature maps as it can preserve spatial information. We replace the convolution operations in the ConvLSTM cell with depthwise separable convolutions which reduces the parameter count drastically and makes the cell compact and lightweight. Equations 1 represent the operations inside a SepConvLSTM cell.</p><formula xml:id="formula_0">i t = ?( 1?1 W x i * (W x i x t ) + 1?1 W h i * (W h i h t?1 ) + b i ) f t = ?( 1?1 W x f * (W x f x t ) + 1?1 W h f * (W h f h t?1 ) + b f ) c t = ? ( 1?1 W x c * (W x c x t ) + 1?1 W h c * (W h c h t?1 ) + b c ) o t = ?( 1?1 W x o * (W x o x t ) + 1?1 W h o * (W h o h t?1 ) + b o ) c t = f t ? c t?1 + i t ?c t h t = o t ? ? (c t )<label>(1)</label></formula><p>Here, * represents convolution, ? represents the Hadamard product, ? represents sigmoid activation, ? represents tanh activation and represents depthwise convolution. 1?1 W and W are pointwise and depthwise kernels respectively. Memory cell c t , hidden state h t and the gate activations f t ,i t and o t are all 3-dimensional tensors. In our experiments, SepConvLSTM turned out to be effective in encoding localized Spatio-temporal feature maps that can be used to distinguish between violent and non-violent videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-processing</head><p>On one stream of our network, we pass the difference of adjacent frames as inputs that promotes the model to encode temporal changes between the adjacent frames boosting the capture of motion information. They were shown to be effective in previous works <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b7">[6]</ref>. Frame differences serve as an efficient alternative to computationally expensive optical flow. In equation 2, f rame i denotes ith frame and f d i is the ith time-step of frame difference. A video clip with k frames produces a corresponding frame difference of k ?1 time-steps. On the other stream, instead of using frames directly, we opted to use background suppressed frames. We employed a simple technique to estimate the background to avoid adding computational overhead. We first calculate the average of all the frames. The average frame mostly contains the background information because they remain unvarying across multiple frames. Then we subtract this average from every frame which accentuates the moving objects in the frame by suppressing the background information. As violent actions are mostly characterized by body movements, not the non-moving background features, this promotes the model to focus more on relevant information. Equations 3 represent this procedure formally.</p><formula xml:id="formula_1">f d i = f rame i+1 ? f rame i (2) (a) (b) (c)</formula><formula xml:id="formula_2">avg = N i=0 f rame i N bsf i = |f rame i ? avg|<label>(3)</label></formula><p>Here, f rame i denotes ith frame, avg is the average of all the frames, and bsf i is the ith time-step of background suppressed frames that we use as inputs to our model. <ref type="figure" target="#fig_1">Figure 2</ref> shows the effect of background suppression and frame difference on video frames. Frame difference mostly encodes temporal information like movements by highlighting the change in body positions. On the other hand, background suppressed frames subdue the background pixels while retaining some textural or appearance-based information of the foreground moving objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture</head><p>As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, the proposed network comprises two separate streams with the similar architecture. Each stream has a 2D convolutional network that extracts spatial features from each time-step of the clip. An LSTM layer learns to encode these spatial features to generate Spatio-temporal feature maps which are passed to the classification layers. On the first stream, background suppressed video frames are passed sequentially to the model. Once all the frames are passed we extract the Spatio-temporal features from the hidden state of the last time-step of the LSTM. The same procedure is followed on the second stream but here we use the difference of adjacent frames as inputs. Frame differences serve as an efficient approximation of optical flow avoiding the computational complexity of calculating optical flow. The frame difference stream learns to encode temporal changes capturing the motion in-between frames while the other stream mainly focuses on spatial appearance-based information. The output features of both streams combined produce robust Spatio-temporal feature maps which are capable of distinguishing between violent and non-violent videos.</p><p>We used MobileNetV2(? = 0.35) <ref type="bibr" target="#b24">[22]</ref> pre-trained on Ima-geNet dataset <ref type="bibr" target="#b25">[23]</ref> as the CNN for extracting spatial features where ? is the width multiplier. The last 30 layers from the MobileNet models were truncated as we found them to be redundant in our preliminary experiments. Pretraining improves generalization and speeds up training. We use Separable Convolutional LSTM (SepConvLSTM) for producing localized Spatio-temporal features from the output feature maps of the CNN. Previously, SepConvLSTM has been used to speed up video segmentation tasks <ref type="bibr" target="#b23">[21]</ref> but have not been explored for action classification tasks. Frames of shape 224 ? 224 ? 3 are passed into the model. In each stream, the CNN extracts spatial features of shape 7 ? 7 ? 56. As we used SepConvLSTMs with 64 filters, they output a feature map of shape 7 ? 7 ? 64 each. After passing through a Max-Pooling layer with window size (2,2), the output features maps from the two streams are fused using a Fusion layer which is described in the following section. Then, the combined feature maps are passed to fully connected layers for classification. LeakyRelu <ref type="bibr" target="#b26">[24]</ref> activation is applied in between the fully connected layers. Finally, binary cross-entropy loss is calculated from outputs of the last layer. We also experimented with one-stream variants of our model to analyze the contribution of each stream. One-stream variants are constructed by simply removing the layers of other stream and the Fusion layer from the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fusion Strategies</head><p>We experimented with three fusion strategies to combine the output feature maps of the two streams. These three strategies make three variants of our proposed model -SepConvLSTM-M, SepConvLSTM-C and SepConvLSTM-A. Fusion layers of these three variants are described below.</p><p>SepConvLSTM-M: In this variant of our model, the output of the frames streams is passed through a LeakyRelu activation layer. On the other hand, the feature maps from frame difference stream goes through a Sigmoid activation layer. Then, we perform an element-wise multiplication to generate the final output feature maps.</p><formula xml:id="formula_3">F f used = LeakyRelu(F f rames ) ? Sigmoid(F dif f ) (4)</formula><p>Here, F f rames and F dif f denotes the feature maps from frames stream and frame difference stream respectively. F f used is the output feature map of the Fusion layer. SepConvLSTM-C: In this variant, we simply concatenate the two output features of two streams and pass it to the classification layers.</p><formula xml:id="formula_4">F f used = Concat(F f rames , F dif f )<label>(5)</label></formula><p>Here, the Concat function concatenates F f rames and F dif f along the channel axis. SepConvLSTM-A: In the last variant of fusion layer, the output feature maps of the two streams are added element-wise to generate the final video representation.</p><formula xml:id="formula_5">F f used = F f rames ? F dif f<label>(6)</label></formula><p>Here, ? refers to element-wise addition operation combining the output feature maps of the two streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULT ANALYSES</head><p>To evaluate the performance of our proposed models, we used three standard benchmarks datasets for violence detection. <ref type="bibr" target="#b29">[27]</ref> is the largest dataset on violence detection containing 2000 real-life surveillance footage. Each video is a 5-second clip with various resolutions and a framerate of 30 fps. The videos have diverse backgrounds and lighting conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RWF-2000</head><p>Hockey <ref type="bibr" target="#b8">[7]</ref> contains 1000 videos collected from different footage of ice hockey. Each video has 50 frames. All the videos have similar backgrounds and violent actions.</p><p>Movies <ref type="bibr" target="#b8">[7]</ref> is relatively smaller dataset containing 200 video clips with various resolutions. The videos are diverse in content. The videos with the 'violent' label are collected from different movie clips.</p><p>The mentioned datasets contain an equal number of videos containing violent and non-violent action to prevent class imbalance. We found RWF-2000 to be the most challenging one because of its wide variety in its content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Methodology</head><p>Adjacent frames in a video tend to contain redundant information. So, we extracted only 32 frames from each video using uniform sampling and resized to 320 ? 320. Before passing onto the model they were cropped with random sizes and resized to 224 ? 224. This produces video frames of shape 32?224?224?3. Performing element-wise subtraction between adjacent frames, we got frame differences of shape 31 ? 224 ? 224 ? 3. We were restricted to a batch size of 4 due to the limitation of memory. Various data augmentation techniques like random brightness, random cropping, gaussian blurring, random horizontal flipping were employed in the training phase to prevent overfitting.</p><p>The proposed method was implemented using the Tensorflow Python library <ref type="bibr" target="#b30">[28]</ref>. We trained the proposed networks on Google Colab with a single Tesla P100-16GB GPU. For all the models, we performed training for about 150 epochs or until the model starts overfitting. On our machine configuration, on average 720 seconds were needed per epoch for training two-stream SepConvLSTM model. The CNNs were initialized using weights pre-trained on the ImageNet dataset. We used Xavier initialization <ref type="bibr" target="#b31">[29]</ref> for the kernel of SepConvLSTM. Hockey and Movies datasets are very small which can cause overfitting. That's why we first trained on the RWF-2000 dataset. Then, we used the weights of this trained model to initialize training on the other two datasets. For model optimization, we used AMSGrad variant of Adam optimizer <ref type="bibr" target="#b32">[30]</ref>. We start training with a learning rate of 4 ? 10 ?4 . After every 5 epochs, we reduced the learning rate to half until it reaches 5 ? 10 ?5 . We keep it unchanged since that epoch. The model is optimized to minimize sigmoid loss between the ground truth and the predicted label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment on Standard Benchmark Datasets</head><p>Performance evaluation of the proposed methods was done on 20% of the dataset. The rest 80% of the clips are used for training our models. From <ref type="table" target="#tab_0">Table I</ref>, we can see that newer deep learning methods outperform the earlier methods which focus on extracting hand-crafted features. All three variants of the proposed model outperforms the previous best result on the larger and more challenging RWF-2000 dataset while matching the state-of-the-art results on the smaller datasets.</p><p>The SepConvLSTM-M model achieved more than 2% margin in terms of accuracy in RWF-2000 dataset which has a fusion strategy of multiplying the LeakyRelu activation of the frames stream with sigmoid activation of the difference stream. In Hockey fights dataset, the SepConvLSTM-C variant of our model performed the best. Out of the three variants, SepConvLSTM-A achieved the lowest accuracy in RWF-2000 dataset which indicates that simple element-wise addition is not as effective as the other fusion strategies. The proposed models were able to achieve good performance due to the use of robust and compact modules like SepConvLSTM and Mo-bileNet which mitigates the chances of overfitting, especially when working with datasets that are not large enough. We have used effective pre-processing techniques that highlight moving objects in the frames making the models focus on more relevant and discriminative features of the inputs. Using a separate stream for the frame difference input makes the model explicitly encode temporal information such as motion patterns. As violent activities are highly correlated with motion patterns, these help the network produce discriminative features. Effective input pre-processing, robust modules like Sep-ConvLSTM, and suitable network architecture all contribute towards boosting the recognition accuracy of the proposed method. Even though many ambiguous body movements in sports are similar to violent behavior, still the proposed models achieve state-of-the-art accuracy on the Hockey dataset indicating the model's effectiveness at handling ambiguous movements. The videos on the two categories of the Movies dataset are easily distinguishable. That's why almost all of the methods achieve very good accuracy on this dataset. Our experiments show that our models can effectively capture Spatio-temporal feature representation to distinguish between violent and non-violent videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>In <ref type="table" target="#tab_0">Table II</ref>, we analyze the individual contribution of each stream to our model's performance by evaluating onestream variants of the model SepConvLSTM-C. Using the variant with only frame difference stream, we get 88.25% accuracy that is better than the previous best result while using only 0.186 million parameters. On the other hand, using the variant with only frames stream, we get an accuracy of 83.75%. The regular variant of SepConvLSTM-C which uses both streams together achieves an accuracy of 89.25%. This indicates that body movements and motion patterns produce more discriminative features than appearance-based features like color, texture, etc.</p><p>In <ref type="table" target="#tab_0">Table III</ref>, we analyze the contribution of the Sep-ConvLSTM module to the proposed models by replacing it with other modules. Replacing the SepConvLSTM module of the SepConvLSTM-C model with a block of some 3D Convolutional layers, we get an accuracy of only 84% which is much lower than our best performing model. It also increases the number of parameters by a factor of 2. Replacing the SepConvLSTM module with a regular ConvLSTM module, we get ConvLSTM-M and ConvLSTM-C variants of the proposed model which produce accuracies slightly lower than our best performing models. But, using the ConvLSTM module increases the parameter count by a great deal. This indicates that SepConvLSTM is a more efficient and robust choice over ConvLSTM for this particular task. <ref type="table" target="#tab_0">Table IV</ref> shows that our model is significantly more lightweight than previous models. Compared to models proposed in <ref type="bibr" target="#b5">[5]</ref> [4], our models have a very low parameter count enabling them to require a drastically fewer number of floating-point operations (FLOPS) and making them faster and computationally efficient. The one-stream variant of our proposed models has the lowest number of parameters. Inspite of that, the one-stream variant of SepConvLSTM-C with difference stream achieves an accuracy higher than the previous best results. Flow Gated Net <ref type="bibr" target="#b29">[27]</ref> uses only 0.27 million parameters but it uses optical flow as inputs which are computationally expensive to calculate. Whereas, the proposed models are light-weight and do not require any computationally expensive pre-processing on the inputs. The low parameters and FLOPs count will be particularly beneficial if they are deployed for time-sensitive applications or in low-end devices like mobile or embedded vision applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation of Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Analysis</head><p>We demonstrate the qualitative results of the proposed method on the RWF-2000 dataset in <ref type="figure" target="#fig_3">Figure 4</ref>. We used  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we propose a novel and efficient method for detecting violent activities in real-life surveillance footage. The proposed network can learn discriminative Spatiotemporal features effectively which is reflected in its high recognition accuracy in the standard benchmark datasets. Furthermore, it is computationally efficient making it suitable to deploy in time-sensitive applications and low-end devices. We showed that the SepConvLSTM cell is a compact and robust alternative to the ConvLSTM cell. As SepConvLSTM uses fewer parameters, stacking multiple layers of LSTM with residual connections seems feasible and may improve the results further. As the datasets for violence detection are not large enough, pre-training on large-scale action recognition datasets like Sports 1M <ref type="bibr" target="#b33">[31]</ref>, UCF-101 <ref type="bibr" target="#b34">[32]</ref> might help achieve better generalization. Extracting Object-level features from object detection models such as YOLO <ref type="bibr" target="#b35">[33]</ref>, Faster R-CNN <ref type="bibr" target="#b36">[34]</ref> and adding them as additional input might help, as object-level features inherently focus on relevant objects like people. Another interesting follow-up of our work can be implementing a violence detection system that can identify the likelihood of violence on a per-frame basis and also localize the regions of the frame or the objects such as people which contribute towards classifying the video as violent. This will be very useful in real-world surveillance as it will enhance the interpretability of the violence detection system. We hope to investigate such possibilities in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A schematic overview of the proposed method for violence detection. The proposed pipeline has two streams consisting of CNN and SepConvLSTM modules. Background suppression and Frame difference are pre-processing modules. The output of the two streams are fused to produce robust Spatiotemporal features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Input pre-processing for the proposed model. (a) shows key-frames of an example video clip. (b) demonstrates the effect of performing background suppression on video frames of (a). The last row (c) shows time-steps of the frame difference derived from the video clip of (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The proposed model is composed of two CNN-LSTM streams with similar architecture. Each stream consists of a truncated MobileNet module generating spatial features from each time-step of the inputs. These features are passed to the SepConvLSTM cell in each stream to produce Spatio-temporal encodings. The outputs from each stream are fused using a Fusion layer and passed to the classifier network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results of the proposed model (SepConvLSTM-M) for violence detection on the RWF-2000 dataset. The first two rows contain examples of video clips for which our model correctly predicts the presence of violence. The last four rows contain examples of failure cases where ambiguous body movements and poor quality of surveillance footage may lead towards incorrect prediction. the variant SepConvLSTM-M of our proposed model as it achieved the best performance on the RWF-2000 dataset. In Figure 4, each row contains six key-frames from a video clip with a corresponding ground truth label and the predicted label. The first two rows contain examples of video clips for which our model gives a correct prediction. The key-frames of first video clip show that the body positions are not aggressive and the body movements are very slow and minimal. These are good indicators of the absence of violence in this video clip which enables our model to give correct prediction. On the other hand, the key-frames of the second clip contain fast fighting movements of multiple persons which helps the model to identify it as a violent clip. The last four rows contain examples of failure cases of our proposed model. The keyframes of the third and fifth row contain ambiguous body movements which may cause incorrect prediction. In the keyframes fourth example video clip, a large portion of the bodies of the people involved in fighting is occluded which may cause the network to incorrectly classify the clip as non-violent. The video clip of the last row has very poor quality and resolution. Moreover, the people involved in the fighting are far from the camera. These factors may contribute towards incorrect classification of this clip by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF CLASSIFICATION RESULTS ON STANDARD BENCHMARK DATASETS</figDesc><table><row><cell>Method</cell><cell cols="3">RWF-2000 Dataset Hockey Dataset Movies Dataset</cell></row><row><cell>ViF [8]</cell><cell>-</cell><cell>82.90%</cell><cell>-</cell></row><row><cell>ViF + OViF [9]</cell><cell>-</cell><cell>87.50%</cell><cell>-</cell></row><row><cell>Radon Transform [10]</cell><cell>-</cell><cell>98.9%</cell><cell>90.1%</cell></row><row><cell>Hough Forest + 2D CNN [11]</cell><cell>-</cell><cell>94.6%</cell><cell>99%</cell></row><row><cell>Improved Fisher Vector [25]</cell><cell>-</cell><cell>93.7%</cell><cell>99.5%</cell></row><row><cell>Three Streams + LSTM [14]</cell><cell>-</cell><cell>93.9%</cell><cell>-</cell></row><row><cell>FightNet [26]</cell><cell>-</cell><cell>97.0%</cell><cell>100%</cell></row><row><cell>ConvLSTM [5]</cell><cell>-</cell><cell>97.1%</cell><cell>100%</cell></row><row><cell>BiConvLSTM [6]</cell><cell>-</cell><cell>98.1%</cell><cell>100%</cell></row><row><cell>Efficient 3D CNN [4]</cell><cell>-</cell><cell>98.3%</cell><cell>100%</cell></row><row><cell>Flow Gated Net [27]</cell><cell>87.25%</cell><cell>98.0%</cell><cell>100%</cell></row><row><cell>Ours (SepConvLSTM-A)</cell><cell>87.75%</cell><cell>99%</cell><cell>100%</cell></row><row><cell>Ours (SepConvLSTM-C)</cell><cell>89.25%</cell><cell>99.50%</cell><cell>100%</cell></row><row><cell>Ours (SepConvLSTM-M)</cell><cell>89.75%</cell><cell>99%</cell><cell>100%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II ANALYZING</head><label>II</label><figDesc>CONTRIBUTION OF EACH STREAM TO OUR MODEL FOR VIOLENCE DETECTION ON RWF-2000 DATASET</figDesc><table><row><cell>Model</cell><cell cols="2">Accuracy Parameters</cell></row><row><cell>SepConvLSTM-C (only frames stream)</cell><cell>83.75%</cell><cell>185,521</cell></row><row><cell>SepConvLSTM-C (only differences stream)</cell><cell>88.25%</cell><cell>185,521</cell></row><row><cell>SepConvLSTM-C (both streams)</cell><cell>89.25%</cell><cell>371,009</cell></row><row><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="3">ANALYZING CONTRIBUTION OF SEPCONVLSTM TO OUR MODEL BY</cell></row><row><cell cols="3">REPLACING IT WITH 3D-CONV AND CONVLSTM LAYERS</cell></row><row><cell>Model</cell><cell cols="2">Accuracy Parameters</cell></row><row><cell>Ours (using 3D-Conv Layers, C Fusion)</cell><cell>84.00%</cell><cell>685,697</cell></row><row><cell>Ours (using ConvLSTM, M Fusion)</cell><cell>87.50%</cell><cell>815,937</cell></row><row><cell>Ours (using ConvLSTM, C Fusion)</cell><cell>88.50%</cell><cell>853,889</cell></row><row><cell>Ours (using SepConvLSTM, M Fusion)</cell><cell>89.75%</cell><cell>333,057</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF EFFICIENCY WITH EARLIER MODELS</figDesc><table><row><cell>Model</cell><cell>Parameters</cell><cell>FLOPs</cell></row><row><cell>AlexNet + ConvLSTM [5]</cell><cell>9.6M</cell><cell>14.40G</cell></row><row><cell>Efficient 3D CNN [4]</cell><cell>7.4M</cell><cell>10.43G</cell></row><row><cell>Flow Gated Net [27]</cell><cell>0.27M</cell><cell>0.54M</cell></row><row><cell>Ours (SepConvLSTM-C, 1 Stream)</cell><cell>0.186M</cell><cell>1.004M</cell></row><row><cell>Ours (SepConvLSTM-C, 2 Streams)</cell><cell>0.371M</cell><cell>2.009M</cell></row><row><cell>Ours (SepConvLSTM-M/A, 2 Streams)</cell><cell>0.333M</cell><cell>1.933M</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowd violence detection using global motion-compensated lagrangian features and scale-sensitive video-level representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Senst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2945" to="2956" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition of aggressive human behavior using binary local motion descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wactlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2008-02" />
			<biblScope unit="page" from="5238" to="5279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Machine cognition of violence in videos using novel outlier-resistant vlad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Firoze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="989" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient violence detection using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to detect violent videos using convolutional long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bidirectional convolutional lstm for the detection of violence in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pnvr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Violence detection in video using computer vision techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Nievas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Computer analysis of images and patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Violent flows: Real-time detection of violent crowd behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Itcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Violence detection using oriented violent flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="37" to="41" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast violence detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 international conference on computer vision theory and applications (VISAPP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fight recognition in video using hough forests and 2d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Espinosa-Aranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bueno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4787" to="4797" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Violence detection in video by using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="551" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-stream deep networks for person to person violence detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="517" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fudan-huawei at mediaeval 2015: Detecting violent scenes and affective impact in movies with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MediaEval</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1608</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Not only look, but also listen: Learning multimodal violence detection under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="322" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal violence detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2957" to="2961" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<ptr target="http://arxiv.org/abs/1704.04861" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Separable convolutional lstms for faster video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pfeuffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1072" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human violence recognition and detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 13th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Violent interaction detection in video based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">844</biblScope>
			<biblScope unit="page">12044</biblScope>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rwf-2000: An open large scale video database for violence detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05913</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
	</analytic>
	<monogr>
		<title level="m">2015, software available from tensorflow.org</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
