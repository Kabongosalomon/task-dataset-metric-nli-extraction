<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MMFN: Multi-Modal-Fusion-Net for End-to-End Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingwen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Geng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Xin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujia</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">MMFN: Multi-Modal-Fusion-Net for End-to-End Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by the fact that humans use diverse sensory organs to perceive the world, sensors with different modalities are deployed in end-to-end driving to obtain the global context of the 3D scene. In previous works, camera and LiDAR inputs are fused through transformers for better driving performance. These inputs are normally further interpreted as high-level map information to assist navigation tasks. Nevertheless, extracting useful information from the complex map input is challenging, for redundant information may mislead the agent and negatively affect driving performance. We propose a novel approach to efficiently extract features from vectorized High-Definition (HD) maps and utilize them in the end-to-end driving tasks. In addition, we design a new expert to further enhance the model performance by considering multi-road rules. Experimental results prove that both of the proposed improvements enable our agent to achieve superior performance compared with other methods. The source code is released as an open-source package.</p><p>? We use a multi-path consideration rule-based expert to arXiv:2207.00186v2 [cs.CV] 3 Aug 2022</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Autonomous driving is conducted via several modules <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, namely localization <ref type="bibr" target="#b2">[3]</ref>, perception <ref type="bibr" target="#b3">[4]</ref>, planning <ref type="bibr" target="#b4">[5]</ref>, and control. However, the system performance is constrained by the manually selected intermediate criteria, e.g., localization and lane detection error. One solution is to use end-to-end driving which optimizes the system from the perspective of the overall system performance, avoiding the potential loss caused by incorrect human-designed intermediate criteria. In this work, we apply imitation learning to train an agent to mimic the expert's action and behavior with the available sensor data inputs.</p><p>In imitation learning, expert drivers are utilized to collect the training data and generate its ground truth of actions before the training phase. Experts in <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> only consider avoidance collisions according to the distance between agents. As illustrated in the left image of <ref type="figure" target="#fig_0">Fig. 1</ref>, it is dangerous to only consider the nearby vehicles in terms of distance, since vehicles may be at a high speed when changing lanes. Hence, more information is added in our expert to enable the expert to capture more potential collisions which are missed in existing methods.</p><p>Apart from the ground truth given by the expert, the agent in imitation learning needs sensor data to have a clear understanding of the surroundings. Existing methods in endto-end driving mostly depend on only a single type of sensor, like camera <ref type="bibr" target="#b5">[6]</ref> or LiDAR <ref type="bibr" target="#b8">[9]</ref>. Recently, Prakash et al. proposed to fuse camera and LiDAR data through an attention mechanism in <ref type="bibr" target="#b6">[7]</ref>. It turns out that taking advantage of the complementary sensors can achieve a satisfactory result. Specifically, cameras can better capture texture information. However, they are susceptible to lighting conditions. LiDAR outperforms cameras in terms of accurate distance information, whereas the sparsity of LiDAR point clouds may cause information loss. Apart from the intrinsic disadvantages of these two sensors, driving scenarios in complicated urban environments, examples of which as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, also reveal the importance of introducing more sensors into end-to-end driving.</p><p>The scenarios presented in <ref type="figure" target="#fig_0">Fig. 1</ref> suggest the benefits of providing lane structures and vehicle velocities for the end-to-end driving task. Thus, we add the HD map and radar on top of the LiDAR and camera as the network inputs in our approach. The HD map offers high-level map data like lanes and roads, which is more closely related to the action or trajectory outputs compared with camera and LiDAR data. Additionally, radar can offer the velocities of other agents directly without any calculation, which are more accurate than the ones calculated from other sensor data. However, little work has been conducted on how to efficiently represent HD maps and radar data in end-toend learning frameworks and extract useful features from them. It is challenging to effectively integrate the complex HD map, which contains not only hierarchical geometric information but also semantic information, into the network. When integrating radar data into the network, the sparsity of radar points can not be ignored. Therefore, we propose a Multi-Modal-Fusion-Network (MMFN) to properly represent and fuse the meaningful information extracted from the complementary sensor raw data in the end-to-end driving task. Our method is proved to be effective in CARLA <ref type="bibr" target="#b9">[10]</ref> leaderboard task and open-sourced in https://github.com/Kin-Zhang/mmfn. The main contributions of this work include the following: improve the performance of existing agents <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr">?</ref> We explore different representations on the HD map as the network input and propose a framework based on <ref type="bibr" target="#b6">[7]</ref> to fuse the four different types of sensor data. ? We experimentally validate the performance of our method both on local driving routes and the online leaderboard <ref type="bibr" target="#b10">[11]</ref>, with an increase in the driving score by 34.67% and a decrease in the infraction rate by 50.8% on average compared with <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In end-to-end driving, many works only concentrate on utilizing a single type of sensor, like cameras. For example, Chekroun et al. <ref type="bibr" target="#b11">[12]</ref> used three cameras and combined the action result and the semantic segmentation result to train the agent jointly. Similarly, Chitta et al. <ref type="bibr" target="#b7">[8]</ref> also employed three cameras to get neural attention fields and predicted the future waypoints. Other works like <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b13">[14]</ref> both focused on camera input and used deep reinforcement learning to circumvent the use of an expert. These works show that applying multiple networks <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref> initially proposed for image detection can promote the driving performance <ref type="bibr" target="#b17">[18]</ref>. As mentioned before, cameras are not robust to lighting conditions, so information resources such as LiDAR, HD maps, and radar, have been added to compensate for the limitations of cameras.</p><p>To use other sensors apart from cameras, processing techniques tailored for each sensor need to be utilized. Images like RGB and RGBD data with semantic labels are processed similarly using off-the-shelf networks like convolutional neural networks (CNNs). In the object detection task using LiDAR, Lang et al. <ref type="bibr" target="#b18">[19]</ref> proposed to extend the dimensions of point data to offer more detailed information. Rhinehart et al. <ref type="bibr" target="#b19">[20]</ref> presented a compact form to describe the LiDAR point cloud data, and this was utilized in our method. Specifically, the LiDAR point clouds are divided into two groups based on their z-axis values and a pseudo image with the number of points inside each pixel of the image is output. Gao et al. <ref type="bibr" target="#b20">[21]</ref> represented the map in a vectorized form when predicting other agents' motion. While Chen et al. <ref type="bibr" target="#b5">[6]</ref> used a bird's-eye-view (BEV) map to express the map, which included more information, like the location of other agents and traffic lights, compared with <ref type="bibr" target="#b20">[21]</ref>.</p><p>Most existing multi-modality fusion networks focus on perception tasks like object detection and motion forecasting. For instance, Liang et al. <ref type="bibr" target="#b21">[22]</ref> designed a network for 3D object detection to jointly process camera and LiDAR inputs, of which the outputs are the results of four sub-tasks. Liang et al. <ref type="bibr" target="#b22">[23]</ref> and Guo et al. <ref type="bibr" target="#b23">[24]</ref> fused the multi-modal features obtained from BEV LiDAR and RGB images in a multiscale fusion fashion for 3D object detection. Their work inspired other researchers to introduce the sensor fusion mechanism into end-to-end driving. For example, Prakash et al. <ref type="bibr" target="#b6">[7]</ref> proposed to embed LiDAR point clouds into a transformer network to cooperate with camera image data. Based on <ref type="bibr" target="#b6">[7]</ref>, we additionally integrate HD map and radar data into the network. We also show the effective use of the transformer network by applying different types of sensors and their corresponding data representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we improve the expert in <ref type="bibr" target="#b5">[6]</ref> by considering time to collision and lane structures. Then, we directly use the dataset expert collected to train the MMFN framework which introduces multiple sensors. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the whole framework for training, details of how to extract sensor data feature can be found in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Expert Principle</head><p>The expert agent in <ref type="bibr" target="#b5">[6]</ref> (AUTO expert) only uses distance to find the nearby obstacles and only considers its front area. This means that the AUTO expert fails to locate its nearby agents using the road or lane ID. To address this issue, the map information provided by the CARLA simulator <ref type="bibr" target="#b9">[10]</ref> is used in our expert agent, where the map refers to the lane information attached to vehicles. Our expert can drive more similarly to human drivers, because the traffic rules that human drivers abide by are based on lane structures that can be obtained from the map input. Based on the AUTO expert, we add time to collision (TTC) and take the vehicles in other lanes into consideration when lane switching happens.</p><p>To calculate the TTC, the expert first gets the relative position vector ?p and relative velocity ?v between two cars in the CARLA simulator, and ?v is projected to ?p through:</p><formula xml:id="formula_0">P v2p = ?p ? ?p ?v ?p ?p , TTC = P v2p ?p ,<label>(1)</label></formula><p>where P v2p is the projected relative velocity. Moreover, ?p ? P p2v , as illustrated in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, is the unreachable distance. When it exceeds the threshold defined by the lane width d, the expert thinks that this nearby vehicle will not collide with the ego vehicle in terms of TTC.</p><p>For the vehicle stop case, not only is the distance considered but also the angle, and the angle from the ego vehicle to the obstacle is calculated by</p><formula xml:id="formula_1">? = arccos (o 1 ? ?p) o 1 ?p ,<label>(2)</label></formula><p>where o 1 corresponds to the orientation of the ego vehicle. When ? &gt; arcsin d ?p , it means that the obstacle may not be in front of the ego car, as <ref type="figure" target="#fig_1">Fig. 2</ref> </p><formula xml:id="formula_2">(b) indicates, where d is the current lane width.</formula><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (c), when the ego vehicle intends to switch to other lanes, apart from the locations of other vehicles, the lane information of these vehicles will also be extracted from the HD map. If another vehicle is in the lane that the ego vehicle intends to switch to, and the distance between these two vehicles is below a certain threshold, the ego vehicle will stop for a while and then execute changing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Input and Output Representation</head><p>To make the best of the fusion mechanism in <ref type="bibr" target="#b6">[7]</ref>, apart from camera and LiDAR data, an OpenDrive HD map and radar data are added as the network inputs in the proposed approach. Even though the HD map in this work is obtained from the CARLA simulator, there are still several approaches to export the HD map automatically <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Additionally, to improve the model's adaptability to dynamic environments, long-range radar is used. In the following, we briefly introduce the processing techniques for each type of sensor data used in our method.</p><p>1) OpenDRIVE HD Map: Considering that maps contain more complex information compared with other sensors, it is worthwhile to investigate the influence of different map representations on the model performance. Here, two kinds of map representations are evaluated, the image-based and vector-based methods. Both extract information from an ASAM OpenDRIVE file, and the difference is the way to describe the information extracted. In the image-based representation, the map is rasterized from the BEV perspective using the lane and road messages in the file, and a map is drawn using the map elements in the raster map, as illustrated in <ref type="figure" target="#fig_2">Fig. 3 (c)</ref>. For the vector-based representation, the nearby center lines of the lanes in the map are vectorized. Here, a window of 28 ? 28 m centered in the ego vehicle position is used to define the surrounding map elements, and the vectorized map is shown schematically in <ref type="figure" target="#fig_2">Fig. 3 (d)</ref>. What distinguishes these two map types from each other are the orientation and semantic information of the map elements, which will be discussed in detail in Section IV-C.</p><p>2) Radar: Endowed with the velocity information, radar contributes to the following and lane switching maneuvers, especially in highly dynamic environments. In our method, two radars with the specification of a 35 ? field of view (FOV) and a maximum range of 100 meters is arranged, one at the front end of the vehicle, and the other at the rear of the vehicle. To avoid radar waves being reflected from the surface of the ground, the pitch angle of the two radars is increased by 5 ? , and their height is set as 1 m. We first calculate the time for each point to reach the radar sensor position by dividing the point depth by the velocity of this point, and then we select the points closer to the radar sensor according to the time calculated. In our setting, the top N = 81 radar points are selected, and if the actual number of radar points is less than N , the remaining feature vectors are padded with 0.</p><p>3) LiDAR Point Cloud: : Our approach to dealing with LiDAR data is similar to <ref type="bibr" target="#b19">[20]</ref>, which converts the 3D LiDAR data into a 2D BEV grid map by calculating the number of LiDAR points inside each grid as <ref type="figure" target="#fig_2">Fig. 3</ref> (a) and (b) show. The total area considered by the 2D BEV grid map is 32?32 m, with 28 m distance in front of the vehicle, 6 m distance behind the vehicle, and 16 m distance on the left and right sides of the vehicle. The reason for considering the LiDAR points at the rear of the agent is that the information in this space is crucial when lane changing happens or when the ego vehicle drives away from the highway. Other settings are the same as <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Camera Images:</head><p>For the RGB input, one camera is deployed in front of the vehicle with a 100 ? FOV and a 400? 300 resolution in pixels. Because of the distortion caused by the rendering of the cameras in the CARLA simulator, the RGB images are cropped to 256 ? 256 ? 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Output</head><p>Representation: : As in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>, the output of the network forecasts the future trajectory w of the vehicle in the BEV space which coincides with the ego vehicle's coordinates frame. The trajectory is represented by a series of 2D waypoints in the form of {w t = (x t , y t ) T t=1 }, where the default number of waypoints is set as 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture Design</head><p>In this section, the network architecture of the proposed approach is discussed. To better fuse the network inputs with multiple modalities, different sensor data inputs are treated in different ways before the fusion layers, as indicated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Thus the remainder of this section introduces the part of the network architecture before the fusion layers, and the fusion mechanism <ref type="bibr" target="#b6">[7]</ref>.</p><p>1) Map: The ASAM OpenDRIVE file is parsed into a discrete HD map, and this process is summarized in Algorithm 1, where S lane refers to the set consisting of the lanes expressed by analytic formulas, L lane node corresponds to a list of discretized points of the lane, and S rough lane is the set composed of discretized lanes called rough lanes and s is the road arc length from the start to the reference point. To select the lanes of interest around the ego vehicle so as to discretize them, all lanes inside a window centered at the ego vehicle are selected. In our setting, the number of lanes is N , and each discretized lane is represented by P = 10 points and P ? 1 vectors, with each vector expressed by:</p><formula xml:id="formula_3">v i = [d i?1 , d i , a i ] , i ? [1, . . . , P ]<label>(3)</label></formula><p>where d After discretizing the lanes selected as mentioned above, these lanes are then vectorized according to <ref type="bibr" target="#b20">[21]</ref>. Briefly speaking, the lane vectors are sent to their own Multilayer Perceptron (MLP), followed by attention layers to concatenate the outputs of the MLPs. We adopt VectorNet to process the lane features. The map feature information output from VectorNet is then fed to ResNet18, as demonstrated in <ref type="figure" target="#fig_2">Fig.  3</ref> to get the OpenDRIVE map data in the same size as the other sensor data before the fusion layers. <ref type="figure">Fig. 4</ref>. Detailed version of the gray fusion layer block in <ref type="figure" target="#fig_2">Fig. 3</ref> 2) Radar: When dealing with radar inputs, if directly resizing them to the same size as that of the camera data before fusion, significant amounts of information will be lost because of the sparsity of radar points. Inspired by <ref type="bibr" target="#b26">[27]</ref>, we choose to connect the radar points to form a graph whose weights are the relative azimuth distances. Then the weights of this graph are multiplied by the radar points to get the radar features as follows:</p><formula xml:id="formula_4">F out = WP in ,<label>(4)</label></formula><p>where F out ? R N ?L is the output radar feature, P in ? R N ?L is the input radar points, W ? R N ?N is the weight matrix obtained from the weights of the graph, N = 81 is the number of radar points after pre-processing mentioned in Section III-B.2, and L = 5 is the number of the feature labels, which include a point's velocity, depth, azimuth, altitude and the label to indicate its location. These radar features are sent to the attention layers to be resized to the same size as other sensor data. The overall processing procedure for radar data mentioned above is the Graph Attention Network shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Fusion:</head><p>With different sensor features of the same size, transform layers <ref type="bibr" target="#b6">[7]</ref> are utilized to offer a chance for these sensor data to 'communicate' with each other. There are four fusion layers in total, with each receiving the concatenated multi-modal sensor features extracted at different stages of their own feature networks. After being blended with other types of sensor data in fusion layers, these features are then sent back to the place from which they are extracted before fusion, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The detailed network architecture inside the fusion layers is demonstrated in <ref type="figure">Fig. 4</ref>. It shows that all the different types of sensor data are concatenated together as the input of the fusion layers F in , which is then multiplied by three matrices:</p><formula xml:id="formula_5">Q = F in M q , K = F in M k , V = F in M v ,<label>(5)</label></formula><p>where Q, K and V refer to the query, key and value respectively, and The overall process of Attention mechanism <ref type="bibr" target="#b27">[28]</ref> in <ref type="figure">Fig.  4</ref> could be generalized as:</p><formula xml:id="formula_6">M q ? R D f ?Dq , M k ? R D f ?D k , M v ? R D f</formula><formula xml:id="formula_7">A = softmax QK T ? D k V,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Output:</head><p>The output layers of the overall network in <ref type="figure" target="#fig_2">Fig. 3</ref> are the same as those in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>, which uses four cascaded Gated Recurrent Units (GRUs) for the final waypoint output. The first GRU takes the differential ego vehicle waypoints and the relative goal location points as inputs, and the last GRU outputs the coordinates of the predicted waypoints relative to the ego vehicle. The loss function of the network is set as the L1 loss between the expert's and the agent's waypoints:</p><formula xml:id="formula_8">L = T t=1 w t ? w gt t 1 ,<label>(7)</label></formula><p>where w gt t is expert waypoint coordinates on time t. After training, the network can receive the sensor data and relative goal location to output the predicted waypoints. Two PID controllers are connected with the network to transform the network outputs to the control signals on the steering wheel, throttle, and brake pedal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Since it is costly to collect large-scale multi-modality sensor data in the real world, the CARLA simulator <ref type="bibr" target="#b9">[10]</ref> is chosen for generating the data needed in the simulation environment. Moreover, it also provides public routes and scenarios for users to evaluate their own agents based on the unified criteria in the online leaderboard platform. To maintain fairness, the AUTO <ref type="bibr" target="#b5">[6]</ref> and MMFN experts collect the same routes for training the models in <ref type="table" target="#tab_0">Table II</ref> which have 207K frames data separately.</p><p>In Section IV-C, to show the generalization performance of the trained model, instead of using a single map in one Town, the routes in <ref type="bibr" target="#b7">[8]</ref> are used for testing instead. Every two routes out of these twelve routes are obtained from the same Town and of the same length. Considering that the main purpose is to test whether the agent can handle complex scenarios, we add all the events under the available driving scenarios in the CARLA leaderboard into the Town maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metric</head><p>The CARLA leaderboard includes unified metrics to evaluate the driving task, the driving score (DS), route completion (RC), and infraction penalty, among which the most important, the DS, is computed by R i ? P i , where R i and P i refer to the RC and infraction penalty coefficient of the i-th route respectively. The DS has an upper limit of 100, and a higher DS indicates a better agent. The RC reveals several facts, e.g., whether the agent can successfully go through junctions without traffic lights. The infraction results shown in <ref type="table" target="#tab_0">Table  I</ref> and II are obtained by calculating the infractions of the vehicle per kilometer (Infra/km). Thus, the lower the Infra/km, the better the agent behaves. More details about infractions can be obtained from CARLA leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Expert Performance</head><p>As seen from <ref type="table" target="#tab_0">Table I</ref>, our MMFN expert outperforms the AUTO expert in most cases. The reason is mainly that the MMFN expert abides by more reasonable rules similar to those in real driving scenarios. Especially for Town 03, 04, and 05, where there are more complex scenarios like lane changing on highways, the MMFN expert performs considerably better than the AUTO expert. Overall, the infractions per kilometer decreased by about 45.27% for all routes in the six towns, indicating that our expert has more awareness of safe driving. Furthermore, the next section also shows that as the upper bound of how well the agent can behave, the expert plays an important role in the driving performance of its agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Local Evaluation</head><p>From <ref type="table" target="#tab_0">Table II</ref>, we can conclude that our MMFN expert can improve the agent's driving performance by comparing the left and right columns. The DS and RC increase by about 9.8% and 10.3% respectively, and the infra/km decreases by around 4.9%. Moreover, comparing the left and right columns of one row in <ref type="table" target="#tab_0">Table II</ref> reveals that the expert performance also affects the agent performance, since the expert limits how well the agent can behave, as mentioned in Section IV-B.</p><p>Before row-wise comparisons, the three baselines given in <ref type="table" target="#tab_0">Table II</ref> need to be introduced. CILRS <ref type="bibr" target="#b28">[29]</ref> and AIM <ref type="bibr" target="#b6">[7]</ref> both use one camera as the network input, which is then sent to a ResNet 34. CILRS outputs the vehicle control signal directly, while AIM outputs waypoints through four GRU decoders followed by PID controllers. Based on AIM, Transfuer <ref type="bibr" target="#b6">[7]</ref> adds the LiDAR input and transform layers to fuse the camera and LiDAR data. Apart from the camera and LiDAR, MMFN adds two more sensor data, HD map, and radar, into the network to offer more information. MMFN (Image) transforms the HD map into a BEV image which is then fed into ResNet 18. By contrast, MMFN (VectorNet) vectorizes the HD map first and extracts features from the vectorized map. Additionally, MMFN (Radar) adds radar input based on MMFN (VectorNet).</p><p>It experimentally shows that how to represent the newly added sensor data is crucial in improving the driving performance, by comparing the results of Transfuser and AIM in <ref type="table" target="#tab_0">Table II</ref>. Although one LiDAR and one camera are used in Transfuser, the driving performance of this model is still worse than that of AIM which only uses one camera. The results of MMFN (Image), MMFN (VectorNet), and Transfuser in <ref type="table" target="#tab_0">Table II</ref> proves the effective use of the additional HD map data, either in the image form or in the vector form, since the remaining sensors except the map used in MMFN are the same as those used in Transfuser.  <ref type="table" target="#tab_0">Table II</ref> further shows that vectorizing the HD map is superior to using images to represent the HD map which can be seen as one of the ablation studies. Possible reasons for it are twofold. One is that convolutional neural networks (CNNs) used in MMFN (Image) are not originally designed to capture long-range geometry structures as discussed in <ref type="bibr" target="#b20">[21]</ref>, while VectorNet can extract more map element features instead. The other one is that the map elements in VectorNet offer some information more directly like orientation or semantic labels, so the agent can give more accurate commands with a clearer understanding of the surroundings.</p><p>In <ref type="table" target="#tab_0">Table II</ref>, MMFN (Radar) fails to compete with MMFN (Image), MMFN (VectorNet), and even with Transfuser <ref type="bibr" target="#b6">[7]</ref>, though it has one more radar sensor compared with the other 3 models. It is demonstrated in <ref type="figure">Fig. 5</ref> that blocking frequently happens in MMFN (Radar), which means the additional use of radar makes the agent more alert to potentially dangerous driving scenarios. One possible reason for the unsatisfactory performance may be that the embeddings of radar data are not good enough to clearly interpret the messages from the raw radar points, and thus it may mislead other types of sensor data in fusion layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Online Leaderboard</head><p>There are two tracks in the CARLA online leaderboard, the sensor track, and the map track. Since the HD map is used in this method, in <ref type="table" target="#tab_0">Table III</ref>, we mainly compare our model with several top models in the map track, but a few models in the sensor track are also evaluated for a more comprehensive comparison. Unlike the experiments in Section IV-C, all the routes and scenarios in the online leaderboard are not public for the sake of fairness. The models are ordered by DS as it is the most important metric to evaluate a model. Another metric P in this table is the infraction penalty defined by the leaderboard, which is initialized as 1 and will be subtracted by different infraction penalties during evaluation. The results in <ref type="table" target="#tab_0">Table III</ref> are all copied from the online leaderboard before the date 25/02/2021. In the online leaderboard, among all the models which use the HD map data in the map track, our model shows the best performance. Compared with other models using multiple sensors in the sensor track, our model has a higher infraction penalty coefficient represented by P , especially even with relatively low RC. Our agent intends to behave more conservatively in complicated driving scenarios, resulting in fewer infractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In conclusion, our proposed approach shows the effective use of the additional HD map data in the end-to-end driving task. It also proves that using VectorNet to represent this data achieves superior driving performance to using a BEV raster image. We explored how to represent all the sensor data and proposed a Multi-Modal-Fusion-Network (MMFN) to use camera images, LiDAR point clouds, an OpenDRIVE map, and radar as the network inputs for the end-to-end autonomous driving. Furthermore, the expert presented in this work also contributes to the agent's performance.</p><p>We hope that this work may arouse the interest in the research community about using HD maps, radar, and other sensors in end-to-end driving. The performance of our model could be further enhanced by setting up more rules for the expert to abide by. As suggested in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b21">[22]</ref>, considering more maneuvers of the expert or replacing our agent with a costly deep reinforcement learning agent <ref type="bibr" target="#b31">[32]</ref> could be helpful in improving the performance. Our future work will also include designing sub-tasks, like classification and segmentation using cameras or LiDAR, before the output layers to help the agent to learn the final driving task more quickly and accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Thanks to HKUST Ramlab's members: Jin Wu, Jie Cheng, Bowen Yang, Xiaodong Mei, and Peide Cai who gave constructive comments on this work. We also thank the anonymous reviewers for their constructive comments. This work was supported by Guangdong Basic and Applied Basic Research Foundation, under project 2021B1515120032, awarded to Prof. Lujia Wang.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Scenarios in complex urban environments. The ego vehicle needs to yield to other vehicles when switching lanes or at the roundabout. These scenarios indicate more abundant environmental information such as the lane structure and the velocities of other vehicles may assist to better facilitate the end-to-end driving task. Figures captured from CARLA leaderboard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustrations of expert rules under different scenarios. (a) the scenario when calculating TTC, and the red dashed line is ?p ? P p2v ; (b) the scenario when the ego vehicle and the nearby vehicle are driving on the same road but in different lanes, where d is the current lane width obtained from the HD map; (c) the scenario when the ego car changes its lane but there is another car in the adjacent lane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Architecture. Process on LiDAR Point Cloud in Section III-B.3. (a) receive all point cloud data, (b) divide point clouds with 2m height into two channels; Two methods for representing the HD map as input to the network in Section III-B.1: (c) rasterize to BEV perspective image, (d) vectorize road elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 5 :</head><label>15</label><figDesc>OpenDrive to VectorNet Input 1: Initialize: S rough lane ? ? 2: S lane ? PARSE(opendrive) 3: for lane in S lane do 4: L lane node ? ? for s in {0, . . . , lane.max s} do 6: lane node ? CALCULATEPROPERTY(lane,s) 7: L lane node .APPEND(lane node) 8: if L lane node .SIZE ? 10 then 9: S rough lane .INSERT(L lane node ) if L lane node = ? then 14: S rough lane .INSERT(L lane node ) 15: end if 16: end for 17: return S rough lane</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>? 1 ,</head><label>1</label><figDesc>d i are the coordinates of each lane point (x, y), and a i is the label of this lane point. In correspondence with the available semantic labels of a point in the OpenDRIVE message, here a i could indicate whether the lane point is at a junction and if it is available for left/right change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>?Dv are weight matrices in the form of linear layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EXPERT</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">DRIVING PERFORMANCE</cell><cell></cell></row><row><cell>No.</cell><cell></cell><cell>DS ?</cell><cell cols="2">RC ?</cell><cell cols="2">Infra/km ?</cell></row><row><cell>Town</cell><cell cols="2">AUTO MMFN</cell><cell>AUTO</cell><cell>MMFN</cell><cell cols="2">AUTO MMFN</cell></row><row><cell>1</cell><cell>85.90</cell><cell>94.00</cell><cell cols="2">100.00 100.00</cell><cell>0.06</cell><cell>0.02</cell></row><row><cell>2</cell><cell>62.58</cell><cell>60.06</cell><cell>95.29</cell><cell>85.09</cell><cell>0.16</cell><cell>0.12</cell></row><row><cell>3</cell><cell>66.36</cell><cell>79.55</cell><cell>80.91</cell><cell>88.07</cell><cell>0.07</cell><cell>0.04</cell></row><row><cell>4</cell><cell>88.79</cell><cell>91.90</cell><cell>98.36</cell><cell>98.7</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>5</cell><cell>71.00</cell><cell>87.60</cell><cell>100.00</cell><cell>94.6</cell><cell>0.07</cell><cell>0.02</cell></row><row><cell>6</cell><cell>60.92</cell><cell>89.60</cell><cell cols="2">100.00 100.00</cell><cell>0.06</cell><cell>0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DRIVING</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">PERFORMANCE OF DIFFERENT MODELS WITH DIFFERENT EXPERT DATA</cell><cell></cell></row><row><cell></cell><cell></cell><cell>AUTO expert</cell><cell></cell><cell></cell><cell>MMFN expert</cell><cell></cell></row><row><cell>Methods</cell><cell>Driving Score ?</cell><cell cols="2">Route Completion ? Infra/km ?</cell><cell>Driving Score ?</cell><cell cols="2">Route Completion ? Infra/km ?</cell></row><row><cell>CILRS [29]</cell><cell>20.52 ? 1.07</cell><cell>33.05 ? 0.08</cell><cell>1.70 ? 0.01</cell><cell>18.18 ? 3.44</cell><cell>30.04 ? 4.17</cell><cell>1.65 ? 0.05</cell></row><row><cell>AIM [7]</cell><cell>67.56 ? 1.77</cell><cell>86.53 ? 5.14</cell><cell>1.03 ? 0.02</cell><cell>73.45 ? 1.04</cell><cell>90.33 ? 0.12</cell><cell>0.73 ? 0.02</cell></row><row><cell>Transfuser [7]</cell><cell>56.25 ? 0.24</cell><cell>62.66 ? 0.34</cell><cell>0.64 ? 0.17</cell><cell>62.36 ? 0.58</cell><cell>79.43 ? 0.29</cell><cell>0.97 ? 0.20</cell></row><row><cell>MMFN (Radar)</cell><cell>46.05 ? 5.95</cell><cell>58.82 ? 8.73</cell><cell>0.91 ? 0.08</cell><cell>58.72 ? 2.47</cell><cell>68.25 ? 3.96</cell><cell>0.67 ? 0.08</cell></row><row><cell>MMFN (Image)</cell><cell>72.22 ? 3.51</cell><cell>88.78 ? 0.43</cell><cell>0.70 ? 0.11</cell><cell>76.56 ? 0.41</cell><cell>89.51 ? 1.09</cell><cell>0.67 ? 0.08</cell></row><row><cell cols="2">MMFN (VectorNet) 75.62 ? 0.32</cell><cell>82.17 ? 1.73</cell><cell>0.50 ? 0.08</cell><cell>88.75 ? 1.42</cell><cell>96.53 ? 0.47</cell><cell>0.41 ? 0.05</cell></row><row><cell>Expert</cell><cell>84.08 ? 1.67</cell><cell>98.62 ? 0.23</cell><cell>0.35 ? 0.04</cell><cell>92.32 ? 1.52</cell><cell>97.32 ? 0.25</cell><cell>0.17 ? 0.02</cell></row><row><cell cols="4">Fig. 5. Blocking scores of each model with different experts on local</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">routes, with lower blocking scores equivalent to higher route completion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Analyzing the results of MMFN (Image) and MMFN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(VectorNet) in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">ONLINE LEADERBOARD RESULTS</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Sensors</cell><cell>DS ?</cell><cell>RC ?</cell><cell>P ?</cell></row><row><cell cols="3">MMFN (Ours) 1 Cameras + 1 LiDAR 22.8</cell><cell>47.22</cell><cell>0.63</cell></row><row><cell>NEAT [8]</cell><cell>3 Cameras</cell><cell cols="3">21.83 41.71 0.65</cell></row><row><cell>AIM-MT [8]</cell><cell>1 Cameras</cell><cell cols="3">19.38 67.02 0.39</cell></row><row><cell>T4AC+ [30]</cell><cell cols="3">1 Cameras + 1 LiDAR 18.75 75.11</cell><cell>0.28</cell></row><row><cell>TransFuser [7]</cell><cell cols="4">1 Cameras + 1 LiDAR 16.93 51.82 0.42</cell></row><row><cell>Pylot [1]</cell><cell cols="2">2 Cameras + 1 LiDAR 16.7</cell><cell cols="2">48.63 0.5</cell></row><row><cell>CaRINA [31]</cell><cell cols="4">2 Cameras + 1 LiDAR 15.55 40.63 0.47</cell></row><row><cell cols="2">means the agent use the HD Map</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pylot: A modular platform for exploring latency-accuracy tradeoffs in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schafhalter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8806" to="8813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hercules: An autonomous logistic vehicle for contact-less goods transportation during the covid-19 outbreak</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07480</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy-based feature selection for efficient lidar slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5222" to="5228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ground-aware monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yixuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="919" to="926" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time trajectory planning for autonomous driving with gaussian process and incremental refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="8999" to="9005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning by cheating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7077" to="7087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neat: Neural attention fields for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Carl-lead: Lidar-based end-toend autonomous driving with contrastive deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08473</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Carla: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Autonomous driving leaderboard</title>
		<ptr target="https://leaderboard.carla.org/leaderboard" />
		<imprint/>
		<respStmt>
			<orgName>CARLA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gri: General reinforced imitation and its application to vision-based autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chekroun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hornauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08575</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to drive from a world on rails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="590" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end model-free reinforcement learning for urban driving using implicit affordances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7153" to="7162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Precog: Predictions conditioned on goals in visual multi-agent scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vectornet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-task multisensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep multi-scale and multi-modal fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="236" to="242" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A light-weight semantic map for visual localization towards autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Hdmapnet: An online hd map construction and evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limitations of behavior cloning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9329" to="9338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How to build and validate a safe and reliable autonomous driving stack? a ros based software modular architecture baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gmez-Hulamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daz-Daz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Araluce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Ortz-Huaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gutirrez-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arango</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Llamazares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Intelligent Vehicles Symposium (IV): In submission</title>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A software architecture for autonomous vehicles: Team lrm-b entry in the first carla autonomous driving challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Rosero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T M</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Os?rio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12598</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-toend urban driving by imitating a reinforcement learning coach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liniger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="222" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
