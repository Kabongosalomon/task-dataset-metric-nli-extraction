<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Kozachkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Ennis</surname></persName>
							<email>mennis@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Division of Medical Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Slotine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Google AI * Equal contribution</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) are widely used throughout neuroscience as models of local neural activity. Many properties of single RNNs are well characterized theoretically, but experimental neuroscience has moved in the direction of studying multiple interacting areas, and RNN theory needs to be likewise extended. We take a constructive approach towards this problem, leveraging tools from nonlinear control theory and machine learning to characterize when combinations of stable RNNs will themselves be stable. Importantly, we derive conditions which allow for massive feedback connections between interacting RNNs. We parameterize these conditions for easy optimization using gradient-based techniques, and show that stability-constrained 'networks of networks' can perform well on challenging sequential-processing benchmark tasks. Altogether, our results provide a principled approach towards understanding distributed, modular function in the brain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The combination and reuse of primitive "modules" has enabled a great deal of progress in computer science, engineering, and biology. Modularity is particularly apparent in the structure of the brain, as different parts are specialized for different functions <ref type="bibr" target="#b22">[Kandel et al., 2000]</ref>. Accordingly, most experimental studies throughout the history of neuroscience have focused on a single brain area in association with a single behavior <ref type="bibr" target="#b0">[Abbott and Svoboda, 2020]</ref>. Similarly, RNN models of brain function have mostly been limited to a single RNN modeling a single area. However, neuroscience is entering an age where recording from many different brain areas simultaneously during complex behaviors is possible. As experimental neuroscience has shifted towards multi-area recordings, computational techniques for analyzing, modeling, and interpreting these multi-area recordings have blossomed <ref type="bibr" target="#b35">[Mashour et al., 2020</ref><ref type="bibr" target="#b0">, Abbott and Svoboda, 2020</ref><ref type="bibr" target="#b43">, Perich et al., 2021</ref><ref type="bibr" target="#b54">, Semedo et al., 2019</ref><ref type="bibr" target="#b75">, Yang and Molano-Maz?n, 2021</ref><ref type="bibr" target="#b33">, Machado et al., 2022</ref>. Despite this, RNN theory has lagged behind.</p><p>The theoretical question of RNN stability is crucial for understanding information propagation and manipulation <ref type="bibr" target="#b68">[Vogt et al., 2020</ref><ref type="bibr" target="#b12">, Engelken et al., 2020</ref><ref type="bibr" target="#b26">, Kozachkov et al., 2022a</ref>. The conditions under which single, autonomous RNNs are chaotic or stable are well-studied, in particular when the RNN weights are randomly chosen and the number of neurons tends to infinity <ref type="bibr" target="#b62">[Sompolinsky et al., 1988</ref><ref type="bibr" target="#b12">, Engelken et al., 2020</ref>. However, there is very little work addressing the theoretical question of stability in 'networks of networks'. Two facts make this question challenging. Firstly, 36th Conference on Neural Information Processing Systems (NeurIPS 2022).</p><p>connecting two stable systems does not, in general, lead to a stable overall system. This is true even for linear systems. Secondly, there is a massive amount of feedback between brain areas, so one cannot reasonably assume near-decomposability <ref type="bibr">[Simon, 1962, Abbott and</ref><ref type="bibr" target="#b0">Svoboda, 2020]</ref>.</p><p>Given that the brain seems to dynamically reorganize and adapt interareal connectivity to meet task demands and environmental constraints <ref type="bibr">Cohen, 2001, Sych et al., 2022]</ref>, this question of how stability is maintained is of the utmost importance. Here we take a bottom-up approach, more specifically asking "what stability properties of the individual modules lend themselves to rapid reorganization?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contraction Analysis</head><p>We focus on a special type of stability, known as contractive stability <ref type="bibr" target="#b32">[Lohmiller and Slotine, 1998</ref>]. Loosely, a contracting system is a dynamical system that forgets its initial conditions exponentially quickly. Contractive stability is a strong form of dynamical stability which implies many other forms of stability, such as certain types of input-to-state stability <ref type="bibr" target="#b63">[Sontag, 2010]</ref>. See Section A1.2 for a mathematical primer on contraction analysis. Contraction analysis has found wide application in nonlinear control theory [Manchester and <ref type="bibr" target="#b34">Slotine, 2017]</ref>, synchronization <ref type="bibr" target="#b44">[Pham and Slotine, 2007]</ref>, and robotics <ref type="bibr" target="#b9">[Chung and Slotine, 2009</ref>], but has only recently begun to find application in neuroscience and machine learning <ref type="bibr" target="#b3">[Boffi et al., 2020</ref><ref type="bibr" target="#b70">, Wensing and Slotine, 2020</ref><ref type="bibr" target="#b25">, Kozachkov et al., 2020</ref><ref type="bibr" target="#b21">, Jafarpour et al., 2021</ref><ref type="bibr" target="#b26">, Kozachkov et al., 2022a</ref><ref type="bibr" target="#b6">, Centorrino et al., 2022</ref><ref type="bibr" target="#b5">, Burghi et al., 2022</ref><ref type="bibr" target="#b27">, Kozachkov et al., 2022b</ref>. Contraction analysis is useful for neuroscience because it is directly applicable to systems with external inputs. It also allows for modular stability-preserving combination properties to be derived <ref type="figure">(Figure 1</ref>). The resulting contracting combinations can involve individual systems with different dynamics, as long as those dynamics are contracting <ref type="bibr" target="#b58">[Slotine and</ref><ref type="bibr">Lohmiller, 2001, Slotine, 2003]</ref>.</p><p>Moreover, modular stability and specifically contractive stability have relevance to evolutionary biology <ref type="bibr">[Simon, 1962, Slotine and</ref><ref type="bibr" target="#b60">Lohmiller, 2001]</ref>. In particular, it is thought that the majority of traits that have developed over the last 400+ million years are the result of evolutionary forces acting on regulatory elements that combine core components, rather than mutations in the core components themselves. This mechanism of action makes meaningful variation in population phenotypes much more feasible to achieve, and is appropriately titled "facilitated variation" <ref type="bibr" target="#b15">[Gerhart and Kirschner, 2007]</ref>. In addition to the biological evidence for facilitated variation, computational models have demonstrated that this approach produces populations which are better able to generalize to new environments <ref type="bibr" target="#b42">[Parter et al., 2008]</ref>, an ability that will be critical to further develop in deep learning systems. However, the tractability of these evolutionary processes hinges on some mechanism for ensuring stability of combinations. Because contraction analysis tools allow complicated contracting systems to be built up recursively from simpler elements, this form of stability would be well suited for biological systems <ref type="bibr" target="#b59">[Slotine and Liu, 2012]</ref>. Our work with the Sparse Combo Net in Section 4 has direct parallels to facilitated variation, in that we train this combination network architecture only through training connections between contracting subnetworks. <ref type="figure">Figure 1</ref>: Contractive stability implies a modularity principle. Because contraction analysis tools allow complicated contracting systems to be built up recursively from simpler elements, this form of stability is well suited for understanding biological systems. Contracting combinations can be made between systems with very different dynamics, as long as those dynamics are contracting.</p><p>Ultimately, our contributions are three-fold:</p><p>? A novel parameterization for feedback combination of contracting RNNs that enables direct optimization using standard deep learning libraries. ? Novel contraction conditions for continuous-time nonlinear RNNs, to use in conjunction with the combination condition. We also identify flaws in stability proofs from prior literature. ? Experiments demonstrating that our 'network of networks' sets a new state of the art for stability-constrained RNNs on benchmark sequential processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Network of Networks Model</head><p>In this paper we analyze rate-based neural networks. Unlike spiking neural networks, these models are continuous and smooth. We consider the following RNN introduced by <ref type="bibr" target="#b73">Wilson and Cowan [1972]</ref>, which may be viewed as an approximation to a more biophysically-detailed spiking network:</p><formula xml:id="formula_0">?? = ?x + W?(x) + u(t)<label>(1)</label></formula><p>Here ? &gt; 0 is the time-constant of the network <ref type="bibr" target="#b11">[Dayan and Abbott, 2005]</ref>, and the vector x ? R n contains the voltages of all n neurons in the network. The voltages are converted into firing-rates through a static nonlinearity ?. We only consider monotonic activation functions with bounded slope: in other words, 0 ? ? ? g (unless otherwise noted). We do not restrain the sign of the nonlinearity. Common example nonlinearities ?(x) that satisfy these constraints are hyperbolic tangent and ReLU. The matrix W ? R n?n contains the synaptic weights of the RNN. It is this matrix that ultimately determines the stability of (1), and will be a main target for our analysis. Finally, u(t) is the potentially time-varying external input into the network, capturing both explicit input into the RNN from the external world, as well as unmodeled dynamics from other brain areas.</p><p>Note that (1) is equivalent to another commonly used class of RNNs where the term Wx + u appears inside the nonlinearity. See Section A1.1 or <ref type="bibr" target="#b39">[Miller and Fumarola, 2012]</ref> for details. Our mathematical results apply equally well to both types of RNNs.</p><p>In order to extend (1) into a model of multiple interacting neural populations, we introduce the index i, which runs from 1 to p, where p is the total number of RNNs in the collection of RNNs. For now we will assume linear interactions between RNNs, because this is the simplest case. The linearity assumption can also be motivated by the fact that RNNs have been found to be well-approximated by linear systems in many neuroscience contexts <ref type="bibr" target="#b64">[Sussillo and</ref><ref type="bibr">Barak, 2013, Langdon and</ref><ref type="bibr" target="#b29">Engel, 2022]</ref>. This leads to the following equation for the 'network of networks':</p><formula xml:id="formula_1">?? i = ?x i + W i ?(x i ) + p j=1 L ij x j + u i (t) ?i = 1 ? ? ? p<label>(2)</label></formula><p>where the matrix L ij captures the interaction between RNN i and RNN j. If RNN i has N i neurons and RNN j has N j neurons, then L ij is an N i ? N j matrix.</p><p>We can now formalize the question posed in the introduction. Namely, "what types of connections between stable RNNs automatically preserve stability?" becomes "what restrictions on L ij must be met in order to ensure overall stability of the network of networks?". We will now derive two combination 'primitives', negative feedback and hierarchical, which allow for recursive connection of contracting modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generalized Negative Feedback Between RNNs Preserves Stability</head><p>We set aside for a moment the problem of determining when (1) is contracting. For now, assume that we have a collection of p contracting RNNs interacting through equation <ref type="formula" target="#formula_1">(2)</ref>. Recall that contraction is defined with respect to a metric, a way of measuring distances between trajectories in state space. Thus, the ith RNN is contracting with respect to some metric M i . We assume for simplicity that this metric is constant, which means that M i is simply a symmetric, positive definite matrix. In the case where every RNN receives feedback from every other, we can preserve stability by ensuring these connections are negative feedback. In the simplest case where all RNN modules are contracting in the identity metric, the negative feedback may be written as:</p><formula xml:id="formula_2">L ij = ?L T ji</formula><p>This is a well known result from the contraction analysis literature <ref type="bibr" target="#b58">[Slotine, 2003]</ref>. Our first novel contribution is a generalization and parameterization of this negative feedback which allows for direct optimization using gradient-based techniques. In particular, if each L ij is parameterized as follows:</p><formula xml:id="formula_3">L ij = B ij ? M ?1 i B T ji M j ?i, j<label>(3)</label></formula><p>for arbitrary matrix B ij , then the overall network of networks retains the assumed contraction properties of the RNN subnetworks. We provide a detailed proof in Section A3.1, but the basic idea relies on ensuring that L ij = ?L T ji in the appropriate metric. This can be achieved via the constraint: <ref type="formula" target="#formula_3">(3)</ref> into the above expression verifies that it is indeed satisfied. Because contraction analysis relies on analyzing the symmetric part of Jacobian matrices, the skew-symmetry of L ij 'cancels out' when computing the symmetric part, and leaves the stability of the subnetwork RNNs untouched.</p><formula xml:id="formula_4">M i L ij = ?L T ji M j Plugging</formula><p>Recursive Properties of Contracting Combinations The feedback combination (3), taken together with hierarchical combinations, may be used as combination primitives for recursively constructing complicated networks of networks while automatically maintaining stability. The recursion comes from the fact that once a modular system is shown to be contracting it may be treated as a single contracting system, which may in turn be combined with other contracting systems, ad infinitum. Note that while the feedback (3) requires linear interareal connections, hierarchical interareal connections may be nonlinear <ref type="bibr" target="#b32">[Lohmiller and Slotine, 1998</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Many Different Ways to Achieve Local Contraction</head><p>In this section we return to the question of achieving contraction in the subnetwork RNNs. Recall that we wish to find restrictions on W i such that the ith subnetwork RNN, described by (1), is contracting. Here we derive five such novel conditions (see Section A3 for detailed proofs). As we will discuss, not all contraction conditions are equally useful -for example some conditions are easier to optimize or have higher model capacity than others. In this section we also point out some flaws in existing stability proofs in the RNN literature, and suggest some pathways towards correcting them.</p><p>Theorem 1 (Absolute Value Restricted Weights). Let |W| denote the matrix formed by taking the element-wise absolute value of W. If there exists a positive, diagonal P such that:</p><formula xml:id="formula_5">P(g|W| ? I) + (g|W| ? I) T P ? 0</formula><p>then (1) is contracting in metric P. If W ii ? 0, then |W | ii may be set to zero to reduce conservatism.</p><p>It is easy to find matrices that satisfy Theorem 1, and given a matrix the condition is as easy to check as linear stability is. Moreover, the condition guarantees we can obtain a metric that the system is known to contract in (see Section 4.1 for details). It is less straightforward to enforce this condition during training, however we found that subnetworks constrained by Theorem 1 can achieve high performance in practice by simply fixing W and only optimizing the connections between subnetworks (Section 4.2). As there are fewer parameters to optimize, this training technique is faster.</p><p>Theorem 2 (Symmetric Weights). If W = W T and gW ? I, and ? &gt; 0, then (1) is contracting.</p><p>It has been known since the early 1990s that if (1) is autonomous (i.e the input u is constant) and has symmetric weights with eigenvalues less than 1/g, then there exists a unique fixed point that the network converges to from any initial condition <ref type="bibr" target="#b36">[Matsuoka, 1992]</ref>. Theorem 2 generalizes this statement to say that if (1) has symmetric weights with eigenvalues less than 1/g, it is contracting. This includes previous results as a special case, because an autonomous contracting system has a unique fixed point which the network converges to from any initial condition.</p><p>Theorem 3 (Product of Diagonal and Orthogonal Weights). If there exists positive diagonal matrices P 1 and P 2 , as well as Q = Q T 0 such that W = ?P 1 QP 2 then (1) is contracting in metric M = (P 1 QP 1 ) ?1 .</p><p>In contrast to the first two contraction conditions, Theorem 3 is very easy to optimize. To meet the constraint that the P matrices are positive, one can parameterize their diagonal elements as P ii = d 2 i + , for some small positive constant , and optimize d i directly. To meet the positive definiteness constraint on Q, one may parameterize it as Q = E T E + I and optimize E directly.</p><p>Theorem 4 (Triangular Weights). If gW ? I is triangular and Hurwitz, then (1) is contracting in a diagonal metric.</p><p>Theorem 4 follows from the fact that a hierarchy of contracting systems is also contracting.</p><p>Theorem 5 (Singular Value Restricted Weights). If there exists a positive diagonal matrix P such that:</p><p>g 2 W T PW ? P ? 0 then (1) is contracting in metric P.</p><p>In the case of discrete-time RNNs, this contraction condition has been proved by many different authors in many different settings. When P = I, it is known as the echo-state condition for discretetime RNNs <ref type="bibr" target="#b20">[Jaeger, 2001]</ref>. This was then generalized to diagonal P by <ref type="bibr" target="#b4">Buehner and Young [2006]</ref>. More recently, the original echo-state condition was rediscovered by <ref type="bibr" target="#b38">Miller and Hardt [2018]</ref> in the machine learning literature. Following this rediscovery, the condition was generalized to P = I by . Here we show that it applies to continuous-time RNNs as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">What do the Jacobian Eigenvalues Tell Us?</head><p>Several recent papers in ML, e.g <ref type="bibr">Ruthotto, 2017, Chang et al., 2019]</ref>, claim that a sufficient condition for stability of the nonlinear system:</p><formula xml:id="formula_6">x = f (x, t)</formula><p>is that the associated Jacobian matrix J(x, t) = ?f ?x has eigenvalues whose real parts are strictly negative, i.e: max i Re(? i (J(x, t)) ? ?? with ? &gt; 0. However, this claim is generally false -see Section 4.4.2 in <ref type="bibr" target="#b61">[Slotine and Li, 1991]</ref>.</p><p>In the specific case of the RNN (1), it appears that the eigenvalues of the symmetric part of W do provide information on global stability in a number of applications. For example, in <ref type="bibr" target="#b36">[Matsuoka, 1992]</ref> it was shown that if W s = 1 2 (W + W T ) has all its eigenvalues less than unity, and u is constant, then (1) has a unique, globally asymptotically stable fixed point. This condition also implies that the real parts of the eigenvalues of the Jacobian are uniformly negative. Moreover, in <ref type="bibr" target="#b7">[Chang et al., 2019]</ref> it was shown that setting the symmetric part of W s = 1 2 (W + W T ) almost equal to zero (yet slightly negative) led to rotational, yet stable dynamics in practice. This leads us to the following theorem, which shows that if the slopes of the activation functions change sufficiently slowly as a function of time, then the condition in <ref type="bibr" target="#b36">[Matsuoka, 1992]</ref> in fact implies global contraction of (1). Theorem 6. Let D be a positive, diagonal matrix with D ii = d?i dxi , and let P be an arbitrary, positive diagonal matrix. If:</p><formula xml:id="formula_7">(gW ? I)P + P(gW T ? I) ?cP and? ? cg ?1 D ??D for c, ? &gt; 0, then (1) is contracting in metric D with rate ?.</formula><p>We stress however, that it is an open question whether or not diagonal stability of W implies that (1) is contracting. It has been conjectured that diagonal stability of gW ? I is a sufficient condition for global contraction of (1) , however this has been difficult to prove. To better characterize this conjecture, we present Theorem 7, which shows by way of counterexample that diagonal stability of gW ? I does not imply global contraction in a constant metric for (1).</p><p>Theorem 7. Satisfaction of the condition gW sym ? I ? 0 is not sufficient to show global contraction of the general nonlinear RNN (1) in any constant metric. High levels of antisymmetry in W can make it impossible to find such a metric, which we demonstrate via a 2 ? 2 counterexample of the following form, with c ? 2 when g = 1: W = 0 ?c c 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Stability-Constrained Network of Networks Perform Well on Benchmarks</head><p>A natural concern is that stability of an RNN may come at the cost of its expressivity, which is particularly relevant for integrating information over long timescales. To investigate whether this might be an issue for our model, we trained a stability-constrained network-of-networks on three benchmark sequential image classification tasks: sequential MNIST, permuted seqMNIST, and sequential CIFAR10. These tasks are often used to measure information processing ability over long sequences <ref type="bibr" target="#b30">[Le et al., 2015]</ref>. Images are presented pixel-by-pixel, and the network makes a prediction at the end of the sequence. In permuted seqMNIST, pixels are input in a fixed but random order.</p><p>All of our experiments were done on networks governed by (2). The nonlinear subnetwork RNNs were connected to each other via linear all-to-all negative feedback, given by <ref type="formula" target="#formula_3">(3)</ref>. For all subnetworks we use the ReLU activation function. To enforce contraction of each individual subnetwork, we focused on two stability constraints from our theoretical results: Theorems 1 and 5. In the case of Theorem 1, we did not train the individual subnetworks' weight matrices, but only trained the connections between subnetworks ( <ref type="figure">Figure 2B</ref>). For Theorem 5, we trained all parameters of the model ( <ref type="figure">Figure 2C</ref>).</p><p>We refer to networks with subnetworks constrained by Theorem 1 as 'Sparse Combo Nets' and to networks with subnetworks constrained by Theorem 5 as 'SVD Combo Nets'. Throughout the experimental results we use the notation 'p ? n network' -such a network consists of p distinct subnetwork RNNs, with each such subnetwork RNN containing n units. Figure 2: Summary of task structure and network architectures. Images from MNIST (or CIFAR10) were flattened into an array of pixels and fed sequentially into the modular 'network of networks', with classification based on the output at the last time-step. For MNIST, each image was also permuted in a fixed manner (A). The subnetwork 'modules' of our architecture were constrained to meet either Theorem 1 via sparse initialization (B) or Theorem 5 via direct parameterization (C). Linear negative feedback connections were trained between the subnetworks according to (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Initialization and Training</head><p>For the Sparse Combo Net we were not able to find a parameterization to continuously update the internal RNN weights during training in a way that preserved contraction. However, it is easy to randomly generate matrices with a particular likelihood of meeting the Theorem 1 condition by selecting an appropriate sparsity level and limit on entry magnitude. Sparsity in particular is of interest due to its relevance in neurobiology and machine learning, so it is convenient that the condition makes it easy to verify stability of many different sparse RNNs. As g = 1 for ReLU activation, we check potential subnetwork matrices W by simply verifying linear stability of |W| ? I.</p><p>Because every RNN meeting the condition has a corresponding well-defined stable LTI system contracting in the same metric, it is also easy to find a metric to use in our training algorithm: solving for M in ?I = MA + A T M will produce a valid metric for any stable LTI system A <ref type="bibr" target="#b61">[Slotine and Li, 1991]</ref>. We utilize the fact that Hurwitz Metzler matrices are diagonally stable to improve efficiency of computing M (as well as in our proof of Theorem 1).</p><p>We therefore randomly generated fixed subnetworks satisfying Theorem 1 and trained only the linear connections between them ( <ref type="figure" target="#fig_1">Figure 3</ref>), as well as the linear input and output layers. More information on network initialization, hyperparameter tuning, and training algorithm is provided in Section A4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) B)</head><p>150 Epochs 150 Epochs For the SVD Combo Net on the other hand, we ensured contraction of each subnetwork RNN by direct parameterization (described in Section A5), thus allowing all weights to be trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The Sparse Combo Net architecture achieved the highest overall performance on both permuted seqMNIST and seqCIFAR10, with 96.94% and 65.72% best test accuracies respectively -thereby setting a new SOTA for stable RNNs <ref type="table">(Table 1</ref>). Furthermore, we were able to reproduce SOTA scores over several repetitions, including 10 trials of seqCIFAR10. Along with repeatability of results, we also show that the contraction constraint on the connections between subnetworks (L in (3)) is important for performance, particularly in the Sparse Combo Net (Section 4.2.3).</p><p>Additionally, we profile how various architecture settings impact performance of our networks. In both networks, we found that increasing the total number of neurons improved task performance, but with diminishing returns (Section 4.2.1). We also found that the sparsity of the hidden-to-hidden weights in Sparse Combo Net had a large impact on the final network performance (Section 4.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experiments with Network Size</head><p>Understanding the effect of size on network performance is important to practical application of these architectures. For both Sparse Combo Net and SVD Combo Net, increasing the number of subnetworks while holding other settings constant (including fixing the size of each subnetwork at 32 units) was able to increase network test accuracy on permuted seqMNIST to a point ( <ref type="figure" target="#fig_2">Figure 4</ref>).</p><p>The greatest performance jump happened when increasing from one module (37.1% Sparse Combo Net, 61.8% SVD Combo Net) to two modules (89.1% Sparse Combo Net, 92.9% SVD Combo Net). After that the performance increased steadily with number of modules until saturating at ? 97% for Sparse Combo Net and ? 95% for SVD Combo Net.</p><p>As the internal subnetwork weights are not trained in Sparse Combo Net, it is unsurprising that its performance was substantially worse at the smallest sizes. However Sparse Combo Net surpassed SVD Combo Net by the 12 ? 32 network size, which contains a modest 384 total units. Due to the better performance of the Sparse Combo Net, we focused additional analyses there. Note also that the SVD Combo Net never reached 55% test accuracy for CIFAR10 in our early experiments.</p><p>We then evaluated task performance as the modularity of a Sparse Combo Net (fixed to have 352 total units) was varied. We observed an inverse U shape ( <ref type="figure">Figure S1B</ref>), with poor performance of a 1 ? 352 net and an 88 ? 4 net, and best performance from a 44 ? 8 net. However, this experiment compared similar sparsity levels, while in practice we can achieve better performance with larger subnetworks by leveraging sparsity in a way not possible for smaller ones.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Experiments with Sparsity</head><p>Because of the link between sparsity and stability as well as the biological relevance of sparsity, we explored in detail how subnetwork sparsity affects the performance of Sparse Combo Net. We ran a number of experiments on the permuted seqMNIST task, varying sparsity level while holding network size and other hyperparameters constant. Here we use "n% sparsity level" to refer to a network with subnetworks that have just n% of their weights non-zero.</p><p>We observed a large (&gt; 5 percentage point) performance boost when switching from a 26.5% sparsity level to a 10% sparsity level in the 11 ? 32 Sparse Combo Net ( <ref type="figure" target="#fig_3">Figure 5</ref>), and subsequently decided to test significantly sparser subnetworks in a 16 ? 32 Sparse Combo Net. We trained networks with sparsity levels of 5%, 3.3%, and 1%, as well as 10% for baseline comparison ( <ref type="figure">Figure S2A)</ref>. A 3.3% sparsity level produced the best results, leading to our SOTA performance for stable networks on both permuted seqMNIST and seqCIFAR10. With a component RNN size of just 32 units, this sparsity level is quite small, containing only one or two directional connections per neuron on average ( <ref type="figure" target="#fig_6">Figure  S7</ref>).</p><p>As sparsity had such a positive effect on task performance, we did additional analyses to better understand why. We found that decreasing the magnitude of non-zero elements while holding sparsity level constant decreased task performance ( <ref type="figure">Figure S2B</ref>), suggesting that the effect is driven in part by the fact that sparsity enables higher magnitude non-zero elements while still maintaining stability. The use of sparsity in subnetworks to improve performance suggests another interesting direction that could enable better scalability of total network size -enforcing sparsity in the linear feedback weight matrix (L). We performed pilot testing of this idea, which showed promise in mitigating the saturation effect seen in <ref type="figure" target="#fig_2">Figure 4</ref>. Those results are detailed in Section A4.2.4 <ref type="table" target="#tab_5">(Table S1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Repeatability and Controls</head><p>Sparse Combo Net does not have the connections within its subnetworks trained, so network performance could be particularly susceptible to random initialization. Thus we ran repeatability studies on permuted sequential MNIST and sequential CIFAR10 using our best network settings (16 ? 32 with subnetwork sparsity level of 3.3%) and an extended training period. Mean performance over 4 trials of permuted seqMNIST was 96.85% with 0.019 variance, while mean performance over 10 trials of seqCIFAR10 was 64.72% with 0.406 variance. Note we also ran a number of additional experiments on size and sparsity settings, described in Section A4.2.</p><p>Across the permuted seqMNIST trials, best test accuracy always fell between 96.65% and 96.94%, a range much smaller than the differences seen with changing sparsity settings and network size. Three of the four trials showed best test accuracy ? 96.88%, despite some variability in early training performance ( <ref type="figure" target="#fig_1">Figure S3</ref>). Similarly, eight of the ten seqCIFAR10 trials had test accuracy &gt; 64.3%, with all results falling between 63.73% and 65.72% ( <ref type="figure" target="#fig_2">Figure S4</ref>). This robustly establishes a new SOTA for stable RNNs, comfortably beating the previously reported (single run) 64.2% test accuracy achieved by Lipschitz <ref type="bibr">RNN [Erichson et al., 2021]</ref>.</p><p>As a control study, we also tested how sensitive the Sparse Combo Net was to the stabilization condition on the interconnection matrix (L in <ref type="formula" target="#formula_3">(3)</ref>). To do so, we initialized the individual RNN modules in a 24 ? 32 network as before, but set L = B and did not constrain B at all during training, thus no longer ensuring contraction of the overall system. This resulted in 47.0% test accuracy on the permuted seqMNIST task, a stark decrease from the original 96.7% test accuracy -thereby demonstrating the utility of the contraction condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Biologists have long noted that modularity provides organisms with stability and robustness <ref type="bibr" target="#b24">[Kitano, 2004]</ref>. The other direction -that stability and robustness provide modularity -is well known to engineers <ref type="bibr" target="#b23">[Khalil, 2002</ref><ref type="bibr" target="#b61">, Slotine and Li, 1991</ref><ref type="bibr" target="#b58">, Slotine, 2003</ref>], but has been less appreciated in biology. We use this principle to build and train provably stable assemblies of recurrent neural networks. Like real brains, the components of our "RNN of RNNs" can communicate with one another through a mix of hierarchical and feedback connections. In particular, we theoretically characterized conditions under which an RNN of RNNs will be stable, given that each individual RNN is stable. We also provided several novel stability conditions for single RNNs that are compatible with these stabilitypreserving interareal connections. Our results contribute towards understanding how the brain maintains stable and accurate function in the presence of massive interareal feedback, as well as external inputs.</p><p>The question of neural stability is one of the oldest questions in computational neuroscience. Indeed, cyberneticists were concerned with this question before the term 'computational neuroscience' existed <ref type="bibr" target="#b71">[Wiener, 1948</ref><ref type="bibr" target="#b1">, Ashby, 1952</ref>. Stability is a central component in several influential neuroscience theories <ref type="bibr" target="#b19">[Hopfield, 1982</ref><ref type="bibr" target="#b55">, Seung, 1996</ref><ref type="bibr" target="#b40">, Murphy and Miller, 2009</ref>, perhaps the most well-known being that memories are stored as stable point attractors <ref type="bibr" target="#b19">[Hopfield, 1982]</ref>. Our work shows that stability continues to be a useful concept for computational neuroscience as the field transitions from focusing on single brain areas to many interacting brain areas.</p><p>While primarily motivated by neuroscience, our approach is also relevant for machine learning. Deep learning models can be as inscrutable as they are powerful. This opacity limits conceptual progress and may be dangerous in safety-critical applications like autonomous driving or human-centered robotics. Given that stability is a fundamental property of dynamical systems -and is intimately linked to concepts of control, generalization, efficiency, and robustness -the ability to guarantee stability of a recurrent model will be important for ensuring deep networks behave as we expect them to <ref type="bibr" target="#b50">[Richards et al., 2018</ref><ref type="bibr" target="#b8">, Choromanski et al., 2020</ref><ref type="bibr" target="#b49">, Revay et al., 2021</ref><ref type="bibr" target="#b51">, Rodriguez et al., 2022</ref>.</p><p>In the case of RNNs, one difficulty is that providing a certificate of stability is often impossible or computationally impractical. However, the stability conditions we derive here allow for recursive construction of complicated RNNs while automatically preserving stability. By parameterizing our conditions for easy optimization using gradient-based techniques, we successfully trained our architecture on challenging sequential processing benchmarks. The high test accuracy our networks achieved with a small number of trainable parameters demonstrates that stability does not necessarily come at the cost of expressivity. Thus, our results likewise contribute towards understanding stability certification of RNNs.</p><p>In future work, we will explore how our contraction-constrained RNNs of RNNs perform on a variety of neuroscience tasks, in particular tasks with multimodal structure <ref type="bibr" target="#b76">[Yang et al., 2019]</ref>. One desiderata for those future models is that they learn representations which are formally similar to those observed in the brain <ref type="bibr" target="#b74">[Yamins et al., 2014</ref><ref type="bibr" target="#b53">, Schrimpf et al., 2020</ref><ref type="bibr" target="#b72">, Williams et al., 2021</ref>, in complement with the structural similarities already shared. Moreover, a "network of networks" approach will be especially relevant to challenging multimodal machine learning problems, such as the simultaneous processing of audio and video. Therefore the advancement of neuroscience theory and machine learning remain hand-in-hand for our next lines of questioning. Indeed, combinations of trained networks have already seen groundbreaking success in DeepMind's AlphaGo <ref type="bibr" target="#b56">[Silver et al., 2016]</ref>.</p><p>As well as the many potential experimental applications, there are numerous theoretical future directions suggested by our work. Networks with more biologically-plausible weight update rules, such as models discussed in <ref type="bibr" target="#b25">[Kozachkov et al., 2020]</ref>, would be a fruitful neuroscience context in which to explore our conditions. One promising avenue of study there is to examine input-dependent stability of the learning process. In the context of machine learning, our stability conditions could be applied to the end-to-end training of multidimensional recurrent neural networks <ref type="bibr" target="#b16">[Graves et al., 2007]</ref>, which have clear structural parallels to our RNNs of RNNs but lack known stability guarantees.</p><p>In sum, recursively building network combinations in an effective and stable fashion while also allowing for continual refinement of the individual networks, as nature does for biological networks, will require new analysis tools. Here we have taken a concrete step towards the development of such tools, not only through our theoretical results, but also through their application to create stable combination network architectures that perform well in practice on benchmark tasks. Note that in neuroscience, the variable x in equation <ref type="formula" target="#formula_0">(1)</ref> is typically thought of as a vector of neural membrane potentials. It was shown in <ref type="bibr" target="#b39">[Miller and Fumarola, 2012</ref>] that the RNN (1) is equivalent via an affine transformation to another commonly used RNN model,</p><formula xml:id="formula_8">?? = ?y + ?(Wy + b(t))<label>(4)</label></formula><p>where the variable y is interpreted as a vector of firing rates, rather than membrane potentials. The two models are related by the transformation x = Wy + b, which yields</p><formula xml:id="formula_9">?? = W(?y + ?(Wy + b)) + ?? = ?x + W?(x) + v where v ? b + ??.</formula><p>Thus b is a low-pass filtered version of v (or conversely, v may be viewed as a first order prediction of b) and the contraction properties of the system are unaffected by the affine transformation. Note that the above equivalence holds even in the case where W is not invertible. In this case, the two models are proven to be equivalent, provided that b(0) and y(0) satisfy certain conditions-which are always possible to satisfy <ref type="bibr" target="#b39">[Miller and Fumarola, 2012]</ref>. Therefore, any contraction condition derived for the x (or y) system automatically implies contraction of the other system. We exploit this freedom freely throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2 Contraction Math</head><p>It can be shown that the non-autonomous syste?</p><formula xml:id="formula_10">x = f (x, t) is contracting if there exists a metric M(x, t) = ?(x, t) T ?(x, t) 0 such that uniforml? M + MJ + J T M ??M</formula><p>where J = ?f ?x and ? &gt; 0. For more details see the main reference <ref type="bibr" target="#b32">[Lohmiller and Slotine, 1998</ref>]. Similarly, a non-autonomous discrete-time system</p><formula xml:id="formula_11">x t+1 = f (x t , t) is contracting if J T M t+1 J ? M t ??M t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2.1 Feedback and Hierarchical Combinations</head><p>Consider two systems, independently contracting in constant metrics M 1 and M 2 , which are combined in feedback:?</p><formula xml:id="formula_12">= f (x, t) + B? y = g(y, t) + Gx (Feedback Combination)</formula><p>If the following relationship between B, G, M 1 , and M 2 is satisfied:</p><formula xml:id="formula_13">B = ?M ?1 1 G T M<label>2</label></formula><p>then the combined system is contracting as well. This may be seen as a special case of the feedback combination derived in <ref type="bibr" target="#b66">[Tabareau and Slotine, 2006</ref>]. The situation is even simpler for hierarchical combinations. Consider again two systems, independently contracting in some metrics, which are combined in hierarchy:?</p><formula xml:id="formula_14">= f (x, t) y = g(y, t) + h(x, t) (Hierarchical Combination)</formula><p>where h(x, t) is a function with bounded Jacobian. Then this combined system is contracting in a diagonal metric, as shown in <ref type="bibr" target="#b32">[Lohmiller and Slotine, 1998</ref>]. By recursion, this extends to hierarchies of arbitrary depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2 Extended Discussion</head><p>Given the paucity of existing theory on modular networks, our novel stability conditions and proof of concept combination architectures are a significant step in an important new direction. The "network of networks" approach is evident in the biological brain, and has seen early practical success in applications such as AlphaGo. There is much evidence this line of questioning will be critical in the future, and our work is the first on stable modular networks.</p><p>Furthermore, we develop an architecture based on such combinations of "vanilla" RNNs that is both stable and achieves high performance on benchmark sequence classification tasks using few trainable parameters (small particularly for sequential CIFAR10). When considering just the facts about the network, it really has no business performing anywhere near as well as it does. Note also that without the stability condition in place, the network performance indeed drops substantially.</p><p>In order to facilitate the extension of this important line of thinking, we provide additional context on the limitations of our current approach as well as promising ideas for future directions in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1 Limitations</head><p>One drawback of our approach is that we parameterize each weight matrix in a special way to guarantee stability. In all cases, this requires us to parameterize matrices as the product of several other matrices. Thus, we gain stability at the cost of increasing the number of parameters, which can slow down training.</p><p>Another current drawback is that we only consider constant metrics. In theory, contraction metrics can be state-dependent as well as time-varying. Thus it is possible that we have overly restricted the space of models we consider. Similarly, negative feedback is not the only way to preserve contraction when combining two contracting systems. There are known small-gain theorems in the contraction analysis literature which accomplish the same task <ref type="bibr" target="#b58">[Slotine, 2003]</ref>. However, parameterizing these conditions is less straightforward than parameterizing the negative-feedback condition.</p><p>A third limitation of the present work is that it does not give a recipe on how to incorporate anatomical knowledge into the building of 'RNNs of RNNs'. Our current approach is 'bottom up', in the sense that we describe complicated networks which can be built from simpler ones while ensuring stability at every level of construction. However, for building biological models of the brain it is important to incorporate known anatomical detail (i.e V4 projects to PFC, PFC projects back to V4, etc). How to do this in a way that preserves stability is an open and interesting question.</p><p>Lastly, we only tested our networks on sequential image classification benchmarks. Future work will include other benchmarks such as character or word-level language modeling. Additionally, while we conducted preliminary experiments exploring the role of scale (i.e number of subnetwork RNNs), we did not pursue this at the sizes reached by many modern deep learning applications. Thus it is currently unclear if the performance of these stability-constrained models will scale well enough with the number of subnetworks (or the number of neurons per subnetwork). Testing this correctly will require extensive experimentation with the various initialization and training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2 Future Directions</head><p>There are numerous future directions enabled by this work. For example, Theorem 6 suggests that a less restrictive contraction condition on W in terms of the eigenvalues of the symmetric part is possible and desirable. Meanwhile, Theorem 7 provides important groundwork in finding such a condition, as it shows the need for a time-varying metric. Investigation of input-dependent metrics could be a fruitful next line of research, and would have far-reaching implications in disciplines such as curriculum learning.</p><p>Furthermore, the beneficial impact of sparsity on training these stable models suggests a potential avenue for additional experimental work -in particular adding a regularizing sparsity term during training. This could allow Sparse Combo Net to have its internal subnetwork weights trained without losing the stability guarantee, and conversely it could allow SVD Combo Net to reap some of the performance benefits of Sparse Combo Net without giving up the flexibility allowed by training said internal weights.</p><p>As described in the limitations above, a major experimental next step will be to test our architectures at greater scale and on more difficult tasks. Since 'network of network' approaches are becoming increasingly popular, our methodology is relevant to a variety of task types, including reinforcement learning applications. Given the biological inspiration, multi-modal learning tasks may also be of particular relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3 Proofs for Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.1 Proof of Feedback Combination Property</head><p>Here we apply existing contraction analysis results to derive equation <ref type="formula" target="#formula_3">(3)</ref>. Because <ref type="formula" target="#formula_3">(3)</ref> is a parameterization of a known contraction conditions <ref type="bibr" target="#b58">[Slotine, 2003]</ref>, we provide the following statement in the form of a corollary.</p><p>Corollary 1 (Network of Networks). Consider a collection of p subnetwork RNNs governed by (1). Assume that these RNNs each have hidden-to-hidden weight matrices {W 1 , . . . , W p } and are independently contracting in metrics {M 1 , . . . , M p }. Define the block matrices W ? BlockDiag(W 1 , . . . , W p ),M ? BlockDiag(M 1 , . . . , M p ),the overall state vectorx T ? (x T 1 ? ? ? x T p ), and finally? T ? (u T 1 ? ? ? u T p ) Then the following 'network of networks' is globally contracting in metricM:</p><formula xml:id="formula_15">?? = ?x +W?(x) +? + Lx L ? B ?M ?1 B TM<label>(5)</label></formula><p>Where B is an arbitrary square matrix. The matrix L is made up of many individual sub-matrices (the L ij terms in <ref type="formula" target="#formula_1">(2)</ref>) which define the interareal connectivity of the overall network.</p><p>Proof. Consider the differential Lyapunov function:</p><formula xml:id="formula_16">V = 1 2 ?x TM ?x</formula><p>The time-derivative of this function is:</p><formula xml:id="formula_17">V = ?x TM ?? = ?x T (MJ +J TM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jacobian of RNNs before interconnection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ML + L TM</head><p>Interconnection Jacobian</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>)?x</head><p>Since we assume that the RNNs are contracting in isolation, the first term in this sum is less that the slowest contracting rate of the individual RNNs, which we call ? &gt; 0. The second term in the sum is the zero matrix, by construction. Thus the time-derivative of V is upper-bounded by:</p><formula xml:id="formula_18">V ? ?2?V</formula><p>This implies that the network of networks is contracting with rate ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.2 Proof of Theorem 1</head><p>Our first theorem is motivated by the observation that if the y-system (described in Section A1.1) is to be interpreted as a vector of firing rates, it must stay positive for all time. For a linear, time-invariant system with positive states, diagonal stability is equivalent to stability. Therefore a natural question is if diagonal stability of a linearized y-system implies anything about stability of the nonlinear system. More formally, given an excitatory neural network (i.e ?ij, W ij ? 0), if the linear syste?</p><formula xml:id="formula_19">x = ?x + gWx</formula><p>is stable, then there exists a positive diagonal matrix P such that:</p><formula xml:id="formula_20">P(gW ? I) + (gW ? I) T P ? 0</formula><p>The following theorem shows that the nonlinear system (1) is indeed contracting in metric P, and extends this result to a more general W by considering only the magnitudes of the weights.</p><p>Theorem 1. Let |W| denote the matrix formed by taking the element-wise absolute value of W. If there exists a positive, diagonal P such that:</p><formula xml:id="formula_21">P(g|W| ? I) + (g|W| ? I) T P ? 0</formula><p>then <ref type="formula" target="#formula_0">(1)</ref> is contracting in metric P. Moreover, if W ii ? 0, then |W | ii may be set to zero to reduce conservatism.</p><p>This condition is particularly straightforward in the common special case where the network does not have any self weights, with the leak term driving stability. While it can be applied to a more general W, the condition will of course not be met if the network was relying on highly negative values on the diagonal of W for linear stability. As demonstrated by counterexample in the proof of Theorem 1, it can be impossible to use the same metric P for the nonlinear RNN in such cases.</p><p>Theorem 1 allows many weight matrices with low magnitudes or a generally sparse structure to be verified as contracting in the nonlinear system (1), by simply checking a linear stability condition (as linear stability is equivalent to diagonal stability for Metzler matrices too [Narendra and Shorten, 2010]).</p><p>Beyond verifying contraction, Theorem 1 actually provides a metric, with little need for additional computation. Not only is it of inherent interest that the same metric can be shared across systems in this case, it is also of use in machine learning applications, where stability certificates are becoming increasingly necessary. Critically, it is feasible to enforce the condition during training via L2 regularization on W. More generally, there are a variety of systems of interest that meet this condition but do not meet the well-known maximum singular value condition, including those with a hierarchical structure.</p><p>Proof. Consider the differential, quadratic Lyapunov function:</p><formula xml:id="formula_22">V = ?x T P?x where P 0 is diagonal. The time derivative of V is: V = 2?x T P? x = 2?x T PJ?x = ?2?x T P?x + 2?x T PWD?x</formula><p>where D is a diagonal matrix such that D ii = d?i dx ? 0. We can upper bound the quadratic form on the right as follows:</p><formula xml:id="formula_23">?x T PWD?x = ij P i W ij D j ?x i ?x j ? i P i W ii D i |?x i | 2 + ij,i =j P i |W ij |D j |?x i ||?x j | ? g|?x| T P|W||?x|</formula><p>If W ii ? 0, the term P i W ii D i |?x i | 2 contributes non-positively to the overall sum, and can therefore be set to zero without disrupting the inequality. Now using the fact that P is positive and diagonal, and therefore ?x T P?x = |?x| T P|?x|, we can upper boundV as:</p><formula xml:id="formula_24">V ? |?x| T (?2P + P|W| + |W|P)|?x| = |?x| T [(P(|W| ? I) + (|W| T ? I)P)]|?x| where |W | ij = |W ij |, and |W | ii = 0 if W ii ? 0 and |W | ii = |W ii | if W ii &gt; 0.</formula><p>This completes the proof.</p><p>Note that W ? I is Metzler, and therefore will be Hurwitz stable if and only if P exists <ref type="bibr" target="#b41">[Narendra and Shorten, 2010]</ref>.</p><p>It is also worth noting that highly negative diagonal values in W will prevent the same metric P from being used for the nonlinear system. Therefore the method used in this proof cannot feasibly be adapted to further relax the treatment of the diagonal part of W.</p><p>The intuitive reason behind this is that in the symmetric part of the Jacobian, PWD+DW T P 2 ? P, the diagonal self weights will also be scaled down by small D, while the leak portion ?P remains untouched by D.</p><p>Now we actually demonstrate a counterexample, presenting a 2 ? 2 symmetric Metzler matrix W that is contracting in the identity in the linear system, but cannot be contracting in the identity in the nonlinear system <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_25">W = ?9 2.5 2.5 0</formula><p>To see that it is not possible for the more general nonlinear system with these weights to be contracting in the identity, take D = 0 0 0 1 . Now (WD) sym ? I = ?1 1.25 1.25 ?1 which has a positive eigenvalue of 1 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.3 Proof of Theorem 2</head><p>While regularization may push networks towards satisfying Theorem 1, strictly enforcing the condition during optimization is not straightforward. This motivates the rest of our theorems, which derive contraction results for specially structured weight matrices. Unlike Theorem 1, these results have direct parameterizations which can easily be plugged into modern optimization libraries.</p><p>Theorem 2. If W = W T and gW ? I, and and ? &gt; 0 then (1) is contracting.</p><p>When W is symmetric, (1) may be seen as a continuous-time Hopfield network. Continuous-time Hopfield networks with symmetric weights were recently shown to be closely related to Transformer architectures <ref type="bibr" target="#b28">[Krotov and</ref><ref type="bibr">Hopfield, 2020, Ramsauer et al., 2020]</ref>. Specifically, the dot-product attention rule may be seen as a discretization of the continuous-time Hopfield network with softmax activation function <ref type="bibr" target="#b28">[Krotov and Hopfield, 2020]</ref>. Our results here provide a simple sufficient (and nearly necessary, see above remark) condition for global exponential stability of a given trajectory for the Hopfield network. In the case where the input into the network is constant, this trajectory is a fixed point. Moreover, each trajectory associated with a unique input is guaranteed to be unique. Finally, we note that our results are flexible with respect to activation functions so long as they satisfy the slope-restriction condition. This flexibility may be useful when, for example, considering recent work showing that standard activation functions may be advantageously replaced by attention mechanisms <ref type="bibr" target="#b10">[Dai et al., 2020]</ref>.</p><p>Proof. We begin by writing W = R ? P for some unknown R = R T and P = P T 0. The approach of this proof is to show by construction that the condition gW ? I implies the existence of an R and P such that the system is contracting in metric P. We consider the y version of the RNN, which as discussed above is equivalent to the x version via an affine transformation.</p><p>The differential Lyapunov condition associated to the RNN is:</p><formula xml:id="formula_26">?x T [?2M + MDW + WDM + ?M]?x ? 0<label>(6)</label></formula><p>Where M, W ? R n?n . Let us now make the substitution M = P and W = R ? P:</p><formula xml:id="formula_27">?x T [?2P + PD(R ? P) + (R ? P)DP + ?P]?x ? 0<label>(7)</label></formula><p>Collecting terms, we get:</p><formula xml:id="formula_28">?x T [?2P + PDR + RDP ? 2PDP + ?P]?x ? 0<label>(8)</label></formula><p>We can rewrite (8) as a quadratic form over a block matrix, as follows:</p><formula xml:id="formula_29">?x T ?x T (? ? 2)P RDP PDR ?2PDP ?x ?x ? 0<label>(9)</label></formula><p>Now the question becomes, when is (9) satisfied? One way to ensure that <ref type="formula" target="#formula_29">(9)</ref> is satisfied is to ensure that the associated block matrix is always (i.e for all D) negative semi-definite. In that case the inequality will hold over all possible vectors, not just ?x ?x T . In other words, the question is now what constraints on the sub-matrices P, D and R ensure that:</p><formula xml:id="formula_30">?y ? R 2n , y T (2 ? ?)P ?RDP ?PDR 2PDP y ? 0<label>(10)</label></formula><p>Note that we have multiplied both sides of the inequality by a minus sign. But this is nothing but the definition of a positive semi-definite matrix. Using the Schur complement <ref type="bibr" target="#b14">[Gallier et al., 2020]</ref>(Proposition 2.1), we know that the block matrix is positive semi-definite iff PDP 0 and:</p><formula xml:id="formula_31">(2 ? ?)P ? RDP(2PDP) ?1 PDR = (2 ? ?)P ? 1 2 (RDR) (2 ? ?)P ? g 2 (RR) 0</formula><p>We continue by setting P = ? 2 RR with ? 2 = g 2(2??) , so that the above inequality is satisfied. At this point, we have shown that if W can be written as: <ref type="formula" target="#formula_0">(1)</ref> is contracting in metric M = ? 2 RR. What remains to be shown is that if the condition:</p><formula xml:id="formula_32">W = R ? ? 2 RR then</formula><formula xml:id="formula_33">gW ? I ? 0</formula><p>Is satisfied, then this implies the existence of an R such that the above is true. To show that this is indeed the case, assume that: 1 4? 2 I ? W 0 Substituting in the definition of ?, this is just the statement that:</p><formula xml:id="formula_34">2(2 ? ?) 4g I ? W 0</formula><p>Setting ? = 2? &gt; 0, this yields:</p><p>(1 ? ?)I gW Since W is symmetric by assumption, we have the eigendecomposition:</p><formula xml:id="formula_35">1 4? 2 I ? W = V( 1 4? 2 I ? ?)V T</formula><p>where V T V = I and ? is a diagonal matrix containing the eigenvalues of W. Denote the symmetric square-root of this expression as S:</p><formula xml:id="formula_36">S = V ( 1 4? 2 I ? ?)V T = S T Which implies that: 1 4? 2 I ? W = S T S</formula><p>We now define R in terms of S as follows:</p><formula xml:id="formula_37">R = 1 ? S + 1 2? 2 I Which means that: 1 4? 2 I ? W = (?R ? 1 2? I)(?R ? 1 2? I)</formula><p>Expanding out the right side, we get:</p><formula xml:id="formula_38">1 4? 2 I ? W = ? 2 RR ? R + 1 4? 2 I Subtracting 1</formula><p>4? 2 I from both sides yields:</p><formula xml:id="formula_39">W = R ? ? 2 RR</formula><p>As desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.4 Proof of Theorem 3</head><p>Theorem 3. If there exists positive diagonal matrices P 1 and P 2 , as well as Q = Q T 0 such that W = ?P 1 QP 2 then (1) is contracting in metric M = (P 1 QP 1 ) ?1 .</p><p>Proof. Consider again a differential Lyapunov function:</p><formula xml:id="formula_40">V = ?x T M?x the time derivative is equal to:V = ?2V + ?x T MWD?x</formula><p>Substituting in the definitions of W and M, we get:</p><formula xml:id="formula_41">V = ?2V ? ?x T P ?1 1 P 2 D?x ? ?2V Therefore V converges exponentially to zero.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.5 Proof of Theorem 4</head><p>Theorem 4. If gW ? I is triangular and Hurwitz, then (1) is contracting in a diagonal metric.</p><p>Note that in the case of a triangular weight matrix, the system (1) may be seen as a feedforward (i.e hierarchical) network. Therefore, this result follows from the combination properties of contracting systems. However, our proof provides a means of explicitly finding a metric for this system.</p><p>Proof. Without loss of generality, assume that W is lower triangular. This implies that W ij = 0 if i ? j. Now consider the generalized Jacobian:</p><formula xml:id="formula_42">F = ?I + ?WD? ?1 with ? diagonal and ? i = i where &gt; 0.</formula><p>Because ? is diagonal, the generalized Jacobian is equal to:</p><formula xml:id="formula_43">F = ?I + ?W? ?1 D Now note that: (?W? ?1 ) ij = i W ij ?j = W ij i?j</formula><p>Where i ? j, we have W ij = 0 by assumption. Therefore, the only nonzero entries are where i ? j. This means that by making arbitrarily small, we can make ?W? ?1 approach a diagonal matrix with W ii along the diagonal. Therefore, if:</p><formula xml:id="formula_44">max i gW ii ? 1 &lt; 0</formula><p>the nonlinear system is contracting. Since W is triangular, W ii are the eigenvalues of W, meaning that this condition is equivalent to gW ? I being Hurwitz.</p><p>A3.6 Proof of Theorem 5</p><p>Theorem 5. If there exists a positive diagonal matrix P such that:</p><formula xml:id="formula_45">g 2 W T PW ? P ? 0</formula><p>then <ref type="formula" target="#formula_0">(1)</ref> is contracting in metric P.</p><p>Note that this is equivalent to the discrete-time diagonal stability condition developed in , for a constant metric. Note also that when M = I, Theorem 5 is identical to checking the maximum singular value of W, a previously established condition for stability of (1). However a much larger set of weight matrices are found via the condition when M = P instead.</p><p>Proof. Consider the generalized Jacobian:</p><formula xml:id="formula_46">F = P 1/2 JP ?1/2 = ?I + P 1/2 WP ?1/2 D where D is a diagonal matrix with D ii = d?i dxi ? 0.</formula><p>Using the subadditivity of the matrix measure ? 2 of the generalized Jacobian we get:</p><formula xml:id="formula_47">? 2 (F) ? ?1 + ? 2 (P 1/2 WP ?1/2 D)</formula><p>Now using the fact that ? 2 (?) ? || ? || 2 we have:</p><formula xml:id="formula_48">? 2 (F) ? ?1 + ||P 1/2 WP ?1/2 D)|| 2 ? ?1 + g||P 1/2 WP ?1/2 || 2</formula><p>Using the definition of the 2-norm, imposing the condition ? 2 (F) ? 0 may be written:</p><formula xml:id="formula_49">g 2 W T PW ? P ? 0</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.7 Proof of Theorem 6</head><p>Theorem 6. Let D be a positive, diagonal matrix with D ii = d?i dxi , and let P be an arbitrary, positive diagonal matrix. If:</p><formula xml:id="formula_50">(gW ? I)P + P(gW T ? I) ?cP and? ? cg ?1 D ??D</formula><p>for c, ? &gt; 0, then (1) is contracting in metric D with rate ?.</p><p>Proof. Consider the differential, quadratic Lyapunov function:</p><formula xml:id="formula_51">V = ?x T PD?x</formula><p>where D 0 is as defined above. The time derivative of V is:</p><formula xml:id="formula_52">V = ?x T P??x + ?x T (?2PD + PDWD + DW T DP)?x</formula><p>The second term on the right can be factored as:</p><formula xml:id="formula_53">?x T (?2PD + PDWD + DW T DP)?x = ?x T D(?2PD ?1 + PW + W T P)D?x ? ?x T D(?2Pg ?1 + PW + W T P)D?x = ?x T D[P(W ? g ?1 I) + (W T ? g ?1 I)P]D?x ? ?cg ?1 ?x T PD 2 ?x</formula><p>where the last inequality was obtained by substituting in the first assumption above. Combining this with the expression forV , we have:</p><formula xml:id="formula_54">V ? ?x T P??x ? cg ?1 ?x T PD 2 ?x</formula><p>Substituting in the second assumption, we have:</p><formula xml:id="formula_55">V ? ?x T P(? ? cg ?1 D 2 )?x ? ???x T PD?x = ??V</formula><p>and thus V converges exponentially to 0 with rate ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.8 Proof of Theorem 7</head><p>Theorem 7. Satisfaction of the condition</p><formula xml:id="formula_56">gW sym ? I ? 0</formula><p>is NOT sufficient to show global contraction of the general nonlinear RNN (1) in any constant metric. High levels of antisymmetry in W can make it impossible to find such a metric, which we demonstrate via a 2 ? 2 counterexample of the form</p><formula xml:id="formula_57">W = 0 ?c c 0 with c ? 2.</formula><p>Note that gW sym ? I = g W+W T 2 ? I ? 0 is equivalent to the condition for contraction of the system with linear activation in the identity metric.</p><p>The main intuition behind this counterexample is that high levels of antisymmetry can prevent a constant metric from being found in the nonlinear system. This is because D is a diagonal matrix with values between 0 and 1, so the primary functionality it can have in the symmetric part of the Jacobian is to downweight the outputs of certain neurons selectively. In the extreme case of all 0 or 1 values, we can think of this as selecting a subnetwork of the original network, and taking each of the remaining neurons to be single unit systems receiving input from the subnetwork. For a given static configuration of D (think linear gains), this is a hierarchical system that will be stable if the subnetwork is stable. But as D can evolve over time when a nonlinearity is introduced, we would need to find a constant metric that can serve completely distinct hierarchical structures simultaneously -which is not always possible.</p><p>Put in terms of matrix algebra, D can zero out columns of W, but not their corresponding rows. So for a given weight pair w ij , w ji , which has entry in W sym = wij +wji 2 , if D i = 0 and D j = 1, the i, j entry in (WD) sym will be guaranteed to have lower magnitude if the signs of w ij and w ji are the same, but guaranteed to have higher magnitude if the signs are different. Thus if the linear system would be stable based on magnitudes alone D poses no real threat, but if the linear system requires antisymmetry to be stable, D can make proving contraction quite complicated (if possible at all).</p><p>Proof. The nonlinear system is globally contracting in a constant metric if there exists a symmetric, positive definite M such that the symmetric part of the Jacobian for the system, (MWD) sym ? M is negative definite uniformly. Therefore (MWD) sym ? M ? 0 must hold for all possible D if M is a constant metric the system globally contracts in with any allowed activation function, as some combination of settings to obtain a particular D can always be found.</p><p>Thus to prove the main claim, we present here a simple 2-neuron system that is contracting in the identity metric with linear activation function, but can be shown to have no M that simultaneously satisfies the (MWD) sym ? M ? 0 condition for two different possible D matrices.</p><p>To begin, take</p><formula xml:id="formula_58">W = 0 ?2 2 0</formula><p>Note that any off-diagonal magnitude ? 2 would work, as this is the point at which 1 2 of one of the weights (found in W sym when the other is zeroed) will have magnitude too large for (WD) sym ? I to be stable.</p><p>Looking at the linear system, we can see it is contracting in the identity because Working out the matrix multiplication, we get</p><formula xml:id="formula_59">(MWD 1 ) sym ? M = 2m ? a b ? m b ? m ?b and (MWD 2 ) sym ? M = ?a ?(a + m) ?(a + m) ?2m ? b</formula><p>We can now check necessary conditions for negative definiteness on these two matrices, as well as for positive definiteness on M, to try to find an M that will satisfy all these conditions simultaneously. In this process we will reach a contradiction, showing that no such M can exist.</p><p>A necessary condition for positive definiteness in a real, symmetric n ? n matrix X is x ii &gt; 0, and for negative definiteness x ii &lt; 0. Another well known necessary condition for definiteness of a real symmetric matrix is |x ii + x jj | &gt; |x ij + x ji | = 2|x ij | ?i = j. See <ref type="bibr">[Weisstein]</ref> for more info on these conditions. Thus we will require a and b to be positive, and can identify the following conditions as necessary for our 3 matrices to all meet the requisite definiteness conditions:</p><formula xml:id="formula_60">2m &lt; a (11) ?2m &lt; b (12) |2m ? (a + b)| &gt; 2|b ? m| (13) | ? 2m ? (a + b)| &gt; 2|a + m|<label>(14)</label></formula><p>Note that the necessary condition for M to be PD, a + b &gt; 2|m|, is not listed, as it is automatically satisfied if <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref> are.</p><p>It is easy to see that if m = 0, conditions <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(14)</ref> will result in the contradictory conditions a &gt; b and b &gt; a respectively, so we will require a metric with off-diagonal elements. To make the absolute values easier to deal with, we will check m &gt; 0 and m &lt; 0 cases independently.</p><p>First we take m &gt; 0. By condition (11) we must have a &gt; 2m, so between that and knowing the signs of all unknowns are positive, we can reduce many of the absolute values. Condition (13) becomes a + b ? 2m &gt; |2b ? 2m|, and condition (14) becomes a + b + 2m &gt; 2a + 2m, which is equivalent to b &gt; a. If b &gt; a we must also have b &gt; m, so condition (13) further reduces to a + b ? 2m &gt; 2b ? 2m, which is equivalent to a &gt; b. Therefore we have again reached contradictory conditions.</p><p>A very similar approach can be applied when m &lt; 0. Using condition (12) and the known signs we reduce condition <ref type="formula" target="#formula_0">(13)</ref>  Therefore it is impossible for a single constant M to accommodate both D 1 and D 2 , so that no constant metric can exist for W to be contracting in when a nonlinearity is introduced that can possibly have derivative reaching both of these configurations. One real world example of such a nonlinearity is ReLU. Given a sufficiently high negative input to one of the units and a sufficiently high positive input to the other, D can reach one of these configurations. The targeted inputs could then flip at any time to reach the other configuration.</p><p>An additional condition we could impose on the activation function is to require it to be a strictly increasing function, so that the activation function derivative can never actually reach 0. We will now show that a very similar counterexample applies in this case, by taking D 1 * = 1 0 0 and D 2 * = 0 0 1 Note here that the W used above produced a (WD) sym ? I that just barely avoided being negative definite with the original D 1 and D 2 , so we will have to increase the values on the off-diagonals a bit for this next example. In fact anything with magnitude larger than 2 will have some &gt; 0 that will cause a constant metric to be impossible, but for simplicity we will now take W * = 0 ?4 4 0</p><p>Note that with W * , even just halving one of the off-diagonals while keeping the other intact will produce a (WD) sym ? I that is not negative definite. Anything less than halving however will keep the identity metric valid. Therefore, we expect that taking in D 1 * and D 2 * to be in the range 0.5 ? &gt; 0 will also cause issues when trying to obtain a constant metric.</p><p>We will now actually show via a similar proof to the above that M is impossible to find for W * when ? 0.5. This result is compelling because it not only shows that does not need to be a particularly small value, but it also drives home the point about antisymmetry -the larger in magnitude the antisymmetric weights are, the larger the where we will begin to encounter problems.</p><p>Working out the matrix multiplication again, we now get</p><formula xml:id="formula_61">(MW * D 1 * ) sym ? M = 4m ? a 2b ? m ? 2a b ? m ? 2a ?4m ? b and (MW * D 2 * ) sym ? M = 4m ? a ?(2a + m ? 2b ) ?(2a + m ? 2b ) ?4m ? b</formula><p>Resulting in two new main necessary conditions:</p><formula xml:id="formula_62">|4m ? a ? b ? 4m | &gt; 2|2b ? m ? 2a | (15) |4m ? a ? b ? 4m| &gt; 2|2a + m ? 2b |<label>(16)</label></formula><p>As well as new conditions on the diagonal elements:</p><formula xml:id="formula_63">4m ? a &lt; 0 (17) ?4m ? b &lt; 0<label>(18)</label></formula><p>We will now proceed with trying to find a, b, m that can simultaneously meet all conditions, setting = 0.5 for simplicity.</p><p>Looking at m = 0, we can see again that M will require off-diagonal elements, as condition <ref type="formula" target="#formula_0">(15)</ref> is now equivalent to the condition a + b &gt; |4b ? 2a| and condition <ref type="formula" target="#formula_0">(16)</ref> is similarly now equivalent to a + b &gt; |4a ? 2b|.</p><p>Evaluating these conditions in more detail, if we assume 4b &gt; 2a and 4a &gt; 2b, we can remove the absolute value and the conditions work out to the contradicting 3a &gt; 3b and 3b &gt; 3a respectively. As an aside, if &gt; 0.5, this would no longer be the case, whereas with &lt; 0.5, the conditions would be pushed even further in opposite directions.</p><p>If we instead assume 2a &gt; 4b, this means 4a &gt; 2b, so the latter condition would still lead to b &gt; a, contradicting the original assumption of 2a &gt; 4b. 2b &gt; 4a causes a contradiction analogously. Trying 4b = 2a will lead to the other condition becoming b &gt; 2a, once again a contradiction. Thus a diagonal M is impossible So now we again break down the conditions into m &gt; 0 and m &lt; 0 cases, first looking at m &gt; 0.</p><p>Using condition (17) and knowing all unknowns have positive sign, condition (15) reduces to a + b ? 2m &gt; |4b ? 2(a + m)| and condition (16) reduces to a + b + 2m &gt; |4a ? 2(b ? m)|. This looks remarkably similar to the m = 0 case, except now condition (15) has ?2m added to both sides (inside the absolute value), and condition (16) has 2m added to both sides in the same manner. If 4b &gt; 2(a + m) the ?2m term on each side will simply cancel, and similarly if 4a &gt; 2(b ? m) the +2m terms will cancel, leaving us with the same contradictory conditions as before.</p><p>Therefore we check 2(a + m) &gt; 4b. This rearranges to 2a &gt; 2(2b ? m) &gt; 2(b ? m), so that from condition (16) we get b &gt; a. Subbing condition (17) in to 2(a + m) &gt; 4b gives 8b &lt; 4a + 4m &lt; 5a i.e. b &lt; 5 8 a, a contradiction. The analogous issue arises if trying 2(b ? m) &gt; 4a. Trying 2(a + m) = 4b gives m = 2b ? a, which in condition (16) results in 5b ? a &gt; |6a ? 6b|, while in condition (17) leads to 5a &gt; 8b, so (16) can further reduce to 5b ? a &gt; 6a ? 6b i.e. 11b &gt; 7a. But b &gt; 7 11 a and b &lt; 5 8 a is a contradiction. Thus there is no way for m &gt; 0 to work.</p><p>Finally, trying m &lt; 0, we now use condition (18) and the signs of the unknowns to reduce condition (15) to a + b + 2|m| &gt; |4b ? 2(a ? |m|)| and condition (16) to a + b ? 2|m| &gt; |4a ? 2(b + |m|)|. These two conditions are clearly directly analogous to in the m &gt; 0 case, where b now acts as a with condition (18) being b &gt; 4|m|. Therefore the proof is complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4 Sparse Combo Net Details</head><p>Here we provide comprehensive information on the methodology and results for Sparse Combo Net, including some supplementary experimental results. See the Appendix table of contents for a guide to this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1 Extended Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1.1 Initialization and Training</head><p>As described in the main text, the nonlinear RNN weights for Sparse Combo Net were randomly generated based on given sparsity and entry magnitude settings, and then confirmed to meet the Theorem 1 condition (or discarded if not). For a sparsity level of x and a magnitude limit of y, each subnetwork W was generated by drawing uniformly from between ?y and y with x% density using scipy.sparse.random, and then zeroing out the diagonal entries. For various potential x and y settings, we quantified both the likelihood that a generated W would satisfy Theorem 1, and the resulting network performance. Of course this is also dependent on subnetwork size, as larger subnetworks enable greater sparsity. The information we have obtained so far is documented in Section A4.2, in particular A4.2.2.</p><p>In training the linear connections between the described nonlinear RNN subnetworks, we constrained the matrix B in (3) to reflect underlying modularity assumptions. In particular, we only train the off-diagonal blocks of B and mask the diagonal blocks. We do this to maintain the interpretation of L as the matrix containing the connection weights between different modules, as diagonal blocks would correspond to self-connections. Furthermore, we only train the lower-triangular blocks of B while masking the others, to increase training speed.</p><p>To obtain the subnetwork RNN metrics necessary for training these linear connections, scipy.integrate.quad was used with default settings to solve for M in the equation ?I = MW + W T M, as described in the main text. This was done by integrating e W T t Qe Wt dt from 0 to ?. For efficiency reasons, and due to the guaranteed existence of a diagonal metric in the case of Theorem 1, integration was only performed to solve for the diagonal elements of M. Therefore a check was added prior to training to confirm that the initialized network indeed satisfied Theorem 1 with metric M. However, it was never triggered by our initialization method.</p><p>Initial training hyperparameter tuning was done primarily with 10 ? 16 combination networks on the permuted seqMNIST task, starting with settings based on existing literature on this task, and verifying promising settings using a 15 ? 16 network <ref type="table">(Table S2</ref>). Initialization settings were held the same throughout, as was later done for the size comparison trials (described in Section A4.2.1).</p><p>Once hyperparameters were decided upon, the trials reported on in the main text began. Most of these experiments were also done on permuted seqMNIST, where we characterized performance of networks with different sizes and sparsity levels/entry magnitudes. When we moved to the sequential CIFAR10 task, we began by simply training with the same best settings that were found from these experiments. The results of all attempted trials are reported in Section A4.4.</p><p>Unless specified otherwise, all networks reported on in the main text were trained for 150 epochs, using an Adam optimizer with initial learning rate 1e-3 and weight decay 1e-5. The learning rate was cut to 1e-4 after 90 epochs and to 1e-5 after 140. After identifying the most promising settings, we ran repetitions trials on the best networks for 200 epochs with learning rate cuts after epochs 140 and 190.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1.2 Code and Datasets</head><p>All Sparse Combo Nets described in the main text were trained using a single GPU on Google Colab. Code to replicate all experiments can be found here: https://colab.research.google.com/ drive/1JCT5OMgaMVK_Xh8BDFNRrEsyF0Ojvg10?usp=sharing Runtime for the best performing architecture settings on the sequential CIFAR10 task was ? 24 hours. A Colab Pro+ account was used to limit timeouts and prioritize GPU access.</p><p>The datasets we used for our tasks were MNIST and CIFAR10, downloaded via PyTorch. MNIST is a handwritten digits classification task, consisting of 60,000 training images and 10,000 testing images (each 28x28 and grayscale). It is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license. CIFAR10 is a dataset of 32x32 color images, split evenly among the following 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. It also contains 60,000/10,000 training/test images, and is distributed under the MIT License.</p><p>As mentioned in the main text, we presented these images to our networks pixel by pixel. In the case of MNIST, we used an additional modification to the dataset by permuting the pixels in a randomly determined (but fixed across the dataset) way. The use of these datasets is included in the above link to our code.</p><p>Extended results information begins on the next page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2 Extended Results</head><p>In this subsection, we describe additional results we did not get to in the main text, related to scalability, modularity, sparsity, and repeatability. We also add some further discussion of these results, along with more detailed information on the respective experimental set ups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2.1 Network Size and Modularity Comparison</head><p>For the Sparse Combo Net specifically we had additional experiments on architecture size and unit distribution besides what was depicted in the main text. The results of these supplemental experiments both replicated the observed effect in <ref type="figure" target="#fig_2">Figure 4</ref> of network size instead using subnetworks with 16 units each as well as higher density ( <ref type="figure">Figure S1A</ref>), and evaluated how task performance varies with modularity of a network fixed to have 352 total units ( <ref type="figure">Figure S1B</ref>). In the modularity experiment we observed an inverse U shape, with poor performance of a 1 ? 352 net and an 88 ? 4 net, and best performance from a 44 ? 8 net. Note that this experiment compared similar sparsity levels across the different subnetwork sizes. In practice we can achieve better performance with larger subnetworks by leveraging sparsity in a way not possible in smaller subnetworks. These additional experiments are now described in more detail below.</p><p>For the initial round of size comparison trials using subnetworks of 16 units each ( <ref type="figure">Figure S1A</ref>), the nonlinear RNN weights were set by drawing uniformly from between ?0.4 and 0.4 with 40% density using scipy.sparse.random, and then zeroing out the diagonal entries. These settings were chosen because they resulted in ? 1% of 16 by 16 weight matrices meeting the Theorem 1 condition. During initialization only the matrices meeting this condition were kept, finishing when the desired number of component RNNs had been set -producing a block diagonal W like pictured in <ref type="figure" target="#fig_1">Figure 3A</ref>. This same initialization process was used throughout our experiments. In later experiments we vary the density and magnitude settings.</p><p>For the size experiments, we held static the number of units and initialization settings for each component RNN, and tested the effect of changing the number of components in the combination network. <ref type="bibr">1,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">15,</ref><ref type="bibr">20,</ref><ref type="bibr">22,</ref><ref type="bibr">25</ref>, and 30 components were tested in this experiment ( <ref type="figure">Figure  S1A</ref>). Increasing the number of components initially lead to great improvements in test accuracy, but had diminishing returns -test accuracy consistently hit ? 93% with a large enough number of subnetworks, but neither loss nor accuracy showed meaningful improvement past the 22 ? 16 network. Interestingly, early training loss and accuracy became substantially worse once the number of components increased past a certain point, falling from 70% to 43% epoch 1 test accuracy between the 22 ? 16 and 30 ? 16 networks. The complete set of results can be found in <ref type="table">Table S3</ref>.</p><p>Note that the size experiment described in the main text ( <ref type="figure" target="#fig_2">Figure 4A</ref>) was a repetition of this original experiment, but now using 32 unit subnetworks with the best performing sparsity settings. The results for the repetition can be found in <ref type="table" target="#tab_8">Table S5</ref>.</p><p>To better understand how the modularity of the combination networks affects performance, the next experiment held the number of total units constant at 352, selected due to the prior success of the 22 ? 16 network, and tested different allocations of these units amongst component RNNs. Thus 1 ? 352, 11 ? 32, 44 ? 8, and 88 ? 4 networks were trained to compare against the 22 ? 16 ( <ref type="figure">Figure  S1B</ref>). Increasing the modularity improved performance to a point, with the 44 ? 8 network resulting in final test accuracy of 94.44%, while conversely the 11 ? 32 resulted in decreased test accuracy. However, the 88 ? 4 network was unable to learn, and a 352 ? 1 network would theoretically just be a scaled linear anti-symmetric network.</p><p>Because larger networks require different sparsity settings to meet the Theorem 1 condition, these were not held constant between trials in the modularity comparison experiment ( <ref type="figure">Figure S1B</ref>), but rather selected in the same way between trials -looking for settings that keep density and scalar balanced and result in ? 1% of the matrices meeting the condition. The scalar was applied after sampling non-zero entries from a uniform distribution between -1 and 1. The resulting settings were 7.5% density and 0.077 scalar for 352 unit component RNN, 26.5% density and 0.27 scalar for 32 unit component RNN, 60% density and 0.7 scalar for 8 unit component RNN, and 100% density and 1.0 scalar for 4 unit component RNN. The complete set of results for the modularity experiment can be found in <ref type="table">Table S4</ref>.</p><p>Figure S1: Performance of Sparse Combo Nets on the Permuted seqMNIST task by combination network size. We test the effects on final and first epoch test accuracy of both total network size and network modularity. The former is assessed by varying the number of subnetworks while each subnetwork is fixed at 16 units (A), and the latter by varying the distribution of units across different numbers of subnetworks with the total sum of units in the network fixed at 352 (B). Note that these experiments were run prior to optimizing the sparsity initialization settings. Experiments on total network size were later repeated with the final sparsity settings ( <ref type="figure" target="#fig_2">Figure 4A</ref>). The results of both the size experiments are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2.2 Sparsity Settings Comparison</head><p>Density and scalar settings for the component nonlinear RNNs were initially chosen for each network size using the percentage of random networks that met the Theorem 1 condition. For scalar s, a component network would have non-zero entries sampled uniformly between ?s and s.</p><p>When we began experimenting with sparsity in the initialization (after seeing the large performance difference in <ref type="figure" target="#fig_3">Figure 5</ref>), we split the previously described scalar setting into two different scalarsone applied before a random matrix was checked against the Theorem 1 condition, and one applied after a matrix was selected. Of course the latter must be ? 1 to guarantee stability is preserved. The scalar was separated out after we noticed that at 5% density, random 32 by 32 weight matrices met the condition roughly 1% of the time whether the scalar was 10 or 100000 -? 85% of sampled matrices using scalar 10 would continue to meet the condition even if multiplied by a factor of 10000. Therefore we wanted a mechanism that could bias selection towards matrices that are stable due to their sparsity and not due to magnitude constraints, while still keeping the elements to a reasonable size for training purposes.</p><p>Ultimately, both sparsity and magnitude had a clear effect on performance ( <ref type="figure">Figure S2</ref>). Increases in both had a positive correlation with accuracy and loss through most parameters tested. Best test accuracy overall was 96.79%, which was obtained by both a 16 ? 32 network with 5% density and entries between -5 and 5, and a 16 ? 32 network with 3.3% density and entries between -6 and 6. The latter also achieved the best epoch 1 test accuracy observed of 86.79%. Thus we chose to go with these settings for our extended training repetitions on permuted seqMNIST.</p><p>It is also worth noting that upon investigation of the subnetwork weight matrices across these trials, the sparser networks had substantially lower maximum eigenvalue of |W|, suggesting that stronger stability can actually correlate with improved performance on sequential tasks. This could be due to a mechanism such as that described in <ref type="bibr" target="#b45">[Radhakrishnana et al., 2020]</ref>.</p><p>Results from the initial trials of different sparsity levels across different network sizes can be found in <ref type="table" target="#tab_10">Table S6</ref>. Results from the more thorough testing of different sparsity levels and magnitudes in a 16 ? 32 network can be found in <ref type="table">Table S7</ref>. <ref type="figure">Figure S2</ref>: Permuted seqMNIST performance by component RNN initialization settings. Test accuracy is plotted over the course of training for four 16 ? 32 networks with different density levels and entry magnitudes (A), highlighting the role of sparsity in network performance. Test accuracy is then plotted over the course of training for two 3.3% density 16 ? 32 networks with different entry magnitudes (B), to demonstrate the role of the scalar. When the magnitude becomes too high however, performance is out of view of the current axis limits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2.3 Repeatability Tests</head><p>Here we give additional details on the repeatability tests described in the main text, as well as introducing some additional results using other hyperparameters (done after the initial SOTA-setting experiments) on sequential CIFAR10.</p><p>To further improve performance once network settings were explored on permuted seqMNIST, an extended training run was tested on the best performing option. Settings were kept the same as above using a 3.3% density 16 ? 32 network, except training now ran for over 200 epochs, with just a single learning rate cut occurring after epoch 200 (exact number of epochs varied based on runtime limit). This experiment was repeated four times and resulted in 96.94% best test accuracy, as described in the main text ( <ref type="figure" target="#fig_1">Figure S3</ref>). <ref type="table" target="#tab_11">Table S8</ref> reports additional details on these trials.</p><p>As mentioned, we also characterized Sparse Combo Net performance on the more challenging CIFAR10 sequential image classification task, across a greater number of trials. Ten trials were run for 200 epochs with learning rate scaled after epochs 140 and 190. All other settings were the same as for the permuted seqMNIST repeatability trials. As reported in the main text, the mean test accuracy observed was 64.72%, with variance 0.406, and range 63.73%-65.72%. Training loss and test accuracy over the course of learning are plotted for all ten trials in <ref type="figure" target="#fig_2">Figure S4</ref>, and exact numbers for each trial are provided in <ref type="table" target="#tab_5">Table S10</ref>.</p><p>As we encountered difficulties with Colab GPU assignment when we started working on these repetitions, we also trained nine networks over a smaller number of epochs ( <ref type="figure" target="#fig_3">Figure S5</ref>). These networks were trained using the 150 epoch paradigm previously described, although only four of the nine completed training within the 24 hour runtime limit. Complete results for these trials can be  <ref type="table" target="#tab_11">Table S8</ref>.</p><p>found in <ref type="table" target="#tab_5">Table S11</ref>. Mean performance among the shorter training trials was 62.82% test accuracy with variance 0.95.</p><p>Prior to beginning the repeatability experiments on seqCIFAR10, we explored alternative hyperparameters on this task <ref type="table" target="#tab_12">(Table S9</ref>). While we ultimately ended up using the same hyperparameters as for permuted seqMNIST, these results further support the robustness of our architecture.  <ref type="table" target="#tab_5">Table  S10</ref>.</p><p>To complete our benchmarking table, we also ran a single 150 epoch trial of our best network settings on the sequential MNIST task. Test accuracy over the course of training for this trial is depicted in <ref type="figure">Figure S6</ref>. <ref type="figure" target="#fig_3">Figure S5</ref>:</p><p>seqCIFAR10 performance on repeated trials with shorter training (done to complete more trials). Nine different 16 ? 32 networks with 3.3% density and entries between -6 and 6 were set up to train for 150 epochs, with learning rate divided by 10 after epochs 90 and 140. Most of these networks hit runtime limit before completing, however they all got through at least 100 epochs and all had test accuracy exceed 61%. This figure depicts test accuracy for each of the networks over the course of training. Networks that completed training are plotted as solid lines, while those that were cut short are dashed. <ref type="figure">Figure S6</ref>: Performance over training on the seqM-NIST task for a 16 ? 32 network with best settings (using 150 epoch training protocol). Final test accuracy exceeded 99%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2.4 Scalability Pilot: Feedback Sparsity Tests</head><p>Not only did the 16 ? 32 Sparse Combo Net with 3.3% density achieve test accuracy sequential CIFAR10 that is the highest to date for a provably stable RNN, it was higher than the 1 million parameter CKConv network, which set a recent SOTA for permuted seqMNIST accuracy <ref type="table">(Table 1)</ref>. Our network has 130, 000 trainable parameters by comparison. Thus we achieve very impressive results given the characteristics of our architecture.</p><p>However, the results of the network size experiments do raise some concern about the scalability of the approach. Here we provide pilot results suggesting that this issue can likely be addressed via the introduction of sparsity in the linear inter-subnetwork connectivity matrix L. While all results in the main text use all-to-all negative feedback, we have early results suggesting that in larger networks, fewer negative feedback connections may perform better, thus preventing performance saturation with scaling.</p><p>Below are the results of testing of this idea in a 24 ? 32 Sparse Combo Net on the sequential CIFAR10 task. We varied the number of feedback connections that were fixed at 0 while all other settings remained static <ref type="table" target="#tab_5">(Table S1</ref>). The resulting performance took an inverse U shape, where the network with only 50% of possible feedback connections non-zero had the best test accuracy of 65.14%, achieved in just 124 epochs of training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.3 Architecture Interpretability</head><p>In this subsection, we give additional information on trainable parameters and properties of the network weights, to assist in interpreting the Sparse Combo Net architecture and its training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.3.1 Number of Parameters</head><p>To report on the number of trainable parameters, we used the following formula:</p><formula xml:id="formula_64">n 2 ?M * C 2 2 + i * n + n * o + n + o</formula><p>Where n is the total number of units in the M ? C combination network, o is the total number of output nodes for the task, and i is the total number of input nodes for the task. Thus for the 16 ? 32 networks highlighted here, we have 129034 trainable parameters for the MNIST tasks, and 130058 trainable parameters for sequential CIFAR10.</p><p>Note that the naive estimate for the number of trainable parameters would be n 2 + i * n + n * o + n + o, corresponding to the number of weights in L, the number of weights in the feedforward linear input layer, the number of weights in the feedforward linear output layer, and the bias terms for the input and output layers, respectively. However, because of the combination property constraints on L, only the lower triangular portion of a block off-diagonal matrix is actually trained, and L is then defined in terms of this matrix and the metric M. Thus we subtract M * C 2 to remove the block diagonal portions corresponding to nonlinear RNN components, and then divide by 2 to obtain only the lower half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.3.2 Inspecting Trained Network Weights</head><p>After training was completed, we inspected the state of all networks described in the main text, pulling both the nonlinear (W) and linear (L) weight matrices from both initialization time and the final model. For W, we confirmed it did not change over training, and inspected the max real part of the eigenvalues of |W| in accordance with Theorem 1. The densest tested matrices tended to have ? max (|W|) &gt; 0.9, while the sparsest ones tended to have ? max (|W|) &lt; 0.1. For L, we checked the maximum element and the maximum singular value before and after training. In general, both went up over the course of training, but by a modest amount.  <ref type="table">Table S2</ref> shows all trials run on permuted sequential MNIST before beginning the more systematic experiments reported on in the main text. Notably, our networks did not require an extensive hyperparameter tuning process.</p><p>Tables S3 and S4 report additional details on the initial size and modularity experiments ( <ref type="figure">Figure S1</ref>). <ref type="table" target="#tab_8">Table S5</ref> reports the results of the repeated experiment on Sparse Combo Net size, this time using 32 unit component subnetworks with best sparsity settings ( <ref type="figure" target="#fig_2">Figure 4A)</ref>. <ref type="table" target="#tab_10">Tables S6 and S7</ref> report results from all trials related to our sparsity experiments ( <ref type="figure" target="#fig_3">Figures 5 and S2)</ref>. <ref type="table" target="#tab_11">Table S8</ref> provides further information on the four trials in the permuted seqMNIST repeatability experiments ( <ref type="figure" target="#fig_1">Figure S3</ref>). <ref type="table" target="#tab_12">Table S9</ref> reports the results of all trials of different hyperparameters on the sequential CIFAR10 task, in chronological order. Ultimately the same settings as those used for permuted seqMNIST were chosen. <ref type="table" target="#tab_5">Table S10</ref> shows the results of all ten trials in the seqCIFAR10 repeatability experiments ( <ref type="figure" target="#fig_2">Figure S4</ref>). <ref type="table" target="#tab_5">Table S11</ref> shows results from nine additional seqCIFAR10 trials of shorter training duration ( <ref type="figure" target="#fig_3">Figure  S5</ref>), run to increase sample size while unable to access the higher quality Colab GPUs.</p><p>Finally, <ref type="table" target="#tab_5">Table S1</ref> reports the results of our pilot trial on introducing sparsity into the linear feedback connection matrix -as this    <ref type="table">Table S3</ref>. All trials were run to completion. A control trial was also run with the largest tested network size -the connections between subnetworks were no longer constrained, and thus this control combination network is not certifiably stable.   <ref type="table">Table S7</ref>: Further optimizing the sparsity settings for high performance using a 16 ? 32 network. The final scalar is the product of the pre-selection and post-selection scalars. Note that the 5% density and 7.5 scalar network was killed after 18 epochs due to exploding gradient. All other trials ran for a full 150 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epochs</head><p>Best Test Acc.</p><p>Epoch 1 Test Acc.    <ref type="table" target="#tab_5">Table S11</ref>: An additional nine trials investigating the repeatability of our results on the se-qCIFAR10 task. For these trials we used the same 150 epoch training paradigm as previously, although only four of the networks were able to fully complete training. These trials were done to expand our sample size while access was limited to only slower GPUs. The mean observed test accuracy among the shorter trials was 62.82%, with variance of 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5 SVD Combo Net Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5.1 Parameterization Information</head><p>For the SVD Combo Net, we ensured contraction by directly parameterizing each of the W i (i = 1, 2 . . . p) as:</p><formula xml:id="formula_65">W i = ? ?1 i U i ? i V T i ? i<label>(19)</label></formula><p>where ? i is diagonal and nonsingular, U i and V i are orthogonal, and ? i is diagonal with ? ii ? [0, g ?1 ). We ensure orthogonality of U i and V i during training by exploiting the fact that the matrix exponential of a skew-symmetric matrix is orthogonal, as was done in <ref type="bibr" target="#b31">[Lezcano-Casado and Mart?nez-Rubio, 2019]</ref>. The network constructed from these subnetworks using <ref type="formula" target="#formula_1">(2)</ref> is contracting in metricM = BlockDiag(? 2 1 , . . . , ? 2 p ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5.2 Control Experiment</head><p>To do the SVD Combo Network stability control experiment, we still constrained the weights of the subnetwork modules using the above parameterization, but we removed any constraint on the connections between modules. Thus each individual module was still guaranteed to be contracting, but the overall system had no such guarantee in this control trial. The experiment was run using a 24 ? 32 combination network and the permuted sequential MNIST task.</p><p>In contrast to the primary control experiment run on Sparse Combo Net, for the SVD Combo Net we saw only a very slight performance decrease when the between-module connections were no longer constrained for the overall stability certificate. Test accuracy in the control run was 94.56%, as compared to the 94.9% observed with the standard SVD Combo Net.</p><p>This disparity in performance decrease makes sense when considering that the hidden-to-hidden weights of SVD Combo Network are trainable, while those of the Sparse Combo Network are not.</p><p>Whatever instabilities are introduced by the lack of constraint on the inter-subnetwork connections likely cannot be adequately compensated for in the Sparse Combo Network. <ref type="figure">Figure S8</ref>: Pytorch Lightning code for SVD Combo Net cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5.3 Model Code</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Example 3x16 Sparse Combo Net. Nonlinear intra-subnetwork weights are initialized using a set sparsity, and do not change in training (A). Linear inter-subnetwork connections are constrained to be antisymmetric with respect to the overall network metric, and are updated in training (B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Permuted seqMNIST performance plotted against the number of subnetworks. Each subnetwork has 32 neurons. Results are shown for both Sparse Combo Net and SVD Combo Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Permuted seqMNIST performance over the course of training for two 11 ? 32 Sparse Combo Nets with different sparsity levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Now consider (MWD) sym ? M with D taking two possible values of We want to find some symmetric, positive definite M = a m m b such that (MWD 1 ) sym ? M and (MWD 2 ) sym ? M are both negative definite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>to 2|m| + a + b &gt; 2b + 2|m|, i.e. a &gt; b. Meanwhile condition (14) works out to a + b ? 2|m| &gt; 2a ? 2|m|, i.e. b &gt; a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure S7 :</head><label>S7</label><figDesc>Weight matrices for each of the 32 unit nonlinear component RNNs that were used in the best performing 16 ? 32 network on permuted sequential MNIST.A4.4 Tables of Results by Trial (SupplementaryDocumentation)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Supplementary Section A2.1 (c) Did you discuss any potential negative societal impacts of your work? [No] There are no unique potential negative societal impacts of this work (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Our code is linked in Supplementary Section A4.1.2, and can be run directly on Google Colab. We used only extremely common benchmark image datasets in our experiments, which can be utilized directly via the provided Colab link. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] All initialization and training details are reported in Supplementary Section A4.1.1. The results for all hyperparameter tuning trials are included in Supplementary Section A4.4 (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Where applicable in the main text, we reported mean, min, max, and variance of test accuracy across our repeated trials (see Section 4.2.3 in particular). We also provide extended information on repeatability tests in Supplementary Section A4.2.3, and tables that report the results of each individual trial we ran in Supplementary Section A4.4 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See information on the code in Supplementary Section A4.1.2 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We cite the article that introduced the sequential image classification tasks when introducing them in Section 4 (b) Did you mention the license of the assets? [Yes] We include the licenses for MNIST and CIFAR10 datasets as part of Supplementary Section A4.1.2 (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]Our code is linked in Supplementary Section A4.1.2 (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating?[N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... Two Different RNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A1.2 Contraction Math . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A1.2.1 Feedback and Hierarchical Combinations . . . . . . . . . . . . . . . . . . 17 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A2.2 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Proof of Feedback Combination Property . . . . . . . . . . . . . . . . . . . . . . 19 A3.2 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A3.3 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 A3.4 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A3.5 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 A3.6 Proof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 A3.7 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 A3.8 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 A4 Sparse Combo Net Details 29A4.1 Extended Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 A4.1.1 Initialization and Training . . . . . . . . . . . . . . . . . . . . . . . . . . 29 A4.1.2 Code and Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 A4.2 Extended Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 A4.2.1 Network Size and Modularity Comparison . . . . . . . . . . . . . . . . . 31 A4.2.2 Sparsity Settings Comparison . . . . . . . . . . . . . . . . . . . . . . . . 32 A4.2.3 Repeatability Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 A4.2.4 Scalability Pilot: Feedback Sparsity Tests . . . . . . . . . . . . . . . . . . 35 A4.3 Architecture Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 A4.3.1 Number of Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 A4.3.2 Inspecting Trained Network Weights . . . . . . . . . . . . . . . . . . . . . 36 A4.4 Tables of Results by Trial (Supplementary Documentation) . . . . . . . . . . . . . 37 A4.4.1 Permuted seqMNIST Trials . . . . . . . . . . . . . . . . . . . . . . . . . 38 A4.4.2 seqCIFAR10 Trials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 Parameterization Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 A5.2 Control Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 A5.3 Model Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42</figDesc><table><row><cell>(a) Did you state the full set of assumptions of all theoretical results? [Yes] See mathemat-ical details within the paper, Section 2 and Section 3 (b) Did you include complete proofs of all theoretical results? [Yes] See Supplementary Section A3 3. If you ran experiments... (a) (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] Appendix A1 Extended Background 17 A1.1 A2 Extended Discussion 18 A2.1 A3 Proofs for Main Results 19 A3.1 A5 SVD Combo Net Details 41 A5.1 A1 Extended Background A1.1 Two Different RNNs</cell></row><row><cell>(c) Did you include the estimated hourly wage paid to participants and the total amount</cell></row><row><cell>spent on participant compensation? [N/A]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Figure S4: seqCIFAR10 performance on repeated trials. Ten different 16 ? 32 networks with 3.3% density and entries between -6 and 6 were trained for 200 epochs, with learning rate divided by 10 after epochs 140 and 190. (A) depicts test accuracy for each of the networks over the course of training. (B) depicts the training loss for the same networks. Exact numbers are reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S1 :</head><label>S1</label><figDesc>Results from pilot testing on the sparsity of negative feedback connections in a 24 ? 32 Sparse Combo Net and a 16 ? 32 Sparse Combo Net. Feedback Density refers to the percentage of possible subnetwork pairings that were trained in negative feedback, while the remaining internetwork connections were held at 0. All networks were trained with the same 150 epoch training paradigm as mentioned in the main text, but were stopped after hitting a 24 hour runtime limit. Decreasing Feedback Density is a promising path towards further improving performance as the size of Sparse Combo Nets is scaled. The ideal amount of feedback density will likely vary with the size of the combination network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>table was presented in Section A4.2.4 however, we do not reproduce it here.</figDesc><table><row><cell cols="3">A4.4.1 Permuted seqMNIST Trials</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>Epochs</cell><cell>Adam WD</cell><cell>Initial LR</cell><cell>LR Schedule</cell><cell>Final Test Acc.</cell></row><row><cell cols="2">10?16 150</cell><cell>5e-5</cell><cell>5e-3</cell><cell>0.1 after 91</cell><cell>84%</cell></row><row><cell cols="2">10?16 150</cell><cell>1e-5</cell><cell>1e-2</cell><cell>0.1 after 50,100</cell><cell>85%</cell></row><row><cell cols="2">15?16 150</cell><cell>2e-4</cell><cell>5e-3</cell><cell>0.1 after 50,100</cell><cell>84%</cell></row><row><cell cols="2">10?16 150</cell><cell>2e-4</cell><cell>1e-2</cell><cell>0.5 every 10</cell><cell>81%</cell></row><row><cell cols="2">10?16 200</cell><cell>2e-4</cell><cell>1e-2</cell><cell>0.5 after 10 then every 30</cell><cell>81%</cell></row><row><cell cols="2">10?16 171*</cell><cell>5e-5</cell><cell>1e-2</cell><cell>0.75 after 10,20,60,100 then every 15</cell><cell>84%</cell></row><row><cell cols="2">15?16 179*</cell><cell>1e-5</cell><cell>1e-3</cell><cell>0.1 after 100,150</cell><cell>90%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S2 :Table S4 :</head><label>S2S4</label><figDesc>Training hyperparameter tuning trials, presented in chronological order. * indicates that training was cut short by the 24 hour Colab runtime limit. LR Schedule describes the scalar the learning rate was multiplied by, and at what epochs. The best performing network is highlighted, and represents the training settings we used throughout most of the main text. 86% 47.99% 0.7104 10?16 85.82% 61.38% 0.4736 15?16 90.28% 69.09% 0.3156 20?16 92.26% 71.72% 0.2392 22?16 93.01% 70.11% 0.2073 25?16 92.99% 61.81% 0.2017 30?16 93.16% 43.21% 0.1991 Table S3: Results for combination networks containing different numbers of component 16-unit RNNs. Training hyperparameters and network initialization settings were kept the same across all trials, and all trials completed the full 150 epochs. Results for different distributions of 352 total units across a combination network. This number was chosen based on prior 22 ? 16 network performance. For each component RNN size tested, the same procedure was used to select appropriate density and scalar settings. All networks otherwise used the same settings, as in the size experiments.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Final</cell></row><row><cell></cell><cell></cell><cell cols="2">Name</cell><cell>Size</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Acc.</cell></row><row><cell></cell><cell></cell><cell cols="2">Sparse Combo Net</cell><cell>1 ? 32</cell><cell>37.1%</cell></row><row><cell></cell><cell></cell><cell cols="2">Sparse Combo Net</cell><cell>4 ? 32</cell><cell>89.1%</cell></row><row><cell></cell><cell></cell><cell cols="2">Sparse Combo Net</cell><cell>8 ? 32</cell><cell>93.6%</cell></row><row><cell></cell><cell></cell><cell cols="2">Sparse Combo Net</cell><cell>12?32 94.4%</cell></row><row><cell></cell><cell></cell><cell cols="2">Sparse Combo Net</cell><cell>16?32 96%</cell></row><row><cell></cell><cell></cell><cell cols="2">Sparse Combo Net</cell><cell>22?32 96.8%</cell></row><row><cell></cell><cell></cell><cell cols="2">Sparse Combo Net</cell><cell>24?32 96.7%</cell></row><row><cell></cell><cell></cell><cell cols="2">Control</cell><cell>24?32 47%</cell></row><row><cell></cell><cell>Final</cell><cell>Epoch</cell><cell>Final</cell></row><row><cell>Size 1 ? 16 3 ? 16 5 ? 16</cell><cell cols="3">Test Acc. 38.69% 24.61% 1.7005 1 Test Acc. Train Loss 70.56% 40.47% 0.9033 77.Size 1?352 40.17% 26.97% 1.662 Final Epoch Final Test Acc. 1 Test Acc. Train Loss 11?32 89.12% 61.29% 0.3781</cell></row><row><cell></cell><cell></cell><cell></cell><cell>22?16 93.01% 70.11% 0.2073</cell></row><row><cell></cell><cell></cell><cell></cell><cell>44 ? 8</cell><cell>94.44% 25.78% 0.1500</cell></row><row><cell></cell><cell></cell><cell></cell><cell>88 ? 4</cell><cell>10.99% 10.99% 2E+35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S5 :</head><label>S5</label><figDesc>Results for Sparse Combo Nets containing different numbers of component 32-unit RNNs with best found initialization settings, using the standard 150 epoch training paradigm. This experiment was run to demonstrate repeatability of the size results seen in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S6 :</head><label>S6</label><figDesc>Results for different initialization settings -varying sparsity and magnitude of the component RNNs for different network sizes. All other settings remained constant across trials, using our selected 150 epoch training paradigm.</figDesc><table><row><cell></cell><cell>Pre-</cell><cell>Post-</cell><cell>Final</cell><cell>Epoch</cell><cell>Final</cell></row><row><cell>Density</cell><cell>select</cell><cell>select</cell><cell>Test</cell><cell>1 Test</cell><cell>Train</cell></row><row><cell></cell><cell>Scalar</cell><cell>Scalar</cell><cell>Acc.</cell><cell>Acc.</cell><cell>Loss</cell></row><row><cell>10%</cell><cell>1.0</cell><cell>1.0</cell><cell cols="3">95.87% 73.67% 0.074</cell></row><row><cell>5%</cell><cell>10.0</cell><cell>0.1</cell><cell cols="3">95.11% 73.10% 0.1311</cell></row><row><cell>5%</cell><cell>10.0</cell><cell>0.2</cell><cell cols="3">96.15% 82.50% 0.0051</cell></row><row><cell>5%</cell><cell>10.0</cell><cell>0.5</cell><cell cols="3">96.69% 75.76% 0.0001</cell></row><row><cell>5%</cell><cell>6.0</cell><cell>1.0</cell><cell cols="3">96.41% 21.55% 3.3E-5</cell></row><row><cell>5%</cell><cell>7.5</cell><cell>1.0</cell><cell cols="3">16.75% 11.39% 3068967</cell></row><row><cell>3.3%</cell><cell>30.0</cell><cell>0.1</cell><cell cols="3">96.24% 83.89% 0.0005</cell></row><row><cell>3.3%</cell><cell>30.0</cell><cell>0.2</cell><cell cols="3">96.54% 86.79% 4E-5</cell></row><row><cell>1%</cell><cell>10.0</cell><cell>1.0</cell><cell cols="2">96.04% 81.2%</cell><cell>0.0001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S8 :</head><label>S8</label><figDesc>Repeatability of the best network settings on permuted seqMNIST. Four trials of 16 ? 32 networks with 3.3% density and entries between -6 and 6, trained for a 24 hour period with a single learning rate cut (0.1 scalar) after epoch 200. All other training settings remained the same as our selected hyperparameters. Trials are presented in chronological order. The mean test accuracy achieved was 96.85% with Variance 0.019.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Best Train</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Loss</cell><cell></cell></row><row><cell></cell><cell></cell><cell>208</cell><cell>96.65%</cell><cell cols="2">61.15%</cell><cell>0.00224</cell><cell></cell></row><row><cell></cell><cell></cell><cell>255</cell><cell>96.94%</cell><cell cols="2">84.19%</cell><cell>5e-5</cell><cell></cell></row><row><cell></cell><cell></cell><cell>250</cell><cell>96.88%</cell><cell cols="2">81.08%</cell><cell>5e-5</cell><cell></cell></row><row><cell></cell><cell></cell><cell>210</cell><cell>96.93%</cell><cell cols="2">67.19%</cell><cell>0.00069</cell><cell></cell></row><row><cell cols="3">A4.4.2 seqCIFAR10 Trials</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Density</cell><cell>Pre-select Scalar</cell><cell>Post-select Scalar</cell><cell>Epochs</cell><cell>Adam WD</cell><cell>Initial LR</cell><cell>LR Schedule</cell><cell>Best Test Acc.</cell></row><row><cell>3.3%</cell><cell>30</cell><cell>0.2</cell><cell>150</cell><cell>1e-5</cell><cell>1e-3</cell><cell>0.1 after 90,140</cell><cell>64.63%</cell></row><row><cell>3.3%</cell><cell>30</cell><cell>0.2</cell><cell>34*</cell><cell>1e-5</cell><cell>5e-3</cell><cell>0.1 after 90,140</cell><cell>35.42%</cell></row><row><cell>5%</cell><cell>6</cell><cell>1</cell><cell>150</cell><cell>1e-5</cell><cell>1e-3</cell><cell>0.1 after 90,140</cell><cell>60.9%</cell></row><row><cell>5%</cell><cell>10</cell><cell>0.5</cell><cell>150</cell><cell>1e-5</cell><cell>1e-4</cell><cell>0.1 after 90,140</cell><cell>54.86%</cell></row><row><cell>3.3%</cell><cell>30</cell><cell>0.2</cell><cell>150</cell><cell>1e-5</cell><cell>5e-4</cell><cell>0.1 after 90,140</cell><cell>61.83%</cell></row><row><cell>3.3%</cell><cell>30</cell><cell>0.2</cell><cell>200</cell><cell>1e-6</cell><cell>2e-3</cell><cell>0.1 after 140,190</cell><cell>62.31%</cell></row><row><cell>3.3%</cell><cell>30</cell><cell>0.2</cell><cell>186*</cell><cell>1e-5</cell><cell>1e-3</cell><cell>0.1 after 140,190</cell><cell>64.75%</cell></row><row><cell>3.3%</cell><cell>30</cell><cell>0.2</cell><cell>132*</cell><cell>1e-6</cell><cell>1e-3</cell><cell>0.1 after 140,190</cell><cell>62.31%</cell></row><row><cell>5%</cell><cell>10</cell><cell>0.5</cell><cell>195*</cell><cell>1e-5</cell><cell>1e-3</cell><cell>0.1 after 140,190</cell><cell>64.68%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S9 :</head><label>S9</label><figDesc>Additional hyperparameter tuning for the seqCIFAR10 task, presented in chronological order. * indicates that training was cut short by the 24 hour Colab runtime limit, or in the case of high learning rate was killed intentionally due to exploding gradient. LR Schedule describes the scalar the learning rate was multiplied by, and at what epochs. The best performing network is highlighted. Ultimately we decided on the same network settings and training hyperparameters for further testing, just extending the training period to 200 epochs with the learning rate cuts occurring after epochs 90 and 140.</figDesc><table><row><cell></cell><cell>Best</cell><cell>Epoch</cell><cell>Best</cell></row><row><cell>Epochs</cell><cell>Test</cell><cell>1 Test</cell><cell>Train</cell></row><row><cell></cell><cell>Acc.</cell><cell>Acc.</cell><cell>Loss</cell></row><row><cell>186</cell><cell cols="3">64.75% 10.53% 0.83</cell></row><row><cell>200</cell><cell cols="3">64.04% 26.35% 0.787</cell></row><row><cell>200</cell><cell cols="3">64.32% 26.76% 0.778</cell></row><row><cell>170</cell><cell cols="3">64.88% 20.65% 0.857</cell></row><row><cell>200</cell><cell cols="3">65.72% 27.28% 0.813</cell></row><row><cell>200</cell><cell cols="3">63.73% 10.47% 0.826</cell></row><row><cell>200</cell><cell cols="3">65.03% 18.75% 0.83</cell></row><row><cell>200</cell><cell cols="3">64.71% 32.36% 0.799</cell></row><row><cell>200</cell><cell>64.4%</cell><cell cols="2">10.09% 0.83</cell></row><row><cell>200</cell><cell cols="3">65.63% 30.25% 0.792</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S10 :</head><label>S10</label><figDesc>Repeatability of the best network settings on seqCIFAR10. Ten trials of 16 ? 32 networks with 3.3% density and entries between -6 and 6, trained for 200 epochs with learning rate scaled by 0.1 after epochs 140 and 190. All other training settings remained the same as before. Trials are presented in chronological order. The mean test accuracy achieved was 64.72% with Variance 0.406. Most trials completed all 200 epochs, but two were cut short due to runtime limits.</figDesc><table><row><cell></cell><cell>Best</cell><cell>Epoch</cell><cell>Best</cell></row><row><cell>Epochs</cell><cell>Test</cell><cell>1 Test</cell><cell>Train</cell></row><row><cell></cell><cell>Acc.</cell><cell>Acc.</cell><cell>Loss</cell></row><row><cell>150</cell><cell cols="3">64.63% 28.91% 0.837</cell></row><row><cell>150</cell><cell cols="2">63.51% 9.7%</cell><cell>0.9</cell></row><row><cell>148</cell><cell cols="3">62.35% 17.51% 0.89</cell></row><row><cell>126</cell><cell cols="3">63.33% 23.61% 0.903</cell></row><row><cell>129</cell><cell cols="3">61.45% 28.93% 0.937</cell></row><row><cell>150</cell><cell cols="3">62.21% 25.82% 0.9</cell></row><row><cell>150</cell><cell cols="3">63.42% 23.51% 0.86</cell></row><row><cell>147</cell><cell cols="3">62.05% 27.67% 0.882</cell></row><row><cell>131</cell><cell cols="3">62.41% 30.33% 0.912</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work benefited from stimulating discussions with Michael Happ, Quang-Cuong Pham, and members of the Fiete lab at MIT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Brain-wide interactions between neural circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Svoboda</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conb.2020.12.012</idno>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<editor>iii-v</editor>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Design for a brain: The origin of adaptive behaviour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ashby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning stability certificates from data. CoRL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques E</forename><surname>Matni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Slotine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindhwani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A tighter bound for the echo state property</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Buehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="820" to="824" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distributed online estimation of biophysical neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy O&amp;apos;</forename><surname>Burghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepulchre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01472</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Contraction analysis of hopfield neural networks with hebbian learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Centorrino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Bullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Russo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05382</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09689</idno>
		<title level="m">Antisymmetricrnn: A dynamical system view on recurrent neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ode to an ode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Slotine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3338" to="3350" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cooperative robot control and concurrent synchronization of lagrangian systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon-Jo</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques E</forename><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="686" to="700" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention as activation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Oehmcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Gieseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiquan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobus</forename><surname>Barnard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07729v2</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Theoretical neuroscience: computational and mathematical modeling of neural systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laurence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Lyapunov spectra of chaotic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Engelken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry F</forename><surname>Abbott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02427</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Hodgkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<title level="m">Lipschitz recurrent neural networks. ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The schur complement and symmetric positive semidefinite (and definite) matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Gallier</surname></persName>
		</author>
		<ptr target="https://www.cis.upenn.edu/jean/schur-comp.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The theory of facilitated variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Gerhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Kirschner</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0701035104</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8582" to="8589" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The &quot;echo state&quot; approach to analysing and training recurrent neural networks-with an erratum note</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust implicit networks via non-euclidean contractions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saber</forename><surname>Jafarpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Proskurnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Bullo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9857" to="9868" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Principles of neural science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric R Kandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Jessell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Siegelbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Hudspeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">4</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Nonlinear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khalil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Biological robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Kitano</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrg1471</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="826" to="837" />
			<date type="published" when="1111-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Achieving stable dynamics in neural circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Kozachkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Lundqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Slotine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1007659</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Robust working memory through short-term synaptic plasticity. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Kozachkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Lundqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Brincat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slotine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Kozachkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Wensing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slotine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.06656</idno>
		<title level="m">Generalization in supervised learning through riemannian contraction</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Large associative memory problem in neurobiology and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Krotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hopfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06996</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Latent circuit inference from heterogeneous neural responses during cognitive tasks. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Langdon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><forename type="middle">A</forename><surname>Engel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lezcano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mart?nez-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3794" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On contraction analysis for non-linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winfried</forename><surname>Lohmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques E</forename><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="683" to="696" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiregion neuronal activity: the forest and the trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">V</forename><surname>Kauvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Deisseroth</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41583-022-00634-0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2022-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Control contraction metrics: Convex and intrinsic criteria for nonlinear feedback design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques E</forename><surname>Manchester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3046" to="3053" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conscious processing and the global neuronal workspace hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>George A Mashour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>Roelfsema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Changeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dehaene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="776" to="798" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stability conditions for nonlinear continuous neural networks with asymmetric connection weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotoshi</forename><surname>Matsuoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="495" to="500" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An integrative theory of prefrontal cortex function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="202" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Stable recurrent models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10369</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mathematical equivalence of two common forms of firing rate models of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fumarola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="31" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Balanced amplification: a new mechanism of selective amplification of neural activity patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth D</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="635" to="648" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hurwitz stability of metzler matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumpati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shorten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions On Automatic Control</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1484" to="1487" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Facilitated variation: How evolution learns from past environments to generalize to new environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merav</forename><surname>Parter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Inferring brain-wide interactions using data-constrained recurrent neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Perich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofia</forename><surname>Arlt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><forename type="middle">E</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><forename type="middle">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Mosher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Minxha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">H</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Rudebeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanaka</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajan</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.12.18.423348</idno>
		<ptr target="https://www.biorxiv.org/content/early/2021/03/11/2020.12.18.423348" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stable concurrent synchronization in dynamic system networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Cuong</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="77" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Overparameterized neural networks implement associative memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adityanarayanan</forename><surname>Radhakrishnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="27162" to="27170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?fl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milena</forename><surname>Pavlovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<title level="m">Geir Kjetil Sandve, et al. Hopfield networks is all you need</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Contracting implicit recurrent neural networks: Stable models with improved trainability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Revay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Manchester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning for Dynamics and Control</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="393" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Revay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">R</forename><surname>Manchester</surname></persName>
		</author>
		<title level="m">Lipschitz bounded equilibrium networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Revay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">R</forename><surname>Manchester</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05942</idno>
		<title level="m">Recurrent equilibrium networks: Unconstrained learning of stable and robust dynamical models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The lyapunov neural network: Adaptive stability certification for safe learning of dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Berkenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lyanet: A lyapunov framework for training neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan Dario Jimenez</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ames</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="18687" to="18703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ckconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02611v1</idno>
		<title level="m">Continuous kernel convolution for sequential data</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Brain-score: Which artificial neural network for object recognition is most brain-like?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Najib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Majaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajalingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohitij</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bashivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Prescott-Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>BioRxiv</publisher>
			<biblScope unit="page">407007</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cortical areas interact through a communication subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Semedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zandvakili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Machens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">How the brain keeps the eyes still</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>H Sebastian Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="13339" to="13344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The architecture of complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<idno>0003-049X</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Philosophical Society</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="467" to="482" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Modular stability tools for distributed computation and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adaptive Control and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The missing link</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Slotine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang-Yu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Physics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="512" to="513" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Modularity, evolution, and the binding problem: a view from stability theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Slotine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winfried</forename><surname>Lohmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="145" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Applied Nonlinear Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques E</forename><surname>Slotine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Chaos in random neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Sompolinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Crisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Jurgen</forename><surname>Sommers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">259</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Contractive systems with inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Perspectives in mathematical system theory, control, and signal processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Opening the black box: low-dimensional dynamics in highdimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="649" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dynamic reorganization of the cortico-basal ganglia-thalamo-cortical network during task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Sych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksejs</forename><surname>Fomins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Novelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fritjof</forename><surname>Helmchen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Reports</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">111394</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Tabareau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Slotine</surname></persName>
		</author>
		<title level="m">Notes on contraction theory. arXiv preprint nlin/0601011</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00144v3</idno>
		<title level="m">Learning longer-term dependencies in rnns with auxiliary losses</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">On lyapunov exponents for rnns: Understanding information propagation using dynamical systems tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><forename type="middle">Puelma</forename><surname>Touzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14123</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Positive definite matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Weisstein</surname></persName>
		</author>
		<ptr target="https://mathworld.wolfram.com/PositiveDefiniteMatrix.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Beyond convexity-contraction and global convergence of gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Jacques</forename><surname>Wensing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slotine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos one</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">236661</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Cybernetics or Control and Communication in the Animal and the Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Generalized shape metrics on neural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4738" to="4750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Excitatory and inhibitory interactions in localized populations of model neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0006-3495(72</idno>
		<idno>0006-3495. doi</idno>
		<ptr target="https://doi.org/10.1016/S0006-3495(72" />
	</analytic>
	<monogr>
		<title level="j">Biophysical Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86068" to="86073" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Performance-optimized hierarchical models predict neural responses in higher visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="8619" to="8624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Towards the next generation of recurrent network models for cognitive neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Guangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Molano-Maz?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="182" to="192" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Task representations in neural networks trained to perform many cognitive tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guangyu Robert Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Madhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jing</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="306" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
