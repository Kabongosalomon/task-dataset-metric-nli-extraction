<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MIntRec: A New Dataset for Multimodal Intent Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 10-14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlei</forename><surname>Zhang</surname></persName>
							<email>zhang-hl20@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
							<email>xuhua@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianrui</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">School of Information Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Hebei University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">School of Information Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Hebei University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MIntRec: A New Dataset for Multimodal Intent Recognition</title>
					</analytic>
					<monogr>
						<title level="m">MM &apos;22</title>
						<meeting> <address><addrLine>Lisboa, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 10-14, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3503161.3547906</idno>
					<note>* Corresponding author. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike International 4.0 License.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems ? Multimedia and multimodal re- trieval;</term>
					<term>Computing methodologies ? Object detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal intent recognition is a significant task for understanding human language in real-world multimodal scenes. Most existing intent recognition methods have limitations in leveraging the multimodal information due to the restrictions of the benchmark datasets with only text information. This paper introduces a novel dataset for multimodal intent recognition (MIntRec) to address this issue. It formulates coarse-grained and fine-grained intent taxonomies based on the data collected from the TV series Superstore. The dataset consists of 2,224 high-quality samples with text, video, and audio modalities and has multimodal annotations among twenty intent categories. Furthermore, we provide annotated bounding boxes of speakers in each video segment and achieve an automatic process for speaker annotation. MIntRec is helpful for researchers to mine relationships between different modalities to enhance the capability of intent recognition. We extract features from each modality and model cross-modal interactions by adapting three powerful multimodal fusion methods to build baselines. Extensive experiments show that employing the non-verbal modalities achieves substantial improvements compared with the text-only modality, demonstrating the effectiveness of using multimodal information for intent recognition. The gap between the best-performing methods and humans indicates the challenge and importance of this task for the community. The full dataset and codes are available for use at https://github.com/thuiar/MIntRec.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>may be beneficial in analyzing human intentions from multiple perspectives and provide more friendly services.</p><p>Taking <ref type="figure">Figure 1</ref> as an example, we might infer the speaker to be complaining about someone based on the text information. After combining the video and audio information, we find the real intention is joking, as the speaker's expression and tone are cheerful rather than indignant. It indicates that using text alone has difficulties satisfying the requirements of identifying complex human intents in practical situations. It is essential to use complementary knowledge of different modalities to improve the performance of intent comprehension.</p><p>Multimodal language understanding has attracted much attention in recent years. A series of multimodal datasets have been proposed in many areas such as sentiment analysis <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>, humor detection <ref type="bibr" target="#b16">[17]</ref>, sarcasm detection <ref type="bibr" target="#b8">[9]</ref>, semantic comprehension <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b52">53]</ref>, etc. These benchmark datasets have extensively promoted the research and application of multimodal methodologies in related fields. However, there is still a lack of multimodal datasets for intent analysis. Most of the existing intent benchmark datasets contain merely the text modality <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref> or the visual modality <ref type="bibr" target="#b22">[23]</ref>. MDID <ref type="bibr" target="#b24">[25]</ref> used image-caption pairs from Instagram posts to analyze multimodal intents, but the caption-based text information is different from spoken languages in the real world.</p><p>The scarcity of data has seriously restricted the development of multimodal intent recognition. Nevertheless, constructing such a multimodal intent benchmark dataset faces two main challenges. Firstly, we need to design appropriate multimodal intent categories. Current intent taxonomies are mainly based on the text information or image-caption pairs, which have limitations when applied in multimodal scenes. Secondly, it requires distinguishing the visual information of the speaker as there is usually more than one person in the same situation. However, it will take much cost to perform manual annotation. To solve these problems, we propose a novel dataset, MIntRec, to fill the gap in multimodal intent recognition. The process of building the dataset is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Firstly, we prepare the original multimodal data for the dataset. The TV series SuperStore is selected as the data source due to its superiority for this task. After collecting the raw videos and subtitles, we process them into text utterances with respective video and audio segments. Then, we design both coarse-grained and fine-grained intent taxonomies for the multimodal scene. The coarse-grained taxonomies contain "Express emotions or attitudes" and "Achieve goals", which are inspired by the human intention philosophy <ref type="bibr" target="#b3">[4]</ref>. Eleven and nine fine-grained intents are respectively summarized for these two coarse-grained categories based on the video segments and highfrequency intent tags.</p><p>Next, we perform multimodal intent annotation with the prepared data and intent taxonomies. Five well-trained workers are employed for the annotation task. They label each sample among twenty intent tags with a convenient annotation platform, and the majority voting determines the multimodal labels. Finally, we obtain 2,224 high-quality samples for MIntRec. Besides, we propose </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intent Categories Interpretations</head><p>Express emotions or attitudes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complain</head><p>Express dissatisfaction with someone or something (e.g., saying unfair encounters with a sad expression and helpless motion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Praise</head><p>Express admiration for someone or something (e.g., saying with an appreciative expression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apologise</head><p>Express regret for doing something wrong (e.g., saying words of apology such as sorry).</p><p>Thank Express gratitude in word or deed for the convenience or kindness given or offered by others (e.g., saying words of appreciation such as thank you).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Criticize</head><p>Point out someone's mistakes harshly (e.g., yelling out someone's problems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Care</head><p>Concern about someone or be curious about something (e.g., worrying about someone's health).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agree</head><p>Have the same attitude about something (e.g., saying affirmative words such as yeah and yes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Taunt</head><p>Use metaphors and exaggerations to accuse and ridicule (e.g., complimenting someone with a negative expression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flaunt</head><p>Boast about oneself to gain admiration, envy, or praise (e.g., saying something complimentary about oneself arrogantly).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oppose</head><p>Have an inconsistent attitude about something (e.g., saying negative words to express disagreement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joke</head><p>Say something to provoke laughter (e.g., saying something funny and exaggerated with a cheerful expression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Achieve goals</head><p>Inform Tell someone to make them aware of something (e.g., broadcasting something with a microphone).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advise</head><p>Offer suggestions for consideration (e.g., saying words that make suggestions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arrange</head><p>Plan or organize something (e.g., requesting someone what they should do formally).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduce</head><p>Communicate to make someone acquaintance with another or recommend something (e.g., describing a person's identity or the properties of an object).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comfort</head><p>Alleviate pain with encouragement or compassion (e.g., describing something is hopeful).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leave</head><p>Get away from somewhere (e.g., saying where to go while turning around or getting up).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prevent</head><p>Make someone unable to do something (e.g., stopping someone from doing something with a hand).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greet</head><p>Express mutual kindness or recognition during the encounter (e.g., waving to someone and saying hello).</p><p>Ask for help Request someone to help (e.g., asking someone to deal with the trouble).</p><p>an automatic process for speaker annotation. The detected object boundings are used to get the visual information of persons in each video frame. To identify the bounding boxes of speakers, we first detect and track faces within bounding boxes in different visual scenes and then predict the indexes of speakers with the active speaker detection algorithm. This process achieves high performance on our constructed testing set. After extracting features for each modality, we build baselines with three strong multimodal fusion methods. The experimental results show that leveraging the nonverbal information achieves 1% ? 2% stable improvements on both binary and multi-class classification. However, the results of the best methods are still far from human performance, indicating the challenge of the multimodal intent recognition task.</p><p>Our contributions are summarized as follows: <ref type="bibr" target="#b0">(1)</ref> In this work, we build a novel multimodal intent recognition dataset, MIntRec, containing 2,224 high-quality samples with multimodal intent annotations. To the best of our knowledge, it is the first benchmark dataset for intent recognition in real-world multimodal scenes.</p><p>(2) New intent taxonomies are designed for this task. Concretely, we provide two coarse-grained and twenty fine-grained intent categories for the study of multimodal intent analysis.</p><p>(3) An automatic speaker annotation process is proposed to produce high-quality annotated bounding boxes for speakers under the evaluation of over 12K human-annotated keyframes. It saves much time and laboratory and may benefit similar annotation tasks.</p><p>(4) Extensive experiments conducted on our dataset show utilizing multimodal information is superior to text-based intent recognition. The best-performing methods still have much room for improvement compared with humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MINTREC DATASET</head><p>In this section, we will introduce the process of building the MIntRec dataset, including data preparation, intent taxonomy definition, multimodal intent annotation, and automatic speaker annotation. The detailed statistics of MIntRec are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Preparation</head><p>Multimodal intent recognition requires plenty of nonverbal signals in real-world conversational scenes. For this purpose, we select the TV series Superstore as the source of our dataset, which has two main advantages: On the one hand, it consists of a wealth of characters (including seven prominent and twenty recurring roles) with different identities in the superstore, which is helpful to produce rich body language, expressions, and tones as multimodal information. On the other hand, it contains a mass of stories in various scenes (e.g., shopping mall, warehouse, office), which favor collecting diverse intent categories.</p><p>The raw videos and subtitles of Superstore are accessible on YouTube and OpenSubtitles 1 . To obtain video segments, we first extract each utterance's starting and ending timestamps of a speaker and then split the raw videos according to these timestamps. The corresponding audio segments are extracted from raw videos with the moviepy toolkit 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intent Taxonomy Definition</head><p>Existing intent taxonomies are restricted in specific tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref> or from social media posts <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>, which are uncommon in realworld scenes. Therefore, we design new taxonomies for multimodal intent recognition, including two coarse-grained and twenty finegrained intent categories.</p><p>In artificial intelligence research, intentions are regarded as plans or goals of an agent, accompanied by the corresponding feedback actions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51]</ref>. However, Schr?der <ref type="bibr" target="#b40">[41]</ref> pointed out that the brain's emotional evaluations of situations are also critical components of human intentions. We combine these two aspects and crawl through the raw videos to generalize two representative coarse-grained intent taxonomies for multimodal intent recognition, including "Express emotions or attitudes" and "Achieve goals".</p><p>The coarse-grained intent taxonomy is insufficient to distinguish the complex and diverse types of human intentions in the real world. Thus, we further classify it into fine-grained categories. Firstly, we analyze different video segments as many as possible and collect several rough intent tags as candidates for each coarsegrained category. Then, we discuss and divide similar tags into the same group (e.g., introducing something or someone, worrying about someone or being interested in something). Next, we collect high-frequency intent tags and organize them into twenty finegrained categories, including eleven classes for "Express emotions or attitudes" and nine for "Achieve goals". Some intents may be cued by a single modality such as text (e.g., thank, apologise, greet, agree, praise), video (e.g., leave, prevent), or audio (e.g., complain, criticize). Other intents may be inferred by combining different modalities (e.g., comfort, care, joke, taunt, flaunt). Brief interpretations of each intent category are summarized by observing practical examples and referring to related materials <ref type="bibr" target="#b41">[42]</ref>, as shown in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multimodal Intent Annotation</head><p>After preparing data and defining intent taxonomies, we employ five students with an English foundation for annotation. Employees are offered interpretations and typical examples of each intent category as guidelines. Only well-trained employees are allowed for annotation. As intents usually exist in specific scenes of events <ref type="bibr" target="#b40">[41]</ref>, there are irrelevant utterances among consecutive video segments, so we add a UNK tag to the label set to distinguish them. To improve the labeling efficiency, we build a database to manage all the multimodal data and a convenient platform for annotation. Users only need to click the button of the intent tag to complete annotation for a piece of data.</p><p>Each of the five workers is required to complete the annotation task of the same set of data independently. They need to choose the most confident intent tag for each sample by combining the video, audio, and text information. The intent labels are determined by the majority voting (three out of five). The samples with votes larger or equal to three (not UNK) are saved. Finally, we acquired 2,224 high-quality samples to make up the MIntRec dataset.</p><p>The detailed voting statistics are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. It can be observed that intent categories with clear text or video cues (e.g., apologise, thank, leave, prevent, greet, agree) are easier to reach a higher agreement with votes larger than three. We also notice that the dataset is imbalanced. The reason is that it satisfies the distribution of different intents in real-world scenarios. Some intents occur more frequently (e.g., complain, inform, and praise), while others do not (e.g., joke and ask for help). Nevertheless, each intent category still contains at least fifty samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Automatic Speaker Annotation</head><p>As suggested in <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>, we first extract frames from each video segment to represent the video information. Then, we aim to annotate the visual contents related to the speakers, which are the objects of multimodal intent recognition.</p><p>We perform object detection on the video frames to obtain rich visual information containing facial and body features. Specifically, we use a Faster R-CNN <ref type="bibr" target="#b36">[37]</ref> with the backbone ResNet-50 <ref type="bibr" target="#b20">[21]</ref> pretrained on the MS COCO dataset <ref type="bibr" target="#b28">[29]</ref> (containing 250,000 person instances with seventeen keypoints) to predict the bounding boxes of persons in each frame.</p><p>However, there are still two challenges for speaker annotation. For one thing, there may be no or little visual information about the speaker in the extracted frame (e.g., most of the body is covered, or the speaker only appears on the back of the body). For another thing, there are usually multiple persons with detected bounding boxes in each frame, making it hard to distinguish the speaker. To solve these problems, we propose an automatic process to perform speaker annotation with the aid of the audio-visual active speaker detection algorithm <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45]</ref>. It contains the following four steps:</p><p>Firstly, we use the scene detection toolkit 3 to distinguish different visual scenes in a video segment, as there may be a change in contents between adjacent frames. Secondly, a pre-trained Faster R-CNN is used to detect the bounding boxes of persons for each frame in a visual scene. Since facial motion (e.g., lips movement) is critical for detecting speakers, we further use the S 3 FD [64] algorithm to detect faces in the bounding boxes and establish the one-to-one mapping between faces and object boundings. Note that this step also filters the keyframes with clear facial features. Thirdly, a simple and effective method is used to perform face tracking. Concretely, we compute intersection-over-union (IoU) for two faces in adjacent frames and consider two faces are from the same person if IoU is above 0.5. Given that accidental objects may block faces, we tolerate up to ten consecutive frames of missing faces. Finally, we use a pre-trained TalkNet model with tracked faces and corresponding audio information to predict speakers and determine respective bounding boxes with the mapping obtained in the second step. With the aid of this process, we automatically generate more than 120K keyframes with speaker annotations of bounding boxes free from any manual intervention.</p><p>To evaluate the quality of keyframes and bounding box information, we construct a testing set with more than 12K humanannotated keyframes. Specifically, we first uniformly extract one shot every ten frames and manually select keyframes with clear visual information. Then, a pre-trained Faster R-CNN is used to predict the object boundings of persons in each keyframe, and annotators label the speakers by choosing the indexes of corresponding bounding boxes.</p><p>Compared with the human-annotated keyframes, the missing rate of generated keyframes is only 2.3%. Among the hit keyframes, the proportion of high-quality predicted bounding boxes (IoU &gt; 0.9) is 90.9%. The evaluation results demonstrate the reliability of the automatic speaker annotation process. Besides, it is much more efficient without labor-intensive and time-consuming manual annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>After preparing the corresponding text, video, and audio data of speakers, we extract features of each modality and use them for multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extraction</head><p>3.1.1 Text. Due to the excellent performance of the pre-trained BERT language model in the Natural Language Processing (NLP) community <ref type="bibr" target="#b23">[24]</ref>, we use it to extract text features. For each text utterance, we obtain the token embeddings z ? R ? from the last hidden layer of BERT, where is the sequence length of text utterances, and is the feature dimension 768.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Vision.</head><p>The object detection method is used for extracting vision features. For each video segment, we first leverage a pretrained Faster R-CNN with the backbone ResNet-50 to extract the feature representations x of all keyframes. Then, we map x into the regions with the annotated bounding boxes to obtain the vision feature embeddings z ? R ? :</p><formula xml:id="formula_0">z = AvgPool(RoIAlign(x, )),<label>(1)</label></formula><p>where RoIAlign <ref type="bibr" target="#b19">[20]</ref> is used to extract the fixed size feature maps (e.g., 7?7). AvgPool is used to reduce both weight and height sizes to the unit size. is the sequence length of video segments, and is the feature dimension 256.</p><p>3.1.3 Audio. The speech toolkit librosa <ref type="bibr" target="#b31">[32]</ref> is first used to acquire audio time series at 16,000 Hz. Then, the pre-trained model wav2vec 2.0 <ref type="bibr" target="#b2">[3]</ref> is used to extract audio features, which learns powerful representations for speech recognition with self-supervised learning. We obtain the acoustic feature embeddings z ? R ? from the last hidden layer of wav2vec 2.0, where is the sequence length of audio segments, and is the feature dimension 768.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Benchmark Multimodal Fusion Methods</head><p>After feature extraction, we benchmark three powerful multimodal fusion methods for the MIntRec dataset. These methods aim to learn the interactions between different modalities with the extracted features and obtain friendly representations for multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>MulT. The Multimodal Transformer (MulT) <ref type="bibr" target="#b45">[46]</ref> is an end-toend method to deal with non-aligned multimodal sequences. It extends the vanilla Transformer <ref type="bibr" target="#b46">[47]</ref> to the cross-modal Transformer with the pairwise inter-modal attention mechanism, which helps to capture the adaptation knowledge between different modalities in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">MISA.</head><p>Hazarika et al. <ref type="bibr" target="#b18">[19]</ref> proposed the framework MISA to learn multimodal representations with modality-invariant and modality-specific properties. On the one hand, a shared subspace is utilized to learn common features of all modalities. On the other hand, distinct subspaces are designed to capture the unique attributes of each modality. For this purpose, the training objectives contain four aspects: similarity loss, difference loss, reconstruction loss, and task-specific loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">MAG-BERT.</head><p>Rahman et al. <ref type="bibr" target="#b35">[36]</ref> integrated two nonverbal modalities into BERT with an additional multimodal adaptation gate (MAG) module. MAG can produce a position shift in the semantic space adaptive to acoustic and visual information. It can be flexibly placed between layers of BERT to receive inputs from nonverbal modalities. In this work, the features of each modality z ,z , and z can be directly used as the inputs of MulT and MISA. As MAG-BERT needs aligned multimodal data, we pass the features of video and audio (z and z ) through the Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b15">[16]</ref> module to align with the text feature z in the wordlevel as suggested in <ref type="bibr" target="#b45">[46]</ref>. For each method, we use the multimodal annotations as targets and perform the classification task under the supervision of the softmax loss. <ref type="table">Table 4</ref>: Multimodal intent recognition results on the MIntRec dataset. "Twenty-class" and "Binary" denote the multi-class and binary classification over fine-grained and coarse-grained intent taxonomies. ? denotes the most improvement over the text-classifier baseline in the current evaluation metric of each method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>This section introduces the experimental setup, baselines, and experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Dataset Splits.</head><p>We shuffle the video segments in random and split training, validation, and testing sets by multimodal annotations in 3:1:1. The detailed statistics are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics.</head><p>Four metrics are used to evaluate the model performance: accuracy (ACC), F1-score (F1), precision (P), and recall (R). We report the macro score over all classes for the last three metrics. The higher values indicate better performance of all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details.</head><p>For the text and audio modalities, we employ the pre-trained BERT model (bert-base-uncased, with 12 Transformer layers) and pre-trained wav2vec 2.0 model implemented in PyTorch <ref type="bibr" target="#b49">[50]</ref>. For the video modality, we use a pre-trained Faster R-CNN with ResNet-50 backbone implemented in MMDetection Toolbox <ref type="bibr" target="#b9">[10]</ref>. As sequence lengths of the segments in each modality need to be fixed, we use zero-padding for shorter sequences. , , and are 30, 230, and 480, respectively. For all methods, the training batch size is 16, and the number of training epochs is 100. We adjust the hyper-parameters with macro F1-score. For a fair comparison, we report the average performance over ten runs of experiments with random seeds 0-9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We build a series of baselines for the MIntRec dataset. As the text modality is predominant in the intent recognition task, we train a classifier with the text-only modality as the primary baseline. As suggested in <ref type="bibr" target="#b23">[24]</ref>, we use the first special token [CLS] from the last hidden layer as the sentence representation and fine-tune the pre-trained BERT model with the downstream classification task for better performance.</p><p>As introduced in section 3.2, three multimodal fusion methods, MAG-BERT, MulT, and MISA, are used to benchmark our dataset. Besides, we also modify them to use two modalities (Text + Audio and Text + Video) as inputs for ablation studies.</p><p>We have a different set of two annotators to evaluate the human performance on this task. They are provided with the training and validation sets with multimodal annotations for learning and assessment as in baselines. After that, they need to label the unseen testing set, and their average results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We conduct experiments on the MIntRec dataset with several baselines and show the results in <ref type="table">Table 4</ref>. For each multimodal fusion method, the best results are highlighted in bold. The improvements over the text-classifier are shown with ?.</p><p>The multimodal fusion methods achieve substantial improvements on all metrics of twenty-class and binary classification compared with the text-only modality. All the multimodal fusion methods for twenty-class classification stably improve over 1% scores on all metrics. All the baselines achieve much higher performance on binary classification. We suggest the reason is that recognizing coarse-grained intent categories is much easier than distinguishing fine-grained intent categories. Nevertheless, all the multimodal fusion methods still yield over 1% improvements on almost all metrics. The results demonstrate that effectively leveraging the multimodal information helps enhance the intent recognition capability.</p><p>However, even the best-performing methods are still far away from humans. Compared with the text modality, the human performance improves by 14% ? 19% on twenty-class classification and  6% ? 7% on binary classification. The improvements are much more significant than in multimodal fusion methods, indicating this task is very challenging for multimodal research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>This section analyzes the effect of nonverbal modalities and shows the performance of fine-grained intent categories with quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of Nonverbal Modalities</head><p>We conduct ablation studies for each multimodal fusion method to investigate the influence of the video and audio modalities. Specifically, we compare the tri-modality with bi-modality and show results in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Bi-modality.</head><p>After combining text with audio modality, the intent recognition performance achieves overall gains on both twenty-class and binary classification. Specifically, MAG-BERT, MulT, and MISA increase accuracy scores of 1.28%, 0.92%, and 0.72% on twenty-class and 0.74%, 0.65%, and 0.36% on binary classification, respectively. Combining text with video modality also leads to better performance in all settings. Similarly, MAG-BERT, MulT, and MISA increase accuracy scores of 1.21%, 1.10%, and 0.65% on twenty-class and 0.36%, 0.70%, and 0.65% on binary classification. Due to the consistent improvements in leveraging video or audio modality, we suppose the two nonverbal modalities are critical for multimodal intent recognition. The valuable information such as tone of voice and body movements may be helpful to recognize human intents from new dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Tri-modality.</head><p>Though multimodal fusion methods with bimodality have achieved better performance than the text modality, we find utilizing the tri-modality brings more gains. MAG-BERT achieves a slight advantage on the precision score but performs worse on the other metrics. The positive results demonstrate that both video and audio modalities benefit this task. The benchmark multimodal fusion methods can fully use the information from different modalities by modeling cross-modal interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance of Fine-grained Intent Classes</head><p>To investigate the effect of the multimodal information in each fine-grained intent category, we report the average macro F1-score of each class over ten runs of experiments for all baselines and show results in <ref type="table" target="#tab_4">Table 5</ref> and 6. The best results of multimodal fusion methods are highlighted in bold. ? indicates the most improvement of multimodal fusion methods and humans over the text-classifier.</p><p>Firstly, we observe the results of each class in the coarse-grained intent category "Express emotions and attitudes" in <ref type="table" target="#tab_4">Table 5</ref>. It is shown that multimodal fusion methods perform better than textclassifier in most classes. Notably, we find there are some classes with over 3% significant improvements (e.g., complain, criticize, taunt, oppose, joke). The success of these intents is intuitive, as they contain vivid nonverbal signals of expressions and tones, requiring the aid of visual and audio information. However, we also notice that the multimodal information is less helpful with few improvements or even degradation in some classes (e.g., apologise, thank, praise, agree). The reason is that these classes usually contain clear textual cues such as sorry, thank, yeah, etc. In this case, the pre-trained language model is good enough for intent recognition.</p><p>Secondly, we observe the results of each class in the coarsegrained intent category "Achieve goals" in <ref type="table" target="#tab_5">Table 6</ref>. The performance of multimodal fusion methods consistently achieves over 1% ? 6% improvements in all classes. It is reasonable because these classes are highly associated with broad body movements, such as gestures, posture, arm behaviors, etc. By comparison, capturing this information with the text modality is rather challenging.</p><p>Finally, we observe the human performance in <ref type="table" target="#tab_4">Table 5</ref> and 6. As expected, humans have gained an absolute advantage over the text modality in most intent categories. However, the human performance is lower than text-classifier in three classes (apologise, thank, agree). It suggests that even humans may make mistakes in the classes biased to the text modality. In contrast, humans are good at reasoning through different modalities and show significant superiority with over 10% improvements in many intents, such as taunt, flaunt, oppose, joke, etc. The huge gap indicates the necessity of exploring an effective way to leverage nonverbal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORKS 6.1 Multimodal Language Datasets</head><p>Multimodal language understanding is a booming area with a series of emerging benchmark datasets. For example, many datasets have been proposed in multimodal sentiment analysis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref> and emotion recognition <ref type="bibr" target="#b33">[34]</ref>. Some multimodal datasets also detect unique properties of human languages, such as sense of humor <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, metaphor <ref type="bibr" target="#b59">[60]</ref>, sarcasm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. Moreover, multimodal datasets are designed for a series of other tasks in NLP, such as dialogue act classification <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, named entity recognition <ref type="bibr" target="#b42">[43]</ref>, comprehension and reasoning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b52">53]</ref>, comments generation <ref type="bibr" target="#b47">[48]</ref>, fake news detection <ref type="bibr" target="#b32">[33]</ref>, etc. Nevertheless, there is a lack of multimodal datasets for intent analysis in real-world dialogue scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Benchmark Datasets for Intent Analysis</head><p>Intent analysis is a popular research field in NLU, and many important tasks have been proposed, such as joint intent detection and slot filling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b58">59]</ref>, open intent detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b61">62]</ref> and discovery <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">63]</ref>. The booming of this area benefits from several benchmark intent datasets proposed in recent years, such as ATIS <ref type="bibr" target="#b21">[22]</ref>, Snips <ref type="bibr" target="#b12">[13]</ref>, CLINC150 <ref type="bibr" target="#b25">[26]</ref>, HWU64 <ref type="bibr" target="#b29">[30]</ref>, and BANKING77 <ref type="bibr" target="#b7">[8]</ref>. These datasets collected the corpus by interacting with the intelligent assistant or customers in specific domains and used the crowdsourcing task among service requests to determine intent labels. StackOverflow <ref type="bibr" target="#b51">[52]</ref> and StackExchange <ref type="bibr" target="#b4">[5]</ref> gathered data from technical question and answering platforms. Their intent labels are defined as the tags assigned to the questions. SWBD <ref type="bibr" target="#b14">[15]</ref> corpus contained 42 dialogue acts (DAs) for task-independent conversations. Still, many DAs are ambiguous concepts (e.g., statement-opinion and statement-non-opinion), which are difficult to be applied in real applications. Intentonomy <ref type="bibr" target="#b22">[23]</ref> analyzed the visual intents among social media posts and collected an image dataset. However, all these datasets merely contain information from a single modality.</p><p>MDID <ref type="bibr" target="#b24">[25]</ref> integrated image and text information for intent recognition. However, the multimodal information from Instagram posts is limited, and the taxonomies are inappropriate in real-world scenes. In contrast, MIntRec contains rich multimodal information in dialogue scenes with text, video, and audio modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Multimodal Fusion Methods</head><p>Based on the multimodal language datasets, multimodal fusion methods are proposed to capture the interactions between language and nonverbal modalities. Traditional methods, such as MCB <ref type="bibr" target="#b13">[14]</ref> and TFN <ref type="bibr" target="#b54">[55]</ref> obtained representations by learning intra-modality and inter-modality relations. However, the high-dimensional representations suffer from high computational complexity. LMF <ref type="bibr" target="#b30">[31]</ref> designed low-rank multimodal tensors to solve this problem. MFN <ref type="bibr" target="#b55">[56]</ref> first learned view-specific interactions for every single modality and then used the attention mechanism to summarize cross-perspective interactions through the multi-perspective gated memory.</p><p>Recent methods adopt Transformer-based methods for multimodal representation learning. For example, MulT <ref type="bibr" target="#b45">[46]</ref> managed to learn interactions between different modalities with directional cross-modal attention. MISA <ref type="bibr" target="#b18">[19]</ref> performed multimodal fusion with multi-headed self-attention to capture the relations between modality-invariant and modality-specific representations. MAG-BERT <ref type="bibr" target="#b35">[36]</ref> introduced the multimodal adaptation gate for pretrained Transformers to receive information from different modalities. In this work, we adapt the above three algorithms to multimodal intent recognition as benchmark methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Audio-visual Active Speaker Detection</head><p>Active speaker detection (ASD) aims to detect the speaker(s) in a visual scene. In this work, we focus on ASD with audio and visual information. Some studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref> treated this problem as a binary classification task and used a multi-layer perceptron (MLP) for ASD with concatenated audio and visual features. Besides, temporal structures <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44]</ref> such as recurrent neural networks (RNNs) were adopted to obtain better performance with time-series information. MAAS <ref type="bibr" target="#b1">[2]</ref> introduced graph convolutional networks (GCNs) to model interactions between audio and video modalities. TalkNet <ref type="bibr" target="#b44">[45]</ref> introduced an audio-visual cross-attention mechanism for effectively modeling cross-modal interactions and a selfattention mechanism for capturing long-term speech dependencies. In this work, we use TalkNet for automatic speaker annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>This paper first presents a new dataset for multimodal intent recognition. It has 2,224 high-quality annotated samples with corresponding multimodal information. New taxonomies with coarse-grained and fine-grained intent categories are specifically designed for realworld multimodal scenes. We also propose an automatic process to obtain the information of object boundings towards speakers, which vastly reduces the annotation burden. We make great efforts to ensure the quality of our dataset and build baselines with three multimodal fusion methods. Comprehensive experiments verify the superiority of multimodal information for intent recognition. The gap between the best-performing multimodal fusion methods and humans indicates there is still a long way to go for multimodal intent recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This paper is founded by National Natural Science Foundation of China (Grant No. 62173195) and Beijing Academy of Artificial Intelligence (BAAI). We would like to thank Guohui Guan, Wenrui Li, and Xiaofei Chen for their efforts during dataset construction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The process of building the MIntRec dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Voting statistics of 2,224 samples in MIntRec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the MIntRec dataset.</figDesc><table><row><cell>Total number of coarse-grained intents</cell><cell>2</cell></row><row><cell>Total number of fine-grained intents</cell><cell>20</cell></row><row><cell>Total number of videos</cell><cell>43</cell></row><row><cell>Total number of video segments</cell><cell>2,224</cell></row><row><cell>Total number of words in text utterances</cell><cell>15,658</cell></row><row><cell cols="2">Total number of unique words in text utterances 2,562</cell></row><row><cell>Average length of text utterances</cell><cell>7.04</cell></row><row><cell>Maximum length of text utterances</cell><cell>26</cell></row><row><cell>Average length of video segments (s)</cell><cell>2.38</cell></row><row><cell>Maximum length of video segments (s)</cell><cell>9.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Intent taxonomies of our MIntRec dataset with brief interpretations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Dataset splits in MIntRec. The training, validation and testing sets are split into 3:1:1.</figDesc><table><row><cell>Item</cell><cell cols="3">Total Express emotions or attitudes Achieve goals</cell></row><row><cell cols="2">Train 1,334</cell><cell>749</cell><cell>585</cell></row><row><cell>Valid</cell><cell>445</cell><cell>249</cell><cell>196</cell></row><row><cell>Test</cell><cell>445</cell><cell>248</cell><cell>197</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of each fine-grained intent category in "Express emotions and attitudes".</figDesc><table><row><cell>Methods</cell><cell>Complain</cell><cell>Praise</cell><cell>Apologise</cell><cell>Thank</cell><cell>Criticize</cell><cell>Care</cell><cell>Agree</cell><cell>Taunt</cell><cell>Flaunt</cell><cell>Oppose</cell><cell>Joke</cell></row><row><cell>Text-classifier</cell><cell>64.36</cell><cell>85.69</cell><cell>97.93</cell><cell>97.22</cell><cell>47.06</cell><cell>87.42</cell><cell>94.26</cell><cell>15.53</cell><cell>46.12</cell><cell>32.32</cell><cell>27.42</cell></row><row><cell>MAG-BERT</cell><cell>67.65</cell><cell>86.03</cell><cell>97.76</cell><cell>96.52</cell><cell>49.02</cell><cell>85.59</cell><cell>91.60</cell><cell>15.78</cell><cell>47.09</cell><cell>33.97</cell><cell>37.54</cell></row><row><cell>MulT</cell><cell>65.48</cell><cell>84.72</cell><cell>97.93</cell><cell>96.83</cell><cell>49.72</cell><cell>88.12</cell><cell>92.23</cell><cell>26.12</cell><cell>48.91</cell><cell>34.68</cell><cell>33.95</cell></row><row><cell>MISA</cell><cell>63.91</cell><cell>86.63</cell><cell>97.78</cell><cell>98.03</cell><cell>53.44</cell><cell>87.14</cell><cell>92.05</cell><cell>22.15</cell><cell>46.44</cell><cell>36.15</cell><cell>38.74</cell></row><row><cell>?</cell><cell>3.29?</cell><cell>0.94?</cell><cell>0.00</cell><cell>0.81?</cell><cell>6.38?</cell><cell>0.70?</cell><cell>2.03?</cell><cell>10.59?</cell><cell>2.79?</cell><cell>3.83?</cell><cell>11.32?</cell></row><row><cell>Human</cell><cell>80.08</cell><cell>93.44</cell><cell>96.15</cell><cell>96.90</cell><cell>72.21</cell><cell>96.09</cell><cell>87.21</cell><cell>65.55</cell><cell>78.10</cell><cell>69.04</cell><cell>72.22</cell></row><row><cell>?</cell><cell>15.72?</cell><cell>7.75?</cell><cell>1.78?</cell><cell>0.32?</cell><cell>25.15?</cell><cell>8.67?</cell><cell>7.05?</cell><cell>50.02?</cell><cell>31.98?</cell><cell>36.72?</cell><cell>44.80?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results of each fine-grained intent category in "Achieve goals".</figDesc><table><row><cell>Methods</cell><cell>Inform</cell><cell>Advise</cell><cell>Arrange</cell><cell>Introduce</cell><cell>Comfort</cell><cell>Leave</cell><cell>Prevent</cell><cell>Greet</cell><cell>Ask for help</cell></row><row><cell cols="2">Text-classifier 67.74</cell><cell>67.68</cell><cell>64.67</cell><cell>68.64</cell><cell>77.05</cell><cell>73.37</cell><cell>82.47</cell><cell>84.90</cell><cell>66.20</cell></row><row><cell>MAG-BERT</cell><cell>71.00</cell><cell>69.30</cell><cell>63.82</cell><cell>67.42</cell><cell>76.43</cell><cell>75.77</cell><cell>85.07</cell><cell>91.06</cell><cell>64.44</cell></row><row><cell>MulT</cell><cell>70.85</cell><cell>69.43</cell><cell>65.44</cell><cell>71.19</cell><cell>76.44</cell><cell>75.58</cell><cell>81.68</cell><cell>86.65</cell><cell>69.12</cell></row><row><cell>MISA</cell><cell>70.18</cell><cell>69.56</cell><cell>67.32</cell><cell>67.22</cell><cell>78.78</cell><cell>77.23</cell><cell>83.30</cell><cell>82.71</cell><cell>67.57</cell></row><row><cell>?</cell><cell>3.26?</cell><cell>1.88?</cell><cell>2.65?</cell><cell>2.55?</cell><cell>1.73?</cell><cell>3.86?</cell><cell>2.60?</cell><cell>6.16?</cell><cell>2.92?</cell></row><row><cell>Human</cell><cell>79.69</cell><cell>87.14</cell><cell>81.40</cell><cell>84.09</cell><cell>95.95</cell><cell>97.06</cell><cell>86.43</cell><cell>94.15</cell><cell>88.54</cell></row><row><cell>?</cell><cell>11.95?</cell><cell>19.46?</cell><cell>16.73?</cell><cell>15.45?</cell><cell>18.90?</cell><cell>23.69?</cell><cell>3.96?</cell><cell>9.25?</cell><cell>22.34?</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.opensubtitles.org/ 2 https://pypi.org/project/moviepy/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://pypi.org/project/scenedetect/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised learning of audio-visual objects from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="208" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MAAS: Multi-Modal Assignation for Active Speaker Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Juan L?on Alc?zar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2020. wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th Advances in Neural Information Processing Systems</title>
		<meeting>the 33th Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12449" to="12460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intention,-Plans,-and-Practical-Reason</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael E Bratman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">388</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating Natural Language Understanding Services for Conversational Question Answering Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">Hernandez</forename><surname>Mendez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Matthes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Langen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="174" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2506" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient Intent Detection with Dual Sentence Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Temcinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Annual Meeting of the Association for Computational Linguistics WorkShop</title>
		<meeting>the 2nd Annual Meeting of the Association for Computational Linguistics WorkShop</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards Multimodal Sarcasm Detection (An Obviously Perfect Paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ver?nica</forename><surname>P?rez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4619" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bert for joint intent classification and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?odore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<title level="m">Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SWITCHBOARD: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Humor Knowledge Enriched Transformer for Understanding Multimodal Humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwu</forename><surname>Md Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasifur</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12972" to="12980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Md Iftekhar Tanveer, Louis-Philippe Morency, and Mohammed Ehsan Hoque</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasifur</forename><surname>Md Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2046" to="2056" />
		</imprint>
	</monogr>
	<note>UR-FUNNY: A Multimodal Language Dataset for Understanding Humor</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Misa: Modality-invariant and-specific representations for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The ATIS spoken language systems pilot corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Hemphill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">R</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley</title>
		<meeting><address><addrLine>Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Intentonomy: a Dataset and Study towards Human Intent Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12986" to="12996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kruk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Lubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4622" to="4632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An Evaluation Dataset for Intent Classification and Outof-Scope Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Peper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjia</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1311" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Unknown Intent Detection with Margin Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Ting-En Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5491" to="5496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Ting-En Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8360" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Benchmarking natural language understanding services for building conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05566</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient Low-rank Multimodal Fusion With Modality-Specific Factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 26th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcvicar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Python in Science Conference</title>
		<meeting>the 14th Python in Science Conference</meeting>
		<imprint>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6149" to="6157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
	<note>Gautam Naik, Erik Cambria, and Rada Mihalcea</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Integrating multimodal information in large pretrained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasifur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwu</forename><surname>Md Kamrul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfeng</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2359" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ava active speaker: An audio-visual dataset for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharadh</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4492" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Emotion aided dialogue act classification for task-independent conversations in a multi-modal framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tulika</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhawal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="277" to="289" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards emotion-aided multi-modal dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tulika</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4361" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Intention, emotion, and action: A neural theory based on semantic pointers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schr?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Terrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thagard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="851" to="880" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Oxford english dictionary. Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esc</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ja</forename><forename type="middle">&amp;amp;</forename><surname>Weiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>Oxford; English</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Large-Scale Chinese Multimodal NER Dataset with Speech Clues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2807" to="2818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech activity detection with bimodal recurrent neural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Is Someone Speaking? Exploring Long-term Temporal Features for Audio-visual Active Speaker Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Rohan Kumar Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3927" to="3935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multimodal Transformer for Unaligned Multimodal Language Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6558" to="6569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Advances in neural information processing systems</title>
		<meeting>the 30th Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">VideoIC: A Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2599" to="2607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5133" to="5143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</title>
		<meeting>the 2020 conference on empirical methods in natural language processing: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Intelligent agents: Theory and practice. The knowledge engineering review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wooldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jennings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="115" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Short Text Clustering via Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yagcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiele</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyun</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3718" to="3727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tensor Fusion Network for Multimodal Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06259</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Joint Slot Filling and Intent Detection via Capsule Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5259" to="5267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">MultiMET: A Multimodal Dataset for Metaphor Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3214" to="3225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoteng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panpan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep Open Intent Classification with Adaptive Decision Boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-En</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14374" to="14382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Discovering New Intents with Deep Aligned Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ting-En Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14365" to="14373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
