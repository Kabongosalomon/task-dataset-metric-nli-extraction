<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Semi-Supervised Object Detection with Soft Teacher</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
							<email>mdxu@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<email>hanhu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<email>lijuanw@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Semi-Supervised Object Detection with Soft Teacher</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label qualities during the curriculum, and the more and more accurate pseudo labels in turn benefit object detection training. We also propose two simple yet effective techniques within this framework: a soft teacher mechanism where the classification loss of each unlabeled bounding box is weighed by the classification score produced by the teacher network; a box jittering approach to select reliable pseudo boxes for the learning of box regression. On the COCO benchmark, the proposed approach outperforms previous methods by a large margin under various labeling ratios, i.e. 1%, 5% and 10%. Moreover, our approach proves to perform also well when the amount of labeled data is relatively large. For example, it can improve a 40.9 mAP baseline detector trained using the full COCO training set by +3.6 mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the state-ofthe-art Swin Transformer based object detector (58.9 mAP on test-dev), it can still significantly improve the detection accuracy by +1.5 mAP, reaching 60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching 52.4 mAP. Further incorporating with the Object365 pretrained model, the detection accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, pushing the new state-of-the-art. The code and models will be made publicly available at https://github.com/ microsoft/SoftTeacher.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data matters. In fact, large data such as ImageNet has largely triggered the boom of deep learning in computer vi-* Equal contribution. ?This work is done when Mengde Xu was intern in MSRA. ?Contact person.  <ref type="figure">Figure 1</ref>. The proposed end-to-end pseudo-label based semisupervised object detection method outperforms the STAC <ref type="bibr" target="#b26">[27]</ref> by a large margin on MS-COCO benchmark. sion. However, obtaining labels can be a bottleneck, due to the time-consuming and expensive annotation process. This has encouraged learning methods to leverage unlabeled data in training deep neural models, such as selfsupervised learning and semi-supervised learning. This paper studies the problem of semi-supervised learning, in particular for object detection. For semi-supervised object detection, we are concerned with the pseudo-label based approaches, which are the current state-of-the-art. These approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref> conduct a multi-stage training schema, with the first stage training an initial detector using labeled data, followed by a pseudo-labeling process for unlabeled data and a re-training step based on the pseudo labeled unannotated data. These multi-stage approaches achieve reasonably good accuracy, however, the final performance is limited by the quality of pseudo labels generated by an initial and probably inaccurate detector trained using a small amount of labeled data. To address this issue, we present an end-to-end pseudolabel based semi-supervised object detection framework, which simultaneously performs pseudo-labeling for unlabeled images and trains a detector using these pseudo labels along with a few labeled ones at each iteration. Specifically, labeled and unlabeled images are randomly sampled with a preset ratio to form one data batch. Two models are applied on these images, with one conducting detection training and the other in charge of annotating pseudo labels for unlabeled images. The former is also referred to as a student, and the latter is a teacher, which is an exponential moving average (EMA) of the student model. This end-to-end approach avoids the complicated multi-stage training scheme. Moreover, it also enables a "flywheel effect" that the pseudo labeling and the detection training processes can mutually reinforce each other, so that both get better and better as the training goes on.</p><p>Another important benefit of this end-to-end framework is that it allows for greater leverage of the teacher model to guide the training of the student model, rather than just providing "some generated pseudo boxes with hard category labels" as in previous approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>. A soft teacher approach is proposed to implement this insight. In this approach, the teacher model is used to directly assess all the box candidates that are generated by the student model, rather than providing "pseudo boxes" to assign category labels and regression vectors to these student-generated box candidates. The direct assessment on these box candidates enables more extensive supervision information to be used in the student model training. Specifically, we first categorize the box candidates as foreground/background by their detection scores with a high foreground threshold to ensure a high precision of the positive pseudo labels, as in <ref type="bibr" target="#b26">[27]</ref>. This high foreground threshold, however, results in many positive box candidates mistakenly assigned as background. To address this issue, we propose using a reliability measure to weight the loss of each "background" box candidate. We empirically find that a simple detection score produced by the teacher model can well serve as the reliability measure, and is used in our approach. We find that this approach measure performs significantly better than previous hard fore-ground/background assignment methods (see <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table">Table 4</ref>), and we name it "soft teacher".</p><p>Another approach instantiates this insight is to select reliable bounding boxes for the training of the student's localization branch, by a box jittering approach. This approach first jitters a pseudo-foreground box candidate several times. Then these jittered boxes are regressed according the teacher model's location branch, and the variance of these regressed boxes is used as a reliability measure. The box candidate with adequately high reliability will be used for the training of the student's localization branch.</p><p>On MS-COCO object detection benchmark <ref type="bibr" target="#b15">[16]</ref>, our approach achieves 20.5 mAP, 30.7 mAP and 34.0 mAP on val2017 with 1%, 5% and 10% labeled data using the Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> framework with ResNet-50 <ref type="bibr" target="#b7">[8]</ref> and FPN <ref type="bibr" target="#b13">[14]</ref>, surpassing previous best method STAC <ref type="bibr" target="#b26">[27]</ref> by +6.5, +6.4 and +5.4 mAP, respectively.</p><p>In addition, we also perform evaluation on a more challenge setting where the labelled data has been adequately large to train a reasonably accurate object detector. Specifically, we adopt the complete COCO train2017 set as labeled data and the unlabeled2017 set as the unlabeled data. Under this setting, we improve the supervised baseline of a Faster R-CNN approach with ResNet-50 and ResNet-101 backbones by +3.6 mAP and +3.0 mAP, respectively.</p><p>Moreover, on a state-of-the-art Swin-Transformer <ref type="bibr" target="#b17">[18]</ref> based detector which achieves 58.9 mAP for object detection and 51.2 mAP for instance segmentation on COCO test-dev2017, the proposed approach can still improve the accuracy by +1.5 mAP and +1.2 mAP, respectively, reaching 60.4 mAP and 52.4 mAP. Further incorporating with the Object365 <ref type="bibr" target="#b23">[24]</ref> pre-trained model, the detection accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, which is the new state-of-the-art on this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Semi-supervised learning in image classification Semisupervised learning in image classification can be roughly categorized into two groups: consistency based and pseudolabel based. The consistency based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11]</ref>   <ref type="figure">Figure 2</ref>. The overview of the end-to-end pseudo-labeling framework for semi-supervised object detection. Unlabeled images and labeled images form the training data batch. In each training iteration, a soft teacher is applied to perform pseudo-labeling on weak augmented unlabeled images on the fly. Two sets of pseudo boxes are produced: one is used for classification branch by filtering boxes according to the foreground score, and the other is used for box regression branch by filtering boxes according to box regression variance. The teacher model is updated by student model via exponential mean average (EMA) manner. The final loss is the sum of supervised detection loss Ls and unsupervised detection loss Lu.</p><p>leverage the unlabeled images to construct a regularization loss which encourages different perturbations of a same image to produce similar predictions. There are several ways to implement perturbations, including perturbing the model <ref type="bibr" target="#b0">[1]</ref>, augmenting the images <ref type="bibr" target="#b22">[23]</ref> or adversarial training <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, the training target is assembled by predicting different training steps. In <ref type="bibr" target="#b28">[29]</ref>, they develop <ref type="bibr" target="#b10">[11]</ref> by ensembling the model itself instead of the model prediction, the so-called exponential mean average (EMA) of the student model. The pseudo-label approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref> (also named as self-training) annotate unlabeled images with pseudo labels by an initially trained classification model, and the detector is refined by these pseudo labeled images. Unlike our method which focuses on object detection, the pseudo-label does not have to solve the problem of assigning foreground/background labels and box regression when classifying images. Recently, some works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26]</ref> explore the importance of data augmentation in semi-supervised learning, which inspire us to use the weak augmentation to generate pseudo-labels and the strong augmentation for the learning of detection models.</p><p>Semi-supervised learning in object detection Similar to the semi-supervised learning in image classification, semisupervised object detection methods also have two categories: the consistency methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> and pseudo-label methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. Our method belongs to the pseudo-label category. In <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>, the predictions of different data augmentation are ensembled to form the pseudo labels for unlabeled images. In <ref type="bibr" target="#b12">[13]</ref>, a SelectiveNet is trained to select the pseudo-label. In <ref type="bibr" target="#b30">[31]</ref>, a box detected on an unlabeled image is pasted onto a labeled image, and the localization consistency estimation is performed onto the pasted label image. As the image itself is modified, a very thorough detection process is required in <ref type="bibr" target="#b30">[31]</ref>. In our method, only the lightweight detection head is processed. STAC <ref type="bibr" target="#b26">[27]</ref> proposes to use a weak data augmentation for model training and a strong data augmentation is used for performing pseudo-label. However, like other pseudo-label methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>, it also follows the multi-stage training scheme. In contrast, our method is an end-to-end pseudo-labeling framework, which avoids the complicated training process and also achieves better performance.</p><p>Object Detection Object detection focuses on designing efficient and accurate detection framework. There are two mainstreams: single-stage object detectors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> and two-stage object detectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. The main difference between the two types of methods is whether to use a cascade to filter a large number of object candidates (proposals). In theory, our method is compatible with both types of methods. However, to allow a fair comparison with previous works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref> on semi-supervised object detection, we use Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> as our default detection framework to illustrate our method. <ref type="figure">Figure.</ref> 2 illustrates an overview of our end-to-end training framework. There are two models, a student model and a teacher model. The student model is learned by both the detection losses on the labeled images and on the unlabeled images using pseudo boxes. The unlabeled images have two sets of pseudo boxes, which are used to drive the training of the classification branch and the regression branch, respec-tively. The teacher model is an exponential moving average (EMA) of the student model. Within this end-to-end framework, there are two crucial designs: soft teacher and box jittering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">End-to-End Pseudo-Labeling Framework</head><p>We first introduce the end-to-end framework for pseudolabel based semi-supervised object detection. Our approach follows the teacher-student training scheme. In each training iteration, labeled images and unlabeled images are randomly sampled according to a data sampling ratio s r to form a training data batch. The teacher model is performed to generate the pseudo boxes on unlabeled images and the student model is trained on both labeled images with the ground-truth and unlabeled images with the pseudo boxes as the ground-truth. Thus, the overall loss is defined as the weighted sum of supervised loss and unsupervised loss:</p><formula xml:id="formula_0">L = L s + ?L u ,<label>(1)</label></formula><p>where L s and L u denote supervised loss of labeled images and unsupervised loss of unlabeled images respectively, ? controls contribution of unsupervised loss. Both of them are normalized by the respective number of images in the training data batch:</p><formula xml:id="formula_1">L s = 1 N l N l i=1 (L cls (I i l ) + L reg (I i l )),<label>(2)</label></formula><formula xml:id="formula_2">L u = 1 N u Nu i=1 (L cls (I i u ) + L reg (I i u )),<label>(3)</label></formula><p>where I i l indicates the i-th labeled image, I i u indicates the i-th unlabeled image, L cls is the classification loss, L reg is the box regression loss, N l and N u denote the number of labeled images and unlabeled images, respectively.</p><p>At the beginning of training, both the teacher model and student model are randomly initialized. As the training progresses, the teacher model is continuously updated by the student model, and we follow the common practices <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref> that the teacher model is updated by exponential moving average (EMA) strategy.</p><p>In contrast to taking a simple probability distribution as the pseudo-label in image classification, creating pseudolabel for object detection is more complicated since an image usually contains multiple objects and the annotation of objects consists of location and category. Given an unlabeled image, the teacher model is used to detect objects and thousands of box candidates are predicted. The nonmaximum suppression (NMS) is then performed to eliminate redundancy. Although most redundant boxes are removed, there are still some non-foreground candidates left. Therefore, only candidates with the foreground score 1 higher than a threshold are retained as the pseudo boxes.</p><p>In order to generate high-quality pseudo boxes and to facilitate the training of the student model, we draw on FixMatch <ref type="bibr" target="#b25">[26]</ref> which is the latest advancement in semisupervised image classification task. Strong augmentation is applied for detection training of the student model and weak augmentation is used for pseudo-labeling of the teacher model.</p><p>In theory, our framework is applicable to mainstream object detectors, including single-stage object detectors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> and two-stage object detectors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref>. To allow a fair comparison with previous methods, we use Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> as our default detection framework to illustrate our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Soft Teacher</head><p>The performance of the detector depends on the quality of the pseudo-label. In practice, we find that using a higher threshold on foreground score to filter out most of the student-generated box candidates with low-confidence can achieve better results than using a lower threshold. As shown in <ref type="table">Table.</ref> 9, the best performance is achieved when the threshold is set to 0.9. However, while the strict criteria (higher threshold) leads to higher foreground precision, the recall of the retained box candidates also falls off quickly. As shown in <ref type="figure" target="#fig_1">Figure. 3 (a)</ref>, when the foreground threshold is set to 0.9, the recall is low, as 33%, while the precision reaches 89%. In this case, if we use IoU between student generated box candidates and teachergenerated pseudo boxes to assign foreground and background labels, as a general object detection framework does when real box annotations are provided, some foreground box candidates will be mistakenly assigned as negatives, which may hinder the training and harm the performance.</p><p>To alleviate this issue, we propose a soft teacher approach which leverages richer information from the teacher model, thanks to the flexibility of the end-to-end framework. Specifically, we assess the reliability of each studentgenerated box candidate to be a real background, which is then used to weigh its background classification loss. Given two box sets {b fg i } and {b bg i }, with {b fg i } denoting boxes assigned as foreground and {b bg i } denoting the boxes assigned as background, the classification loss of an unlabeled image with the reliable weighting is defined as: </p><formula xml:id="formula_3">L cls u = 1 N fg b N fg b i=1 l cls (b fg i , G cls ) + N bg b j=1 w j l cls (b bg j , G cls ),<label>(4)</label></formula><formula xml:id="formula_4">w j = r j N bg b k=1 r k ,<label>(5)</label></formula><p>where G cls denotes the set of (teacher-generated) pseudo boxes used for classification, l cls is the box classification loss, r j is the reliability score for j-th background box candidate, N fg b and N bg b are the number of box candidates of the box set {b fg i } and {b bg i }, respectively. Estimating the reliability score r is challenging. We find empirically that the background score produced by the teacher model with weak augmented image can well serve as a proxy indicator of r and is easily obtained in our endto-end training framework. Specifically, given a studentgenerated box candidate, its background score can be obtained simply by using the teacher (BG-T) to process the box through its detection head. It is worth noting that this approach, unlike the widely used hard negative mining approaches, e.g., OHEM <ref type="bibr" target="#b24">[25]</ref> or Focal Loss <ref type="bibr" target="#b14">[15]</ref>, is more like a "simple" negative mining. For comparison, we also examine several other indicators:</p><p>? Background score of student model (BG-S): Another natural way to generate the background score is to use the prediction of student model directly.</p><p>? Prediction difference (Pred-Diff): The prediction difference between the student model and teacher model is also a possible indicator. In our approach, we simply use the difference between the background scores of the two models to define the reliability score:</p><formula xml:id="formula_5">r = 1 ? |p bg S (b) ? p bg T (b)|,<label>(6)</label></formula><p>where p bg S and p bg T are the predicted probability of the background class of the student and the teacher model, respectively.</p><p>? Intersection-over-Union: The IoU between groundtruths and box candidates is a commonly used criterion for foreground/background assignment. There are two different hypotheses about how to use IoU to measure whether a box candidate belongs to the background.</p><p>In the first hypothesis, if the IoU between a box candidate and a ground-truth box is less than a threshold (e.g., 0.5), a larger IoU indicates the box candidate has greater probability of being background. This can be viewed as an IoU-based hard negative mining which is adopted by Fast R-CNN <ref type="bibr" target="#b5">[6]</ref> and Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> in the early implementation. In contrast, the other hypothesis suggests that box candidates with a smaller IoU with ground-truths are more likely to be backgrounds. In our experiments, we validate both hypotheses and name them as IoU and Reverse-IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Box Jittering</head><p>As shown in <ref type="figure" target="#fig_1">Figure. 3 (b)</ref>, the localization accuracy and the foreground score of the box candidates do not show a strong positive correlation, which means that the boxes with high foreground score may not provide accurate localization information. This indicates that the selection of the teachergenerated pseudo boxes according to the foreground score is not suitable for box regression, and a better criterion is needed.</p><p>We introduce an intuitive approach to estimate the localization reliability of a candidate pseudo box by measuring the consistency of its regression prediction. Specifically, given a teacher-generated pseudo box candidate b i , we sample a jittered box around b i and feed the jittered box into the teacher model to obtain the refined boxb i , which is formulated as follows:b i = refine(jitter(b i )).</p><p>The above procedure is repeated several times to collect a set of N jitter refined jittered boxes {b i,j }, and we define the localization reliability as the box regression variance:</p><formula xml:id="formula_7">? i = 1 4 4 k=1? k ,<label>(8)</label></formula><formula xml:id="formula_8">? k = ? k 0.5(h(b i ) + w(b i )) ,<label>(9)</label></formula><p>where ? k is the standard derivation of the k-th coordinate of the refined jittered boxes set {b i,j },? k is the normalized ? k , h(b i ) and w(b i ) represent the height and width of box candidate b i , respectively. A smaller box regression variance indicates a higher localization reliability. However, computing the box regression variances of all pseudo box candidates is unbearable during training. Therefore, in practice, we only calculate the reliability for the boxes with a foreground score greater than 0.5. In this way, the number of boxes that need to be estimated is reduced from an average of hundreds to around 17 per image and thus the computation cost is almost negligible.</p><p>In <ref type="figure" target="#fig_1">Figure. 3 (c)</ref>, we illustrate the correlation between the localization accuracy and our box regression variance. Compared with the foreground score, the box regression variance can better measure the localization accuracy. This motivates us to select box candidates whose box regression variance is smaller than a threshold as pseudo-label to train the box regression branch on unlabeled images. Given the pseudo boxes G reg for training the box regression on unlabeled data, the regression loss is formulated as:</p><formula xml:id="formula_9">L reg u = 1 N fg b N fg b i=1 l reg (b fg i , G reg ),<label>(10)</label></formula><p>where b fg i is i-th box assigned as foreground, N fg b is the total number of foreground box, l reg is the box regression loss. Therefore, by substituting Equ. 4 and Equ. 10 into Equ. 3, the loss of unlabeled images is:</p><formula xml:id="formula_10">L u = 1 N u Nu i=1 (L cls u (I i u , G i cls ) + L reg u (I i u , G i reg )).<label>(11)</label></formula><p>Here we use the pseudo boxes G cls and G reg as the inputs of the loss to highlight the fact that the pseudo boxes used in classification and box regression are different in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Protocol</head><p>We validate our method on the MS-COCO benchmark <ref type="bibr" target="#b15">[16]</ref>.</p><p>Two training datasets are provided, the train2017 set contains 118k labeled images and the unlabeled2017 set contains 123k unlabeled images. In addition, the val2017 set with 5k images is also provided for validation. In previous methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref>, there are two settings for validating the performance: Partially Labeled Data: STAC <ref type="bibr" target="#b26">[27]</ref> first introduced this setting. 1%, 5% and 10% images of train2017 set are sampled as the labeled training data, and the remaining unsampled images of train2017 are used as the unlabeled data. For each protocol, STAC provides 5 different data folds and the final performance is the average of all 5 folds. Fully Labeled Data:</p><p>In this setting, the entire train2017 is used as the labeled data and unlabeled2017 is used as the additional unlabeled data. This setting is more challenging. Its goal is to use the additional unlabeled data to improve a well-trained detector on large-scale labeled data.</p><p>We evaluate our method on both settings and follow the convention to report the performance on val2017 with the standard mean average precision (mAP) as the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use the Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> equipped with FPN <ref type="bibr" target="#b13">[14]</ref> (Feature Pyramid Network) as our default detection framework to evaluate the effectiveness of our method, and an ImageNet pre-trained ResNet-50 <ref type="bibr" target="#b7">[8]</ref> is adopted as the backbone. Our implementation and hyper-parameters are based on MMDetection <ref type="bibr" target="#b3">[4]</ref>. Anchors with 5 scales and 3 aspect ratios are used. 2k and 1k region proposals are generated with a non-maximum suppression threshold of 0.7 for training and inference. In each training step, 512 proposals are sampled from 2k proposals as the box candidates to train RCNN. Since the amount of training data of Partially Labeled Data setting and Full Labeled Data setting has large differences, the training parameters under the two settings are slightly different. Partially Labeled Data: The model is trained for 180k iterations on 8 GPUs with 5 image per GPU. With SGD training, the learning rate is initialized to 0.01 and is divided by 10 at 110k iteration and 160k iteration. The weight decay and the momentum are set to 0.0001 and 0.9, respectively. The foreground threshold is set to 0.9 and the data sampling ratio s r is set to 0.2 and gradually decreases to 0 over the last 10k iterations. Fully Labeled Data: The model is trained for 720k iterations on 8 GPUs with 8 image per GPU. In SGD training, the learning rate is initialized to 0.01 and is divided by 10 at 480k iteration and 680k iteration. The weight decay and the momentum are set to 0.0001 and 0.9, respectively. The foreground threshold is set to 0.9 and the data sampling ratio s r is set to 0.5 and gradually decreases to 0 in the last 20k iterations.</p><p>For estimating the box localization reliability, we set N jitter as 10, and threshold is set as 0.02 to select the pseudolabels for box regression. The jittered boxes are randomly sampled by adding the offsets on four coordinates, and the offsets are uniformly sampled from [-6%, 6%] of the height or width of the pseudo box candidates. In addition, we follow STAC and FixMatch to use different data augmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labeled image training</head><p>Unlabeled image training Pseudo-label generation Scale jitter short edge ? (0.5, 1.5) short edge ? (0.5, 1.5) short edge ? (0.5, 1.5) Solarize jitter p=0.25, ratio ? (0, 1) p=0.25, ratio ? (0, 1) -Brightness jitter p=0.25, ratio ? (0, 1) p=0.25, ratio ? (0, 1) -Constrast jitter p=0.25, ratio ? (0, 1) p=0.25, ratio ? (0, 1) -Sharpness jitter p=0.25, ratio ? (0, 1) p=0.25, ratio ? (0, 1) <ref type="figure">5)</ref>, ratio ? (0.05, 0.2) num ? (1, 5), ratio ? (0.05, 0.2) - <ref type="table">Table 2</ref>. The summary of the data augmentation used in our approach. We follow the practice of STAC <ref type="bibr" target="#b26">[27]</ref> and FixMatch <ref type="bibr" target="#b25">[26]</ref> to provide different data augmentation for pseudo-label generation, labeled image training and unlabeled image training. "-" indicates the augmentation is not used.  </p><formula xml:id="formula_11">- Translation - p=0.3, translation ratio ? (0, 0.1) - Rotate - p=0.3, angle ? (0, 30 ? ) - Shift - p=0.3, angle ? (0, 30 ? ) - Cutout num ? (1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">System Comparison</head><p>In this section, we compare our method with previous state-of-the-arts on MS-COCO. We first evaluate on the Partially Labeled Data setting and compare our method with STAC. For benchmarking, we compare the supervised baseline of our method with the results reported in STAC and find they perform similarly, the results are shown in <ref type="table">Table.</ref> 3. In this case, we further compare our method with STAC at the system level, and our method shows a significant performance improvement in different protocols. Specifically, our method outperforms the STAC by 6.5 points, 6.4 points and 5.4 points when there are 1%, 5%, and 10% labeled data, respectively. The qualitative results of our method compared with supervised baseline are shown in <ref type="figure" target="#fig_2">Figure. 4</ref> Then we compare our method with other state-of-theart methods in Fully Labeled Data setting. Since the reported performance of supervised baseline varies in different works, we report the results of the comparison methods and their baseline at the same time. The results are shown in <ref type="table">Table.</ref> 4.</p><p>We first compare with the Proposal Learning <ref type="bibr" target="#b27">[28]</ref> and STAC <ref type="bibr" target="#b26">[27]</ref> which also use unlabeled2017 as additional unlabeled data. Because of the better hyper-parameters and more adequate training, our supervised baseline achieved better performance than other methods. Under the stronger baseline, our method still shows a greater performance gain (+3.6 points) than Proposal Learning (+1.0 points) and STAC (-0.3 points). Self-training <ref type="bibr" target="#b35">[36]</ref> uses Ima-geNet (1.2M images) and OpenImages (1.7M images) as the additional unlabeled data, which is 20? larger than the unlabeled2017 (123k images) that we use. With similar baseline performance, our method also shows better result with less unlabeled data.</p><p>In addition, we further evaluate our method on other stronger detectors, and the results evaluated on val2017 set are shown in <ref type="table">Table.</ref> 5. Our method consistently improves the performance of different detectors by a notable margin. Even in the state-of-the-art detector HTC++ with Swin-L backbone, we still show 1.8 improvement on detection AP and 1.4 improvement on mask AP. Moreover, we also report the results on test-dev2017 set. As shown in Tabel. 1, our method improves the HTC++ with Swin-L backbone by 1.5 mAP on detection, which is the first work to surpass 60 mAP on COCO object detection benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>In this section, we validate our key designs. If not specified, all the ablation experiments are performed on the single data fold provided by <ref type="bibr" target="#b26">[27]</ref> with 10% labeled images from train2017 set.</p><p>Multi-Stage vs. End-to-End. We compare our end-toend method with the multi-stage framework as shown in Table 6. By simply switching from the multi-stage framework to our end-to-end framework, performance is increased by 1.3 points. By updating the teacher model with the student model through the exponential moving average (EMA) strategy, our method further achieves 31.2 mAP.</p><p>Effects of Soft Teacher and Box Jittering. We ablate the effects of soft teacher and box jittering. The results are shown in <ref type="table">Table.</ref> 7. Based on our end-to-end model equipped with EMA (E2E+EMA), integrating the soft teacher improves the performance by 2.4 points. Further applying the box jittering, the performance reaches 34.2 mAP, which is 3 points better than E2E+EMA.</p><p>Different Indicators in Soft Teacher. In Section. 3.2, several different indicators are explored for reliability estimation. Here, we evaluate the different indicators and the results are shown in <ref type="table">Table.</ref> 8. The background score predicted by the teacher model achieves the best performance. Simply switching the model from teacher to student will make the performance worse. In addition, the improvement of IoU and Revearse-IoU is negligible compared with BG-T. These results prove the necessity of leveraging the teacher model.</p><p>Effects of other hyper-parameters. We study the effects of hyper-parameters used in our method. Table. 9 studies the effects of different foreground score thresholds. The best performance is achieved when the threshold is set to 0.9, and lower or higher thresholds will cause significant  <ref type="table" target="#tab_0">Table 11</ref>. Ablation study on the effects of different number of jittered boxes used to estimate the box regression variance. performance degradation. In <ref type="table">Table.</ref> 10, we study the box regression variance threshold. The best performance is achieved when the threshold is set to 0.02. In <ref type="table">Table.</ref> 11, we study the effects of different number of jittered boxes, and the performance is saturated when N jitter is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed an end-to-end training framework for semi-supervised object detection, which discards the complicated multi-stage schema adopted by previous approaches. Our method simultaneously improves the detector and pseudo labels by leveraging a student model for detection training, and a teacher model which is continuously updated by the student model through the exponential moving average strategy for online pseudo-labeling. Within the end-to-end training, we present two simple techniques named soft teacher and box jittering to facilitate the efficient leverage of the teacher model. The proposed framework outperforms the state-of-the-art methods by a large margin on MS-COCO benchmark in both partially labeled data and fully labeled data settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>We would like to thank Yue Cao for his valuable suggestions and discussions; Yutong Lin and Yixuan Wei for help on Swin Transformer experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>We randomly sampled 10k unlabeled training images from train2017 to draw figures based on the model trained with 10% labeled images. (a) precision and recall of foreground under different foreground score thresholds. (b) the correlation between the IoU with ground-truth and box foreground score. (c) the correlation between the IoU with ground-truth and box regression variance. Each point in (b) and (c) represents a box candidate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The qualitative results of our method. (a), (c) are the results of the supervised baseline. (b), (d) are the results of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>arXiv:2106.09018v3 [cs.CV] 6 Aug 2021 On the state-of-the-art detector HTC++(Swin-L), our method surpasses the supervised learning on both val2017 and test-dev2017. * indicates that models are pre-trained with Object365<ref type="bibr" target="#b23">[24]</ref> dataset.</figDesc><table><row><cell>detector</cell><cell>method</cell><cell cols="4">val2017 mAP det mAP mask mAP det mAP mask test-dev2017</cell></row><row><cell></cell><cell>supervised</cell><cell>57.1</cell><cell>49.6</cell><cell>-</cell><cell>-</cell></row><row><cell>HTC++(Swin-L) w/ single-scale</cell><cell>ours</cell><cell>59.1</cell><cell>51.0</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ours  *</cell><cell>60.1</cell><cell>51.9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>supervised</cell><cell>58.2</cell><cell>50.5</cell><cell>58.9</cell><cell>51.2</cell></row><row><cell>HTC++(Swin-L) w/ multi-scale</cell><cell>ours</cell><cell>59.9</cell><cell>51.9</cell><cell>60.4</cell><cell>52.4</cell></row><row><cell></cell><cell>ours  *</cell><cell>60.7</cell><cell>52.5</cell><cell>61.3</cell><cell>53.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>System level comparison with STAC on val2017 under the Partially Labeled Data setting. All the results are the average of all 5 folds. For benchmarking, we also compare the supervised benchmark performance between our method and STAC, and their performance is similar.</figDesc><table><row><cell>Method</cell><cell>Extra dataset</cell><cell></cell><cell>mAP</cell></row><row><cell>Proposal learning [28]</cell><cell>unlabeled2017</cell><cell>37.4</cell><cell>+1.0 ? ? ? 38.4</cell></row><row><cell>STAC [27]</cell><cell>unlabeled2017</cell><cell>39.5</cell><cell>-0.3 ? ? ? 39.2</cell></row><row><cell>Self-training [36]</cell><cell cols="2">ImageNet+OpenImages 41.1</cell><cell>+0.8 ? ? ? 41.9</cell></row><row><cell>Ours</cell><cell>unlabeled2017</cell><cell>40.9</cell><cell>+3.6 ? ? ? 44.5</cell></row><row><cell cols="4">Table 4. Comparison with other state-of-the-arts under the setting of using all data of train2017 set. Particularly, Self-training uses</cell></row><row><cell cols="4">ImageNet (1.2M images) and OpenImages (1.7M images) as additional unlabeled images, which is 20? larger than unlabeled2017</cell></row><row><cell>(123k images).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">for pseudo-label generation, labeled image training and un-</cell><cell></cell><cell></cell></row><row><cell cols="2">labeled image training. The details are summarized in Ta-</cell><cell></cell><cell></cell></row><row><cell>ble .2.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Compared with various supervised trained detectors on val2017. The entire train2017 is used as the labeled images, and the unlabeled2017 is used as the additional unlabeled images.</figDesc><table><row><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .Table 8 .Table 9 .Table 10 .</head><label>78910</label><figDesc>Soft teacher Box jittering mAP mAP@0.5 mAP@0.75 We study the effects of soft teacher and box jittering techniques. Comparison of different indicators in soft teacher. Ablation study on the effects of different foreground thresholds. Ablation study on the effects of different thresholds for selecting pseudo boxes for box regression according to box regression variance.</figDesc><table><row><cell>31.2</cell><cell>48.8</cell><cell>34.0</cell></row><row><cell>33.6</cell><cell>52.9</cell><cell>36.6</cell></row><row><cell>34.2</cell><cell>52.6</cell><cell>37.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The foreground score is defined as the maximum probability of all non-background categories.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4864</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Learning with pseudo-ensembles. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reppoints v2: Verification meets regression for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving object detection with selective selfsupervised self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omnisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04586</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>and confidence. NIPS, 2020. 3, 4</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Proposal learning for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetan</forename><surname>Ramaiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021. 3, 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards human-machine cooperation: Selfsupervised sample mining for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dense reppoints: Representing visual objects with dense point sets. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and self-training. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
