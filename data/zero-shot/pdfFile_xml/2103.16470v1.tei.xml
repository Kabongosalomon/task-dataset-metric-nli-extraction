<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The objective of this paper is to learn context-and depthaware feature representation to solve the problem of monocular 3D object detection. We make following contributions: (i) rather than appealing to the complicated pseudo-LiDAR based approach, we propose a depth-conditioned dynamic message propagation (DDMP) network to effectively integrate the multi-scale depth information with the image context; (ii) this is achieved by first adaptively sampling context-aware nodes in the image context and then dynamically predicting hybrid depth-dependent filter weights and affinity matrices for propagating information; (iii) by augmenting a center-aware depth encoding (CDE) task, our method successfully alleviates the inaccurate depth prior; (iv) we thoroughly demonstrate the effectiveness of our proposed approach and show state-of-the-art results among the monocular-based approaches on the KITTI benchmark dataset. Particularly, we rank 1 st in the highly competitive KITTI monocular 3D object detection track on the submission day (November 16th, 2020). Code and models are released at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is a fundamental problem in computer vision. Although promising progress in 2D object detection has been made <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b43">43]</ref> with convolutional neural networks (CNNs) in recent years, 3D object detection that perceives 3D object location, physical dimension, and orientation, still remains challenging and critical in appli-* The first three authors contributed equally to this work. <ref type="bibr">?</ref>   cations such as autonomous driving <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">15]</ref>, robotic grasp and navigation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">41]</ref>, and Mixed Reality (MR) <ref type="bibr" target="#b39">[39]</ref>.</p><p>LiDAR point cloud based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b53">53]</ref> have excelled in 3D object detection and achieve superior performance, however, they still depend on the expensive LiDAR sensors and sparse data representation to make them scalable. Cheaper alternative such as perceiving RGB images that are captured by the monocular camera is drawing increasing attention. Some image-only based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29]</ref> aim to explore the 2D-3D geometric consistency for recovering reasonable 3D detection. Nevertheless, the performance is still far from satisfactory. This is because (i) scale variance caused by the perspective projection. The monocular views at far and near distance cause significant changes in object scales. It is difficult for conventional CNNs (e.g., 2D conv) to process objects of different scales. (ii) lack of depth cues for the CNNs to capture the depth-aware feature for 3D reasoning.</p><p>Recent efforts have been made to pursue pseudo-LiDAR based approaches <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b49">49]</ref>. The pseudo-LiDAR approaches first transform depth maps estimated from 2D images to point cloud data representations and then adopt existing LiDAR-based 3D detectors for prediction. Although improved performances on monocular 3D detection have been observed, they still suffer from the gap between inaccurate estimated depth and real-world depth. Additionally, LiDAR-based approaches only employ 3D spatial information to generate LiDAR-resembled point cloud but discard the semantic information from RGB images, which is a vital clue to perceive and distinguish objects.</p><p>Another line of research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b37">37]</ref> focuses on the perspective of image and depth fusion learning. Arguably, with the depth assisted, the model can learn depth-aware feature representation, and both semantic and structure knowledge can be integrated for 3D reasoning. Specifically, D 4 LCN <ref type="bibr" target="#b7">[8]</ref> proposes to generate dynamic-depthwisedilated kernels from depth features to integrate with image context. However, two issues still remain unsolved, (i) Its empirical design can not guarantee the discriminative power of the model and the local dilated convolution is not be able to fully capture the object context in the condition of perspective projection and occlusion. (ii) Its 3D detection performance heavily relies on the precision of estimated depth maps. The model has no clue to resolve the inferior 3D localization caused by the inaccurate depth prior.</p><p>To this end, for the first time we present a graph-based formulation, a novel depth-conditioned dynamic message propagation model (DDMP-3D), to effectively learn depthaware feature representations for monocular 3D object detection. As shown in figure 1, specifically, considering each feature pixel as a node within a graph, we first dynamically sample the neighbourhoods of a node from the feature graph. This operation allows the network to gather the object contexts efficiently by adaptively selecting a subset of the most relevant nodes in the graph. For the sampled nodes, we further predict filter weights and affinity matrices dependent on the aligned depth feature to propagate information through the sampled nodes. Moreover, multi-scale depth features are explored over the propagation process, hybrid filter weights and affinity matrices are learned to adapt various scales of objects.</p><p>Additionally, to resolve the challenge of inaccurate depth prior, a center-aware depth encoding (CDE) is augmented as an auxiliary task append at the depth branch. It performs 3D object center regression task which explicitly guides the intermediate features of the depth branch to be instanceaware and further improves the localization of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image-only 3D detection Due to the absence of accurate depth information, the monocular 3D detection task remains challenging. Several works rely only on RGB information and geometry consistency to predict 3D boxes. For instance, Deep3DBox <ref type="bibr" target="#b29">[29]</ref> utilizes bins classification to solve the orientation regression and enforces the 2D-3D box constraint to recover 3D locations and shape. M3D-RPN <ref type="bibr" target="#b0">[1]</ref> leverages the geometric relationship between 2D and 3D perspectives by sharing the prior anchors and classification targets. For a better understanding of the spatial relationship between original 3D bounding boxes and the object, FQNet <ref type="bibr" target="#b25">[25]</ref> infers the 3D IoU between 3D proposals and the object by drawing the projection results on the image plane to bring additional information. However, the orientation predictions can affect the following location prediction greatly, leading to a coupled 3D parameter regression. MonoPair <ref type="bibr" target="#b5">[6]</ref> captures spatial relationships between paired objects to improve accuracy on occluded objects, inspired by the key point-based CenterNet <ref type="bibr" target="#b52">[52]</ref>. Depth-assisted 3D detection An alternative solution to improve monocular detection performance is to utilize depth information. The pioneering work pseudo-LiDAR <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b44">44]</ref> imitates the process of LiDAR-based (Point Cloud) 3D detection approaches by first estimating depth maps for input images from off-the-shelf methods such as DORN <ref type="bibr" target="#b13">[13]</ref>, then transforming it into 3D space and adopting LiDARbased 3D detection approaches. Mono3D-PLiDAR <ref type="bibr" target="#b45">[45]</ref> points out that the noise in pseudo-LiDAR data is a bottleneck to improve performance and adopts the instance mask instead of the bounding box for frustum lifting. With the help of the stereo network, more accurate depth estimation is achieved to aid the pseudo-LiDAR methods in <ref type="bibr" target="#b49">[49]</ref>. Different to pseudo-LiDAR approaches that rely heavily on the accuracy of estimated depth, D 4 LCN [8] carefully designs a local convolutional network, where the depth map was regarded as guidance to learn local dynamic depthwisedilated kernels for images. However, we argue that the local dilated convolution is not able to fully capture the object context in the condition of perspective projection and occlusion, and the model has no mechanism to resolve the inferior 3D localization caused by the inaccurate depth prior.</p><p>In addition to the above-mentioned approaches, multitask or leveraging auxiliary knowledge is exploited to improve the 3D detection performance. For instance, AM3D <ref type="bibr" target="#b27">[27]</ref> designs two modules for background segmentation and RGB information aggregation respectively in order to enhance the 3D box estimation task. Graph neural network A number of methods have been explored to model context for computer vision tasks, such as dilated convolution <ref type="bibr" target="#b50">[50]</ref> and deformable convolution <ref type="bibr" target="#b6">[7]</ref>. Graph neural networks <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b51">51]</ref>, on the other hand, propagate information along graph-structured input data. These networks are superior to both fronts on capturing object context. In this paper, we present a graph-based formulation for depth-aware feature representation learning, with the goal of solving monocular 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We first briefly introduce the graph message passing formulation. Then the overall framework and each crit- ical component are explained in detail, including depthconditioned dynamic message propagation (DDMP) and center-aware depth feature encoding (CDE). Finally, we describe the instantiation of our model and loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph message passing</head><p>The message passing mechanism constructs a feature graph G = {V, E, A}, where V stands for nodes, E for edges and A for adjacency matrices. Each node in graph are represented as the latent feature vector h i , i.e., V = {h i } N 1 , where N is total number of nodes. And A ? R N ?N is a binary or learnable matrix with self-loops describing the connections between nodes. Network aims to refine latent feature vectors h i by extracting hidden structured information among the feature vectors at different node locations. The common message passing phase takes T iterations and is composed of a message calculation step M t and a message updating step U t . Given a latent feature vector h (t) i at iteration t, it samples K locally connected node field v j ? V, v j ? R K?C , where C is vector dimension and K N . Thus the message calculation step for node i is:</p><formula xml:id="formula_0">m t+1 i = M t (A i,j {h (t) 1 , ..., h (t) K }, w j ) = j?N (i) A i,j h (t) j w j<label>(1)</label></formula><p>where A i,j is the connection relationship between latent nodes h (t) i and h (t) j , N (i) contains the K number of sampled nodes for v i , and w j ? R C?C is a transformation matrix for message calculation on the hidden node h </p><formula xml:id="formula_1">h t+1 i = U t (h t i , m t+1 i ) = ?(h t i + ? m i m t+1 i )<label>(2)</label></formula><p>where ? m i is a learnable parameter for scaling the message, and the operation ?(?) is a non-linearity function e.g., ReLU. By updating nodes T times, the network finally obtained refined features via message passing on each nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Framework overview</head><p>We give an overview of the proposed DDMP-3D as in <ref type="figure" target="#fig_1">Figure 2</ref>. It consists of two branches, one for 3D regression (the upper branch colored in blue) while the other for depth feature extraction (the bottom branch colored in green). The RGB images are initially fed into the upper branch for feature extraction while corresponding depth maps estimated via off-the-shelf depth estimator are sent into the depth branch for extracting depth-aware features. Specifically, we first adaptively sample context-aware nodes in the image graph and then dynamically predict hybrid depthdependent filter weights and affinity matrices. After obtaining the context-and depth-aware features from these two branches, we integrate them in a graph message propagation pattern via the DDMP module. Common 3D heads for 3D center, dimension, orientation regression are followed to achieve final 3D object boxes. Moreover, an auxiliary center regression task is performed during the training process to implicitly guide the depth sub-network to learn centeraware depth features for better object localization. Dynamic nodes are first sampled from the image and depth feature graph, for these sampled nodes, the filter weights and affinity matrices are learned from depth features to propagate the depthconditioned message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth-conditioned dynamic message propagation (DDMP)</head><p>We first introduce the process of our DDMP. The input feature maps are regarded as a graph, in which each pixel of the feature map is a node vector v i ? R C and all the pixels make up the set V = {v i } N 1 , where C is the channel numbers of the input feature maps and N is the total number of pixels, and its status at time t is denoted as latent node h (t) i . Hence, depth-conditioned message propagation is employed on feature nodes to deliver the contextand depth-aware information. In particular, it includes two steps: 1) Image feature node sampling, which selects a subset of relevant object nodes in the graph; 2) For these sampled nodes, the hybrid filter weights and affinity matrices are learned from multi-scale depth feature to enrich the propagated message. <ref type="figure" target="#fig_3">Figure 3</ref> shows the depth-conditioned dynamic message propagation process. We first sample the context-aware nodes in the image graph. Due to the diverse scales and occlusion of targets caused by perspective projection, we follow the strategies in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b51">51]</ref> to explore dynamic sampling during the message propagation process. For each node v i , the sampling number K determines the receptive field of v i . To adaptively sample relevant nodes for v i while taking different feature distribution into consideration, the network learns to walk around so as to choose the most effective nodes among the uniformly distributed neighboring nodes. We denote ?d i,j ? R D as predicted walk for a uniformly sampled relevant node v i,j with j ? N (i), where N (i) contains K number of sampled nodes for v i and D = 2 is the space dimension along height and width. Then the node walk can be described as a matrix transformation:</p><formula xml:id="formula_2">?d i,j = W i,j h i + b i,j<label>(3)</label></formula><p>where W i,j and b i,j are matrix transformation parameters learned on image graph nodes, h i is the latent vector for v i . After the above sampling, we obtain the dynamic image relevant nodes v i,j based on v i . According to the Equa-tion 1, we need to learn the affinity matrix A and transformation matrix w for calculating the message. To effectively achieve depth-sensitive features, we generate hybrid depthdependent filter weights and affinity matrices based on the multi-scale depth feature. Because of the ill depth prior, we do not sample the corresponding depth nodes, but generate another walk ?d i,j for uniformly neighboring nodes on the depth feature graph. For stage l, we have obtained dynamic walked nodes v l i,j and? l i,j from image and depth feature graph respectively. Note that we have aligned inconsistent scale depth features via down-or up-sampling operation. Then depth feature nodes? l i,j learns to further generate affinity matrix A l i,j and transformation matrix w l i,j , as:</p><formula xml:id="formula_3">m t+1 i = l?L j?N (i) ? l A l i,j h l,(t) j w l i,j<label>(4)</label></formula><formula xml:id="formula_4">{A l i,j ; w l i,j } =W l i,j? l i,j +b l i,j<label>(5)</label></formula><p>In Equation 4, L stands for the layer from different level stages; h l,(t) j is the latent vector for dynamic nodes v l i,j from stage l with node walk ?d i,j , and ? l is the balance weight for depth feature maps integration from different stages.W l i,j andb l i,j in Equation 5 are matrix transformation parameters generated by depth nodes. The calculated message is summarized as:</p><formula xml:id="formula_5">m t+1 i = l?L j?N (i) ? l A l i,j ?(h l,(t) j |V; j; ?d i,j )w l i,j (6)</formula><p>where ?(?) is a bilinear sampler which samples a new node h l,(t) j calculated by ?d i,j over the whole nodes V of graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model instantiation</head><p>The introduced architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref> and a more detailed scheme of the DDMP module is further depicted in <ref type="figure" target="#fig_3">Figure 3</ref>. DDMP module is embedded in the network within Stage II and III to propagate the context-and depth-aware message. To instantiate it, we first extract the hierarchical depth features from Stage II, III, and IV with different sizes, and adopt stride MaxPooling or interpolation operation to align these feature maps to the same size with corresponding stage image feature maps. Then we transform the channels of RGB and depth features from different stages to a fixed number C (i.e., 256) with 1 ? 1 convolutions before feeding them into DDMP. DDMP accepts RGB features as input feature nodes F, F ? R C?H?W , where C, H and W are the channel, height, and width of the feature map, respectively. We denote H (0) as an initial state of the latent feature map H, which has the same dimension with F, and H (0) = F. The dynamic walk ?d for each image node is generated by applying 3 ? 3 convolutional layers according to <ref type="bibr">Equation 3</ref>. The same operation is done on depth features by generating ?d to obtain the sampled depth nodes. Hybrid dynamic affinity matrices and weights in Equation 5 are also calculated on hierarchical depth features F l dep ? R C?H?W by applying 3 ? 3 convolutional layers, where l stands for stage II / III / IV. Then we obtain affinity matrices A l ? R H?W ?K and another weights W l ? R H?W ?K?G , where K is sampled size (i.e.3 ? 3) and G is group size. All message M ? R C?H?W is calculated as Equation 6 using group convolutional layers and concated with F, to produce a refined feature map H (1) via a 1 ? 1 convolutional, which is described as Equation 2. We perform T = 1 to balance the performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Center-aware depth feature encoding (CDE)</head><p>Generally, the depth map estimated from off-the-shelf algorithms sometimes inevitably lose appearance details or fail to discriminate the depth between the foreground instance and backgrounds, which leads to unreliable depth prior for the depth-assisted 3D object detection. It is already proved that multi-task strategy <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b47">47]</ref> can boost each single task to some degree, benefiting from the multi-fold regularization effect in the joint-optimization. Hence, we augment an auxiliary task to jointly optimize with the main 3D detection task as depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. The augmented task with xyz (Equation 9) supervision in 3D space uniquely determines a point in 2D image plane, which imposes spatial constraints on the network to gain a better 3D instance-level understanding. With the better instance-awareness brought by CDE, our model is able to alleviate the inaccurate depth prior in situation like occlusion and distant objects. We adopt a similar network architecture for depth branch with the head only predicting 3D centers without predefined anchors. During training, we jointly optimize the losses of the two sub-branches in the light of guiding the intermediate depth features to be center-aware and thus enhancing the performance for 3D object detection. In this way, the proposed approach becomes more robust to the estimated depth accuracy, validation experiments can be found in Section 4. The auxiliary task is omitted at the inference phase without extra computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Objective functions</head><p>We follow single-stage approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">35]</ref> to pre-define anchors and then regress the positive samples. Firstly, 2D anchors [x,?,?,?] 2d are defined in 2D space for the 2D bounding box. [x,?] p represents the 2D location projected from the 3D center. 3D anchor parameters [?,?,?,l,?] 3d denote 3D object center depth, physical dimension, and rotation, respectively. Then all ground truth 3D boxes are projected to 2D space to compute the intersection over union (IoU) with 2D anchors. Positive 2D anchors with IoU ? 0.5 combined with 3D anchor parameters are thus selected to participate in the regression for 3D bounding predictions. The predicted 2D-3D parameters include ([x, y, w, h] 2d , [x, y] p , [z, w, h, l, ?] 3d ), which denote the classification scores, 2D bounding box locations, 3D center projections on 2D plane, 3D object center depths, physical dimensions and rotations, respectively.</p><p>Similar to YOLOv3 <ref type="bibr" target="#b33">[33]</ref>, we adopt</p><formula xml:id="formula_6">([t x , t y , t w , t h ] 2d , [t x , t y ] p , [t z , t w , t h , t l , t ? ] 3d</formula><p>) to parameterize the corresponding predictions directly generated by the network. Hence the predicted 3D boxes is,</p><formula xml:id="formula_7">[x, y] 2d = [x,?] 2d + [t x , t y ] 2d * [?,?] 2d [w, h] 2d = [?,?] 2d * exp([t w , t h ] 2d ) [x, y] p = [x,?] p + [t x , t y ] p * [?,?] 2d [w, h, l] 3d = [?,?,l] 3d * exp([t w , t h , t l ] 3d ) [z, ?] 3d = [?,?] 3d + [t z , t ? ] 3d ,<label>(7)</label></formula><p>where we use the same anchor for [x,?] 2d and [x,?] p . Therefore, the loss for the detection branch L det contains classification loss, 2D bounding box regression loss and 3D box regression loss. We apply standard cross-entropy loss for classification and smooth L1 (L1) loss for box regression. The detection branch loss then is,</p><formula xml:id="formula_8">L det = L cls + L 2d + L 3d L 2d =L1 ([t x , t y , t w , t h ] 2d , [t gt x , t gt y , t gt w , t gt h ] 2d ) L 3d =L1 ([t x , t y ] p , [t gt x , t gt y ] p ) +L1 ([t z , t w , t h , t l , t ? ] 3d , [t gt z , t gt w , t gt h , t gt l , t gt ? ] 3d ),<label>(8)</label></formula><p>where ( * ) gt means (*)'s corresponding ground truth target.</p><p>For the auxiliary task of the depth branch, we regress the depth [t z ] 3d of the 3D object as well as the [t x , t y ] p for projecting the 3D center onto the 2D image plane, which shares the same ground truth with the detection branch:</p><formula xml:id="formula_9">L dep =L1 ([t x , t y ] p , [t gt x , t gt y ] p ) +L1 ([t z ] 3d , [t gt z ] 3d ). (9)</formula><p>The final training loss is a summation of L det and L dep . Inspired by focal loss, we utilize classification scores s t to balance the samples:</p><formula xml:id="formula_10">L = (1 ? s t ) ? (L det + L dep ),<label>(10)</label></formula><p>where ? is the focus parameter and set as 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset We conduct experiments on the KITTI dataset <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref> of 7,481 and 7,518 images for training for testing, respectively. As in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>, we use two train-val splits of the KITTI dataset ("val1"/"val2") to evaluate our DDMP-3D.</p><p>Evaluation metrics Precision-recall curves are adopted for evaluation, and we report the average precision (AP) results of 3D and Bird's eye view (BEV) object detection on KITTI validation and test set. The 40 recall positions-based metric AP | R40 has been utilized by the KITTI test server instead of AP | R11 since Aug. 2019. To compare with previous methods that only report AP | R11 results, we also demonstrate AP | R11 in all experiments except the comparison with state-of-the-art (SoTA) methods on KITTI test and validation set. Three levels of difficulty are defined in the benchmark according to the 2D bounding box height, occlusion, and truncation degree, namely, "Easy", "Mod.", and "Hard". The KITTI benchmark ranks all methods based on the AP 3D of "Mod.". As in <ref type="bibr" target="#b4">[5]</ref>, we adopt IoU = 0.7 as threshold for "Car" category. To validate the effectiveness on "Cyclist" and "Pedestrian" categories, we include the experiments with IoU = 0.5 for fair comparison. We denote AP for 3D and BEV as AP 3D and AP BEV , respectively.</p><p>Training details Initial anchors for detection heads are constructed as in D 4 LCN <ref type="bibr" target="#b7">[8]</ref>. For 2D anchors, 12 scales ranging from 30 to 400 pixels in height following the power function of 30 * 1.265 n , n = 0, ..., 11, combining with aspect ratios of [0.5, 1.0, 1.5] to generate a total of 36 anchors. Mean statistics across matched 3D ground truth are also calculated as the initialization of 3D anchor parameters. We employ ResNet-50 <ref type="bibr" target="#b18">[18]</ref> as feature extraction backbone for RGB and depth branch. The input image size is scaled to 512 ? 1760 and only horizontal flipping is applied to data augmentation. 2D space Non-Maximum Suppression (NMS) with an IoU threshold of 0.4 is finally used to drop the predicted bounding box redundancy. Note that the baseline method demonstrated in all experiments stands for simply dot-multiplication on RGB and depth features together, which is equivalent to treating each pixel on depth feature as a depth kernel. The depth maps used for all experiments except for <ref type="table">Table 6</ref> are obtained by off-the-shelf monocular depth estimator DORN <ref type="bibr" target="#b13">[13]</ref>.</p><p>Our model is trained by SGD with an initial learning rate 0.04, momentum 0.9, and weight decay 0.0005 respectively.  We train the network with a batch size of 16 on 8 Nvidia Tesla v100 GPUs for 36k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with state-of-the-arts</head><p>Results on KITTI test and val set <ref type="table" target="#tab_1">Table 1</ref> and 2 show the 3D "Car" detection results on KITTI test and "val1", "val2" set at IoU = 0.7, respectively. We report both AP | R40 and AP | R11 on validation set while only AP | R40 on test set. In the KITTI leaderboard, our proposed DDMP-3D consistently outperforms other alternatives and ranks 1st compared with the top-ranked monocular-based 3D object detection methods. Quantitatively, our method achieves the highest performance on "Moderate" set for both validation and test set, which is the main setting for ranking on the benchmark. Large performance margins are observed over the second top-performed method (D <ref type="figure" target="#fig_4">4 LCN [8]</ref>), 4.19% and 1.06% on validation and test set in view of "Mod.", respectively. This phenomenon indicates that our outstanding performance benefits from the effective context-and depth-aware features learning via depth-conditioned dynamic graph message propagation and center-aware depth feature encoding. Note that some cutting-edge monocular methods use extra information for 3D object detection, e.g., AM3D <ref type="bibr" target="#b27">[27]</ref> designs two extra modules for background points segmentation and RGB information aggregation, while we attain the appealing results and acceptable inference speed without bells and whistles.</p><p>Results on "Cyclist" and "Pedestrian" "Cyclist" and "Pedestrian" categories are much more challenging than the "Car" for monocular 3D object detection due to their nonrigid structures and small scale. We report these two cate-gories respect to baseline and <ref type="bibr" target="#b7">[8]</ref> in <ref type="table" target="#tab_2">Table 3</ref>. Following <ref type="bibr" target="#b7">[8]</ref>, AP 3D of "Cyclist" and "Pedestrian" on the "val1"(AP | R11 ) and test(AP | R40 ) set at IoU = 0.5 are reported. Thanks to our DDMP and CDE which provide better sensitivity to object location, we are able to localize these challenging categories to some degree and clearly outperform the alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>Main ablative analysis In <ref type="table">Table 4</ref>, we conduct ablation experiments to analyze the effectiveness of different components: I) Baseline: simply integrate depth features into RGB features by dot-product. II) DDMP (single-scale): depthconditioned dynamic message propagation without multiscale depth features. For the input of a certain DDMP, depth features from its corresponding single stage are extracted to generate affinity matrices and weights. For fair comparison, we keep the number of affinity matrices and weights the same with the following Group III. III) DDMP (multiscale): dynamic graph message propagation via hybrid and hierarchical depth features. For fair comparison, the parameter numbers are exactly kept the same as Group II (singlescale). The critical difference with respect to Group II lies in that the depth features fed into the DDMP are from different stages rather than a single stage. IV) On the basis of Group III, CDE is added additionally to conduct a 3D center regression task with an auxiliary loss during the training phase, so as to guide the depth features to be center-aware. As depicted in <ref type="table">Table 4</ref>, we can observe that the performance continues to grow with the participation of components. Dynamic message propagation introduces an impres- <ref type="table">Table 4</ref>. Ablative analysis on KITTI "val1" split set for AP3D and APBEV at IoU = 0.7. Experiment group (I) is our baseline method. Different experiment settings are explored: (II) using depth-conditioned dynamic message propagation with single scale, (III) performing multi-scale dynamic message propagation, (IV) our full approach with auxiliary center-regression task. sive increase from 18.82% to 22.36% on the moderate setting which confirms the effectiveness of the graph message mechanism between image and depth. Group III shows that hybrid affinity matrices and weights learning leads to 0.48% performance gain on the moderate setting. It indicates that multi-scale depth features are useful in graph message propagation owing to the better depth perception on various scales of objects. Further, the auxiliary task brings a noticeable gain from 28.12% to 31.14% on the easy setting, which validates the effectiveness of the auxiliary task by learning center-aware depth features. Qualitative comparisons of the baseline and our method are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. The ground truth, baseline, and our method are colored in green, yellow, and red, respectively. For better visualization, the first and second columns show RGB images and BEV images of pseudo point clouds, respectively. Compared with the baseline, our DDMP-3D can produce higher-quality 3D bounding boxes in different kinds of scenes. More quantitative and qualitative results are reported in our supplementary material.</p><p>Auxiliary task-guided depth encoding We explore the effect of different auxiliary tasks. We employ "z", "xy" and "xyz" center regressions on the depth branch (Equation 9). As shown in Table 5, depth task "z" improves the moderate 3D detection performance from 22.84% to 23.07% while auxiliary center estimation task brings substantial improvements on all the three subsets from (22.84% / 28.12% / 19.09%) to (23.13% / 31.14% / 19.45%). The improvement is especially significant on "Easy". This further suggests that center-awareness is useful for object localization.</p><p>Impact of different depth estimators For generalization ability validation on different depth estimation methods, we choose the monocular depth estimator DORN <ref type="bibr" target="#b13">[13]</ref> as well as the more accurate stereo matching method PSM-Net <ref type="bibr" target="#b3">[4]</ref> to obtain depth maps for comparison. As shown in <ref type="table">Table 6</ref>, the performance gain with respect to baseline and D 4 LCN <ref type="bibr" target="#b7">[8]</ref> are enhanced with the increasing accuracy of estimated depth. Besides, improvements can be noticed both in monocular and stereo depth predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a depth-conditioned dynamic message propagation (DDMP-3D) network, a novel graphbased approach that learns context-and depth-aware feature representation for 3D object detection. It dynamically samples context-aware nodes in the image context and predicts the hybrid filter weights and affinities based on the aligned multi-scale depth features for message propagation. A center-aware depth encoding task is augmented and jointly trained with the whole network to resolve the challenge of inaccurate depth prior. This framework allows us to build a new state of the art among the monocular-based approaches, and this is demonstrated by the fact that we are rank 1 st in the highly competitive KITTI monocular 3D object detection track on the submission day (Nov 16th, 2020).</p><p>Loss weight selection of auxiliary tasks. The loss weights for two branches in Equation 10 in our paper determine the influence of the auxiliary task on main task, which is a key hyper-parameter in our DDMP-3D framework. To explore the sensitivity of this parameter, we conduct experiments as shown in <ref type="table" target="#tab_5">Table 8</ref>.</p><p>Paying more attention to the main task or at least equal weights to two tasks can achieve better performance for our model. When the weight for auxiliary task is equal to that of main task, it is favorable to detect objects on moderate and hard settings owing to its sensitivity to the centers. While it is friendly to detect easy objects with higher weight for main task. A relatively high weight to auxiliary task brings slightly negative effect on the final performance. This reflects that L det is essential on detection results whose weight should not be less than that of L dep .</p><p>Statistic analysis on 3D metric. To further demonstrate the effectiveness of the proposed CDE, we compare the errors on the specific metrics (center "xyz") of the baseline method with or without CDE.</p><p>As shown in <ref type="figure">Figure 5</ref>, we can see that our proposed CDE improves the baseline method in "x", "y" and "z", resulting in more accurate monocular 3D object detection. Note that the "x", "y", and "z" indicate the 3D camera coordinates of the object center point.</p><p>Ablation study on the auxiliary task for the depth encoding. We report the experiment results of deploying other auxiliary tasks in <ref type="table">Table 9</ref>. It is observed that various auxiliary tasks have certain effects on the performances. The task of 3D center regression ("xyz") is critical, which introduces notable improvements on all settings.</p><p>However, the performance of adding 3D bounding box regression ("whl + rotation") or classification task experienced a drop at some settings. We consider that it is difficult for bounding box regression and classification on depth map without well-defined boundary and distinctive appearance. Therefore, we further validate the hypothesis that the center-aware depth feature encoding helps monocular 3D object detection.</p><p>Different message propagation strategies. How to effectively deliver the depth information through image feature domain and learn context-and depth-aware feature representation is critical for monocular 3D detection. This is also the objective of this paper. We perform different message propagation strategies to verify the effectiveness of our proposed DDMP-3D.</p><p>As shown in <ref type="table" target="#tab_1">Table 10</ref>, "3DNet" is the baseline in D 4 LCN <ref type="bibr" target="#b8">[9]</ref>, which only contains the single detection branch without the guidance of depth map. "3DNet w/ DGMN <ref type="bibr" target="#b51">[51]</ref>" augments detection branch with the DGMN formulation to perform the effective feature learning in the RGB feature domain. "Baseline" integrates images with depth maps via a common multiplication operation. With the guidance of depth maps, it easily outperforms the above two methods. "3DNet w/ DGMN + Depth" introduces the depth information via the common multiplication operation. "DDMP" is our proposed module for integrating image and depth via graph message propagation.</p><p>The large gains on all settings demonstrate its effectiveness on propagating depth-conditioned messages. Different with DGMN <ref type="bibr" target="#b51">[51]</ref>, our proposed DDMP generates hybrid filters and affinities used for propagating message from the multi-scale sampled depth feature; "DDMP + CDE" augments the "CDE" task which has been discussed in the main paper. Note that thanks to the non-linear Softmax operation on the generated affinity matrix, the network learns from the normalized affinities to further boost the final detection performance, as is shown in the last two rows in <ref type="table" target="#tab_1">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional qualitative results</head><p>Visualization of dynamic sampling points. <ref type="figure" target="#fig_5">Figure 6</ref> shows dynamic sampling points based on the learned ?d and ?d from images and depth maps, respectively. The receiving nodes are shown with red circles. As shown in left figure, our sampled image nodes accurately perceive the semantic context: object boundary of left car and the small object, to enable more effective message passing. Also, in right figure, we demonstrate our dynamically sampled multi-scale depth nodes that dedicate to capture the context of the target objects.</p><p>More qualitative results. <ref type="figure" target="#fig_6">Figure 7</ref> shows more qualitative results on the KITTI dataset. The 3D ground-truth boxes and our DDMP-3D predictions are drawn in green and red, respectively. As clearly observed, DDMP-3D can produce high-quality 3D bounding boxes in various scenes.  <ref type="figure">Figure 5</ref>. The statistic analysis and comparison of the baseline (green) and the baseline with our CDE (red). The vertical axis of the chart represents the number of samples after normalization. Improvements can be observed in the metrics "x", "y", and "z".  <ref type="table">Table 9</ref>. Comparison results (3D "Car" detection) of different auxiliary tasks on val split set (IoU = 0.7). "DDMP + bbox", "DDMP + class", and "DDMP + center" stand for the bounding boxes regression, classification and center regression tasks, respectively.  <ref type="table" target="#tab_1">Table 10</ref>. Comparison results (3D "Car" detection) of different message integration positions on val split set (IoU = 0.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Image   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Left: DDMP adaptively samples context-aware nodes (top) in the image context and dynamically predicting hybrid depth-dependent filter weights and affinity matrices (bottom) for propagating information. Right: the improvement of DDMP-3D (red) over the baseline (yellow) via center-aware depth encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Schematic illustration of our proposed DDMP-3D. Two branches are involved including 3D detection branch (colored in blue) and depth feature extraction branch (colored in green). The DDMP modules in yellow color reveal the depth-conditioned dynamic message propagation. It dynamically samples context-aware nodes in the upper image branch and predicts the hybrid filter weights and affinities based on multi-scale depth features from the bottom branch for message propagation. CDE is the auxiliary task for joint-optimization training and is discarded during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>message updating step U t updates the node h (t)i with a linear combination of the calculated message and the original node status:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of DDMP module in a single scale pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparison of ground truth (green), the baseline (yellow), and our method (red) on KITTI val set. For better visualization, the first and second columns show RGB and BEV images of point clouds converted from pre-estimated depth, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of sampling points on the images and depth maps, and the predicted results on the KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>More qualitative results on the KITTI dataset. The 3D ground-truth boxes and our DDMP-3D predictions are drawn in green and red, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with School of Data Science, Fudan University. Li Wang and Xiangyang Xue are with School of Computer Science, Fudan University. Yanwei Fu is with the School of Data Science, MOE Frontiers Center for Brain Science, and Shanghai Key Lab of Intelligent Information Processing, Fudan University. Liang Du and Jianfeng Feng are with the Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with SoTA methods on the KITTI test set at IoU = 0.7. Our DDMP-3D achieves new SoTA performance. / 23.12 28.12 / 31.14 16.34 / 19.45 19.91 / 22.92 29.17 / 30.66 15.26 / 18.75</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell></cell><cell>Speed (FPS)</cell><cell cols="2">Mod.</cell><cell cols="4">AP3D Easy Hard Mod.</cell><cell>APBEV Easy</cell><cell>Hard</cell><cell>GPU</cell></row><row><cell>FQNet [25]</cell><cell>CVPR 2019</cell><cell></cell><cell>2</cell><cell cols="2">1.51</cell><cell>2.77</cell><cell>1.01</cell><cell cols="2">3.23</cell><cell>5.40</cell><cell>2.46</cell><cell>1080Ti</cell></row><row><cell>ROI-10D [28]</cell><cell>CVPR 2019</cell><cell></cell><cell>5</cell><cell cols="2">2.02</cell><cell>4.32</cell><cell>1.46</cell><cell cols="2">4.91</cell><cell>9.78</cell><cell>3.74</cell><cell>-</cell></row><row><cell>MonoDIS [38]</cell><cell>ICCV 2019</cell><cell></cell><cell>-</cell><cell cols="2">7.94</cell><cell cols="5">10.37 6.40 13.19 17.23 11.12</cell><cell>Tesla V100</cell></row><row><cell>MonoPair [6]</cell><cell>CVPR 2020</cell><cell></cell><cell>17</cell><cell cols="2">9.99</cell><cell cols="5">13.04 8.65 14.83 19.28 12.89</cell><cell>-</cell></row><row><cell>UR3D [37]</cell><cell>ECCV 2020</cell><cell></cell><cell>8</cell><cell cols="2">8.61</cell><cell cols="5">15.58 6.00 12.51 21.85</cell><cell>9.2</cell><cell>GTX Titan X</cell></row><row><cell>RTM3D [24]</cell><cell>ECCV 2020</cell><cell></cell><cell>20</cell><cell cols="7">10.34 14.41 8.77 14.20 19.17 11.99</cell><cell>1080Ti</cell></row><row><cell>AM3D [27]</cell><cell>ICCV 2019</cell><cell></cell><cell>3</cell><cell cols="7">10.74 16.50 9.52 17.32 25.30 14.91</cell><cell>1080Ti</cell></row><row><cell>DA-3Ddet [48]</cell><cell>ECCV 2020</cell><cell></cell><cell>3</cell><cell cols="7">11.50 16.77 8.93 15.90 23.35 12.11</cell><cell>Titan RTX</cell></row><row><cell>D 4 LCN [8]</cell><cell>CVPR 2020</cell><cell></cell><cell>5</cell><cell cols="7">11.72 16.65 9.51 16.02 22.51 12.55</cell><cell>1080Ti</cell></row><row><cell cols="2">Kinematic3D [2] ECCV 2020</cell><cell></cell><cell>8</cell><cell cols="7">12.72 19.07 9.17 17.52 26.69 13.10</cell><cell>-</cell></row><row><cell>Ours</cell><cell>-</cell><cell></cell><cell>6</cell><cell cols="7">12.78 19.71 9.80 17.89 28.08 13.44</cell><cell>Tesla V100</cell></row><row><cell cols="11">Table 2. Comparison on the KITTI "val1", "val2" set. We report the average precision (in %) including AP |R40 and AP |R11 of "Car" on</cell></row><row><cell cols="2">3D object detection (AP3D) at IoU = 0.7.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Mod.</cell><cell cols="4">Val1 [AP |R40 / AP |R11] Easy</cell><cell>Hard</cell><cell></cell><cell>Mod.</cell><cell cols="2">Val2 [AP |R40 / AP |R11] Easy</cell><cell>Hard</cell></row><row><cell>OFT-Net [35]</cell><cell cols="2">-/ 3.27</cell><cell cols="2">-/ 4.07</cell><cell></cell><cell>-/ 3.29</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>FQNet [25]</cell><cell cols="2">-/ 5.50</cell><cell cols="2">-/ 5.98</cell><cell></cell><cell>-/ 4.75</cell><cell></cell><cell cols="2">-/ 5.11</cell><cell>-/ 5.45</cell><cell>-/ 4.45</cell></row><row><cell>ROI-10D [28]</cell><cell cols="2">-/ 6.93</cell><cell cols="2">-/ 10.25</cell><cell></cell><cell>-/ 6.18</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>MonoDIS [38]</cell><cell cols="2">-/ 7.60</cell><cell cols="2">-/ 11.06</cell><cell></cell><cell>-/ 6.37</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>MonoGRNet [32]</cell><cell cols="2">7.56 / 10.19</cell><cell cols="2">11.90 / 13.88</cell><cell cols="2">5.76 / 7.62</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>GS3D [23]</cell><cell cols="2">-/ 10.97</cell><cell cols="2">-/ 13.46</cell><cell cols="2">-/ 10.38</cell><cell></cell><cell cols="2">-/ 10.51</cell><cell>-/ 11.63</cell><cell>-/ 10.51</cell></row><row><cell>shift R-CNN [30]</cell><cell cols="2">-/ 11.29</cell><cell cols="2">-/ 13.84</cell><cell cols="2">-/ 11.08</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>MonoPSR [22]</cell><cell cols="2">-/ 11.48</cell><cell cols="2">-/ 12.75</cell><cell></cell><cell>-/ 8.59</cell><cell></cell><cell cols="2">-/ 12.24</cell><cell>-/ 13.94</cell><cell>-/ 10.77</cell></row><row><cell>SS3D [19]</cell><cell cols="2">-/ 13.15</cell><cell cols="2">-/ 14.52</cell><cell cols="2">-/ 11.85</cell><cell></cell><cell cols="2">-/ 8.42</cell><cell>-/ 9.45</cell><cell>-/ 7.34</cell></row><row><cell>RTM3D [24]</cell><cell cols="2">-/ 16.86</cell><cell cols="2">-/ 20.77</cell><cell cols="2">-/ 16.63</cell><cell></cell><cell cols="2">-/ 16.29</cell><cell>-/ 19.47</cell><cell>-/ 15.57</cell></row><row><cell>M3D-RPN [1]</cell><cell cols="4">11.07 / 17.06 14.53 / 20.27</cell><cell cols="2">8.65 / 15.21</cell><cell cols="4">10.07 / 16.48 14.51 / 20.40</cell><cell>7.51 / 13.34</cell></row><row><cell>Pseudo-LiDAR [44]</cell><cell cols="2">-/18.50</cell><cell cols="2">-/ 28.20</cell><cell cols="2">-/ 16.40</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>Decoupled-3D [3]</cell><cell cols="2">-/ 18.68</cell><cell cols="2">-/ 26.95</cell><cell cols="2">-/ 15.82</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>UR3D [37]</cell><cell cols="10">13.35 / 18.76 23.24 / 28.05 10.15 / 16.55 11.10 / 16.75 22.15 / 26.30</cell><cell>9.15 / 13.60</cell></row><row><cell>AM3D [27]</cell><cell cols="2">-/ 21.09</cell><cell cols="2">-/ 32.23</cell><cell cols="2">-/ 17.26</cell><cell></cell><cell>-/ -</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>D 4 LCN [8]</cell><cell cols="6">16.20 / 21.71 22.32 / 26.97 12.30 / 18.22</cell><cell></cell><cell cols="2">-/ 19.54</cell><cell>-/ 24.29</cell><cell>-/ 16.38</cell></row><row><cell>Ours</cell><cell>20.39</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>3D object detection (AP3D) performance for "Cyclist" and "Pedestrian" on KITTI val split and test set ("val1"/test).</figDesc><table><row><cell>Method</cell><cell>Mod.</cell><cell>Cyclist Easy</cell><cell>Hard</cell><cell>Mod.</cell><cell>Pedestrian Easy</cell><cell>Hard</cell></row><row><cell cols="7">D 4 LCN [8] 4.41 / 1.67 5.85 / 2.45 4.14 / 1.36 11.23 / 3.42 12.95 / 4.55 11.05 / 2.83</cell></row><row><cell>Baseline</cell><cell cols="3">4.07 / 0.17 4.50 / 0.32 4.08 / 0.17</cell><cell>6.93 / 2.32</cell><cell>8.11 / 3.05</cell><cell>6.78 / 1.81</cell></row><row><cell>Ours</cell><cell cols="6">6.47 / 2.50 8.01 / 4.18 6.27 / 2.32 12.11 / 3.55 14.42 / 4.93 12.05 / 3.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>LCN 21.71 / 19.54 26.97 / 24.29 18.22 / 16.38 Baseline 18.82 / 24.18 26.03 / 33.06 16.27 / 19.63 Ours 23.12 / 27.46 31.14 / 37.71 19.45 / 24.53 Baseline 25.41 / 33.34 35.26 / 46.75 20.69 / 27.27 Ours 30.83 / 36.20 41.76 / 52.87 24.78 / 29.34</figDesc><table><row><cell></cell><cell cols="2">Group</cell><cell cols="4">DDMP single-scale multi-scale DDMP</cell><cell>CDE</cell><cell>Mod.</cell><cell>AP3D Easy</cell><cell>Hard</cell><cell>Mod.</cell><cell>APBEV Easy</cell><cell>Hard</cell></row><row><cell></cell><cell>I</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="2">18.82 26.03 16.27 24.18 33.06 19.63</cell></row><row><cell></cell><cell>II</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell cols="2">22.36 28.94 18.86 26.73 36.89 24.00</cell></row><row><cell></cell><cell>III</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell cols="2">22.84 28.12 19.09 27.05 37.11 24.20</cell></row><row><cell></cell><cell>IV</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell cols="2">23.12 31.14 19.45 27.46 37.71 24.53</cell></row><row><cell cols="8">Table 5. Comparison of different auxiliary tasks on val split set.</cell><cell></cell></row><row><cell>Center</cell><cell></cell><cell cols="2">AP3D</cell><cell></cell><cell cols="3">APBEV</cell><cell></cell></row><row><cell cols="2">Regression Mod.</cell><cell cols="2">Easy</cell><cell>Hard</cell><cell>Mod.</cell><cell>Easy</cell><cell>Hard</cell><cell></cell></row><row><cell>-</cell><cell cols="7">22.84 28.12 19.09 27.05 37.11 24.20</cell><cell></cell></row><row><cell>only z</cell><cell cols="7">23.07 28.11 19.19 26.95 36.70 24.06</cell><cell></cell></row><row><cell>xy</cell><cell cols="7">22.51 28.68 18.52 26.80 37.15 21.38</cell><cell></cell></row><row><cell>xyz</cell><cell cols="7">23.13 31.14 19.45 27.46 37.71 24.53</cell><cell></cell></row><row><cell cols="8">Table 6. Comparison of different depth estimators on val split set</cell><cell></cell></row><row><cell cols="8">at IoU = 0.7. The first, second, and third rows show the results of</cell><cell></cell></row><row><cell cols="6">D 4 LCN [8], the baseline, and our method.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Depth</cell><cell>Method</cell><cell></cell><cell>Mod.</cell><cell cols="3">AP3D / APBEV Easy</cell><cell>Hard</cell><cell></cell></row><row><cell cols="2">DORN [13] D 4 PSMNet [4] D 4 LCN</cell><cell></cell><cell cols="2">25.41 / -</cell><cell>30.03 / -</cell><cell cols="2">21.63 / -</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Detailed architecture. The table expands the details of the DDMP process on image stage II (DDMP 1), including the message propagation from image stage II and depth stage II / III / IV, and the message updating on image stage II.</figDesc><table><row><cell>Module</cell><cell>Type / Stride</cell><cell>Input name</cell><cell>Output name: size</cell></row><row><cell></cell><cell>conv1 / s=2</cell><cell>image</cell><cell>img conv1: 64 ? 256 ? 880</cell></row><row><cell></cell><cell>conv2 x / s=2</cell><cell>img conv1</cell><cell>img stage1: 256 ? 128 ? 440</cell></row><row><cell>Detection backbone (ResNet-50)</cell><cell>conv3 x / s=2</cell><cell>img stage1</cell><cell>img stage2: 512 ? 64 ? 220</cell></row><row><cell></cell><cell>conv4 x / s=2</cell><cell>img stage2</cell><cell>img stage3: 1024 ? 32 ? 110</cell></row><row><cell></cell><cell>conv5 x (dilated=2) / s=1</cell><cell>img stage3</cell><cell>img stage4: 2048 ? 32 ? 110</cell></row><row><cell></cell><cell>conv1 / s=2</cell><cell>estimated depth map</cell><cell>dep conv1: 64 ? 256 ? 880</cell></row><row><cell></cell><cell>conv2 x / s=2</cell><cell>dep conv1</cell><cell>dep stage1: 256 ? 128 ? 440</cell></row><row><cell>Depth backbone (ResNet-50)</cell><cell>conv3 x / s=2</cell><cell>dep stage1</cell><cell>dep stage2: 512 ? 64 ? 220</cell></row><row><cell></cell><cell>conv4 x / s=2</cell><cell>dep stage2</cell><cell>dep stage3: 1024 ? 32 ? 110</cell></row><row><cell></cell><cell>conv5 x (dilated=2) / s=1</cell><cell>dep stage3</cell><cell>dep stage4: 2048 ? 32 ? 110</cell></row><row><cell></cell><cell>conv 1 ? 1</cell><cell>img stage2</cell><cell>img stage22: 256 ? 64 ? 220</cell></row><row><cell></cell><cell>conv 3 ? 3</cell><cell>img stage22</cell><cell>img stage22 offset: 18 ? 64 ? 220</cell></row><row><cell></cell><cell>deform unfold 3 ? 3</cell><cell>img stage22</cell><cell>img stage22 sample: 256 ? 9 ? 64 ? 220</cell></row><row><cell></cell><cell>conv 1 ? 1</cell><cell>dep stage2</cell><cell>dep stage22: 256 ? 64 ? 220</cell></row><row><cell>DDMP 1 (stage2 depth2)</cell><cell>deform conv 3 ? 3 (group=1) conv 3 ? 3</cell><cell>dep stage22 dep stage22</cell><cell>dep stage22 affinity: 9 ? 64 ? 220 dep stage22 filter: 9 ? 64 ? 220</cell></row><row><cell></cell><cell>dot</cell><cell>img stage22 sample; dep stage22 filter</cell><cell>stage22 sample: 256 ? 9 ? (64  *  220)</cell></row><row><cell></cell><cell>matmul(group=1)</cell><cell>stage22 sample; dep stage22 affinity</cell><cell>message stage22: 256 ? 64 ? 220</cell></row><row><cell>DDMP 1 (stage2 depth3)</cell><cell>Similar to stage2 depth2 (+ interpolate on dep stage3)</cell><cell>img stage2; dep stage3</cell><cell>message stage23: 256 ? 64 ? 220</cell></row><row><cell>DDMP 1 (stage2 depth4)</cell><cell>Similar to stage2 depth2 (+ interpolate on dep stage4)</cell><cell>img stage2; dep stage4</cell><cell>message stage24: 256 ? 64 ? 220</cell></row><row><cell>DDMP 1 (update)</cell><cell>concat &amp; conv 3 ? 3</cell><cell>img stage2; message stage22/23/24</cell><cell>img stage2: 512 ? 64 ? 220</cell></row><row><cell>DDMP 2</cell><cell>Similar to DDMP 1</cell><cell>img stage3; dep stage2/3/4</cell><cell>img stage3: 1024 ? 32 ? 110</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Comparison results (3D "Car" detection) of different weights of auxiliary tasks on val split set (IoU = 0.7). ? and ? are the weights for L det and L dep , respectively. 28.12 19.09 27.05 37.11 24.20 1:2 22.71 31.35 18.94 27.18 37.96 24.38 2:1 22.85 32.32 19.35 27.36 41.65 24.47 1:1 23.13 31.14 19.45 27.46 37.71 24.53</figDesc><table><row><cell>? : ?</cell><cell>Mod.</cell><cell>AP 3D Easy</cell><cell>Hard Mod.</cell><cell>AP BEV Easy</cell><cell>Hard</cell></row><row><cell>1:0</cell><cell>22.84</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>.36) 28.84 (+0.72) 18.31 (-0.78) 26.06 (-0.99) 36.35 (-0.76) 21.00 (-3.20) DDMP + class 22.69 (-0.15) 28.72 (+0.60) 19.16 (+0.07) 26.94 (-0.09) 36.87 (-0.24) 24.11 (-0.09) DDMP + center 23.13 (+0.29) 31.14 (+3.02) 19.45 (+0.36) 27.46 (+0.41) 37.71 (+0.60) 24.53 (+0.33)</figDesc><table><row><cell></cell><cell></cell><cell>AP 3D</cell><cell></cell><cell></cell><cell>AP BEV</cell><cell></cell></row><row><cell></cell><cell>Mod.</cell><cell>Easy</cell><cell>Hard</cell><cell>Mod.</cell><cell>Easy</cell><cell>Hard</cell></row><row><cell>DDMP</cell><cell>22.84</cell><cell>28.12</cell><cell>19.09</cell><cell>27.05</cell><cell>37.11</cell><cell>24.20</cell></row><row><cell>DDMP + bbox</cell><cell>22.48 (-0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>input Depth map input AP 3D AP BEV Mod. Easy Hard Mod. Easy Hard 3DNet -14.61 17.94 12.74 19.89 24.87 16.14 3DNet w/ DGMN [51] 16.98 20.12 15.17 21.49 26.40 17.96 Baseline (3DNet + Depth) 18.82 26.03 16.27 24.18 33.06 19.63 3DNet w/ DGMN [51] +Depth 19.59 27.78 16.48 25.30 35.59 20.32 DDMP 22.84 28.12 19.09 27.05 37.11 24.20 DDMP + CDE 23.13 31.14 19.45 27.46 37.71 24.53 DDMP (Softmax) + CDE 23.17 32.40 19.35 27.85 42.05 24.91</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Architecture details As described in Section 3 in our paper, DDMP-3D contains image and depth feature encoding branches, with two DDMP modules adopted at Stage II and III, respectively. We demonstrate the architecture details in <ref type="table">Table 7</ref>. Since two DDMP modules ("DDMP 1" and "DDMP 2") share a similar architecture, we only report the details of "DDMP 1". "DDMP 1" first integrates image features from stage II with depth features from stage II / III / IV (stage2 depth2 / 3 / 4) and then concatenate the outputs together. The outputs are scaled to the size of image features ("DDMP 1 (update)"). Note that the codes of constructing the model are attached in the supplementary material.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection with decoupled structured polygon estimation and heightguided depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3dcfs: Fast and robust joint 3d semantic-instance segmentation via coupled feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ssf-dan: Separated semantic feature based domain adaptation network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongye</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Associate-3ddet</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual-to-conceptual association for 3d point cloud object detection</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets robotics: The kitti dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Monocular 3d object detection and box fitting trained end-toend using intersection-over-union loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eskil</forename><surname>J?rgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Byeong-Moon Jeon, and Marius Leordeanu. Shift r-cnn: Deep monocular 3d object detection with closed-form geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andretti</forename><surname>Naiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Paunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic depth fusion and transformation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distancenormalized unified representation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuepeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>L?pez-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What is mixed reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Speicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Nebeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Crossing the gap: A deep dive into zero-shot sim-to-real transfer for dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Valassakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evolving boxes for fast vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with pseudo-lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hdnet: Exploiting hd maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection via feature domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic graph message passing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
