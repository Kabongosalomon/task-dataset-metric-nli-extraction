<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Line as a Visual Sentence: Context-aware Line Descriptor for Visual Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoung</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">Line as a Visual Sentence: Context-aware Line Descriptor for Visual Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Localization</term>
					<term>SLAM</term>
					<term>Deep Learning for Visual Perception</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Along with feature points for image matching, line features provide additional constraints to solve visual geometric problems in robotics and computer vision (CV). Although recent convolutional neural network (CNN)-based line descriptors are promising for viewpoint changes or dynamic environments, we claim that the CNN architecture has innate disadvantages to abstract variable line length into the fixed-dimensional descriptor. In this paper, we effectively introduce Line-Transformers dealing with variable lines. Inspired by natural language processing (NLP) tasks where sentences can be understood and abstracted well in neural nets, we view a line segment as a sentence that contains points (words). By attending to well-describable points on a line dynamically, our descriptor performs excellently on variable line length. We also propose line signature networks sharing the line's geometric attributes to neighborhoods. Performing as group descriptors, the networks enhance line descriptors by understanding lines' relative geometries. Finally, we present the proposed line descriptor and matching in a Point and Line Localization (PL-Loc). We show that the visual localization with feature points can be improved using our line features. We validate the proposed method for homography estimation and visual localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V ISUAL features are widely utilized in many robotics and computer vision (CV) applications such as visual tracking, simultaneous localization and mapping (SLAM), and structure-from-motion (SFM). While the keypoint features are well-studied and a base of many applications, keypoints that are not well distributed in the image may result in unstable and inaccurate pose estimation.</p><p>Recent research has reported that SLAM performance can be enhanced by using both points and lines <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> even for low-textured environments. For example, line band descriptor (LBD) <ref type="bibr" target="#b5">[5]</ref> is one of the widely-used line descriptors in SLAM. Reliable performance of LBD has been reported for continuous frames, whereas the performance degrades under a wide baseline, which prevented the line-based approach from adapting line features directly in visual localization <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref>. Tackling this limitation, approaches leveraged convolutional neural network (CNN) to learn the representation of line descriptors <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10]</ref>    has innate difficulties in dealing with variable line lengths because the dimension of CNN should be predefined by its architecture. Another issue of line occlusion is also related to this length variation and was examined in SOLD 2 <ref type="bibr" target="#b11">[11]</ref>.</p><p>In our work, we overcome this fixed length requirement and achieve flexibility to the length variation by interpreting a line segment from the natural language processing (NLP) point of view. We view a line consisting of points as a sentence consisting of words, thereby allowing various lengths for lines. The word2vec in NLP learns vector representations that encapsulate each distinct word, leveraging them as base inputs of RNN, LSTM, or transformers for later tasks such as text classification and text summarization. Similarly, we utilize a learned descriptor map as a transition from RGB pixels to point vectors.</p><p>Based on this key idea, we propose a novel line descriptor using transformers <ref type="bibr" target="#b12">[12]</ref>. The transformers in our model learn the context of the point vectors within a line segment to summarize it in the form of a line descriptor. Unlike SOLD 2 , a line is not just a simple sequence of points but is handled with attention to its context dynamically, enabling reliable performance even under occlusion. The attributes of the proposed arXiv:2109.04753v1 [cs.CV] 10 Sep 2021 method can be summarized below.</p><p>? We present a novel line segment descriptor using a transformer architecture by considering line segments as sentences and points as words. Leveraging NLP for line descriptor, we can handle lines with various lengths. ? The proposed line descriptor understands the line segment's context by paying attention to more meaningful points on a line. It effectively abstracts various length lines to a fixed-sized descriptor. ? We suggest line signature networks that share the message of line attributes (e.g., position, angle, length, and descriptors) between neighborhoods. Serving as a grouped line descriptor, the line descriptors can learn geometric attributes of their neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Line Descriptors</head><p>As a handcrafted line descriptor, Wang et al. <ref type="bibr" target="#b13">[13]</ref> proposed a mean-standard deviation line descriptor (MSLD) observing gradients around line segments. LBD <ref type="bibr" target="#b5">[5]</ref> enhanced the precision and computing time by investigating gradients on bands that are parallel to a line. Recently, Vakhitov and Lempitsky <ref type="bibr" target="#b8">[8]</ref> presented a learnable line descriptor (LLD) for visual SLAM in a deep-learning manner. They first built a full-sized descriptor map using CNN and uniformly split a line segment into a fixed-number of subsegments. Lange et al. proposed a learning-based line descriptor named DLD <ref type="bibr" target="#b9">[9]</ref>. They also split a line segment and trained the CNN with a selfgenerated dataset. Extended works from the same group <ref type="bibr" target="#b10">[10]</ref> adapted wavelets to extract more meaningful information from an image, referred to as the Wavelet-based line descriptor (WLD). Pautrat et al. <ref type="bibr" target="#b11">[11]</ref> introduced SOLD 2 for joint line detection and description. To find a line correspondence, SOLD 2 sampled uniformly-spaced point descriptors on a line and performed the sequence matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Point Descriptors</head><p>The recent point descriptor also focused on learning-based methods. Like a handcrafted feature SIFT <ref type="bibr" target="#b14">[14]</ref>, learned descriptors often utilized the patch as their inputs as in L2-Net <ref type="bibr" target="#b15">[15]</ref> and HardNet <ref type="bibr" target="#b16">[16]</ref>. Examining keypoint detection and description together, LIFT <ref type="bibr" target="#b17">[17]</ref> introduced an end-to-end learning pipeline implementing keypoint detection, orientation estimation, and description simultaneously. SuperPoint <ref type="bibr" target="#b18">[18]</ref> proposed self-supervised learning for detector and descriptor using synthetic pretraining and homography adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Point-and-Line SLAM</head><p>Exploiting both point and line for SLAM has strong advantages, robustly performing in challenging environments such as low-textured, motion blurred, and defocused cases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. PL-SVO <ref type="bibr" target="#b0">[1]</ref> extended the SVO method by adapting its semidirect approach to line segments. PL-SLAM <ref type="bibr" target="#b1">[2]</ref> presented line features on the monocular ORB-SLAM. In <ref type="bibr" target="#b2">[3]</ref>, stereo camerabased PL-SLAM was presented to improve the bag-of-words method using points and lines.</p><p>In this paper, we utilize visual localization to evaluate line descriptors. Visual localization <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref> is a problem of estimating a camera pose given a query image and a prior 3D feature map, which is similar to relocalization in SLAM. It differs from the previous works in considering only discrete scenes, including large viewpoint changes, and it is effective for evaluating the robustness of line descriptors in various situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transformers in NLP and CV</head><p>We briefly summarize recent studies connecting NLP and CV. Vaswani et al. proposed the transformer for language translation <ref type="bibr" target="#b12">[12]</ref>, and it became a base architecture of many state-of-the-art methods in NLP tasks. Bidirectional Encoder Representations from Transformers (BERT) is one of the widely used models utilizing transformer encoders <ref type="bibr" target="#b19">[19]</ref>. One of the recent trends in CV is the adoption of the transformers <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. Dosovitskiy et al. proposed a Vision Transformer (ViT) <ref type="bibr" target="#b20">[20]</ref> in which 16x16 patches from an image are used as inputs of a standard transformer for image classification. Visual Transformer (VT) <ref type="bibr" target="#b21">[21]</ref> adopted a transformer using semantic tokens from convolutions and spatial attention so as to treat an image focussing more on important regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>The proposed Line-Transformers aim to build a line descriptor given points located on a line segment ( <ref type="figure" target="#fig_2">Fig. 2)</ref>.</p><p>A. Line Transformers 1) Line Tokenizer: We briefly illustrate NLP terminologies and explain our method based on a similar concept. In NLP, word tokenization is a process used to divide a sentence into smaller words called tokens. The tokens are utilized as minimum units for model inputs. Special tokens are also used to perform special tasks. For example, the classification token [CLS] is applied to aggregate sequence representation for classification tasks and the separator token [SEP] is used to differentiate two sentences. These tokens are converted into vector representations by allocating words with similar meaning into a similar vector space. The vector representation of tokens is called word embedding, and NLP models exploit them to understand natural languages effectively. In our problem, we formulate a point-and-line segment as the relationship between a word and a sentence in the natural language.</p><p>As in <ref type="figure" target="#fig_3">Fig. 3</ref>, the line tokenizer aims to generate point tokens and their embeddings that describe a line segment. After detecting line segments from an image <ref type="bibr" target="#b22">[22]</ref>, we uniformly select points p on a line segment. The position of point p consists of its image coordinates and keypoint confidence: p i = (x, y, c) i . The inter-points interval (v) is then determined depending on the level of discriminability. When the interval of points is small, the model can receive more information to describe a line but needs larger computations and memory. The number of points (n) on a line is n = /v + 1 when a line length is . As the [CLS] token in BERT <ref type="bibr" target="#b19">[19]</ref>, we introduced a special line token [LINE] to aggregate contextual information of  point tokens. The line token is prepended on the series of point tokens.</p><p>We encode each point token into a vector representation, point embedding E ? R 1?D , where D is the descriptor dimension. This is done by associating each point token with a vector that matched in a dense descriptor map <ref type="bibr" target="#b18">[18]</ref> encoded with a descriptor vector at each pixel. The embedding of the special token [LINE], E line ? R 1?D , expresses the initial state of a line descriptor, and its weight values are learned during the training process. Finally, we add the embeddings to the positional embeddings E pos ? R (n+1)?D which is obtained by multilayer perceptron (MLP) using each point's position.</p><p>2) Transformer: Given the token embeddings, we model a line descriptor using transformers <ref type="bibr" target="#b12">[12]</ref>. The transformer encoder is composed of two sublayers: multi-head self-attention (MSA) layers and MLP layers, whereas each sub-layer has a residual connection and layer normalization (LN).</p><p>We stack the transformer L times as in <ref type="bibr" target="#b0">(1)</ref>. Here, the token embeddings z 0 serve as the encoder inputs. The line embedding E line is located at the first element of z 0 , and is expressed with the superscript 0. Then, at the last layer z L , the line embedding contains the line context (i.e., </p><formula xml:id="formula_0">E line = z 0 0 This is a line [CLS] [LINE] Descriptor Map ? ? ? 0.1 0.1 0.2 0.1 0.1 0.2 0.2 0.2 ? 0.1 0.3 ? (a) (b) (c) v ? ? ? 0.0 0.1 0.1 0.1 0.2 0.2 0.3 0.3 ? ? 0 0 MLP Posi?onal Encoder</formula><formula xml:id="formula_1">= z 0 L ). z 0 = [E line ; E 1 ; E 2 ; ...; E n ] + E pos , (1) z i?1 = LN(MSA(z i?1 , mask 0 ) + z i?1 ), z i = LN(MLP(z i?1 ) + z i?1 ), i = 1...L d = z 0 L</formula><p>Each line segment has a various number of point tokens based on the length . To handle the various sizes at once, we use a mask vector mask 0 for the MSA to reject the unrelated point embeddings inside of scaled dot-product attention. The mask has the size 1 ? n max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Line Signature Networks</head><p>Inspired by line signatures <ref type="bibr" target="#b23">[23]</ref> and message-passing <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref> in graph neural networks (GNN), the proposed deep line signature networks are designed for line signatures' messages to be shared with each line segment using graph attention networks. The line signature is originally proposed as a grouped line descriptor. It clusters neighbor line segments as a group and takes relative positions by a series of angles and length ratios. While the line signature needs to define the clustering range of neighbor line segments and the mandefined attributes of lines, we exploit a graph attention network that can implicitly assign neighbor line segments and pass the messages of their descriptions including positions.</p><formula xml:id="formula_2">d i = d i + MLP(x i , y i , i , cos ? i , sin ? i ) (2) s 0 = [d 1 ; d 2 ; ...; d m ] s j = s j?1 + MLP s j?1 ||MSA(s j?1 ) , j = 1...M</formula><p>We first make an attribute embedding by feeding the line's attributes such as midpoints (x, y), angle ?, and length to MLP. Then, we add it on descriptor d i and share the messages with each line segment using the message-passing network as in <ref type="bibr" target="#b1">(2)</ref>. The operator || represents concatenation, and m denotes the number of line descriptors within an image. We also stack the message-passing layers M times. Finally, we normalize line descriptors in z M after feeding them to another MLP.  <ref type="figure">Fig. 4</ref>: Keylines, sublines, and adjacency matrix. Points are selected with the interval v on a line segment. When the number of points n is greater than n max , the keyline is divided into two sublines as in (b). The adjacency matrix describes relationship between keylines and sublines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sublines to Keylines</head><p>For transformers, the number of input tokens is limited by its maximum size (n max ). Overcoming the limited text length of transformers, the longer sequences were often truncated in NLP. Instead of truncation, we handle long line segments utilizing the keyline and subline concepts. Let us call the original line segment a keyline. When the keyline is longer than the maximum ( &gt; n max ? v), we divide it into multiple sublines. In doing so, we can build an adjacency matrix A img via the relationship between keylines and sublines. The values of the adjacency matrix are one divided by the number of sublines, as presented in <ref type="figure">Fig. 4(b)</ref>. Then, the distance matrix of sublines C sublines can be transformed into a distance matrix of keylines C keylines as follows:</p><formula xml:id="formula_3">C keylines = A img1 ? C sublines ? A img2 ,</formula><p>where the adjacency matrices of two images are A img1 and A img2 . The distance matrix of sublines includes the distance between descriptors from two images, which can be calculated by a matcher such as nearest neighbor.</p><p>Similar to the ensemble averaging, (III-C) averages the multiple sublines' distances with respect to a keyline via matrix multiplication of adjacency matrix with distance matrix. For example, the distance between keylines in image1 and sublines in image2 can be represented in A img1 ? C sublines .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>We use a triplet loss function with semi-hard negative sampling strategy <ref type="bibr" target="#b26">[26]</ref>. The basic idea of triplet loss is to make the distance between an anchor descriptor a i and its matched (positive) descriptor p i closer and to simultaneously further the distance with a nonmatched (negative) descriptor n i . In line segment matching, one line in one image can be matched with more than two lines in another image and this means that a single anchor line can have multiple positive lines. In our implementation, we choose the most overlapped line as the positive p i . The overall loss function is given as:</p><formula xml:id="formula_4">L = 1 n n i max(0, ? + a i ? p i 2 ? a i ? n i 2 ),<label>(3)</label></formula><p>where the semi-hard negative sample n i is chosen to the hard negative further away from the positive p i . We observed that semi-negative sampling helped converging loss values stably. The margin value ? provides the capacity to increase the negative distances, and we set it to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>To detect line segments on images, we used Line Segment Detector (LSD) <ref type="bibr" target="#b22">[22]</ref> for its high generalizability over various environments. We selected SuperPoints <ref type="bibr" target="#b18">[18]</ref> for our front-end descriptor map. Because its raw descriptor map is sized H/ 8 ? W/8 given an image sized H ? W , we set 8 to the interval of points on a line. We limited the number of point tokens greater than 2 and smaller than 21 for a subline. A line descriptor, key, query, and value in MSA have the same dimension D = 256 as the SuperPoint. The MSA has four heads, and the line transformers and line signature networks have L = 12 and M = 7 layers, respectively. Our networks contain 14M parameters, and they run at an average speed of 20 ms on an NVIDIA GTX 2080Ti GPU for 256 line descriptors in an image. It is implemented in Pytorch using the Adam optimizer with a learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate our line descriptor in terms of homography estimation and visual localization performance. For two test scenarios, we compare the proposed method against Super-Point, handcrafted line descriptor LBD <ref type="bibr" target="#b5">[5]</ref>, learning-based line descriptors LLD <ref type="bibr" target="#b8">[8]</ref>, WLD <ref type="bibr" target="#b10">[10]</ref>, and SOLD 2 <ref type="bibr" target="#b11">[11]</ref>. We use a nearest neighbor (NN) matcher for LBD, LLD, and WLD, and SOLD 2 by means of its own line matcher. We include SuperPoint as a reference for the point-based matching. We used a maximum of 512 and 1,024 points at each dataset. For lines, we extracted 256 line segments in the longer order. Further results can be seen in the video line-as-a-visual-sentence.mp4.</p><p>A. Homography Estimation 1) Datasets: For homography estimation, we use Oxford and Paris dataset <ref type="bibr" target="#b27">[27]</ref>. The dataset includes 5K (Oxford) and 6K (Paris) images and we split them into the training, validatation, and test sets. For self-supervised learning, we augment images using synthetic homographies and photometric distortions <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28]</ref>.</p><p>To establish ground-truth line correspondences from an image pair, we first detect line segments from both the original image and its augmented image. Then, we project two endpoints of each line onto one another using a known homography matrix. The criteria for the true correspondence are (i) overlap existence, (ii) reprojection error (&lt; 4 px), and (iii) angle difference (&lt; 2 ? ). The resulting true correspondences were expressed as an overlap-similarity matrix. The overlap similarity between two lines measures the ratio between overlapped line length and smaller line length as O(L 1 , L 2 ) = L 1 ? L 2 / min(L 1 , L 2 ). The overlapped line length L 1 ? L 2 is the distance between two middle endpoints  among four endpoints of two lines. For SuperPoint, true point correspondences are defined by the point-wise reprojection errors (&lt; 4 px) following <ref type="bibr" target="#b24">[24]</ref>.</p><p>2) Metrics: We implement a line-based homography matrix estimation <ref type="bibr" target="#b29">[29]</ref>. With the homography estimation utilizing a random sample consensus (RANSAC) of 2,000 iterations, we compute average reprojection errors of the four image corners and report the area under the cumulative error curve (AUC) at the thresholds (5, 10, and, 20 pixels). We also compute matching precision (P) and recall (R) based on the groundtruth matches.</p><p>3) Results: <ref type="table">Table.</ref> I lists the quantitative comparison. Our Line-Transformers outperform other line descriptor methods in terms of F-score by a large margin (10.1%). Our method also outperforms every homography estimation metric except AUC under five pixels. Compared to SuperPoint, the Line-Transformers yielded more stable performance at the AUC under 10 and 20 pixels. LLD has low performances on this dataset because it originally trained in continuous frames without large viewpoint changes.</p><p>The precision and recall are a direct and explicit measure for line-matching performance depending solely on the number of correct/wrong matches. More implicit performance could be captured from homography estimation when the performance depends on the number, distribution, and quality of the matches. In that sense, the proposed method fulfilled the quantity and quality of the reliable matches. We discuss the matching evaluation in more detail in ?IV-D. <ref type="figure" target="#fig_5">Fig. 5</ref> shows qualitative results for the homography estimation-based line matching. Line-Transformers have better performance by producing more correct matches and fewer false matches than other line descriptors. It also shows that LBD in particular has many incorrect matches, resulting in lower matching precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Localization</head><p>Next, we evaluate line descriptors by estimating a camera pose in 3D line maps. The evaluations were performed using the ScanNet <ref type="bibr" target="#b30">[30]</ref> and Oxford Radar RobotCar <ref type="bibr" target="#b31">[31]</ref> datasets for indoor and outdoor experiments. For indoor environments, we trained our networks in supervised learning, and initialized them with the homography estimation's weights. Then, the weights trained using ScanNet were applied directly to outdoor localization to observe our networks' generalizability.</p><p>1) Indoor: The ScanNet dataset <ref type="bibr" target="#b30">[30]</ref> includes 2.5M views of RGB-D data and 3D camera poses in 1,513 indoor scenes. We generate ground-truth line correspondences and 3D line maps. Similarly, as in the research <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b24">24]</ref>, we selected sufficiently overlapping image pairs (40-80%) by investigating the depth maps. Due to the potential inaccuracy in the depth maps, we decomposed a line into point arrays and examined the intermediate points to validate the line correspondences. Then, we randomly select 350 image pairs from each of the four overlapped scenes for the test set, a total of 1,400 image pairs. Because depth maps are not sufficiently accurate to find all ground-truth line pairs, some correct line correspondences may be counted as a false negative. Therefore, we mitigate this issue by checking scene depth quality, as well as observing the ratio between the numbers of all extracted line segments and the validated line segments during projection/unprojection procedures.</p><p>2) Outdoor: The Oxford Radar RobotCar dataset <ref type="bibr" target="#b31">[31]</ref> is built upon the Oxford RobotCar dataset <ref type="bibr" target="#b33">[33]</ref>. We selected two sequences (2019-01-11-12-26-55 and 2019-01-16-13-09-37) for our reference and query sets. Then, we randomly selected 300 query images in a sequence and performed visual place recognition <ref type="bibr" target="#b34">[34]</ref> to identify corresponding reference images that have a 3D line feature map. Instead of using the global positioning system (GPS) potential unreliable signals, we have computed the ground-truth relative pose between query and reference images using their point clouds via Iterated Closest    Point (ICP). In the final evaluation sets, we excluded queryreference image pairs with poor ICP fitness. The 3D line maps were generated following procedures similar to those for our indoor evaluation. Unlike the indoor RGB-D camera, however, the projected LiDAR points are so sparse that 2D line segments can be difficult to find their corresponding depth value. To alleviate this, we utilized a depth completion <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Metrics:</head><p>We report the percentage of correctly localized query images while using different thresholds (i.e., 0.25m, 10?/ 0.5m, 10?/ 1.0m, 10?for indoor and 0.25m, 2?/ 0.5m, 5?/ 5.0m, 10?for outdoor). We estimated the camera pose by Perspective-n-Point-Line (PnPL) <ref type="bibr" target="#b36">[36]</ref> with a RANSAC of 20 iterations. For SuperPoint, we leveraged Perspectiven-Point (PnP). We analyze the pose estimation results that use points, lines, and points-and-lines, respectively. We also report matching precision (P) and recall (R) based on the groundtruth matches at indoor. 4) Results: For both indoor and outdoor tests, Line-Transformers achieve the highest performance among other line descriptors in visual localization and precision-recall metrics <ref type="table" target="#tab_2">(Table. II</ref>). The qualitative results in <ref type="figure" target="#fig_5">Fig. 5</ref> illustrate that the Line-Transformers perform robustly in imaging changes such as blurring, viewpoints, and illuminations. Our method can be generalized well performing reliably using the same weights trained in ScanNet datasets.</p><p>Unlike in the homography estimation, the point-based method with PnP outperforms all line-based methods. One of the reasons is the small number of 3D line inliers during depth validation. While the 3D feature point is directly determined by its corresponding depth pixel, some 3D line features are filtered out during depth linearity's validation in RANSAC. Thus, within our mapping method, line-based localization is prone to performance degradation than point-based. However, as will be discussed in ?IV.F, the line features can complement the performance of the points, especially when point feature numbers are small or biased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance by Line Length</head><p>To examine the robustness of our model with respect to various line lengths, we further investigate performance comparisons against CNN-based methods while varying line length. From each dataset, we evenly divide line segment sets into three groups (i.e., short, mid, and long) by their lengths. Each group has 33% numbers of all line segments, and the mid-length group ranged from about 30 to 50 pixels.</p><p>As presented in <ref type="figure" target="#fig_7">Fig. 6(b)</ref>, the proposed Line-Transformers outperform other descriptors for all line lengths. Furthermore, as shown in <ref type="figure" target="#fig_7">Fig. 6(a)</ref>, the matching performance difference between Line-Transformers and other CNN-based line descriptors (LLD and WLD) is increased by line length. This tendency indicates that the performance of the proposed method is preserved even with longer line segments, unlike the handcrafted descriptor and CNN-based descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion on Evaluation Metrics</head><p>Unlike point features that assume one-to-one matching, line matching is a many-to-many problem because a line detector tends to break the same line segment into small lines differently at each image pair. For example, two nonoverlapping lines may originate from a single line due to the occlusion and segmentation; thus, they should be considered the correct correspondences semantically.</p><p>Evaluation of line segment matches often depends on human inspections <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b5">5]</ref>, assuming that line matches are one-to-one correspondences <ref type="bibr" target="#b5">[5]</ref>, not many-to-many. Therefore, when a matcher finds only the closest matching pair, the precision and recall should be carefully considered cautioning  many-to-many correspondences. In that sense, the precisionrecall metrics may be limited because they cannot consider non-overlapping line correspondences. Visual localization and homography estimation could be more proper metrics in this aspect. In visual localization, matching with the non-overlapped but semantically same line is also considered a good match because a Perspective-n-Line (PnL) algorithm does not consider the endpoint's positions to ensure the robustness for changing endpoints. Similarly, the line-based homography estimation does not consider endpoints <ref type="bibr" target="#b29">[29]</ref> but is limited to planar scenes in the real 3D world. Hence, we found that line-based visual localization is a better alternative that can inspect both overlapped and nonoverlapped line matches with large perspective changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Understanding Line-Transformers</head><p>To examine the attention scores of Line-Transformers, we visualize the maximum attention score of each multi-heads within a line segment <ref type="figure" target="#fig_9">(Fig. 8</ref>). The point embeddings on a line are not considered equally for a line descriptor, but they have their own attention patterns. We also observe that the matched lines have similar attention patterns and the line descriptors tend to refer to the endpoints of a line.</p><p>The <ref type="figure" target="#fig_9">Fig. 8 (b)</ref> illustrates the attention between line descriptors in line signature networks. We observe that the attentions gradually focus on a small number of neighbor lines at a later layer. To study the quantitative effect of the line signature networks, we evaluate our models without line signature networks. As presented in <ref type="table" target="#tab_2">Table. II and Table.</ref> III, the line signature networks improve the localization results from 47.4% to 53.0% under the threshold of 0.25m and 10?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visual Localization Using Feature Points and Lines</head><p>Despite the generally better performance in <ref type="table">Table.</ref> II, the point-based localization may be deteriorated due to the small number of points (e.g., feature-less environment) or biased feature distribution as in the sample cases in <ref type="figure" target="#fig_8">Fig. 7</ref>. This section examines how the line-based approach can enhance point-based localization in a complementary fashion.</p><p>For the analysis, we define point-based localization failure using the reprojection errors of 3D features and count the inlier when the reprojection error is less than four pixels. Then, the PL-Loc was additionally performed for the cases when the number of inliers is less than 5 or 20 (i.e., when point-based inliers drop).</p><p>As presented in the table in <ref type="figure" target="#fig_8">Fig. 7(b)</ref>, the PL-Loc provides additional enhancements to visual localization. More interestingly, the point outperformed over the line for 61% of cases, which indicates the remaining 39% of cases are with potential to be improved with lines. This also implies the proper combination of point and line would improve the overall localization performance. For example, better metrics followed by a strong model selection could be examined to complete robust PnPL in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper presented a novel line descriptor handling variable line length effectively using attention mechanisms, inspired by NLP tasks handling sentences and paragraphs of various lengths. We also presented a PL-Loc pipeline leveraging keypoints and keylines simultaneously for visual localization. Our experiments demonstrated that our line descriptor achieved state-of-the-art performance in homography estimation and visual localization datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) For both low and high-textured environments, our line descriptor performs accurate matching. (b) Attention scores for points in a line with matched lines having similar attentional contexts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Line-Transformers consist of two major components: line transformers and line signature networks. The first component uses a line tokenizer to extract point tokens and embeddings from a line segment. Considering the context of the point embeddings, transformers summarize it into a line embedding, or a line descriptor. The second component enhances the line descriptor by sharing lines' positional context with its neighborhoods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>(a) In BERT<ref type="bibr" target="#b19">[19]</ref>, a sentence is tokenized, and a [CLS] is prepended for a sentence classification task. (b) Similar to BERT, a line segment is tokenized, and a special line token [LINE] is used for aggregating points information. (c) The point embedding is a descriptor vector located on each point token on a descriptor map. and a line descriptor d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative line segment matches for homography estimation and visual localization. The last three columns represent blurring, viewpoint, and illumination changes. More correct matches (green) and fewer wrong matches (red) indicate better performance. Unmatched lines are colored in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Performance difference by line length. The figure (a) illustrates that the overall graph has an upward trajectory, thus showing that our method performs better than other CNN-based line descriptors when line segments extend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Points and lines are complementary for better localization, especially when keypoints are biased or in small numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Visualizing attention scores. (a) Attention patterns in lines describe how much the points embeddings contribute to building line descriptors. The matched lines follow similar attention patterns. (b) Attention scores between line descriptors are initially low and widely spread, and they are gradually converged onto a small number of neighbor lines at a later layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>yielding superior performance. Still, CNN Manuscript received: April 29, 2021; Revised August 2, 2021; Accepted September 7, 2021. This paper was recommended for publication by Editor Sven Behnke upon evaluation of the Associate Editor and Reviewers' comments. This work was fully supported by [Localization in changing city] project funded by Naver Labs Corporation.</figDesc><table /><note>1 S. Yoon is with the Robotics Program, KAIST, Daejeon, S. Korea sungho.yoon@kaist.ac.kr2 A. Kim is with the Department of Mechanical Engineering, SNU, Seoul, S. Korea ayoungk@snu.ac.kr Digital Object Identifier (DOI): see top of this page.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Homography estimation evaluation. The bestperforming values are in bold font.</figDesc><table><row><cell></cell><cell cols="3">Homography AUC AUC@5px AUC@10px AUC@20px</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>(SuperPoint)</cell><cell>(38.5)</cell><cell>(43.3)</cell><cell>(46.3)</cell><cell>(37.6)</cell><cell>(38.6)</cell><cell>(39.6)</cell></row><row><cell>LBD</cell><cell>2.2</cell><cell>7.6</cell><cell>17.5</cell><cell>20.6</cell><cell>55.2</cell><cell>30.1</cell></row><row><cell>LLD</cell><cell>0.7</cell><cell>2.6</cell><cell>6.6</cell><cell>5.9</cell><cell>13.6</cell><cell>8.3</cell></row><row><cell>WLD</cell><cell>16.7</cell><cell>35.2</cell><cell>54.5</cell><cell>48.0</cell><cell>51.3</cell><cell>49.6</cell></row><row><cell>SOLD 2</cell><cell>31.8</cell><cell>51.5</cell><cell>67.1</cell><cell>41.1</cell><cell>45.8</cell><cell>43.3</cell></row><row><cell>LT (Ours)</cell><cell>29.5</cell><cell>52.1</cell><cell>69.4</cell><cell>57.7</cell><cell>61.5</cell><cell>59.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">: Visual localization</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">Visual Localization AUC (Indoor) 0.25m/10? 0.50m/10? 1.00m/10?</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>(SuperPoint)</cell><cell cols="2">(83.6 PnP)</cell><cell cols="2">(86.4 PnP)</cell><cell cols="2">(86.8 PnP)</cell><cell cols="2">(49.5) (69.1)</cell><cell>(57.7)</cell></row><row><cell></cell><cell>PnL</cell><cell>PnPL</cell><cell>PnL</cell><cell>PnPL</cell><cell>PnL</cell><cell>PnPL</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LBD</cell><cell>15.7</cell><cell>38.4</cell><cell>19.0</cell><cell>45.9</cell><cell>19.3</cell><cell>46.6</cell><cell>18.5</cell><cell>49.8</cell><cell>27.0</cell></row><row><cell>LLD</cell><cell>12.0</cell><cell>35.8</cell><cell>16.1</cell><cell>41.6</cell><cell>16.4</cell><cell>42.5</cell><cell>10.5</cell><cell>31.8</cell><cell>15.8</cell></row><row><cell>WLD</cell><cell>28.0</cell><cell>55.9</cell><cell>35.4</cell><cell>61.6</cell><cell>36.1</cell><cell>62.4</cell><cell>27.9</cell><cell>37.3</cell><cell>31.9</cell></row><row><cell>SOLD 2</cell><cell>46.4</cell><cell>73.5</cell><cell>57.4</cell><cell>77.9</cell><cell>59.2</cell><cell>78.6</cell><cell>35.0</cell><cell>40.5</cell><cell>37.5</cell></row><row><cell>LT (w/o LS)</cell><cell>47.4</cell><cell>76.9</cell><cell>59.8</cell><cell>81.6</cell><cell>61.4</cell><cell>82.5</cell><cell>42.4</cell><cell>62.2</cell><cell>50.4</cell></row><row><cell>LT (Ours)</cell><cell>53.0</cell><cell>79.2</cell><cell>65.0</cell><cell>83.3</cell><cell>66.6</cell><cell>83.9</cell><cell>49.4</cell><cell>68.4</cell><cell>57.3</cell></row><row><cell></cell><cell cols="6">Visual Localization AUC (Outdoor) 0.25m/2? 0.50m/5? 5.00m/10?</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>(SuperPoint)</cell><cell cols="2">(37.1 PnP)</cell><cell cols="2">(63.6 PnP)</cell><cell cols="2">(90.8 PnP)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PnL</cell><cell>PnPL</cell><cell>PnL</cell><cell>PnPL</cell><cell>PnL</cell><cell>PnPL</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LBD</cell><cell>1.9</cell><cell>4.5</cell><cell>2.6</cell><cell>18.5</cell><cell>9.1</cell><cell>59.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LLD</cell><cell>1.9</cell><cell>7.5</cell><cell>6.4</cell><cell>30.2</cell><cell>20.0</cell><cell>71.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>WLD</cell><cell>9.4</cell><cell>21.5</cell><cell>29.4</cell><cell>45.7</cell><cell>54.7</cell><cell>83.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SOLD 2</cell><cell>21.5</cell><cell>28.7</cell><cell>54.0</cell><cell>53.2</cell><cell>82.3</cell><cell>90.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LT (w/o LS)</cell><cell>21.9</cell><cell>29.4</cell><cell>55.1</cell><cell>58.9</cell><cell>87.9</cell><cell>91.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LT (Ours)</cell><cell>26.8</cell><cell>30.9</cell><cell>57.7</cell><cell>61.1</cell><cell>90.2</cell><cell>91.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Ablation of Line-Transformers.</figDesc><table><row><cell>Line-Transformers</cell><cell cols="3">Line-based Visual Localization 0.25m/10? 0.50m/10? 1.00m/10?</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>No Line Signature Net (LS)</cell><cell>47.4</cell><cell>59.8</cell><cell>61.4</cell><cell cols="2">42.4 62.2 50.4</cell></row><row><cell>No Positional encoding in LS</cell><cell>49.4</cell><cell>62.8</cell><cell>64.3</cell><cell cols="2">44.3 66.4 53.1</cell></row><row><cell>No mid-point in LS</cell><cell>51.1</cell><cell>62.6</cell><cell>64.9</cell><cell cols="2">47.7 69.0 56.4</cell></row><row><cell>No length, angle in LS</cell><cell>51.9</cell><cell>64.2</cell><cell>65.8</cell><cell cols="2">44.0 68.4 53.6</cell></row><row><cell>Full</cell><cell>53.0</cell><cell>65.0</cell><cell>66.6</cell><cell cols="2">49.4 68.4 57.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pl-svo: Semi-direct monocular visual odometry by combining points and line segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gomez-Ojeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Briales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</title>
		<meeting>IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4211" to="4216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pl-slam: Real-time monocular visual slam with points and lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakhitov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. on Robot. and Automat</title>
		<meeting>IEEE Intl. Conf. on Robot. and Automat</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4503" to="4508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pl-slam: A stereo slam system through the combination of points and line segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gomez-Ojeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zu?iga-No?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="734" to="746" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visualinertial odometry with point and line features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eckenhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee/Rsj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. on Intell. Robots and Sys</title>
		<imprint>
			<biblScope unit="page" from="2447" to="2454" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An efficient and robust line segment matching approach based on lbd descriptor and pairwise geometric consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Visual Comm. and Img. Rep</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="794" to="805" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term visual localization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pat. Anal. and Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From coarse to fine: Robust hierarchical localization at large scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dymczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learnable line segment descriptor for visual SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakhitov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dld: A deep learning based line descriptor for line feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schweinfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</title>
		<meeting>IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5910" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">WLD: A Wavelet and Learning based Line Descriptor for Line Feature Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Symp. on Vis., Mod., and Vis</title>
		<meeting>Intl. Symp. on Vis., Mod., and Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sold?: Self-supervised occlusion-aware line description and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pautrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Juan-Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Sys</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MSLD: A robust descriptor for line matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="941" to="953" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. of Comput. Vision</title>
		<imprint>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">L2-net: Deep learning of discriminative patch descriptor in euclidean space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6128" to="6136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Sys</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4826" to="4837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Comput. Vision</title>
		<meeting>European Conf. on Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="224" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. of the North American Chapter of the Assoc. for Comput. Linguistics: Human Lang</title>
		<meeting>Conf. of the North American Chapter of the Assoc. for Comput. Linguistics: Human Lang</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note type="report_type">Tech</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Learning Representations</title>
		<meeting>Intl. Conf. on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LSD: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pat. Anal. and Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wide-baseline image matching using line signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. on Comput</title>
		<meeting>IEEE Intl. Conf. on Comput</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1311" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4937" to="4946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Learning Representations</title>
		<meeting>Intl. Conf. on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Revisiting oxford and paris: Large-scale image retrieval benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5706" to="5715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">R2D2: repeatable and reliable detector and descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Sys</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="405" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Combining line and point correspondences for homography estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubrofsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Symp. Visual Computing</title>
		<meeting>Intl. Symp. Visual Computing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">5359</biblScope>
			<biblScope unit="page" from="202" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2432" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Murcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01300</idno>
		<title level="m">The oxford radar robotcar dataset: A radar extension to the oxford robotcar dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">D2-net: A trainable CNN for joint detection and description of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8092" to="8101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. of Robot. Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning with average precision: Training image retrieval with a listwise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. on Comput. Vision</title>
		<meeting>IEEE Intl. Conf. on Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5106" to="5115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Comput. Vision, 2020</title>
		<meeting>European Conf. on Comput. Vision, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="120" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cvxpnpl: A unified convex solution to the absolute pose estimation problem from point and line correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agostinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Line segment matching: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. on Appl. of Computer Vision</title>
		<meeting>IEEE Winter Conf. on Appl. of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
