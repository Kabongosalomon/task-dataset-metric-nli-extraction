<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OCTUPLET LOSS: MAKE FACE RECOGNITION ROBUST TO IMAGE RESOLUTION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-15">July 15, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Knoche</surname></persName>
							<email>martin.knoche@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Chair of Human-Machine Communication Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elkadeem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chair of Human-Machine Communication Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>H?rmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chair of Human-Machine Communication Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chair of Human-Machine Communication Technical University Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OCTUPLET LOSS: MAKE FACE RECOGNITION ROBUST TO IMAGE RESOLUTION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-15">July 15, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image resolution, or in general, image quality, plays an essential role in the performance of today's face recognition systems. To address this problem, we propose a novel combination of the popular triplet loss to improve robustness against image resolution via fine-tuning of existing face recognition models. With octuplet loss, we leverage the relationship between high-resolution images and their synthetically down-sampled variants jointly with their identity labels. Fine-tuning several state-of-the-art approaches with our method proves that we can significantly boost performance for cross-resolution (high-to-low resolution) face verification on various datasets without meaningfully exacerbating the performance on high-to-high resolution images. Our method applied on the Face-Transformer network achieves 95.12% face verification accuracy on the challenging XQLFW dataset while reaching 99.73% on the LFW database. Moreover, the low-to-low face verification accuracy benefits from our method. We release our code 1 to allow seamless integration of the octuplet loss into existing frameworks.</p><p>Knoche et al.</p><p>[7] extensively analyzed the susceptibility of face recognition systems to image resolution. Their work demonstrates that face verification accuracy for the popular ArcFace [1] approach with a ResNet50 [8] as the backbone network is dropping significantly for image resolutions below about 50?50 px. As illustrated later in our experiments, we confirm this effect also on other architectures such as MobileNet <ref type="bibr" target="#b8">[9]</ref> or iResNet50 <ref type="bibr" target="#b9">[10]</ref>. In <ref type="bibr" target="#b6">[7]</ref>, the authors also stated that the face transformer structure <ref type="bibr" target="#b10">[11]</ref> is less affected by varying image resolution, which is in line with our findings in Sec. 4.</p><p>Generally, one can distinguish between two face recognition scenarios concerning image resolution: 1) Low-resolution face verification considers two facial images with the same low resolution. 2) The validation of two images with different resolutions is described as cross-resolution face verification. Despite the increased amount of information 1 Code</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the continuous development of face recognition systems has opened various applications such as automatic phone unlocking, border control, public surveillance, and many more convenient applications. Current stateof-the-art face recognition systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> achieve impressive performance on popular benchmark datasets as LFW <ref type="bibr" target="#b2">[3]</ref>, MegaFace <ref type="bibr" target="#b3">[4]</ref>, or IJB-B <ref type="bibr" target="#b4">[5]</ref>. However, these systems are primarily designed to operate in controlled environments, e.g., on images with high quality or resolution, and their performance significantly deteriorates in uncontrolled environments, e.g., on low-resolution images <ref type="bibr" target="#b5">[6]</ref>. With the advances towards ever more robust face recognition systems applicable in such crucial scenarios, more and more approaches are being published. Various approaches focus on robust face recognition against age gaps, head pose variances, alignments, adversarial attacks, occlusions, and masks. Only a few authors focus on image resolution (cf. Sec. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HIGH RESOLUTION LOW RESOLUTION</head><p>Octuplet Loss present in high-resolution images, the latter problem is even more challenging due to the distinct inherent visual properties of high-and low-resolution images. This emerges in the context of surveillance applications where, e.g., low-resolution surveillance images are compared with high-quality passport images. Another example is the automatic tagging of people in movies or social media, where image resolution is often compromised due to compression.</p><p>In this work, we tackle cross-resolution face recognition with a novel metric learning approach called octuplet loss fine-tuning. This objective constitutes fine-tuning an existing network to increase its robustness against image resolution while maintaining its performance in controlled scenarios. As depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>, we exploit the advantages of the widespread triplet loss <ref type="bibr" target="#b11">[12]</ref> and build upon it. Our key innovation is the combination of four triplet loss terms, which exploit the relationship between high-and low-resolution images and identity labels.</p><p>Our main contributions are summarized as follows:</p><p>? We propose a novel loss function called octuplet loss that leverages four triplet loss terms to capture the relationships between high-and low-resolution faces.</p><p>? A fine-tuning strategy is introduced, which can be easily applied to existing networks to improve their robustness against image resolution while maintaining comparable performance on high-resolution images.</p><p>? We demonstrate that fine-tuning several state-of-the-art networks with our proposed octuplet loss leads to significant improvements for cross-resolution and low-resolution face verification on numerous popular datasets.</p><p>The rest of our paper is organized as follows: in Sec. 2, we review the literature related to this area; Sec. 3 introduces the triplet loss concept, describes the applied mining strategy, and presents the octuplet loss function in detail; in Sec. 4, we describe datasets and experimental settings, followed by quantitative results, i.e., improvements on existing networks with our approach and ablation studies; finally, Sec. 5 concludes this work and indicates possible directions for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a wide variety of works related to robust face recognition. Reviewing it would be out of the scope of this paper, so we only briefly describe the most relevant recent work in cross-resolution face recognition. Those methods can be divided into transformation-based and non-transformation-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformation-Based Approaches</head><p>Transformation-based or hallucination-based approaches tackle this challenging problem by super-resolving lowresolution faces prior to matching them with high-resolution images. Jiang et al. <ref type="bibr" target="#b12">[13]</ref> provided an exhaustive review of face super-resolution in general.</p><p>Recently, prior guided <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> and attribute constrained <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> face super-resolution approaches were presented. However, they aim for a visually pleasant reconstruction ignoring identity-related information. Therefore, numerous works <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> leveraged face recognition networks to ensure face feature similarity and optimized the super-resolution to preserve identity information.</p><p>To cope with weakly labeled datasets, Hsu et al. <ref type="bibr" target="#b33">[34]</ref> apply an identity-preserving contrastive loss, whereas Kazemi et al. <ref type="bibr" target="#b34">[35]</ref> utilize an adversarial face verification loss. Very recently, Ghosh et al. <ref type="bibr" target="#b35">[36]</ref> presented an end-to-end supervised resolution enhancement and recognition network using a heterogeneous quadruplet loss metric to train a generative adversarial network (GAN), which super-resolves images without corrupting the discriminative information of the low-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-Transformation-Based Approaches</head><p>Non-transformation-based approaches aim to directly project facial features from arbitrary resolution images into a common feature space. In <ref type="bibr" target="#b36">[37]</ref>, this was accomplished by a non-linear coupled mapping architecture using two deep convolutional neural networks (CNNs). <ref type="bibr" target="#b37">[38]</ref> approached the problem differently with a student-teacher method. A deep coupled ResNet model containing one trunk network and two branch networks was introduced by <ref type="bibr" target="#b38">[39]</ref>. The trunk network extracts facial features, while the two branch networks transform high-resolution and the corresponding low-resolution features into a common feature space. In <ref type="bibr" target="#b39">[40]</ref>, <ref type="table">Talreja</ref>  Knoche et al. <ref type="bibr" target="#b6">[7]</ref> provided the BT-M model, which was trained straightforwardly with half the number of images within each batch being low-resolution. Additionally, they contributed two networks (ST-M1 and ST-M2), which both incorporate a siamese network structure, enabling optimization with respect to an additional feature distance loss. Similar to the BT-M model of <ref type="bibr" target="#b6">[7]</ref>, Zeng et al. <ref type="bibr" target="#b43">[43]</ref> presented a resolution-invariant deep network and trained it directly with unified low-and high-resolution images. In <ref type="bibr" target="#b44">[44]</ref>, the authors applied a cross-resolution contrastive loss on higher-level features of two separate network branches, with each branch focusing precisely on one resolution (high and low). The following two methods go one step further: <ref type="bibr" target="#b45">[45]</ref> tackled the problem with a deep siamese network structure and combined a classification loss with a cross-resolution triplet loss. Zha and Chao <ref type="bibr" target="#b46">[46]</ref> also applied cross-resolution triplet loss, but in contrast to <ref type="bibr" target="#b45">[45]</ref>, they used a two-branch network similar to <ref type="bibr" target="#b44">[44]</ref>.</p><p>In the recent past, <ref type="bibr" target="#b47">[47]</ref> proposed a multi-scale parallel deep CNN feature fusion architecture. In contrast to most other face recognition systems, they provide an end-to-end approach and directly predict the similarity score of two input images. Very recently, Li et al. <ref type="bibr" target="#b48">[48]</ref> proposed a novel deep rival penalized competitive learning strategy for low-resolution face recognition.</p><p>The work of Terh?rst et al. <ref type="bibr" target="#b49">[49]</ref> pursued a distinct goal. They focused on a more general quality-aware face recognition, i.e., they do not concentrate solely on the physical image quality but also consider pose and age variations. Their approach combines a quality-aware comparison score, utilizing model-specific face image qualities, with a face recognition model based on a magnitude-aware angular margin loss. A rather unusual method in this field of research but still relevant is the work of Zhao <ref type="bibr" target="#b50">[50]</ref>, which shows a new technique for correlation feature-based face recognition.</p><p>Despite the recent advances in resolution robust face recognition, a closer look at the works on this topic reveals that there is no standard benchmark method. Not only are different datasets for training and cross/same resolution evaluation used, but the synthetically down-sampling is also different across coding platforms/tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Triplet Loss</head><p>Recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b51">51]</ref> showed that triplet-based learning helps extract more discriminative face embeddings.</p><p>Being the fundament of our octuplet loss function, we first review the concept of triplet loss <ref type="bibr" target="#b11">[12]</ref> in the face recognition domain in more detail.</p><p>Let B = {I 1 , I 2 , . . . , I B } be a mini-batch of B facial images I ? R 112?112?3 , whereby each image belongs to a particular identity id(I ). Given that every identity within the mini-batch is represented by at least two images, we define a set of triplets according to the following rule:</p><formula xml:id="formula_0">T (B 1 , B 2 , B 3 ) := (A, P , N ) : A ? B 1 , P ? B 2 , N ? B 3 , id(A) = id(P ), id(A) = id(N ), A = P ,<label>(1)</label></formula><p>with A denoting an anchor image, P being its related positive image, which belongs to the same identity, and N being its related negative image of a different identity. Using a feature extractor f (I ), we obtain facial embeddings f = f (I ) in a d-dimensional Euclidean space. Then, the triplet loss aims to indirectly enlarge the feature distance d(?, ?) between P and N by pulling f (P ) and f (A) together and simultaneously repelling the f (N ) from f (A). In this work, we consider three different feature distance metrics: cosine d cos , Euclidean d euc , and Euclidean squared d euc 2 , which are defined by</p><formula xml:id="formula_1">d cos (f 1 , f 2 ) = 1 ? f 1 ? f 2 f 1 2 f 2 2 , (2) d euc (f 1 , f 2 ) = f 1 ? f 2 2 , and<label>(3)</label></formula><formula xml:id="formula_2">d euc 2 (f 1 , f 2 ) = d euc (f 1 , f 2 ) 2 .<label>(4)</label></formula><p>A margin m, as a minimum distance between the positive and negative image, enforces that triplets of which the distance of the negative and positive image is already larger than the margin will not affect the loss (cf. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">52]</ref>). The objective triplet loss function L tri can then be formulated as follows:</p><formula xml:id="formula_3">L tri (T ) = 1 |T | (A,P ,N ) ?T d f(A), f(P ) ? d f(A), f(N ) + m + ,<label>(5)</label></formula><p>where [?] + denotes max(0, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hard Sample Mining</head><p>Given the constraint that in all our experiments, a mini-batch strictly contains two randomly selected images of the same identity, the number of identities within each mini-batch is B/2. For each anchor image A, we can find exactly one positive image P and B ? 2 negative images N . Hence, the cardinality of the set |T | = B 2 ? 2B (cf. Equation <ref type="formula" target="#formula_0">(1)</ref>).</p><p>With this set of triplets T , the maximum information within each mini-batch is exploited by the triplet loss. However, the majority of triplets within T , according to Equation (1), do not contribute towards L tri as they are already correctly classified and thus fulfill d f (A), f (P ) + m &lt; d f (A), f (N ) (cf. Equation <ref type="formula" target="#formula_3">(5)</ref>). To accelerate the training procedure, we follow Hermans et al. <ref type="bibr" target="#b53">[53]</ref> and select only the most relevant negative sample N * , which is obtained for a given anchor image A by</p><formula xml:id="formula_4">N * = arg min N d f (A), f (N ) .<label>(6)</label></formula><p>This additional constraint leads to a more meaningful set T and hence a less costly minor cardinality |T | = B. However, selecting the most challenging sample is prone to include outliers, e.g., incorrectly labeled data, and thus hinders f in learning meaningful associations. Nevertheless, in line with <ref type="bibr" target="#b53">[53]</ref>, we observed in our experiments that a large number of triplets mitigates this effect within each mini-batch. Thus, we consider this hard sample mining strategy a valid method for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Octuplet Loss</head><p>The primary purpose of this work constitutes improving the robustness of existing face recognition models by elegantly exploiting the triplet loss. Inspired by <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b54">54]</ref>, we formulate four different triplet loss terms combining high-and low-resolution images. In contrast to <ref type="bibr" target="#b45">[45]</ref>, we follow the idea of fine-tuning rather than training from scratch utilizing a classification loss. With our octuplet loss, we aim to allow any network to directly learn the connection between highand low-resolution while maintaining its performance on high-resolution images. The concept of applying triplet loss to features from different image resolutions is also proposed in <ref type="bibr" target="#b46">[46]</ref>. However, their features are computed via two separate branches of the network, thus increasing the computational costs. We aim to directly project embeddings from images with arbitrary resolutions r into a common feature space.</p><p>Nowadays, benchmarks and applications typically utilize the distance between facial embeddings to distinguish between same or different identities. Therefore, it is reasonable to employ the feature distances directly in the training phase.</p><p>Due to the lack of large face recognition training datasets containing both low and high-resolution images, we simulate a lower image resolution by synthetically down-sampling images to a particular resolution r ? 7, 14, 28 and subsequent up-sampling to restore the original resolution (in our experiments 112?112, cf. Subsec. 3.1). For both operations, we apply a bicubic kernel and anti-aliasing. Since we only use square images, we specify the image resolution by the first dimension for the remaining part of this work. With this image degradation method, we double the size of every mini-batch, such that it comprises B high-resolution images B with their corresponding low-resolution images B lr .</p><p>Together with the hard sample mining strategy (cf. Subsec. 3.2), we define the following four sets of triplets:</p><formula xml:id="formula_5">T hhh := (A, P , N * 1 ) ? T (B, B, B) : N * 1 = arg min N d f(A), f(N ) ,<label>(7)</label></formula><p>which exclusively consists of high-resolution images.</p><formula xml:id="formula_6">T hll := (A, P lr , N * 2lr ) ? T (B, B lr , B lr ) : N * 2lr = arg min N lr d f(A), f(N lr )<label>(8)</label></formula><p>and</p><formula xml:id="formula_7">T lhh := (A lr , P , N * 3 ) ? T (B lr , B, B) : N * 3 = arg min N d f(A lr ), f(N ) ,<label>(9)</label></formula><p>which contain a mix of low-and high-resolution images. Lastly,</p><formula xml:id="formula_8">T lll := (A lr , P lr , N * 4lr ) ? T (B lr , B lr , B lr ) : N * 4lr = arg min N lr d f(A lr ), f(N lr ) .<label>(10)</label></formula><p>which comprises solely low-resolution images. With this configuration, we ensure that the P and N are both either degraded or non-degraded.</p><p>Note that the hard sample mining strategy is applied separately for each set of triplets. Simultaneously calculating the triplet loss for each set will result in considering the feature distances between up to eight different images for every A ? B. Thus, the combination of all four triplet losses consequently depends on the octuplet (A, A lr , P , P lr , N * 1 , N * 2lr , N * 3 , N * 4lr ). As a result, our novel loss is named octuplet loss L oct and is computed by</p><formula xml:id="formula_9">L oct = L tri (T hhh ) + L tri (T hll ) + L tri (T lhh ) + L tri (T lll ) .<label>(11)</label></formula><p>This way, Equation <ref type="formula" target="#formula_0">(11)</ref> encompasses all three cases: low-resolution face pairs (L tri (T lll )), cross-resolution face pairs (L tri (T hll ) and L tri (T lhh )), and high-resolution face pairs (L tri (T hhh )). Consequently, we not only increase the robustness against low-and cross-resolution face pairs but also guarantee that the network does not forget to handle high-resolution face pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>This work uses the MS1M-V2 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">55]</ref> database for training and validation, comprising 5.7M images of 87k identities. The vast majority (? 99.9%) is used for our fine-tuning strategy, and only ? 1 is retained for validation. From the latter subset, we randomly generated 3000 genuine and 3000 imposter image pairs to measure face verification performance during training. Due to our condition that each identity within a mini-batch must appear exactly twice (cf. Subsec. 3.2), we employ an algorithm that creates the mini-batches. Images are picked from the entire dataset according to the number of unpicked images per identity. By updating the underlying probability distribution after every batch, we ensure diverse batches even at the end of every epoch.</p><p>We evaluate all models on the well-known face verification dataset Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b2">[3]</ref>. Moreover, we apply our models to several publicly available variants of LFW: XQLFW <ref type="bibr" target="#b56">[56]</ref> (large image quality difference), CALFW <ref type="bibr" target="#b57">[57]</ref> (large age gap), CPLFW <ref type="bibr" target="#b58">[58]</ref> (large pose variations), and SLLFW <ref type="bibr" target="#b59">[59]</ref> (similar faces). Finally, we evaluate the face verification accuracy on AgeDB <ref type="bibr" target="#b60">[60]</ref> (large age gap) and CFP-FP/CFP-FF <ref type="bibr" target="#b61">[61]</ref> (frontal-profile/frontal-frontal image pairs). All protocols consist of 3000 genuine and 3000 imposter pairs, except for CFP-FP/CFP-FF, which contain 3500 genuine and 3500 imposter pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>To demonstrate the effectiveness of our octuplet loss, we employ it on various pre-trained approaches, i.e., we take a pre-trained model and fine-tune it only with our proposed octuplet loss function.  <ref type="bibr" target="#b10">[11]</ref>, and 2 hours for the MobileNetV2 <ref type="bibr" target="#b8">[9]</ref> architecture. We follow <ref type="bibr" target="#b0">[1]</ref> in data preprocessing and generate normalized face crops (112?112 px) with five facial landmarks extracted with the MTCNN <ref type="bibr" target="#b64">[64]</ref> for all our experiments. Additionally, besides horizontal flipping, random brightness and saturation variation are applied as data augmentation. For the generation of deteriorated images, bicubic down-sampling with anti-aliasing is used. To retrieve the face verification performance for different image resolutions, we deteriorate the second (according to the protocol) image of each pair to the particular resolution in the evaluation protocol (cf. Subsec. 3.3).</p><p>We assess the robustness of the face recognition systems to image resolution in terms of their face verification accuracy. We employ the cosine distance as our distance metric for all evaluations and determine the absolute accuracy with 10-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Improvements on SOTA Methods</head><p>We apply our fine-tuning strategy to several state-of-the-art face recognition models. For evaluation purposes, XQLFW <ref type="bibr" target="#b56">[56]</ref> fits the purpose of our investigation perfectly since the pairs in the evaluation protocol show a large difference in resolution. In addition, we synthetically deteriorate images of several other datasets to analyze the robustness of our approach to specific image resolutions r. <ref type="table" target="#tab_2">Table 1</ref> summarizes the results and highlights the tremendous robustness increase originating from the octuplet loss L oct .</p><p>Without L oct , the models (BT-M, ST-M1, and ST-M2) <ref type="bibr" target="#b6">[7]</ref> are already trained to be resolution invariant and perform best on XQLFW <ref type="bibr" target="#b56">[56]</ref> and very low-resolution images (7 px) of the other datasets. All remaining models are very susceptible to image resolution and show a decrease in accuracy for low-resolution images. However, although the FaceTransformer <ref type="bibr" target="#b10">[11]</ref> network tends to be more robust than structures solely based on CNNs, its performance is still worse for very low resolution images. This is in line with the findings of <ref type="bibr" target="#b56">[56]</ref> and renders a reliable real-world application impossible.</p><p>After fine-tuning with our proposed octuplet loss L oct , all models perform significantly better on images with low resolutions while maintaining their performance on high-resolution images.</p><p>Only a few minor deteriorations can be observed for the FaceTransformer <ref type="bibr" target="#b10">[11]</ref> and iResNet <ref type="bibr" target="#b9">[10]</ref> architecture, which we investigate in Subsec. 4.4. The most considerable improvement holds for the ResNet50 <ref type="bibr" target="#b7">[8]</ref> architecture pre-trained with the ArcFace <ref type="bibr" target="#b0">[1]</ref> method. We boost the accuracy from 74.22% to 93.27% on the most realistic cross-resolution dataset XQLFW <ref type="bibr" target="#b56">[56]</ref> while even slightly surpassing the baseline accuracy on LFW <ref type="bibr" target="#b2">[3]</ref> with 99.55%. Our method further improves the face verification accuracy for BT-M <ref type="bibr" target="#b6">[7]</ref>, ST-M1 <ref type="bibr" target="#b6">[7]</ref>, and ST-M2 [7] on high-resolution images, i.e., it recovers the prior drop in accuracy reported in <ref type="bibr" target="#b6">[7]</ref>. This behavior shows that our method better exploits the available network capabilities and makes them more robust. With the exception of the 7 px resolution, the best overall performance after fine-tuning with L oct is accomplished by the FaceTransformer network. The vast increase in accuracy, which is observed on four different architectures and four unique pre-training loss functions, demonstrates that our approach is universally applicable and works on various network architectures.</p><p>Finally, we measure the face verification accuracy with pairs of images with the same image resolution. Our experimental results in <ref type="table" target="#tab_3">Table 2</ref> indicate that our baseline model is slightly worse in same-resolution face verification than in the cross-resolution scenario (cf. <ref type="table" target="#tab_2">Table 1</ref>). This discrepancy is understandable due to the reduced information content of both low-resolution images. However, our approach substantially increases the performance from 77.57% to 89.74% on average across all image resolutions. These outcomes show that our technique is not limited to cross-resolution scenarios and can also be applied in same-resolution scenarios.</p><p>In conclusion, these improvements testify to a further contribution toward universal, resolution-independent face recognition systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with other SOTA Approaches</head><p>After demonstrating that our octuplet loss L oct improves the robustness of various face recognition models in crossresolution scenarios, we compare L oct with state-of-the-art cross-resolution methods. For this purpose, we evaluated our two best-performing approaches (FaceTransformer <ref type="bibr" target="#b10">[11]</ref> and MagFace <ref type="bibr" target="#b1">[2]</ref> with L oct ) on LFW <ref type="bibr" target="#b2">[3]</ref> with particular resolutions to match the evaluation conditions of other approaches and enable a direct comparison. The results are reported in <ref type="table" target="#tab_4">Table 3</ref> and show that our approaches outperform all other methods except for r = 8 px image resolution, where Lai and Lam <ref type="bibr" target="#b45">[45]</ref> achieved a higher accuracy. However, a notable drawback of their approach is the weak performance for high-resolution images. We must interpret the results of Ge et al. <ref type="bibr" target="#b41">[41]</ref> carefully as their approach is based on a teacher model that performs worse on high-resolution images (only 97.15%) and they report numbers of specific models for each image resolution. Moreover, the training resolution is inconsistent across the compared methods and can lead to slight deviations. Concluding, these results provide a reasonable classification of our approach as the state-of-the-art and underline its advantages. In addition, we compare our octuplet loss L oct with the approach of Terh?rst et al. <ref type="bibr" target="#b49">[49]</ref>. While they perform worse on XQLFW (83.95%), they report a slightly higher accuracy on LFW and much better results on AgeDB and CFP-FP. However, they aim at general quality-robust face recognition encompassing resolution, age, and pose. In contrast, we focus exclusively on the images' resolution; hence, this is not a fair comparison and should be considered with caution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis and Characteristics</head><p>For the analysis, we are using a re-implementation of the popular ArcFace <ref type="bibr" target="#b0">[1]</ref> approach, pre-trained on MS1M-V2 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">55]</ref>. It consists of a ResNet50 <ref type="bibr" target="#b7">[8]</ref> backbone network followed by a 512-dimensional fully connected layer, which acts as a bottleneck layer during pre-training and provides the facial features f for the octuplet loss L oct . We denote this in the following as our baseline network. We provide the receiver operating characteristics curve in <ref type="figure" target="#fig_1">Fig. 2</ref> to obtain deeper insights into the fine-tuning effect on our baseline network on XQLFW <ref type="bibr" target="#b56">[56]</ref> and the LFW <ref type="bibr" target="#b2">[3]</ref> database at different scales. Primarily at very low false acceptance rates (FAR), the performance gain after the fine-tuning is tremendous. While the baseline model fails for challenging situations, the fine-tuned version achieves superior results. On the XQLFW dataset, our approach increases the true acceptance rate (TAR) for very low FARs from 0% to over 65%. This improvement is similar to the behavior on the LFW dataset at 7 px. The effect vanishes the higher the resolution until, at 112 px, the rates remain nearly equal.</p><p>Overall, this improvement shows the benefit of our method, especially in security applications, e.g., manhunts via surveillance cameras.</p><p>Moreover, we investigate the deviation in the accuracy change between several datasets. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we fanned out the increase for several datasets and different image resolutions. We observe a significant performance reduction of the baseline model on challenging datasets that focus on age, pose, person similarity, or low image quality. This indicates that the image resolution is even more critical in combination with other adverse conditions. Our proposed octuplet loss fine-tuning strategy accomplishes the best accuracy for LFW <ref type="bibr" target="#b2">[3]</ref> and CFP-FF <ref type="bibr" target="#b61">[61]</ref> with over 90% at all resolutions, which are the easiest benchmarks. In contrast, CPLFW <ref type="bibr" target="#b58">[58]</ref> seems to be the most challenging dataset, with a performance below 90% at all scales. The chart also reveals that for large pose variations datasets such as CPLFW <ref type="bibr" target="#b58">[58]</ref> and CFP-FP <ref type="bibr" target="#b61">[61]</ref>, there is still a moderate increase of accuracy at 28 px image resolution, whereas the boost at that scale is marginal for all other datasets. Only on data with a large age gap (AgeDB <ref type="bibr" target="#b60">[60]</ref> and CALFW <ref type="bibr" target="#b57">[57]</ref>) and similar faces (SLLFW <ref type="bibr" target="#b59">[59]</ref>), our approach marginally reduces the accuracy on 56 px and 112 px image resolution. In conclusion, this analysis uncovers that our proposed approach is not limited to relatively simple datasets like LFW <ref type="bibr" target="#b2">[3]</ref> but also offers immense benefits to more challenging datasets, which involve, e.g., large age gaps or head pose variances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>We conduct multiple ablation studies to understand the influence of our loss terms, distance metric, feature normalization, margins, and batch size. Firstly, we study each single triplet loss term's contribution and then investigate the distance metric's influence, followed by clarifying the impact of normalizing features. Since the margin is crucial in our proposed octuplet loss, we empirically search for the optimal value for fine-tuning the baseline network. This study, combined with the effect of the batch size, is finally presented in this section. As in Subsec. 4.4, we use the re-implementation of ArcFace <ref type="bibr" target="#b0">[1]</ref> with a ResNet50 <ref type="bibr" target="#b7">[8]</ref> as the pre-trained network for all ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Loss Terms</head><p>Our proposed octuplet loss consists of four different triplet loss functions (cf. Equation <ref type="formula" target="#formula_0">(11)</ref>), and each term affects the overall performance. Hence, we conduct experiments to obtain the contribution of each term. In this study, we use the Euclidean distance and no feature normalization. As depicted in the upper part of <ref type="table" target="#tab_5">Table 4</ref>, we start from the best mean accuracy across all datasets and image resolutions, which is obtained by including all triplet loss terms. As expected, utilizing only L tri (T hhh ) leads to the worst results on XQLFW <ref type="bibr" target="#b56">[56]</ref> and images with resolutions (7 -28 px), but interestingly, it does not improve accuracy on the high-resolution dataset LFW <ref type="bibr" target="#b2">[3]</ref>, whereas it does improve the accuracy on average across all datasets. We suspect that: 1) The performance is already saturating for LFW, and 2) there might be a few lower-quality images in the LFW dataset, although we expect them to be exclusively in high resolution.</p><p>However, this term is essential to constrain the network and not focus entirely on low resolution. In contrast, utilizing only L tri (T lll ) significantly improves the performance on low-resolution images. Nevertheless, it drastically reduces the verification accuracy on high-quality images and thus is not considered preferable. Considering only L tri (T hll ), L tri (T lhh ), or the inclusion of both terms, leads to a moderate increase of robustness to image resolution but also comes with the trade-off of reducing the accuracy on high-resolution images. A similar effect occurs for the combination of L tri (T hhh ) and L tri (T lll ). Interestingly, the L tri (T hhh ) + L tri (T hll ) or L tri (T hhh ) + L tri (T lhh ) configuration yields the best performance on intermediate image resolutions (28 px and 56 px). Furthermore, experiments with three L tri terms reveal that removing L tri (T hhh ) or L tri (T lll ) leads to a marginal decline in performance.</p><p>This breakdown of the individual loss terms shows that each term contributes to the overall performance. However, the benefit of including both L tri (T hhh ) and L tri (T lll ) instead of simply one of them is only minor since they both connect high-resolution images with low-resolution images (cf . <ref type="figure" target="#fig_0">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Distance Metric and Feature Normalization</head><p>As described in Subsec. 4.2, our proposed approach follows the work of Hermans et al. <ref type="bibr" target="#b53">[53]</ref> and uses the Euclidean distance metric without feature normalization. However, as proposed in other works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b66">66]</ref>, we experimented with the squared Euclidean distance. Additionally, we conduct experiments with the cosine distance metric. Since those configurations consequently affect the magnitude of the margin, we empirically determine the best margins for each configuration. <ref type="table" target="#tab_4">Table 3</ref> illustrates the face verification accuracies and points out that the Euclidean distance is best for low-resolution images. In contrast, for intermediate and high-resolution images (from 28 px up to 112 px), the Euclidean squared distance and feature normalization leads to the best results. The improvement for this configuration is also evident in the XQLFW <ref type="bibr" target="#b56">[56]</ref> dataset and might be preferred for real-world applications. Due to the utilization of the cosine distance in our evaluation protocols, one would expect that utilizing this metric in our octuplet loss fine-tuning strategy leads to the best results. However, this is not true, as seen in the bottom row of <ref type="table" target="#tab_4">Table 3</ref>. We can achieve similar performance on LFW <ref type="bibr" target="#b2">[3]</ref> and XQLFW, but for lower image resolutions, fine-tuning with cosine distance leads to a smaller improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Margin and Batch Size</head><p>To conclude our ablation studies, we report the face verification accuracy after fine-tuning our baseline network with different margins m and batch sizes B. Keeping in mind that our baseline network achieves 84.90% accuracy, in <ref type="figure">Fig. 4</ref>, it is obvious that the improvement is more prominent for a larger number of samples within each batch. This effect is unsurprising since a larger batch size increases the probability of the hard sample mining algorithm finding even more challenging samples. Batches containing less than 64 samples lead to even worse accuracy, and hence, they are not further investigated in this work. Due to hardware limitations, we were unable to conduct experiments with larger batch sizes. Nevertheless, we expect this trend to continue until our hard sample mining algorithm only selects outliers, i.e., incorrectly labeled images.  <ref type="figure">Figure 4</ref>: Cross-resolution face verification accuracy [%] with our proposed octuplet loss L oct comparison of different values for margin m and batch-size, evaluated on our validation split of MS1M-V2. Note that we report mean accuracy for images resolutions r ? {7, 14, 28, 56, 112}.</p><p>In addition, we evaluated the face verification accuracy for margin values between 1 and 500. A value of m = 25 leads to peak performance. This maximum is also consistent for different batch sizes and indicates that the margin is independent of the batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work conducts further research on low-/cross-resolution face recognition and proposes a novel fine-tuning strategy with an octuplet loss function for existing models to boost their robustness against varying image resolutions. Our contribution involves a combination of four triplet loss terms applied simultaneously to high-and low-resolution images. This interaction exploits not only the relationship between different resolutions of the same image but also between different images of the same identity. The most significant advantage compared to other approaches is that this method can be built on top of existing approaches instead of a costly re-training.</p><p>We demonstrated the effectiveness of our fine-tuning strategy with several state-of-the-art face recognition approaches and observed a vast increase of robustness against image resolution without any significant trade-off on high-resolution images. Our approach performs best on the recently published cross-quality labeled faces in the wild dataset achieving 95.12% accuracy. Additionally, we exhaustively analyzed the improvements on several popular datasets and concluded that our method is universally applicable. Moreover, our ablation study revealed that all four triplet loss terms are needed to perform superior. We discovered that the distance metric and feature normalization plays a less important role as long as the margin for the triplet loss terms is chosen correctly.</p><p>Our future work will focus on reducing the amount of data needed for the octuplet loss fine-tuning strategy to further reduce the training time. In other words, we want to follow up on the hard sample mining strategy. An intelligent distillation process of the training set, i.e., keeping only the most relevant images, could potentially achieve even faster convergence. In masked face recognition, we witness many analogies to cross-resolution face recognition, so it would be interesting to explore if our octuplet loss concept could be beneficial there.</p><p>We believe that our contribution can help the community build more robust face recognition systems in the future. Code and details are released under the MIT license.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed octuplet loss exploits the relation between four high-resolution images (upper left) and four low-resolution images (lower right) incorporating four triplet loss (L tri ) terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Cross-resolution receiver operating characteristics (ROC) curve comparison of the baseline model (dashed) with our proposed octuplet loss L oct fine-tuning (solid) on XQLFW, and LFW for selected image resolutions. The equal error rate (EER) is indicated by a dotted line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Cross-resolution face verification accuracy comparison of the baseline model and our proposed octuplet loss L oct fine-tuning on several datasets (see Subsec. 4.1) for different image resolutions. An improvement is highlighted with darker colors, whereas a deterioration is indicated with lighter colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Improvement of cross-resolution face verification accuracy [%] with our proposed octuplet loss L oct , evaluated on several datasets (see Subsec. 4.1) for different image resolutions. L oct fine-tuning 99.55 (+0.05) 83.07 (+32.25) 90.72 (+20.89) 93.65 (+2.16) 94.46 (-0.21) 94.65 (-0.36) 91.31 (+10.95) 93.27 (+19.05) +0.08) 84.54 (+14.61) 91.59 (+7.13) 93.91 (+1.22) 94.47 (+0.58) 94.56 (+0.73) 91.81 (+4.85) 94.20 (+10.60) +1.60) 84.17 (+9.24) 90.34 (+5.88) 92.21 (+5.11) 92.86 (+5.02) 92.74 (+4.77) 90.46 (+6.00) 93.47 (+2.50) +2.93) 81.72 (+9.28) 88.41 (+6.01) 90.55 (+6.66) 90.80 (+6.75) 90.71 (+6.89) 88.44 (+7.12) 92.93 (+2.11) 79.41 (+25.03) 87.03 (+16.85) 90.30 (+2.73) 91.44 (+0.25) 91.35 (-0.20) 87.91 (+8.94) 91.70 (+18.97)</figDesc><table><row><cell></cell><cell></cell><cell cols="6">?(LFW, CALFW, CPLFW, SLLFW, CFP-FF, CFP-FP, AgeDB)</cell><cell></cell></row><row><cell>Model</cell><cell>LFW</cell><cell>7 px</cell><cell>14 px</cell><cell>28 px</cell><cell>56 px</cell><cell>112 px</cell><cell>mean</cell><cell>XQLFW</cell></row><row><cell>ResNet50 (ArcFace [1])</cell><cell>99.50</cell><cell>50.82</cell><cell>69.83</cell><cell>91.49</cell><cell>94.67</cell><cell>95.01</cell><cell>80.36</cell><cell>74.22</cell></row><row><cell>+ ResNet50 (BT-M [7])</cell><cell>99.30</cell><cell>69.93</cell><cell>84.46</cell><cell>92.69</cell><cell>93.89</cell><cell>93.83</cell><cell>86.96</cell><cell>83.60</cell></row><row><cell cols="2">+ L oct fine-tuning 99.38 (ResNet50 (ST-M1 [7]) 97.30</cell><cell>74.93</cell><cell>84.46</cell><cell>87.10</cell><cell>87.84</cell><cell>87.97</cell><cell>84.46</cell><cell>90.97</cell></row><row><cell cols="2">+ L oct fine-tuning 98.90 (ResNet50 (ST-M2 [7]) 95.87</cell><cell>72.44</cell><cell>82.40</cell><cell>83.89</cell><cell>84.05</cell><cell>83.82</cell><cell>81.32</cell><cell>90.82</cell></row><row><cell cols="2">+ L oct fine-tuning 98.80 (MobileNetV2 (ArcFace [1]) 98.85</cell><cell>54.38</cell><cell>70.18</cell><cell>87.57</cell><cell>91.19</cell><cell>91.55</cell><cell>78.97</cell><cell>72.73</cell></row><row><cell cols="2">+ L oct fine-tuning 98.78 (-0.07) FaceTransformer [11] 99.70</cell><cell>60.53</cell><cell>84.82</cell><cell>96.03</cell><cell>97.21</cell><cell>97.28</cell><cell>87.17</cell><cell>87.88</cell></row><row><cell>+ L oct fine-tuning</cell><cell cols="8">99.73 (+0.03) 82.96 (+22.43) 91.72 (+6.90) 95.13 (-0.90) 96.35 (-0.86) 96.52 (-0.76) 92.54 (+5.37) 95.12 (+7.24)</cell></row><row><cell>iResNet50 (MagFace[2])</cell><cell>99.63</cell><cell>52.82</cell><cell>73.71</cell><cell>94.32</cell><cell>96.71</cell><cell>96.87</cell><cell>82.89</cell><cell>76.95</cell></row><row><cell>+ L oct fine-tuning</cell><cell>99.63 (0)</cell><cell>81.69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(+28.87) 90.22 (+16.51) 93.84 (-0.48) 94.61 (-2.10) 94.72 (-2.15) 91.01 (+8.12) 92.92 (+15.97)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Improvement of same-resolution face verification accuracy [%] with our proposed octuplet loss L oct . Values are averaged across several datasets (see Subsec. 4.1) for each image resolution.</figDesc><table><row><cell>AgeDB)</cell></row></table><note>Model ?(LFW, CALFW, CPLFW, SLLFW, CFP-FF, CFP-FP,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Cross-resolution face verification accuracy [%], evaluated on LFW for different image resolutions. The best accuracy per resolution is marked in bold.</figDesc><table><row><cell>Model</cell><cell>8 px</cell><cell cols="4">12 px 16 px 32 px high resolution</cell></row><row><cell>Lai and Lam [45]</cell><cell>94.8</cell><cell>97.6</cell><cell>98.2</cell><cell>?</cell><cell>99.1 (128 px)</cell></row><row><cell>Sun et al. [42]</cell><cell>90.0</cell><cell>94.9</cell><cell>97.2</cell><cell>?</cell><cell>99.1 (112?96 px)</cell></row><row><cell>DCR [39]</cell><cell>93.6</cell><cell>95.3</cell><cell>96.6</cell><cell>?</cell><cell>98.7 (112?96 px)</cell></row><row><cell>TCN [46]</cell><cell>90.5</cell><cell>94.7</cell><cell>97.2</cell><cell>?</cell><cell>98.8 (112?96 px)</cell></row><row><cell>Ge et al. [41]</cell><cell>?</cell><cell>?</cell><cell cols="2">85.87 89.72</cell><cell>97.15 (224 px)</cell></row><row><cell>ResNet50 (ArcFace [1])</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ L oct fine-tuning</cell><cell cols="4">90.38 96.88 98.28 99.48</cell><cell>99.55 (112 px)</cell></row><row><cell>FaceTransformer [11]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ L oct fine-tuning</cell><cell cols="4">94.02 98.17 99.08 99.57</cell><cell>99.63 (112 px)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Two ablation studies: Influence on the cross-resolution face verification accuracy [%] from each triplet loss term (upper part) and the influence of the distance metric and normalization of features (lower part) using our proposed octuplet loss L oct fine-tuning evaluated on several datasets (see Subsec. 4.1) and different image resolutions. The best performance within each study is marked in bold.<ref type="bibr" target="#b61">61</ref>.17(-21.90)  78.77(-11.95) 92.21(-1.44)  94.77 (+0.31) 95.11 (+0.46) 84.40 (-6.91) 80.98(-12.29)   99.52 (-0.03) 79.93(-3.14)  89.39(-1.33) 93.57 (-0.08) 94.57 (+0.11) 94.64 (-0.01) 90.42 (-0.89) 92.15 (-1.12) 99.48 (-0.07) 80.10 (-2.97) 89.48 (-1.24) 93.53 (-0.12) 94.56 (+0.10) 94.62 (-0.03) 90.46 (-0.85) 92.32 (-0.95) 98.95 (-0.60) 82.49 (-0.58) 88.80 (-1.92) 91.40 (-2.25) 92.21 (-2.25) 92.06 (-2.59) 89.39 (-1.92) 93.20 (-0.07) 99.57 (+0.02) 79.60 (-3.47) 89.50 (-1.22) 93.93 (+0.28) 95.19 (+0.73) 95.18 (+0.53) 90.68 (-0.63) 91.93 (-1.34) 99.58 (+0.03) 79.42 (-3.65) 89.75 (-0.97) 93.97 (+0.32) 95.11 (+0.65) 95.14 (+0.49) 90.68 (-0.63) 92.18 (-1.09) 99.57 (+0.02) 81.66 (-1.41) 90.05 (-0.67) 93.66 (+0.01) 94.65 (+0.19) 94.84 (+0.19) 90.97 (-0.34) 92.77 (-0.50) 99.45 (-0.10) 80.51 (-2.56) 89.90 (-0.82) 93.52 (-0.13) 94.48 (+0.02) 94.54 (-0.11) 90.59 (-0.72) 92.55 (-0.72) 99.58 (+0.03) 80.60 (-2.47) 90.11 (-0.61) 93.94 (+0.29) 94.97 (+0.51) 94.94 (+0.29) 90.91 (-0.40) 92.55 +0.05) 94.52 (+0.06) 94.60 (-0.05) 91.29 (-0.02) 93.30 (+0.03) 99.37 (-0.18) 82.49 (-0.58) 90.35 (-0.37) 93.58 (-0.07) 94.47 (+0.01) 94.56 (-0.09) 91.09 (-0.22) 93.48 (+0.21) 99.20 (-0.35) 83.09 (+0.02) 90.62 (-0.10) 93.18 (-0.47) 93.93 (-0.53) 93.95 (-0.70) 90.96 (-0.35) 93.42 (+0.15) +0.05) 79.53 (-3.54) 88.64 (-2.08) 93.59 (-0.06) 94.80 (+0.34) 94.95 (+0.30) 90.30 (-1.01) 93.27 (0) 99.63 (+0.08) 80.08 (-2.99) 89.63 (-1.09) 93.83 (+0.18) 95.11 (+0.65) 95.24 (+0.59) 90.78 (-0.53) 93.58 (+0.31) 99.53 (-0.02) 81.36 (-1.71) 89.75 (-0.97) 93.55 (-0.10) 94.61 (+0.15) 94.71 (+0.06) 90.80 (-0.51) 93.23 (-0.04) 99.58 (+0.03) 79.57 (-3.50) 88.85 (-1.87) 93.70 (+0.05) 94.92 (+0.46) 95.03 (+0.38) 90.41 (-0.90) 93.27 (0)</figDesc><table><row><cell>L tri (?)</cell><cell></cell><cell></cell><cell cols="6">?(LFW, CALFW, CPLFW, SLLFW, CFP-FF, CFP-FP, AgeDB)</cell><cell></cell></row><row><cell>T hhh T hll T lhh</cell><cell>T lll</cell><cell>LFW</cell><cell>7 px</cell><cell>14 px</cell><cell>28 px</cell><cell>56 px</cell><cell>112 px</cell><cell>mean</cell><cell>XQLFW</cell></row><row><cell></cell><cell></cell><cell>99.55</cell><cell>83.07</cell><cell>90.72</cell><cell>93.65</cell><cell>94.46</cell><cell>94.65</cell><cell>91.31</cell><cell>93.27</cell></row><row><cell></cell><cell></cell><cell cols="8">99.42 (-0.13) (-0.72)</cell></row><row><cell cols="6">99.50 (-0.05) 82.92 (-0.15) 93.70 (d euc d euc 2 d cos 90.72 (0) f 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>99.55</cell><cell>83.07</cell><cell>90.72</cell><cell>93.65</cell><cell>94.46</cell><cell>94.65</cell><cell>91.31</cell><cell>93.27</cell></row><row><cell></cell><cell></cell><cell>99.60 (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Magface: A universal representation for face recognition and quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhida</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14225" to="14234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Faces in &apos;Real-Life&apos; Images: Detection, Alignment, and Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-b face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition in low quality images: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loreto</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domingo</forename><surname>Mery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Image resolution susceptibility of face recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Knoche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>H?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03769</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved residual networks for image and video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ionut Cosmin Duta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9415" to="9422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Face transformer for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14803</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning-based face super-resolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Component attention guided face super-resolution network: Cagface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratheesh</forename><surname>Kalarot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="370" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heatmap-aware pyramid face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Organ-branched cnn for robust face super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahetiyaer</forename><surname>Bare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shili</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dclnet: Dual closed-loop networks for face super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengdong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianning</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page">106987</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face super-resolution network with incremental enhancement of facial parsing information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7537" to="7543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attribute-guided face generation using conditional cyclegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="282" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Super-resolving very low-resolution face images with supplementary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="908" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic face hallucination: Super-resolving very low-resolution face images with supplementary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2926" to="2943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facial attribute capsules for noise face super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinrui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12476" to="12483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super-identity convolutional neural network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="183" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fh-gan: Face hallucination and recognition using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bayram</forename><surname>Bayramli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usman</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Te</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wavelet domain generative adversarial network for multi-scale face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="763" to="784" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Low-resolution face recognition based on identity-preserved face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun-Cheung</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Hang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1173" to="1177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimizing super resolution for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusto</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Abello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 32nd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face hallucination using cascaded super-resolution and identity priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klemen</forename><surname>Grm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitomir</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>?truc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2150" to="2165" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face super-resolution through dual-identity constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanduo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Edge and identity preserving network for face super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyong</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheolkon</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joongkyu</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">446</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Verification of very low-resolution faces using an identity-preserving deep face super-resolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esra</forename><surname>Ataer-Cansizoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Sullivan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10974</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning face image super-resolution through facial semantic attribute transformation and self-attentive structure enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="468" to="483" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Identity-preserving face hallucination via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4796" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sigan: Siamese generative adversarial network for identity-preserving face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weng-Tai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6225" to="6236" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Identity-aware deep face hallucination via adversarial face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariborz</forename><surname>Hadi Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Suprear-net: Supervised resolution enhancement and recognition network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biometrics, Behavior, and Identity Science</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Low resolution face recognition using a two-branch deep convolutional neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erfan</forename><surname>Zangeneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalda</forename><surname>Mohsenzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">112854</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-resolution learning for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Valerio Massoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">103927</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep coupled resnet for low-resolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="526" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attribute-guided coupled gan for crossresolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veeru</forename><surname>Talreja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariborz</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Low-resolution face recognition in the wild via selective knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengwei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2051" to="2062" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Classifier shared deep network with multi-hierarchy loss for low resolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingna</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingmin</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">115766</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards resolution invariant face recognition in uncontrolled scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics (ICB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Genlr-net: Deep framework for very low resolution face and object recognition with generalization to unseen categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sivaram Prasad Mudunuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soma</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. IEEE</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="602" to="60209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep siamese network for low-resolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Cheung</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin-Man</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1444" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tcn: Transferable coupled network for cross-resolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3302" to="3306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiscale parallel deep cnn (mpdcnn) architecture for the real low-resolution face recognition for surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mainak</forename><surname>Nayaneesh Kumar Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish Kumar</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">104290</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep rival penalized competitive learning for low-resolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Qmagface: Simple and accurate quality-aware face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Terh?rst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Ihlefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13475</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Homogeneous low-resolution face recognition method based correlation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13175</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Triplet distillation for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoji</forename><forename type="middle">Roland</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="808" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep metric learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmut</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasan?akir</forename><surname>Bilge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1066</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Triplet permutation method for deep learning of single-shot person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?a Jos? G?mez-</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Armingol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo</forename><surname>De La Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Imaging for Crime Detection and Prevention (ICDP)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cross-quality LFW: A database for analyzing cross-resolution image face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Knoche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>H?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Automatic Face and Gesture Recognition (FG). 2021</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Cross-age LFW: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fine-grained face verification: Fglfw database, baselines, and human-dcmn partnership</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="63" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Agedb: The first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations (ICLR). 2019, OpenReview.net</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Self-restrained triplet loss for accurate masked face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadi</forename><surname>Boutros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">108473</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
