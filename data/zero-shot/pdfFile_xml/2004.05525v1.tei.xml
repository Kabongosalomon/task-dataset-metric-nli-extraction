<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BUILDING DISASTER DAMAGE ASSESSMENT IN SATELLITE IMAGERY WITH MULTI-TEMPORAL FU- SION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Weber</surname></persName>
							<email>ejweber@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institue of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Kan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">WL Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BUILDING DISASTER DAMAGE ASSESSMENT IN SATELLITE IMAGERY WITH MULTI-TEMPORAL FU- SION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted for presentation at the ICLR 2020 AI For Earth Sciences Workshop</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic change detection and disaster damage assessment are currently procedures requiring a huge amount of labor and manual work by satellite imagery analysts. In the occurrences of natural disasters, timely change detection can save lives. In this work, we report findings on problem framing, data processing and training procedures which are specifically helpful for the task of building damage assessment using the newly released xBD dataset. Our insights lead to substantial improvement over the xBD baseline models, and we score among top results on the xView2 challenge leaderboard. We release our code used for the competition 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>When a natural disaster occurs, quick and accurate information is critical to an effective response. To better deploy resources in affected areas, it is important for emergency responders to know the location and severity of the damages.</p><p>Satellite imagery offers a powerful source of information and can be used to assess the extent and areas of damages. However, a common bottleneck in the current workflows involves the time it takes for human analysts to observe an affected area and identify damaged zones. This process can take hours in a situation where time is of the essence. It therefore presents room to be accelerated through leveraging artificial intelligence.</p><p>In the last several years, Convolutional Neural Networks <ref type="bibr" target="#b10">(Krizhevsky et al., 2012)</ref> have achieved human level performance on a variety of computer vision tasks, including object recognition and image segmentation <ref type="bibr" target="#b12">(LeCun et al., 2015)</ref>. These techniques are highly relevant and applicable in the case of satellite image analysis for disaster damage assessment <ref type="bibr" target="#b9">(Ji et al. (2018);</ref><ref type="bibr" target="#b1">Cooner et al. (2016)</ref>).</p><p>The main contributions of this work is a characterization of the importance of mono-temporal vs. multi-temporal settings when performing damage assessment with natural disaster along with insights on helpful image pre-processing techniques. The specific insights in this work are that substantially better performance is obtained by independently feeding the pre and post-disaster images through a CNN with shared weights. The features are then fused before a semantic segmentation final layer. Furthermore, working on smaller image crops and weighting error on damage classes inversely proportional to their statistical occurrence in the training dataset leads to models strongly improving over baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we highlight related work in computer vision and large-scale datasets for disaster damage analysis. In particular, we focus on building disaster damage assessment.  For building damage assessment, we feed pre and post-disaster images through a ResNet50 backbone with shared weights. We then concatenate features before semantic segmentation to obtain our final building damage predictions. The output is a 5-class damage map ranging from no building (0) to destroyed building (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">COMPUTER VISION TECHNIQUES FOR BUILDING DAMAGE ASSESSMENT</head><p>Researchers have applied machine learning approaches to building damage assessment in satellite imagery. Xu et al. (2019) described a method to build convolutional neural networks that automatically detect damaged buildings in satellite images using pre and post images and showed that the model can generalize well to new regions and disasters if it is fine-tuned on a small set of examples from that region. Their work is done on a proprietary dataset spanning three disasters and is framed as a binary pixel classification over damaged/not damaged buildings. <ref type="bibr" target="#b1">Cooner et al. (2016)</ref> compared the performance of multiple machine learning methods in building damage detection with both pre and post-event satellite imagery of the 2010 Haiti earthquake, and found that a feed-forward neural network achieved the lowest error rate of 40%. <ref type="bibr" target="#b9">Ji et al. (2018)</ref> developed a convolutional network to identify collapsed buildings from post-event satellite imagery of the Haiti earthquake, and reached an overall accuracy of 78.6%. <ref type="bibr" target="#b3">Duarte et al. (2018)</ref> combine drone and satellite images of disasters to improve the accuracy of their convolutional networks, with a best reported accuracy of 94.4%. In these settings damaged building detection over images is framed as a binary pixel classification problem. <ref type="bibr" target="#b19">Yang et al. (2018)</ref> show that convolutional neural network (CNN) can perform feature-based multitemporal remote sensing image registration and can outperform four state-of-the-art methods in most scenarios. <ref type="bibr" target="#b14">Nia &amp; Mori (2017)</ref> show that convolutional neural networks can perform damage assessment on post disaster image. However, their dataset uses ground-level images instead of satellite images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DATASETS FOR DAMAGE ASSESSMENT</head><p>Until xBD <ref type="bibr" target="#b6">(Gupta et al., 2019)</ref>, adequate satellite imagery addressing building damage was not generally available. Other satellite imagery datasets were often limited to single disaster types and did not have common criteria for assessing damage assessment <ref type="bibr" target="#b5">(Fujita et al. (2017)</ref>; <ref type="bibr" target="#b0">Chen et al. (2018);</ref><ref type="bibr" target="#b4">Foulser-Piggott et al. (2012)</ref>). This made comparing model and agreeing on a problem framing difficult.</p><p>The xview dataset <ref type="bibr" target="#b11">(Lam et al., 2018)</ref> is a precursor to the xBD dataset. It is one of the largest and most diverse publicly available object detection datasets, with over 1 million objects across 60 classes in over 1,400 km 2 of imagery. It focuses on object detection and not on building damage assessment.</p><p>The complete xBD dataset contains satellite images from 19 different natural disasters across 22,068 images and contains 850,736 building polygons. Each image has a 1024 by 1024 pixels resolution.</p><p>Images are collected from WorldView-3 satellites at 0.3m spatial resolution. The imagery covers a total of 45,361.79 km 2 . It also introduces a four-level damage annotation scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head><p>In this section, we explain the task of building damage assessment and data pre-processing. Given the nature of the xBD dataset, with pre-disaster and post-disaster images, we exploit the multitemporal information to predict both building locations and damage level with the same network. For localization the task is to predict 0 or 1 (background or building) and for damage assessment the task is to predict a 5 dimensional output: 0 is no building and 1-4 is the damage level (1: undamaged, 2: minor, 3: major: 4: destroyed). An alternative approach, which we found to be the best, is focus on only damage assessment but say damage levels 1-4 are "buildings". This avoids the problem of using completely separate networks, as done in the xBD baselines <ref type="bibr" target="#b6">(Gupta et al., 2019)</ref>. We find this very important to our final results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BASELINE ARCHITECTURE</head><p>The first decision in this work is related to problem framing. For this work, we are focused on building damage assessment, which means we care about both (i) building localization and (ii) building per-pixel damage classification. Because we have two tasks (i.e. identifying the building and determining the damage level), we decided to use the same model architecture for both tasks. However, the baseline model uses two separate networks.</p><p>The task and baseline models are described in the xBD paper <ref type="bibr" target="#b6">(Gupta et al., 2019)</ref>. For localization, a U-Net <ref type="bibr" target="#b16">(Ronneberger et al., 2015)</ref> architecture is used for binary pixel classification of "background" or "building". For damage assessment, the model is less straightforward. It uses post-damage images fed into a ResNet-50 backbone <ref type="bibr" target="#b7">(He et al., 2016)</ref> pre-trained on ImageNet <ref type="bibr" target="#b2">(Deng et al., 2009)</ref> and additional features from a shallow CNN. All convolutional layers use a ReLU activation. The output is a one-hot encoded vector where each element represents the probability of an ordinal class. The model uses an ordinal cross-entropy loss function. Unlike traditional cross-entropy, ordinal cross-entropy penalizes relative to the distance between true and predicted ordinal class. Since the difference between any two classes is not interchangeable, this loss function allows the model to distinguish between the different levels of damage.</p><p>Extending beyond this baseline, we decided that using the same network for both building detection and damage assessment was a more natural formulation of the problem. This way the model can jointly reason about similar features. For this reason, we use a Mask R-CNN backbone  augmented with a Feature Pyramid Network module <ref type="bibr" target="#b13">(Lin et al., 2017</ref>) and a semantic segmentation head. The model was pretrained on the ImageNet dataset <ref type="bibr" target="#b2">(Deng et al., 2009</ref>). The implementation of this Mask R-CNN architecture was obtained using the Detectron2 <ref type="bibr" target="#b17">(Wu et al., 2019)</ref> library from Facebook AI Research which has PyTorch <ref type="bibr" target="#b15">(Paszke et al., 2019)</ref> implementations of other models as well. With this backbone, we tried both instance segmentation and semantic segmentation to generate the per-pixel classification output. Our final model uses semantic segmentation, as it's a more natural damage-assessment formulation without the notion of instances. Often time, the buildings are too small for instance segmentation to be appropriate.</p><p>For the loss function, we use cross-entropy loss on the predicted classes with the ground truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DATA PROCESSING TECHNIQUE</head><p>With the xBD dataset, we have both pre-disaster and post-disaster images available, which can be used for the two tasks of building localization and damage assessment. The images are 1024 by 1024, but we found that the buildings were often too small resolution for the model to accurately draw building boundaries. For this reason, we trained and ran models on 4 512 by 512 images forming the top-left, top-right, bottom-left and bottom-right quadrants.</p><p>Furthermore, given this pre and post data, damage assessment can be framed as either a monotemporal and multi-temporal task <ref type="bibr" target="#b18">(Xu et al., 2019)</ref>. In the mono-temporal setting, only the post images are fed to a model that has to predict damage level per pixel. In the multi-temporal setting, both the pre and post images are fed to the model that has to predict damage levels on the post images. In this case we fed both the pre and post images independently through the R-CNN base, with shared weights. These features were then concatenated before being fed through the semantic segmentation head of the networks. We show our best architecture in <ref type="figure" target="#fig_1">Figure 1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we explain the dataset and experiments used to inform the final model design. The metrics used are for building localization and damage assessment from the xView2 competition 2 ( <ref type="figure" target="#fig_1">(Gupta et al., 2019)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">XBD DATASET</head><p>This work is one of the first to apply recent deep learning techniques to the xBD dataset. xBD is the largest and first building damage assessment dataset released to date. It contains 1024 x 1024 satellite imagery with 30cm per pixel resolution covering a diverse set of disasters-including earthquakes, floods, volcanic eruption, wildfire and wind-and geographical locations (16) with over 850,000 building annotations (on a 1 to 4 damage scale) across over 45,000 km 2 of imagery before and after disasters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ABLATION STUDY</head><p>For the task of multi-class pixel classification, both instance segmentation and semantic segmentation are common solutions. The goal of instance segmentation is assign a class and instance ID to every pixel. Similarly, the goal of semantic segmentation is to assign a class to every pixel, but in this case the notion of instance ID is not used. The nature of damage assessment is more suited for semantic segmentation, where instances IDs are not relevant.</p><p>Given the success of instance segmentation networks such as Mask R-CNN , we perform experiments with both instance segmentation and semantic segmentation architectures. Furthermore, we experiment with using image crops, both pre and post images as the input, predicting localization and damage together, and weighting the class loss inversely proportional to the training set distribution.</p><p>Looking at <ref type="table" target="#tab_0">Table 1</ref>, we see the different experiments used to choose our best model. Notice that the F1 score is computed with the xBD test set withheld for the xView2 competition. Localization F1 score is for building detection (0-1) and damage F1 is for damage assessment (0-4). When using a joint prediction, we classify all pixels with damage level at least 1, to be a building. This is how we use the same network for both tasks. Overall F1 is a weighted combination of 30% localization F1 and 70% damage F1.</p><p>In the first row (row 1), instance segmentation is used on full 1024 x 1024 images. In the case of overlapping bounding boxes, we use the label with higher damage prediction. By switching to semantic segmentation (row 2), we notice a large improvement in localization (0.114). This is due to the building box predictions suffering on small building sizes. By using 4 512 x 512 crops per image (row 3), we obtain a boost in localization F1 of 0.016. By using our multi-temporal input with pre and post images and adding the joint prediction (row 4), we see an increase for all metrics. Lastly (row 5), we add class-specific cross-entropy weighting to obtain an increase in damage and overall F1, while only losing slight performance in localization F1. This is because we put more weighting on building damage classification. The reason for this weighting is that the 73.6% of polygons have no damage. Furthermore, most of the image is the "no building" class.</p><p>We don't show this in the table, but simple concatenation of pre and post images to produce a 6channel input produced very poor results. Similarly, subtracting the pre and post images before input did not work well either. Our best results were obtained when processing pre and post images individually, and then concatenating the features before segmentation. Instead of a single-network 256 channel feature embedding being used with the semantic segmentation head, we use 512 features from stacking the pre and post image features coming from the FPN before the semantic segmentation head, which predicts the final 5 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TRAINING</head><p>We performed most experiments on a machine with 4 NVIDIA 1080 TIs, and training takes roughly 6 hours to convergence when using ImageNet pretrained weights. When training for too long, the network will collapse into predicting all 0 (no building) labels. Our inversely proportional class weightings allows the network to train longer before collapsing, but it doesn't solve the issue entirely. Code will be released for replication of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>With our final model, we obtain a localization F1 of 0.835 and damage F1 of 0.697. These metrics are combined to obtain an overall F1 of 0.738 in the xView2 competition with the xBD test set.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we show the breakdown of the building damage assessment result compared to the xBD baseline. The metrics are computed on the holdout set, and we outperform baselines by a large margin. Notice that this is compared to the xBD baseline, but our results are competitive on the xView2 leaderboard, in which we were ranked place 2 in Track 3: "Evaluation Only", and 40th place before non-validated submissions were removed. Our code is available at https:// github.com/ethanweber/xview2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In conclusion, the four main insights behind our performance include working with image quadrants instead of the full image, using one architecture trained on both the pre and post images and fused before the final segmentation layer, using the Mask R-CNN with FPN architecture and engineering our loss function to weight errors on classes inversely proportional to their occurrence on the dataset. Future research directions include using an ordinal cross-entropy loss function to penalize errors in the damage scales differently and also experimenting with other ways to combine the information from the pre and post images at different stages of feature extraction. Incorporating the disaster type (e.g. flood, fire, etc.) into the building damage assessment prediction could also be an interesting direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>https://github.com/ethanweber/xview2 1 arXiv:2004.05525v1 [cs.CV] 12 Apr 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Our model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Our ablation study. In this table, we show the experiments that inform our final model. The best model uses semantic segmentation, both pre and post image as input, 512 x 512 image crops, joint building localization and damage prediction, and cross-entropy loss weightings to handle class-imbalance.</figDesc><table><row><cell cols="4">Architectures Two-image input (pre and post) 512 x 512 images Joint prediction Class-specific weighting Overall F1 Localization F1 Damage F1</cell></row><row><cell>Instance seg.</cell><cell>0.492</cell><cell>0.705</cell><cell>0.401</cell></row><row><cell>Semantic seg.</cell><cell>0.536</cell><cell>0.819</cell><cell>0.414</cell></row><row><cell>Semantic seg.</cell><cell></cell><cell>0.835</cell><cell></cell></row><row><cell>Semantic seg.</cell><cell>0.729</cell><cell>0.847</cell><cell>0.679</cell></row><row><cell>Semantic seg.</cell><cell>0.738</cell><cell>0.835</cell><cell>0.697</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with xBD baseline. On all metrics, our best model outperforms the xBD baseline model by a large margin.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://xview2.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Benchmark dataset for automatic damaged building detection from posthurricane remotely sensed imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">Andrew</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Escay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tessa</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Staneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjun</forename><surname>Choe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05581</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detection of urban damage using remote sensing and machine learning algorithms: Revisiting the 2010 haiti earthquake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cooner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James B</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">868</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Satellite image classification of building damages using airborne and satellite image samples in a deep learning approach. ISPRS Annals of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Nex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vosselman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The use of remote sensing for postearthquake damage assessment: lessons from recent events, and future prospects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Foulser-Piggott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifthteenth World Conference on Earthquake Engineering</title>
		<meeting>the Fifthteenth World Conference on Earthquake Engineering</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shuhei Hikosaka, and Ryosuke Nakamura. Damage detection from aerial images via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aito</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riho</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hosfelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Sajeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirav</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryce</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jigar</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howie</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gaston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09296</idno>
		<title level="m">A dataset for assessing building damage from satellite imagery</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying collapsed buildings using post-earthquake satellite imagery and convolutional neural networks: A case study of the 2010 haiti earthquake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanfa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Buchroithner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1689</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kuzma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dooley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Klaric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mccord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07856</idno>
		<title level="m">Objects in context in overhead imagery</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building damage assessment using deep learning and groundlevel image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nia</forename><surname>Karoon Rashedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation. CoRR, abs/1505.04597</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1505.04597" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Building damage detection in satellite imagery using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zebo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeriya</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaytseva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06444</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-temporal remote sensing image registration using deep convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="38544" to="38555" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
