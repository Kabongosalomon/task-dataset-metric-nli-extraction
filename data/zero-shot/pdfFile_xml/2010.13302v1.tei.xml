<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation in the Wild Image Initial Pose Ours (a) (b) (c) (d) (e) (f) Ref View 1 Ref View 2 Ref View 3</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
							<email>zhangzhecnjs@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Qiu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Qin</surname></persName>
							<email>qinwenhu@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
						</author>
						<title level="a" type="main">AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation in the Wild Image Initial Pose Ours (a) (b) (c) (d) (e) (f) Ref View 1 Ref View 2 Ref View 3</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) * Corresponding Author ? Zhe Zhang and Chunyu Wang have contributed equally. Work done when Zhe Zhang is an intern at Microsoft Research Asia</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Occlusion is probably the biggest challenge for human pose estimation in the wild. Typical solutions often rely on intrusive sensors such as IMUs to detect occluded joints. To make the task truly unconstrained, we present Ada-Fuse, an adaptive multiview fusion method, which can enhance the features in occluded views by leveraging those in visible views. The core of AdaFuse is to determine the point-point correspondence between two views which we solve effectively by exploring the sparsity of the heatmap representation. We also learn an adaptive fusion weight for each camera view to reflect its feature quality in order to reduce the chance that good features are undesirably corrupted by "bad" views. The fusion model is trained endto-end with the pose estimation network, and can be directly applied to new camera configurations without additional adaptation. We extensively evaluate the approach on three public datasets including Human3.6M, Total Capture and CMU Panoptic. It outperforms the state-of-the-arts on all of them. We also create a large scale synthetic dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Pose</head><p>Occlusion-Person, which allows us to perform numerical evaluation on the occluded joints, as it provides occlusion labels for every joint in the images. The dataset and code are released at https://github.com/zhezh/adafuse-3d-human-pose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurately estimating 3D human pose from multiple cameras has been a longstanding goal in computer vision <ref type="bibr" target="#b30">(Liu et al., 2011;</ref><ref type="bibr" target="#b3">Bo and Sminchisescu, 2010;</ref><ref type="bibr" target="#b14">Gall et al., 2010;</ref><ref type="bibr" target="#b44">Rhodin et al., 2018;</ref><ref type="bibr" target="#b0">Amin et al., 2013;</ref><ref type="bibr" target="#b5">Burenius et al., 2013;</ref><ref type="bibr" target="#b36">Pavlakos et al., 2017;</ref><ref type="bibr" target="#b2">Belagiannis et al., 2014)</ref>. The ultimate goal is to recover absolute 3D locations of the body joints in a world coordinate system from multiple cameras placed in natural environments. The task has attracted a lot of attention because it can benefit many applications such as augmented and virtual reality <ref type="bibr" target="#b48">(Starner et al., 2003)</ref>, humancomputer-interaction and intelligent player analysis in sport videos <ref type="bibr" target="#b4">(Bridgeman et al., 2019)</ref>. The task is often addressed by a simple two-step framework. In the first step, it tries to detect the 2D poses in all camera views, for example, by a convolutional neural network <ref type="bibr" target="#b6">(Cao et al., 2017;</ref>. Then in the second step, it recovers the 3D pose from the multiview 2D poses either by analytical methods <ref type="bibr" target="#b5">(Burenius et al., 2013;</ref><ref type="bibr" target="#b36">Pavlakos et al., 2017;</ref><ref type="bibr" target="#b2">Belagiannis et al., 2014;</ref><ref type="bibr" target="#b42">Qiu et al., 2019;</ref><ref type="bibr" target="#b0">Amin et al., 2013)</ref> or by discriminative models <ref type="bibr" target="#b22">(Iskakov et al., 2019;</ref><ref type="bibr" target="#b54">Tu et al., 2020)</ref>. The camera parameters are usually assumed known in these approaches. The development of powerful network architectures such as <ref type="bibr" target="#b35">(Newell et al., 2016)</ref> has notably improved the 2D pose estimation quality, which in turn reduces the 3D error remarkably. For example, in <ref type="bibr" target="#b42">(Qiu et al., 2019)</ref>, the 3D error on Human3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2014)</ref> decreases significantly from 52mm to 26mm.</p><p>However, obtaining small errors on benchmark datasets does not imply that the task has been truly solved unless the challenges such as background clutter, human appearance variation and occlusion encountered in real world applications are well addressed. In fact, a growing amount of efforts <ref type="bibr" target="#b9">Ci et al., 2019;</ref><ref type="bibr" target="#b46">Rogez and Schmid, 2016;</ref><ref type="bibr" target="#b37">Pavlakos et al., 2018;</ref><ref type="bibr" target="#b10">Ci et al., 2020)</ref> have been devoted to improving the pose estimation performance in challenging scenarios, for example, by augmenting the training dataset <ref type="bibr" target="#b55">Varol et al., 2017)</ref> with more images or by using more robust sensors such as IMUs . We will discuss about this type of work in more details in section 2.</p><p>In this work, we propose to solve the problem in a different way by multiview feature fusion. The approach is orthogonal to the previous efforts. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our approach can accurately detect the joints even when they are occluded in certain views. The motivation behind our approach is that a joint occluded in one view may be visible in other views. So it is generally helpful to fuse the features at the corresponding locations in different views. To that end, we present a flexible multiview fusion approach termed Ada-Fuse. <ref type="figure">Figure 2</ref> shows the pipeline. It first uses camera parameters to compute the point-line correspondence between a pair of views. Then it "finds" the matched point on the line by exploring the sparsity of the heatmap representation without performing the challenging point-point matching. Finally, the features of the matched points in different views are fused. The approach can effectively improve the feature quality in occluded views. In addition, for a new environment with different camera poses, we can directly use AdaFuse without re-training as long as the camera parameters are available. This improves the applicability of the approach in real applications.</p><p>The performance of AdaFuse is further boosted by learning an adaptive fusion weight for each view to reflect its feature quality. This weight is leveraged in fusion in order to reduce the impact of low-quality views. If a joint is occluded in one view, its features are also likely corrupted. In this case, we hope to give a small weight to this view when performing multiview fusion such that the high-quality features in the visible views are dominant, and are free from being corrupted by low-quality features. We add some simple layers to the pose estimation network to predict heatmap quality based on the heatmap distribution and cross view consistency. We observe in our experiments that the use of adaptive fusion notably improves the performance.</p><p>We evaluate our approach on three public datasets including Human3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2014)</ref>, Total Capture  and CMU Panoptic . It outperforms the state-of-the-arts demonstrating the effectiveness of our approach. In addition, we also compare it to a number of standard multiview fusion methods such as RANSAC in order to give more detailed insights. We evaluate the generalization capability of our approach by training and testing on different datasets. We also create a synthetic human pose dataset in which human are purposely occluded by objects. The dataset allows us to perform evaluation on the occluded joints.</p><p>The rest of the paper is organized as follows. In section 2, we discuss the related work on multiview 3D human pose estimation with special focus on the approaches that aim to improve the performance in challenging environments. Section 3 introduces the basics for multiview feature fusion to lay the groundwork for AdaFuse. Then we describe how we learn adaptive weight for each camera view to reflect the feature quality, as well as the details of AdaFuse. In sections 5 and 6, we introduce the experimental datasets and results, respectively. Section 7 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We first review the related work on multiview 3D human pose estimation in section 2.1. Then section 2.2 summarizes the techniques that are used to improve the in-the-wild performance. Finally, in section 2.3, we discuss the approaches on consensus learning such as RANSAC. This is necessary for multiple sensor fusion because the sensors could have contradictory predictions and the outliers should be removed to ensure the good fusion quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multiview 3D Human Pose Estimation</head><p>We briefly classify the multiview 3D human pose estimation methods into two classes. The first class is model-based approaches which are also known as analysis-by-synthesis approaches <ref type="bibr" target="#b30">(Liu et al., 2011;</ref><ref type="bibr" target="#b14">Gall et al., 2010;</ref><ref type="bibr" target="#b34">Moeslund et al., 2006;</ref><ref type="bibr" target="#b47">Sigal et al., 2010;</ref><ref type="bibr" target="#b40">Perez et al., 2004</ref>). They first model human body by simple primitives such as sticks and cylinders. Then the parameters of the model (i.e. poses) are continuously updated according to the observations in multiview images until the model can be explained by the image features. The resulted optimization problem is usually non-convex. So expensive sampling techniques are often used. The main difference among those approaches lies in the adopted image features and the optimization algorithms. We refer the interested readers to earlier survey papers such as <ref type="bibr" target="#b34">(Moeslund et al., 2006)</ref>. The advantage of the model-based approaches lies in its capability to handle occlusion because of the inherent structure prior embedded in human model. These approaches aggregate the local features as evidence to infer the global model parameters with the inherent human body structure as constraints. So if a joint is occluded, it can still rely on other joints to guess the possible locations that are consistent with the prior. However, the model-based approaches get larger 3D errors than the model-free approaches due to the difficult optimization problems.</p><p>The second class is model-free approaches <ref type="bibr" target="#b42">(Qiu et al., 2019;</ref><ref type="bibr" target="#b22">Iskakov et al., 2019;</ref><ref type="bibr" target="#b5">Burenius et al., 2013;</ref><ref type="bibr" target="#b36">Pavlakos et al., 2017;</ref><ref type="bibr" target="#b11">Dong et al., 2019;</ref><ref type="bibr" target="#b0">Amin et al., 2013;</ref><ref type="bibr" target="#b2">Belagiannis et al., 2014;</ref><ref type="bibr" target="#b58">Xie et al., 2020)</ref> which often follow a two-step framework. They first detect 2D poses in images of all camera views. Then with the aid of camera parameters, they recover the 3D pose using either triangulation <ref type="bibr" target="#b0">(Amin et al., 2013;</ref><ref type="bibr" target="#b22">Iskakov et al., 2019)</ref> or pictorial structure models <ref type="bibr" target="#b5">(Burenius et al., 2013;</ref><ref type="bibr" target="#b36">Pavlakos et al., 2017;</ref><ref type="bibr" target="#b11">Dong et al., 2019)</ref>. Recursive pictorial structure model is introduced in <ref type="bibr" target="#b42">(Qiu et al., 2019)</ref> to speed up the inference process. The authors in <ref type="bibr" target="#b22">(Iskakov et al., 2019)</ref> also propose to use learnable triangulation <ref type="bibr" target="#b17">(Hartley and Zisserman, 2003)</ref> for human pose estimation which is more robust to inaccurate 2D poses. If the 2D poses are accurate, the recovered 3D poses are guaranteed to be accurate without worrying about being trapped in local optimum as the model-based methods.</p><p>The development of more powerful network architectures <ref type="bibr" target="#b35">(Newell et al., 2016;</ref><ref type="bibr" target="#b49">Sun et al., 2019)</ref> has dramatically improved the 2D pose estimation accuracy on benchmark datasets, which in turn also decreases the 3D pose estimation error. For example, on the most popular benchmark Hu-man3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2014)</ref>, the 3D MPJPE error has decreased to about 20mm which can meet the requirements of many real-life applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Improving "In the Wild" Performance</head><p>Sensors Occlusion is probably the biggest challenge for inthe-wild scenarios. One straightforward solution is to use additional sensors such as IMUs  and radio signals , which are not impacted by occlusion. For example, <ref type="bibr" target="#b45">Roetenberg et al. (Roetenberg et al., 2009)</ref> place 17 IMUs at the rigid bones. If the measurements are accurate, the 3D pose is fully determined. In practice, however, the accuracy is limited by the drifting problem. To that end, some approaches <ref type="bibr" target="#b32">von Marcard et al., 2018;</ref><ref type="bibr" target="#b15">Gilbert et al., 2019;</ref><ref type="bibr" target="#b31">Malleson et al., 2017;</ref><ref type="bibr" target="#b60">Zhang et al., 2020)</ref> propose to fuse images and IMUs to achieve more robust pose estimation. Some works <ref type="bibr" target="#b28">Li et al., 2019;</ref><ref type="bibr" target="#b61">Zhao et al., 2018)</ref> leverage the fact that wireless signals in the WiFi frequencies traverse walls and reflect off the human body, and propose a radiobased system that can estimate 2D poses even when persons are completely occluded by walls. However, these approaches also have their own problems. For example, how to effectively fuse visual and inertial signals for IMU-based approaches? Besides, wearing sensors on the body is intrusive, and is not acceptable in some scenarios such as football games. On the other hand, the WiFi-based solutions cannot deal with self-occlusion which is a big limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>Collecting more images for model training is an effective approach to improve the generalization performance. For example in <ref type="bibr" target="#b42">Qiu et al., 2019)</ref>, the authors propose to use the MPII  and the COCO <ref type="bibr" target="#b29">(Lin et al., 2014)</ref> datasets to help train the 2D module of the 3D pose estimators which effectively reduces the risk of over-fitting to simple training datasets. However, annotating a sufficiently large pose dataset is expensive and time consuming. So some approaches <ref type="bibr" target="#b46">(Rogez and Schmid, 2016;</ref><ref type="bibr" target="#b55">Varol et al., 2017;</ref><ref type="bibr" target="#b19">Hoffmann et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2016;</ref><ref type="bibr" target="#b27">Lassner et al., 2017)</ref> propose to generate synthetic images. The main issue is to bridge the gap between the synthetic and real images such that the model trained on synthetic images can be applied to real images. To that end, some approaches such as <ref type="bibr" target="#b39">(Peng et al., 2018)</ref> propose to use generative adversarial networks to generate realistic images.</p><p>Spatial-Temporal Context Models Some approaches propose to use spatial-temporal context models to jointly detect all joints in a video sequence such that each joint can benefit from other joints in the same or neighboring frames. Intuitively, if a body joint is occluded thus is difficult to be detected according to its own appearance, they can use the locations of other joints to guess the possible location. For example, in a previous work <ref type="bibr" target="#b6">(Cao et al., 2017;</ref><ref type="bibr" target="#b25">Kreiss et al., 2019)</ref>, the authors propose to detect body parts, i.e. the links connecting two joints, in addition to the individual joints. This provides a chance to mutually enhance the detection of the two linked joints. In <ref type="bibr" target="#b8">(Cheng et al., 2019;</ref><ref type="bibr" target="#b38">Pavllo et al., 2019)</ref>, temporal convolution is utilized to deal with occlusion in current frames. Some works such as <ref type="bibr" target="#b42">(Qiu et al., 2019)</ref> propose to establish the spatial correspondence across multiple camera views, and leverage multi-view features for robust joint detection. Significant performance improvement has been achieved for the occluded joints on several benchmark datasets. The main drawback of the approach <ref type="bibr" target="#b42">(Qiu et al., 2019)</ref> is the lack of flexibility in practice since it needs to train a separate fusion network for every possible camera placement. Our work differs from <ref type="bibr" target="#b42">(Qiu et al., 2019)</ref> in that it can be applied to new environments with different numbers of cameras and different camera poses without additional adaptation. We will compare the two methods in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Consensus Learning</head><p>A fundamental problem in multi-sensor fusion is to detect and remove outliers as the sensors may produce inconsistent measurements. RANSAC <ref type="bibr" target="#b12">(Fischler and Bolles, 1981)</ref> is the most commonly used outlier detection method. The main assumption is that the dataset consists of inliers. It produces reasonable results only with a certain probability which increases as the number of inliers. In practice, when the number of sensors is small, the probability of detecting the real outliers is also small. For example, in multiview human pose estimation, the number of cameras is only four to eight for most benchmark datasets <ref type="bibr" target="#b21">(Ionescu et al., 2014;</ref><ref type="bibr" target="#b52">Trumble et al., 2017)</ref>. For such cases, we observe that RANSAC may not be the best option.</p><p>In recent years, uncertainty learning <ref type="bibr" target="#b24">(Kendall and Gal, 2017;</ref><ref type="bibr">Gal and Ghahramani, 2015;</ref><ref type="bibr" target="#b26">Lakshminarayanan et al., 2017;</ref><ref type="bibr" target="#b59">Zafar et al., 2019;</ref><ref type="bibr" target="#b26">Lakshminarayanan et al., 2017;</ref><ref type="bibr" target="#b41">Pleiss et al., 2017)</ref> has attracted a lot of attention which is particularly important for high-risk applications such as autonomous driving and medical diagnosis <ref type="bibr" target="#b13">(Gal, 2016;</ref><ref type="bibr" target="#b15">Ghahramani, 2016)</ref>. The main idea is that, when a model makes a prediction, it also outputs a score reflecting the confidence of the prediction. Consider an autonomous car that uses a neural network to detect people. If the network is not confident about the prediction, the car could probably rely on other sensors for making the correct decision. Uncertainty is introduced to computer vision in <ref type="bibr" target="#b24">(Kendall and Gal, 2017;</ref><ref type="bibr" target="#b25">Kreiss et al., 2019;</ref><ref type="bibr" target="#b18">He et al., 2019;</ref><ref type="bibr" target="#b20">Ilg et al., 2018)</ref>. Another branch of approaches such as <ref type="bibr" target="#b16">(Guo et al., 2017;</ref><ref type="bibr" target="#b41">Pleiss et al., 2017)</ref> propose to learn uncertainty by calibration. They propose to train the model such that the probability associated with the predicted class label agrees with its ground truth correctness likelihood.</p><p>The concept of uncertainty can be leveraged to reduce the impact of outliers. For example, in <ref type="bibr" target="#b22">(Iskakov et al., 2019)</ref>, the authors propose to predict an uncertainty score for each joint in each view. The score is used to weigh each view when doing triangulation. This dramatically reduces the 3D pose estimation error. Inspired by the success of uncertainty learning in computer vision tasks, we propose to learn uncertainty for multiview feature fusion. The predicted uncertainty is used as a weight when fusing multiview features. We show this adaptive feature fusion could effectively improve the fusion quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Basics for Multiview Fusion</head><p>We first introduce the basics for multiview fusion to lay the groundwork for AdaFuse. In particular, we discuss how to establish the point-point correspondence between two views such that the features correspond to the same 3D space point can be fused together. The narrow baseline correspondence can be solved efficiently by local feature matching. However, in the context of multiview human pose estimation where only a small number of cameras are placed far away from each other, the local features cannot be robustly detected and matched especially for texture-less human regions. This poses a serious challenge.</p><p>To solve the problem, we present a coarse-to-fine approach to find matched points. It first establishes the pointto-line correspondence between two views by epipolar geometry, and then implicitly determine the point-to-point correspondence by exploring the sparsity of the heatmap representations. The approach notably simplifies the task because it avoids the challenging step of finding the exact correspondence. We first introduce epipolar geometry in section 3.1 in order to determine the point-to-line correspondence. Then in section 3.2, we describe how we adapt epipolar geometry to perform multiview heatmap fusion. Finally, we discuss the side effect caused by the simplified fusion strategy and our solution in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Epipolar Geometry</head><p>Let us denote a point in 3D space as X ? R 4?1 as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. This could be the location of a body joint in the context of pose estimation. Note that homogeneous coordinate and column vector are used to represent a point. The 3D point is imaged in two camera views, at x = PX in the first, and x = P X in the second, where x and x ? R 3?1 represent 2D points in images, P and P ? R 3?4 are the projection matrix for each camera. Since the two 2D points correspond to the same 3D point and have the same semantic <ref type="figure">Fig. 4</ref> Epipolar geometry based heatmap fusion. For each location x in the first view, we first compute the corresponding epipolar lines in the other two views. Then we find the largest responses on the two lines, respectively and add them to the original response at x.</p><formula xml:id="formula_0">x I I (a) (b) (c)</formula><p>meanings, their features can be safely fused such that each view benefits from the other view. The epipolar geometry <ref type="bibr" target="#b17">(Hartley and Zisserman, 2003</ref>) between two views is essentially the geometry of the intersection of the image planes with the pencil of planes having the baseline as axis. The baseline is the line joining the camera centers C 1 and C 2 . In particular, for each location x in the first view, it helps us to determine the location of the corresponding point x in the second view without having to know X.</p><p>We can see from <ref type="figure" target="#fig_1">Figure 3</ref> that the image points x and x , the 3D point X, and the camera centers C 1 and C 2 lie on the same plane ?. The plane intersects with the two image planes at epipolar lines I and I , respectively. In particular,</p><formula xml:id="formula_1">I = Fx I = F x ,<label>(1)</label></formula><p>where F ? R 3?3 is fundamental matrix which can be derived from P and P . Readers can refer to <ref type="bibr" target="#b17">(Hartley and Zisserman, 2003)</ref> for detail derivation. In addition, the rays back-projected from x and x intersect at X, and the rays are coplanar, lying in ?. It is straightforward to derive that the location of x which corresponds to x is guaranteed to lie on the epipolar line I . However, we have to leverage additional information such as appearance to determine the exact location of x on I .</p><p>In the context of multiview feature fusion, for every image point x, we need to find the corresponding point x in the second view so that we can fuse the features at x with those at x and obtain more robust pose estimations. Since we do not know the depth of X, it could move freely on the line defined by the camera center C 1 and image point x. However, we know that x cannot span the entire image plane but is restricted to the line I . In the following section 3.2, we will describe how we perform multiview feature fusion based on epipolar geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampson Distance</head><p>In practice, usually we have 2D measurements x and x corresponding to the same 3D location X which is unknown. Due to measurement noise and errors, the line C 1 x and C 2 x might not intersect exactly at location X. To obtain the optimal estimation for X, we search forX subject to</p><formula xml:id="formula_2">d 2 Reproj = min X d 2 x, PX + d 2 x , P X ,<label>(2)</label></formula><p>where d(?) denotes Euclidean distance, d Reproj represents the reprojection distance between x and x . Since there is optimization process when obtaining d Reproj , we adopt an one-step method which is its first-order approximation <ref type="bibr" target="#b17">(Hartley and Zisserman, 2003)</ref>. This approximation is also called Sampson distance as</p><formula xml:id="formula_3">d Sampson = x Fx (Fx) 2 1 + (Fx) 2 2 + (F x ) 2 1 + (F x ) 2 2 ,<label>(3)</label></formula><p>where F is fundamental matrix, the subscript 1 or 2 denotes the first or second element of a vector. By using Sampson distance, we can directly obtain distance between a pair of locations without knowing the intermediateX. In AdaFuse, we use Sampson distance to represent to what extent a pair of 2D joint detections support each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Heatmap Fusion</head><p>Multiview fusion is applied to heatmaps rather than intermediate features as shown in <ref type="figure">Figure 2</ref>. This is because heatmap has the nice property of sparsity which can simplify the pointpoint matching. A heatmap produces a per-pixel likelihood for joint locations in the image. Specifically, it is generated as a two-dimensional Gaussian distribution centered at the coordinate of the joint. So it has a small number of large responses near the joint location, and a large number of zeros at other locations. See <ref type="figure">Figure 4</ref> (a) for an example heatmap of the right knee joint. The sparse heatmaps allow us to safely skip the exact point-point matching because the features at the "zero" locations on the epipolar line are not contributing to the feature fusion. As a result, instead of trying to find the exact corresponding location in the other view, we simply select the largest response on the epipolar line as the matched point. This is a reasonable simplification because the corresponding point usually has the largest response. For example, in <ref type="figure">Figure 4</ref>, for each location x, we first compute the corresponding epipolar lines in the other two camera views. Then we find the largest responses on the two epipolar lines, respectively and fuse them with the response at x.</p><p>Let us denote the heatmap in view v as H v . The response at the location x of the heatmap is denoted as H v (x). The corresponding epipolar line of x in view u is denoted as I u (x) which consists of a number of discrete locations on The ambiguity problem in our simplified multiview fusion approach and our solution. We can see from the "fused heatmap" that the correct location has the largest response which is as expected. However, for an incorrect location x, there is also a chance that the response is also enhanced by at most one view. Fortunately, the correct location will be enhanced more times (three times in this example) leading to the largest response. So we apply the SoftMax operator to the fused heatmap to reduce the responses at incorrect locations.</p><p>the heatmap H u . The epipolar line can be analytically computed based on the camera parameters for every location x.</p><p>Then we formulate multiview fusion a?</p><formula xml:id="formula_4">H v (x) = ?H v (x) + 1 ? ? N N u=1 max x ?I u (x) H u (x ),<label>(4)</label></formula><p>where? denotes the fused heatmap and N is the number of camera views which contribute to the fusion of current view. The parameter ? balances the responses in the current and other views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Side Effect and Solution</head><p>One side effect caused by the simplified fusion model (i.e. Eq. <ref type="formula" target="#formula_4">(4)</ref>) is that some background locations may be enhanced undesirably. We visualize an example in the second row of <ref type="figure">Figure 5</ref>. We can see that many background pixels, for example x, have non-zero responses which are caused by fusion. This phenomenon happens because multiple epipolar lines (in other views) may pass the ground truth joint location which has large responses, and some of the epipolar lines actually correspond to background pixels in the current view. This is explained in <ref type="figure">Figure 5</ref>. For a location x in the current view, the corresponding epipolar lines in the other three views are drawn in the first row. We can see that although x is not at a meaningful joint location, the epipolar line in the first view passes the ground truth knee joint and leads to a large unexpected response for x. Fortunately, there are patterns for the background pixels that could be undesirably impacted. In general, the pixels that are impacted by a high response location in another view are guaranteed to lie on the same line. More importantly, the lines that correspond to different views do not overlap. It means, for a location x in the background, its response can only be enhanced by at most one view. In contrast, the location which corresponds to meaningful body joints will be enhanced by multiple views. In other words, the correct location is guaranteed to have the largest response for general cases. So we take advantage of this observation and directly apply the SoftMax operator to remove the small responses. See the third row in <ref type="figure">Figure 5</ref> for the effect. We can see that only the large responses around the joint location are preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>It is worth noting that the above fusion method does not have learnable parameters. So we only need to train the backbone network such as SimpleBaseline  to estimate pose heatmaps. The loss function for training the backbone network is defined as MSE loss between the estimated heatmaps and ground truth heatmaps. In the testing stage, given the heatmaps estimated by SimpleBaseline, we fuse them deterministically by our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptive Weight for Multiview Fusion</head><p>The fusion strategy introduced in the previous section treats all views evenly without considering the feature quality of each view. Note that the fusion weight is 1?? N for the N views in Eq. (4). However, the strategy is problematic in some cases where the heatmaps of some camera views are incorrect. This is because those features may undesirably mess up the features in good views, leading to a completely incorrect 2D pose estimation results.</p><p>To solve this problem, we present a weight learning network to learn an adaptive weight for each view to faithfully reflect its heatmap quality. It takes inputs of the heatmaps of N -views extracted by the pose estimation network, and regresses N weights ? u . Then multiview fusion is rewritten to consider the weights as follow?</p><formula xml:id="formula_5">H v (x) = ? v H v (x) + N u=1 ? u max x ?I u (x) H u (x ),<label>(5)</label></formula><p>The prediction of the adaptive fusion weight ? is implemented by a lightweight neural network as shown in <ref type="figure">Figure  6</ref>. On top of the heatmaps H provided by the pose estimation network, we extract two types of information for making the prediction. The first is the appearance embedding which extracts information such as the distribution characteristics of the heatmaps. The second is the geometry embedding which considers the cross-view location consistency. The two terms are complementary to each other. The proposed weight learning network can be joined with the pose estimation network for end-to-end training without enforcing supervision on the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Appearance Embedding</head><p>The heatmap of each joint actually contains rich information to infer its heatmap quality. For example, if the predicted heatmap has a desired shape of Gaussian kernel, then in many cases, the heatmap quality is good. In contrast, if the predicted heatmap has random and small responses all over the space (for example, when the joint is occluded), then the quality is likely to be bad.</p><p>We propose a simple network to extract appearance embeddings for each joint in each camera view. <ref type="figure">Figure 7</ref> shows the network structure. Starting from the heatmaps H i , we apply a convolutional layer to extract features. Then the features are down-sampled by average pooling and fed to a Fully Connected (FC) layer for extracting the appearance embeddings. Different joint types and camera views share the same weights. We only show the network for a single view and a single joint for simplicity. The appearance embedding network is jointly learned end-to-end with the pose estimation network.  <ref type="figure">Fig. 7</ref> The appearance embedding network for predicting the fusion weight. i is the index of camera views. The parameters in the network are shared for all views and joints. See also <ref type="figure">Figure 6</ref> for how the appearance embedding A i is used for determining the fusion weight. Soft Argmax <ref type="figure">Fig. 8</ref> The geometry embedding network for predicting the fusion weight. For each joint in each camera view (three views are shown in this example), it generates a 256-dimensional embedding to reflect the heatmap (pose) quality. Note that the FC is shared for all branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Geometry Embedding</head><p>The appearance embedding alone is not sufficient for some challenging cases where the heatmaps have the desired shape of Gaussian kernel but at the wrong locations. One such example is when the left knee is detected at the location of right knee which is usually known as the "double counting" problem to the community. To solve this problem, we propose to leverage the location consistency information among all camera views. Our core motivation is that the predicted joint location in one camera view is more reliable if it agrees with the locations in other views.</p><p>We implement this idea by a geometry embedding network as shown in <ref type="figure">Figure 8</ref>. Starting from the heatmaps H, we first apply the "soft-argmax" operator <ref type="bibr" target="#b50">(Sun et al., 2018)</ref> to obtain the location (x, y) of the joint in each view. We also get the heatmap response value s in that location to reflect its confidence. Then we compute the Sampson distance <ref type="bibr" target="#b17">(Hartley and Zisserman, 2003)</ref> dist i?j between the current view and other views to measure the correspondence or consistency error. A small dist i?j means the joint locations in the two views are consistent. Intuitively, the location that is consistent with most views is more reliable. Finally, we propose to use a FC layer to embed the Sampson distance into a feature vector. The feature vectors of all camera pairs are then averaged to obtain the final geometry embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Weight Learning Network</head><p>We propose a simple network consisting of three FC layers to transform the concatenated appearance and geometric embeddings to regress the final weight. It is worth noting that we do not train the weight learning network indepen-dently. Instead, we join it with the pose estimation network to minimize the fused 2D heatmap loss without enforcing intermediate supervision on the fusion weights. The first column in <ref type="figure">Figure 9</ref> shows some example weights predicted by our approach. We can see that when the joints are occluded, and are localized at incorrect locations, the corresponding fusion weights are indeed smaller than other joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets and Metrics</head><p>We introduce the three datasets used for evaluation and the corresponding metrics. We also describe how we construct the synthetic person dataset Occlusion-Person which has a large amount of human-object occlusion.  <ref type="figure">Fig. 9</ref> We visualize the predicted fusion weights by the size of the markers in the first column. A large marker denotes a larger weight. The rest two columns show the poses estimated by HeuristicFuse and AdaFuse, respectively. Our AdaFuse has clearly better estimations due to the consideration of the feature quality in every view. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The Human3.6M Dataset <ref type="bibr" target="#b21">(Ionescu et al., 2014)</ref> It provides synchronized images captured by four cameras. There are seven subjects performing daily actions. We use a crosssubject evaluation scheme where subjects 1, 5, 6, 7, 8 are used for training and 9, 11 for testing. We also use the MPII dataset  to augment the training data to avoid over-fitting to the simple background. Since the MPII dataset provides only monocular images, we only train the backbone network before multiview fusion.</p><p>The Total Capture Dataset  It provides synchronized person images captured by eight cameras. Following the dataset convention, the training set consists of "ROM1,2,3", "Freestyle1,2", "Walking1,3", "Act-ing1,2" and "Running1" on subjects 1, 2 and 3. The testing set consists of "Freestyle3 (FS3)", "Acting3 (A3)" and "Walking2 (W2)" on subjects 1,2,3,4 and 5.</p><p>The CMU Panoptic Dataset  This recently introduced dataset provides images captured by dozens of cameras. We uniformly select six cameras to evaluate the impact of the number of cameras on 3D pose estimation. In particular, the cameras 1, 2, and 10 are firstly selected to construct a 3-view experiment setting. Then the cameras 13, 3 and 23 are sequentially added to the previous three cameras to construct a four, five and six view experiment setting, respectively. We follow the practice of the previous work <ref type="bibr" target="#b57">(Xiang et al., 2019)</ref> to select the training and testing sequences which consist of only one person. Since few works have reported numerical results on this dataset, we only compare our approach to the baselines.</p><p>The Occlusion-Person Dataset The previous benchmarks do not provide occlusion labels for the joints in images which prevents us from performing numerical evaluation on the occluded joints. In addition, the amount of occlusion in the benchmarks is limited. To address the limitations, we propose to construct this synthetic dataset Occlusion-Person. We adopt UnrealCV <ref type="bibr" target="#b43">(Qiu et al., 2017)</ref> to render multiview images and depth maps from 3D models. In particular, thirteen human models of different clothes are put into nine different scenes such as living rooms, bedrooms and offices. The human models are driven by the poses selected from the CMU Motion Capture database. We purposely use objects such as sofas and desks to occlude some body joints. Eight cameras are placed in each scene to render the multiview images and the depth maps. The eight cameras are placed evenly every 45 degree on a circle of two meters radius at about 0.9 and 2.3 meters high, respectively. We provide the 3D locations of 15 joints as ground truth. <ref type="figure" target="#fig_0">Figure  10</ref> shows some sample images from the dataset and spacial configuration of the cameras. The occlusion label for each joint in an image is obtained by comparing its depth value (available in the depth map), to the depth of the 3D joint in the camera coordinate system. If the difference between the two depth values is smaller than 30cm, then the joint is not occluded. Otherwise, it is occluded. <ref type="table" target="#tab_2">Table 1</ref> compares this dataset to the existing benchmarks. In particular, about 20% of the body joints are occluded in our dataset. We use 75% of the dataset for training and 25% for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metrics</head><p>2D Metrics The Percentage of Correct Keypoints (PCK) metric introduced in <ref type="bibr" target="#b1">Andriluka et al. (2014)</ref> is commonly used for 2D pose evaluation. PCKh@t measures the percentage of the estimated joints whose distance from the ground-truth joints is smaller than t times of the head length. Following the previous works, we report results when t is 1 2 . Since the head length is not provided in the used three benchmarks, we approximately set it to be 2.5% of the human bounding box width for all benchmarks. </p><formula xml:id="formula_6">= [p 3 1 , ? ? ? ,p 3 M ]: MPJPE = 1 M M i=1 p 3 i ?p 3 i 2</formula><p>where M is the number of joints in a pose. We do not align the estimated 3D poses to the ground truth by Procrustes. This is referred to as protocol 1 in some works <ref type="bibr" target="#b33">(Martinez et al., 2017;</ref><ref type="bibr" target="#b51">Tome et al., 2018)</ref>  <ref type="table">Table 2</ref> The 2D pose estimation accuracy (PCKh@t) of the baseline methods and our approach on the Human3.6M dataset. We report results for each individual joint and the average over all joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Root  <ref type="table">Table 3</ref> The 3D pose estimation error (mm) of the baseline methods and our approach on the Human3.6M dataset. We compare our approach to four baselines. The first is No-Fuse which estimates 2D poses independently for each view without multiview fusion. The second is HeuristicFuse which assigns a fixed fusion weight for each view according to Eq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>(4). The parameter ? is set to be 0.5 by cross-validation. The third baseline is ScoreFuse which uses the same formulation as AdaFuse, i.e. Eq. (5), for feature fusion. It differs from AdaFuse only in the way we compute ?. In particular, ScoreFuse computes ? as the maximum value of the heatmap H. Our approach is denoted as AdaFuse which uses the predicted weight for fusion as in Eq. (5). All of the four methods use triangulation <ref type="bibr" target="#b17">(Hartley and Zisserman, 2003)</ref> to estimate 3D pose from the multiview 2D poses. We also compare to a baseline RANSAC which does not perform multiview fusion, but uses RANSAC to remove the outliers in triangulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results on Human3.6M</head><p>2D Pose Estimation Results The 2D pose estimation results are presented in <ref type="table">Table 2</ref>. All multiview fusion methods remarkably outperform NoFuse. The improvement is most significant for the Elbow and Wrist joints because they are frequently occluded by human body. The results demonstrate that multiview fusion is an effective strategy to handle occlusion. AdaFuse achieves the highest average accuracy among all fusion methods validating that learning appropriate fusion weights can effectively reduce the negative impact caused by the features of low-quality views. <ref type="table">Table 3</ref> shows the 3D pose estimation errors of the baselines and our approach. We can see that NoFuse gets an average error of 22.9mm. This is a very strong baseline whose error is only slightly larger than the  <ref type="figure" target="#fig_0">Fig. 11</ref> We divide the test set of Human3.6M into to six groups according to the error of NoFuse. We compute the average error for every baseline and every group, respectively. state-of-the-arts (see <ref type="table">Table 4</ref>). On top of this strong baseline, we observe that adding multiview fusion can further reduce the 3D pose estimation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Pose Estimation Results</head><p>HeuristicFuse gets a smaller error than NoFuse which is consistent with the 2D results in <ref type="table">Table 2</ref>. The mean error only decreases by 1.9mm because most examples are relatively easy leaving little space for improvement. However, significant improvement is achieved for the challenging joints such as Wrist. The ScoreFuse gets a smaller error than HeuristicFuse. It means assigning small weights to low-quality views helps improve the quality of the fused heatmaps. Finally, our approach AdaFuse, which determines the fusion weight by considering both appearance cues and geometry consistency, notably decreases the average error to 19.5mm. Considering the baseline is already very strong, the improvement is significant. We notice that AdaFuse achieves slightly worse results on a small number of joints such as hip and head. This is mainly because these joints are rarely oc-cluded in the datasets so the 2D pose estimator can obtain very accurate estimations for them. Further applying cross view fusion will introduce small noise to heatmaps leading to slightly worse 2D pose estimation accuracy. But when occlusion occurs which is often the case in practice, the benefit brought by cross view fusion will be much more significant than the harm caused by the small noise.</p><p>RANSAC is the de facto standard for solving robust estimation problems. As shown in <ref type="table">Table 3</ref>, it outperforms No-Fuse by removing some outlier 2D poses in triangulation. However, it is not as effective as the multiview fusion methods because the latter also attempt to refine, in addition to removing, the outlier poses. Another reason is that the number of cameras in this task is small which reduces the chance of finding the true outliers. In addition, we find that RANSAC is very sensitive to the threshold used for determining whether a data point is inlier or outlier. In our experiments, we set the threshold by cross validation.  <ref type="figure" target="#fig_0">Fig. 12</ref> We visualize the weights predicted by the ScoreFuse and Ada-Fuse, respectively. For example, in the first example (left sub- <ref type="figure">figure)</ref>, the pose estimation network generates a high response at the wrong location for the first view. Consequently, ScoreFuse undesirably gives a large weight. In contrast, AdaFuse gives a small weight by identifying that its location are inconsistent with other views.</p><p>To better understand the improvement brought by Ada-Fuse, we divide the testing samples of the Human3.6M dataset into six groups according to the 3D errors of NoFuse. Then we compute the average error for each group. <ref type="figure" target="#fig_0">Figure 11</ref> shows the results of various baselines. We can see that Ada-Fuse achieves the most significant improvement when the original error of NoFuse is large. However, even when the pose estimations of NoFuse are already accurate, AdaFuse can still reduce the error slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study on Fusion Weights One typical situation where</head><p>ScoreFuse fails is when the pose estimation network generates large scores at inaccurate locations. In this case, Ada-Fuse can outperform ScoreFuse by leveraging the multiview geometry consistency. To support this conjecture, we visualize some typical heatmaps and the corresponding fusion weights predicted by the two methods, respectively, in <ref type="figure" target="#fig_0">Figure 12.</ref> We find that the heatmap responses are large for the four views although the locations are inaccurate for the first and third view. ScoreFuse gives large weights for all views which finally leads to a corrupted heatmap. In contrast, Ada-Fuse identifies that the predicted locations in the first and third view are inconsistent with the other two views in spite of their large scores. So it decreases the weights to ensure the good quality of the fused heatmap.</p><p>In addition, we also conduct ablation study on AdaFuse by using only one of two embedding networks. When we only use either the appearance embedding or geometry embedding, the 3D errors increase to 20.3mm and 19.9mm, respectively. Note that the improvement is actually much larger on those challenging examples. The results validate that the two embeddings are complementary.</p><p>Comparison to the State-of-the-arts <ref type="table">Table 4</ref> compares our approach to the state-of-the-arts. We can see that our approach outperforms all of them. Note that two approaches, i.e. Triangulation and Volumetric, are used in <ref type="bibr" target="#b22">(Iskakov et al., 2019)</ref> to lift 2D poses to 3D. The Triangulation approach is more comparable to ours. Our approach AdaFuse decreases the error of <ref type="bibr" target="#b22">(Iskakov et al., 2019)</ref> by about 13% <ref type="bibr">(= 22.6?19.5 22.6</ref> ).</p><p>The improvement is significant considering that the error of the state-of-the-art is already very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results on Panoptic</head><p>We evaluate the impact of the number of cameras on this dataset. <ref type="figure" target="#fig_0">Figure 13</ref> shows the mean 3D errors when three to six cameras are used, respectively. In general, the error decreases when more cameras are used for most baselines. However, we observe that the error of NoFuse actually becomes larger when the camera number increases from three to four. This undesirable phenomenon happens because the new camera view is very challenging thus the 2D pose estimation results are inaccurate. However, for our approach AdaFuse, the negative impact of low-quality heatmaps in individual views is limited due to the adaptive multiview fusion. We can see that the error of AdaFuse consistently decreases when the number of cameras increases. Since there is not a commonly adopted evaluation protocol and very few works have reported results on this new dataset, we do not compare our approach to the other approaches. <ref type="table">Table 4</ref> The 3D pose estimation errors (mm) of the state-of-the-arts and our approach on the Human3.6M dataset. We report results for each of the 15 actions individually and also the average error over all actions. T- <ref type="bibr" target="#b22">Iskakov et al. (2019)</ref>   <ref type="table">Table 5</ref> shows the results on the occluded joints. Only about 30.9% of the occluded joints can be accurately detected by NoFuse. The result is reasonable because the features of the occluded joints are severely corrupted. All of the three multiview fusion methods remarkably improve the accuracy. In particular, more than 90% of the occluded joints are correctly detected by AdaFuse.</p><p>The results demonstrate the advantages of our strategy for learning the fusion weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Pose Estimation Results</head><p>We show the 3D pose estimation error (mm) for each joint type in <ref type="table">Table 6</ref>. NoFuse results in a large error of 48.1mm. By improving the 2D pose estimation results on the occluded joints, the 3D errors are also significantly reduced, especially for the joints on the limbs such as Ankles and Wrists. In particular, our approach decreases the 3D error significantly to 12.6mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Number of Occluded Views</head><p>We also evaluate the impact of the number of occluded views on this dataset. In particular, we classify each joint into one of five groups according to the number of occluded views, and report the average joint error for each group, respectively. The results are shown in <ref type="table">Table 7</ref>. We can see that when the joints are visible in all views, the simple baseline NoFuse also achieves a very small error of 13.0mm. However, the error increases dramatically to 82.6mm when four views are occluded. Recall that there are eight views in total for this dataset. In contrast, the multiview fusion methods, especially our AdaFuse, achieves consistently smaller errors than NoFuse. More importantly, the error increase is much slower than NoFuse when more camera views are occluded which validates the robustness of our approach to occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Power</head><p>The only learnable parameters in our fusion approach are in the appearance embedding and geometry embedding networks. In this section, we evaluate whether the AdaFuse weight prediction network learned on Occlusion-Person can be directly applied to the other datasets.</p><p>In particular, we append the AdaFuse weight prediction network learned on Occlusion-Person to the 2D pose estimators trained on each dataset itself as the final model for evaluation. <ref type="table">Table 8</ref> shows the 3D pose estimation results on various datasets. We find that the fusion network learned on the synthetic Occlusion-Person dataset achieves similar performance on the three realistic datasets compared to the networks learned on each of the target dataset, respectively. The promising results validate that the fusion model has strong generalization power. It is also worth noting that our approach can naturally handle different numbers of cameras for two reasons. First, the parameters in the appearance embedding network and the geometry embedding network are shared for all camera views. Second, the "Mean" operator in the geometry embedding network makes it independent of the number of views as shown in <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref>. In summary, AdaFuse is ready to be deployed in new environ- <ref type="table">Table 6</ref> The 3D pose estimation error (mm) of the baselines and our approach on the Occlusion-Person dataset. We report the result on each joint individually and also the average over all joints. The second row shows the percentage of the joints that are occluded for each joint type.  <ref type="table">Table 7</ref> The 3D pose estimation error (mm) of the baseline methods and our approach on the Occlusion-Person dataset. We group the the 3D joints by number of occluded views (8 views in all). We show each group's joint number percentage in the second row. ments of different camera poses without additional adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results on Total Capture</head><p>We report the 3D pose estimation results on the Total Capture dataset in <ref type="table">Table 9</ref>. It is worth noting that some methods also use IMUs in addition to the multiview images. We can see that our approach outperforms all of the previous methods. We notice that the error of our approach is slightly larger than LSTM-AE <ref type="bibr" target="#b53">(Trumble et al., 2018)</ref> for the "W2 (walking)" action of S4,5. We tend to think it is because LSTM can get significant benefits when it is applied to periodic actions such as "walking". This is also observed independently in another work <ref type="bibr" target="#b15">(Gilbert et al., 2019)</ref>. We show some 3D pose estimation examples in <ref type="figure" target="#fig_0">Figure  14</ref>. In most cases, our approach can accurately estimate the 3D poses. One typical situation where the approach fails is when 2D pose estimation results are inaccurate for many camera views. For example in the Panoptic dataset, when human begin to enter the dome, they may be occluded in multiple views. In this case, the heatmaps in each view are of low-quality. Therefore the fused heatmaps will also have degraded quality, leading to inaccurate 2D pose estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary and Future Work</head><p>We present a multiview fusion approach AdaFuse to handle the occlusion problem in human pose estimation. AdaFuse has practical values in that it is very simple and can be flexibly applied to new environments without additional adaptation. In addition, it can be combined with any 2D pose estimation networks. We extensively evaluate the effectiveness of the approach on three benchmark datasets. The approach <ref type="table">Table 8</ref> The 3D pose estimation errors MPJPE (mm) when AdaFuse weight prediction network is trained on Occlusion-Person or directly trained on the Evaluation dataset, respectively. The 2D pose estimators for generating the initial heatmaps are trained on each Evaluation dataset separately. outperforms the state-of-the-arts remarkably. We also construct a large scale human dataset which has severe occlusion to promote more research along this direction. Our next step of work is to leverage temporal information to further improve the pose estimation accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Our approach accurately detects the poses even though they are occluded by leveraging the features in other views. The bottom three rows are images from other view angles of the scene for readers to better perceive the 3D poses of the actors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Illustration of the point-line correspondence in two views. For an arbitrary point x in one view, the corresponding point x in another view has to lie on the epipolar line I . This is the core of AdaFuse for finding corresponding points in other views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 5 The ambiguity problem in our simplified multiview fusion approach and our solution. We can see from the "fused heatmap" that the correct location has the largest response which is as expected. However, for an incorrect location x, there is also a chance that the response is also enhanced by at most one view. Fortunately, the correct location will be enhanced more times (three times in this example) leading to the largest response. So we apply the SoftMax operator to the fused heatmap to reduce the responses at incorrect locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10</head><label>10</label><figDesc>We show some typical images, ground-truth 2D joint locations and the depth maps from the Occlusion-Person dataset. The joint represented by red "x" means it is occluded. The bottom row shows spacial configuration of the eight cameras used in the dataset from different view angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3D</head><label></label><figDesc>Metrics The 3D pose estimation accuracy is measured by Mean Per Joint Position Error (MPJPE) between a ground truth 3D pose y = [p 3 1 , ? ? ? , p 3 M ] and an estimated 3D pos? y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13</head><label>13</label><figDesc>The 3D pose estimation errors on the Panoptic dataset when different numbers of cameras are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Overview of AdaFuse. It takes multiview images as input and outputs 2D poses of all views jointly. It first uses a pose estimation network to obtain 2D heatmaps for each view. Then on top of epipolar geometry, the heatmaps from all camera views are fused. Finally, we apply the SoftMax operator to suppress the small noises introduced in fusion. Consequently, pose estimation in each view benefits from other views.</figDesc><table><row><cell>Heatmaps</cell><cell>AdaFuse</cell><cell></cell><cell></cell></row><row><cell>Pose Estimation</cell><cell>C 2</cell><cell></cell><cell>SoftMax</cell></row><row><cell>Network</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pose Estimation Network</cell><cell>M</cell><cell>C 1</cell><cell>SoftMax</cell></row><row><cell>Pose Estimation</cell><cell></cell><cell></cell><cell>SoftMax</cell></row><row><cell>Network</cell><cell>C 3</cell><cell></cell><cell></cell></row><row><cell>Fig. 2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>The statistics of the public multiview pose estimation datasets. Only the Occlusion-Person dataset provides occlusion labels.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Frames Cameras Occluded Joints</cell></row><row><cell>Human3.6M</cell><cell>784k</cell><cell>4</cell><cell>-</cell></row><row><cell>Total Capture</cell><cell>236k</cell><cell>8</cell><cell>-</cell></row><row><cell>Panoptic</cell><cell>36k</cell><cell>31</cell><cell>-</cell></row><row><cell>Occlusion-Person</cell><cell>73k</cell><cell>8</cell><cell>20.3%</cell></row><row><cell>Initial Pose with Weight</cell><cell>Heuristic Fusion</cell><cell>Ours</cell><cell></cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(d)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>means triangulation is used. V-<ref type="bibr" target="#b22">Iskakov et al. (2019)</ref> means volumetric method is used.MethodsDirect Disc. Eat Greet Phone Photo Pose Purch Sit SitD Smoke Wait WalkD Walk WalkT MPJPE The 2D pose estimation accuracy (PCKh@t) of the baselines and our approach for the occluded joints on the Occlusion-Person dataset. We report results for each joint type individually, and also the average accuracy over all joint types.</figDesc><table><row><cell cols="2">Trumble et al. (2017)</cell><cell cols="6">92.7 85.9 72.3 93.2 86.2 101.2 75.1 78.0 83.5 94.8 85.8 82.0 114.6 94.9 79.7</cell><cell>87.3</cell></row><row><cell cols="2">Pavlakos et al. (2017)</cell><cell cols="6">41.2 49.2 42.8 43.4 55.6 46.9 40.3 63.7 97.6 119.0 52.1 42.7 51.9 41.8 39.4</cell><cell>56.9</cell></row><row><cell cols="2">Tome et al. (2018)</cell><cell cols="6">43.3 49.6 42.0 48.8 51.1 64.3 40.3 43.3 66.0 95.2 50.2 52.2 51.1 43.9 45.3</cell><cell>52.8</cell></row><row><cell cols="2">Qiu et al. (2019)</cell><cell cols="6">24.0 26.7 23.2 24.3 24.8 22.8 24.1 28.6 32.1 26.9 30.9 25.6 25.0 28.0 24.4</cell><cell>26.2</cell></row><row><cell cols="8">T-Iskakov et al. (2019) 20.4 22.6 20.5 19.7 22.1 20.6 19.5 23.0 25.8 33.0 23.0 21.6 20.7 23.7 21.3</cell><cell>22.6</cell></row><row><cell cols="8">V-Iskakov et al. (2019) 18.8 20.0 19.3 18.7 20.2 19.3 18.7 22.3 23.3 29.1 21.2 20.3 19.3 21.6 19.8</cell><cell>20.8</cell></row><row><cell>NoFuse</cell><cell></cell><cell cols="6">20.1 22.2 20.2 22.2 23.9 18.2 20.6 25.9 37.0 24.6 22.4 22.5 18.2 22.8 18.5</cell><cell>22.9</cell></row><row><cell cols="2">AdaFuse (Ours)</cell><cell cols="6">17.8 19.5 17.6 20.7 19.3 16.8 18.9 20.2 25.7 20.1 19.2 20.5 17.2 20.5 17.3</cell><cell>19.5</cell></row><row><cell>Table 5 Methods</cell><cell>Hip</cell><cell>Knee</cell><cell>Ankle</cell><cell>Shlder</cell><cell>Elbow</cell><cell>Wrist</cell><cell>Avg.</cell></row><row><cell>NoFuse</cell><cell>63.4</cell><cell>21.5</cell><cell>17.0</cell><cell>29.5</cell><cell>14.6</cell><cell>12.4</cell><cell>30.9</cell></row><row><cell>HeuristicFuse</cell><cell>76.9</cell><cell>59.0</cell><cell>73.4</cell><cell>63.5</cell><cell>49.0</cell><cell>54.8</cell><cell>65.0</cell></row><row><cell>ScoreFuse</cell><cell>90.9</cell><cell>88.6</cell><cell>88.1</cell><cell>86.0</cell><cell>93.2</cell><cell>86.8</cell><cell>89.8</cell></row><row><cell>AdaFuse</cell><cell>96.5</cell><cell>96.0</cell><cell>92.5</cell><cell>94.1</cell><cell>98.3</cell><cell>93.2</cell><cell>95.5</cell></row><row><cell cols="4">6.3 Results on Occlusion-Person</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">2D Pose Estimation Results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>3% 13.7% 7.6% 23.0% 25.0% 23.5% 16.8% 25.3% 21.7%Fig. 14 We demonstrate some 3D pose estimation examples obtained by AdaFuse. The last row shows some failure cases.</figDesc><table><row><cell></cell><cell>Root</cell><cell>Belly</cell><cell>Neck</cell><cell>Hip</cell><cell>Knee</cell><cell cols="3">Ankle Shlder Elbow</cell><cell>Wrist</cell><cell>Mean</cell></row><row><cell cols="2">Occluded (%) 14.NoFuse 10.0</cell><cell>12.2</cell><cell>12.5</cell><cell>16.8</cell><cell>61.1</cell><cell>113.9</cell><cell>28.0</cell><cell>63.7</cell><cell>60.3</cell><cell>48.1</cell></row><row><cell>HeuristicFuse</cell><cell>8.8</cell><cell>10.7</cell><cell>11.5</cell><cell>14.2</cell><cell>21.1</cell><cell>19.2</cell><cell>17.5</cell><cell>23.6</cell><cell>24.1</cell><cell>18.0</cell></row><row><cell>ScoreFuse</cell><cell>8.4</cell><cell>12.6</cell><cell>12.6</cell><cell>14.7</cell><cell>17.5</cell><cell>17.1</cell><cell>16.1</cell><cell>13.2</cell><cell>16.9</cell><cell>15.0</cell></row><row><cell>RANSAC</cell><cell>8.6</cell><cell>11.2</cell><cell>11.7</cell><cell>12.9</cell><cell>18.8</cell><cell>17.9</cell><cell>17.1</cell><cell>14.5</cell><cell>19.7</cell><cell>15.5</cell></row><row><cell>AdaFuse (Ours)</cell><cell>7.2</cell><cell>10.6</cell><cell>11.6</cell><cell>11.7</cell><cell>13.8</cell><cell>15.7</cell><cell>14.2</cell><cell>9.9</cell><cell>14.4</cell><cell>12.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table 9The 3D pose estimation errors MPJPE (mm) of different methods on the Total Capture dataset.</figDesc><table><row><cell>Evaluation Dataset</cell><cell>AdaFuse</cell><cell></cell><cell cols="6">NoFuse HeuristicFuse ScoreFuse RANSAC</cell></row><row><cell></cell><cell>Trained on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Evaluation Dataset Occlusion-Person</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human3.6M</cell><cell>19.5</cell><cell>19.4</cell><cell></cell><cell>22.9</cell><cell>21.0</cell><cell></cell><cell>20.1</cell><cell>21.8</cell></row><row><cell>Panoptic 4 views</cell><cell>14.7</cell><cell>14.6</cell><cell></cell><cell>33.2</cell><cell>22.5</cell><cell></cell><cell>21.9</cell><cell>16.9</cell></row><row><cell>Panoptic 6 views</cell><cell>13.6</cell><cell>13.9</cell><cell></cell><cell>29.6</cell><cell>19.8</cell><cell></cell><cell>19.4</cell><cell>15.5</cell></row><row><cell>Total Capture</cell><cell>19.2</cell><cell>20.1</cell><cell></cell><cell>29.4</cell><cell>20.0</cell><cell></cell><cell>20.5</cell><cell>20.5</cell></row><row><cell>Methods</cell><cell>IMUs Temporal</cell><cell cols="3">Subjects(S1,2,3)</cell><cell cols="3">Subjects(S4,5)</cell><cell>Mean</cell></row><row><cell></cell><cell></cell><cell>W2</cell><cell>A3</cell><cell>FS3</cell><cell>W2</cell><cell>A3</cell><cell>FS3</cell></row><row><cell>(Trumble et al., 2017)</cell><cell></cell><cell>48.3</cell><cell>94.3</cell><cell cols="5">122.3 84.3 154.5 168.5 107.3</cell></row><row><cell>(Wei et al., 2016)</cell><cell></cell><cell cols="4">79.0 106.5 112.1 79.0</cell><cell>73.7</cell><cell>149.3</cell><cell>99.8</cell></row><row><cell>(Gilbert et al., 2019)</cell><cell></cell><cell>19.2</cell><cell>42.3</cell><cell>48.8</cell><cell>24.7</cell><cell>58.8</cell><cell>61.8</cell><cell>42.6</cell></row><row><cell>(Trumble et al., 2018)</cell><cell></cell><cell>13.0</cell><cell>23.0</cell><cell>47.0</cell><cell>21.8</cell><cell>40.9</cell><cell>68.5</cell><cell>34.1</cell></row><row><cell>(Qiu et al., 2019)</cell><cell></cell><cell>19</cell><cell>21</cell><cell>28</cell><cell>32</cell><cell>33</cell><cell>54</cell><cell>29</cell></row><row><cell>NoFuse</cell><cell></cell><cell>15.9</cell><cell>18.5</cell><cell>29.9</cell><cell>33.9</cell><cell>33.8</cell><cell>60.0</cell><cell>29.4</cell></row><row><cell>HeuristicFuse</cell><cell></cell><cell>7.8</cell><cell>11.6</cell><cell>19.6</cell><cell>23.3</cell><cell>26.9</cell><cell>44.8</cell><cell>20.0</cell></row><row><cell>ScoreFuse</cell><cell></cell><cell>9.7</cell><cell>13.1</cell><cell>19.9</cell><cell>23.9</cell><cell>27.2</cell><cell>41.4</cell><cell>20.5</cell></row><row><cell>RANSAC</cell><cell></cell><cell>8.4</cell><cell>11.6</cell><cell>20.5</cell><cell>23.3</cell><cell>27.2</cell><cell>45.7</cell><cell>20.5</cell></row><row><cell>AdaFuse (Ours)</cell><cell></cell><cell>7.2</cell><cell>10.8</cell><cell>18.5</cell><cell>22.8</cell><cell>26.6</cell><cell>42.9</cell><cell>19.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiview pictorial structures for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">3D pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Locally connected network for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<editor>T-PAMI</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dropout as a bayesian approximation: Insights and applications</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Gal Y, Ghahramani Z</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization and filtering for human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusing visual and inertial sensors with semantics for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z ;</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
	<note>A history of bayesian neural networks</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, JMLR. org</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2888" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to train with synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="609" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Uncertainty estimates and multihypotheses networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galesso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="652" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Hu-man3. 6m: Large scale datasets and predictive methods for 3D human sensing in natural environments. T-PAMI pp</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<idno>arXiv:190505754</idno>
		<title level="m">Learnable triangulation of human pose</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="190" to="204" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="11977" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Making the invisible visible: Action recognition through walls and occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Markerless motion capture of interacting characters using multiview image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR, IEEE</publisher>
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time full-body motion capture from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV, IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="449" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey of advances in vision-based human motion capture and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kr?ger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="90" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1253" to="1262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data fusion for visual tracking with particles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vermaak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="495" to="513" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On fairness and calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5680" to="5689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unrealcv: Virtual worlds for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Z</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia, ACM</title>
		<meeting>the 25th ACM international conference on Multimedia, ACM</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1221" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Xsens mvn: full 6dof human motion tracking using miniature inertial sensors. Xsens Motion Technologies BV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roetenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luinge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Slycke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech Rep 1</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3108" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The perceptive workbench: Computer-visionbased gesture tracking, object tracking, and 3d reconstruction for augmented desks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Westyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weeks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="71" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Rethinking pose in 3D: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Total capture: 3D human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deep autoencoder for combined human pose estimation and body model upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Voxelpose: Towards multicamera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y ; Cvpr</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
	<note>Simple baselines for human pose estimation and tracking</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Metafuse: A pre-trained fusion model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C ; Cvpr</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
	<note>3d human pose estimation in the wild by adversarial learning</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Face recognition with bayesian convolutional networks for robust surveillance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zia</forename><forename type="middle">T</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Fusing wearable imus with multi-view images for human pose estimation: A geometric approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2200" to="2209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Through-wall human pose estimation using radio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7356" to="7365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Through-wall human mesh recovery using radio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="10113" to="10122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
