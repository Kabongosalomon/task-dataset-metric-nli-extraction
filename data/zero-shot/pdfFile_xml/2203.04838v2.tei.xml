<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
						</author>
						<title level="a" type="main">CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic Segmentation</term>
					<term>Scene Parsing</term>
					<term>Cross-Modal Fusion</term>
					<term>Vision Transformers</term>
					<term>Scene Understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pixel-wise semantic segmentation of RGB images can be advanced by exploiting informative features from supplementary modalities. In this work, we propose CMX, a vision-transformer-based cross-modal fusion framework for RGB-X semantic segmentation. To generalize to different sensing modalities encompassing various supplements and uncertainties, we consider that comprehensive cross-modal interactions should be provided. CMX is built with two streams to extract features from RGB images and the complementary modality (X-modality). In each feature extraction stage, we design a Cross-Modal Feature Rectification Module (CM-FRM) to calibrate the feature of the current modality by combining the feature from the other modality, in spatial-and channel-wise dimensions. With rectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix them for the final semantic prediction. FFM is constructed with a cross-attention mechanism, which enables exchange of long-range contexts, enhancing both modalities' features at a global level. Extensive experiments show that CMX generalizes to diverse multi-modal combinations, achieving state-of-the-art performances on five RGB-Depth benchmarks, as well as RGB-Thermal and RGB-Polarization datasets. Besides, to investigate the generalizability to dense-sparse data fusion, we establish an RGB-Event semantic segmentation benchmark based on the EventScape dataset, on which CMX sets the new state-of-the-art. Code is available at https://github.com/huaaaliu/RGBX Semantic Segmentation. <ref type="figure">Fig. 1</ref>: RGB-X semantic segmentation across diverse sensing modality combinations: RGB-D, -T, -P, and -E segmentation. network to extract features from RGB images, meanwhile information derived from another specific modality assist the extraction of RGB features <ref type="figure">(Fig. 2(a)</ref>). This is a typical strategy in RGB-D semantic segmentation with depth-guided operators <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, which are usually elaborately tailored for specific modalities (e.g., depth), yet hard to be extended to operate with other modality combinations. The second type of approaches [7], [8], <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref> deploys two backbones to perform feature extraction from RGBand another modality separately, then fuses the extracted two features into one feature for semantic prediction <ref type="figure">(Fig. 2(b)</ref>). This arXiv:2203.04838v2 [cs.CV] 12 Apr 2022 14 Acc=66.09% Acc=66.62% Acc=71.78% Acc=71.89%</p><p>Acc=78.04% Acc=79.57% Acc=91.15% Acc=92.24%</p><p>Acc=83.25% Acc=81.02%</p><p>Acc=83.38% Acc=81.16%</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S EMANTIC segmentation is an essential task in computer vision, which aims to transform an image input into its underlying semantically meaningful regions and enables a pixelwise dense scene understanding for many real-world applications such as automated vehicles, robotics navigation, and augmented reality <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Over the last years, pixel-wise semantic segmentation of RGB images has gained an increasing amount of attention and made significant progress on segmentation accuracy <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Yet, due to the characteristics of RGB images, current deep semantic segmentation models cannot always extract high-quality features under some certain circumstances, e.g., when two objects have similar colors or textures, it is difficult to distinguish them through pure RGB images <ref type="bibr" target="#b6">[7]</ref>.</p><p>However, different types of sensors can supply RGB images with rich complementary information (see <ref type="figure">Fig. 1</ref>). For example, depth measurement can help identify the boundaries of objects and offer geometric information of dense scene elements <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Thermal images facilitate to discern different objects through their specific infrared imaging <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Besides, polarimetric-and event information are advantageous for perception in specularand dynamic real-world scenes <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Therefore, multi-modal semantic segmentation models, that can effectively exploit such supplementary information, are often promising to yield better performances than single-modal RGB-based segmentation <ref type="bibr" target="#b12">[13]</ref>.</p><p>Existing multi-modal semantic segmentation methods can be divided into two categories. The first category employs a single structure offers the extensibility to replace the other modal-stream with alternative complementary modalities. Nevertheless, most previous methods are predominantly crafted for specific modalities, e.g., they only focus on tackling RGB-D scene parsing <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref> or RGB-T segmentation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>. While inspiring, they fail to work well on different multi-modal scenarios. For example, according to <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b18">[19]</ref>, ACNet <ref type="bibr" target="#b6">[7]</ref> and SA-Gate <ref type="bibr" target="#b7">[8]</ref>, designed for RGB-D data, perform less satisfactorily in RGB-T tasks (see <ref type="figure">Fig. 3</ref>). To enable robust real-world scene understanding, a general and flexible architecture suitable for a diverse mix of sensor data combinations enhancing RGB segmentation ( <ref type="figure">Fig. 1)</ref>, i.e. RGB-X semantic segmentation, is desirable and advantageous. It not only saves research and engineering efforts on optimizing architectures for a specific modality combination scenario, but also makes it possible that a system equipped with multi-modal sensors can readily leverage new sensors when they become available <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, which is conductive to robust scene perception. However, such universal modality-agnostic RGB-X semantic segmentation architectures are rarely investigated in the state of the art.</p><p>Recently, with the emergence of powerful non-local feature extraction backbones like vision transformers <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, the performances of different dense prediction tasks, e.g., semantic segmentation, have been significantly advanced. Multi-modal data contain patterns that require modeling dependencies across modalities. Transformers handle inputs as sequences and have the ability to acquire long-range correlations <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, offering the possibility for a unified framework for diverse multi-modal tasks. However, existing multi-modal fusion modules <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref> are mostly architected for Convolutional Neural Networks (CNNs) that fail in establishing long-range contextual dependencies in pixel-rich image data, which prove essential for accurate semantic segmentation <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Moreover, multi-modal data often encompass a great deal of noisy measurements in different sensing modalities, e.g., low-quality distance estimation regions caused by limited effective depth ranges <ref type="bibr" target="#b7">[8]</ref> and uncertainties resulted from various event representations <ref type="bibr" target="#b11">[12]</ref>. It remains unclear whether potential improvements on RGB-X semantic segmentation can be materialized via vision transformers. Crucially, while some previous works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> use a simple global multi-modal interaction strategy, it does not generalize well across different sensing data combinations <ref type="bibr" target="#b9">[10]</ref>. We hypothesize that for RGB-X semantic segmentation with various supplements and uncertainties, comprehensive cross-modal interactions should be provided, to fully exploit the potential of cross-modal complementary features.</p><p>In light of these challenges, we propose CMX, a cross-modal fusion framework for RGB-X semantic segmentation. CMX uses RGB image and another modal image as inputs, with the aim of achieving better segmentation than using only RGB image. To extract bi-modal features, our vision-transformer-based model is built as a two-stream architecture (RGB-and X-modal stream). Two specific modules are designed for feature interaction and feature fusion in between. The first module, Cross-Modal Feature Rectification Module (CM-FRM), is put forward to calibrate the bi-modal features by leveraging their spatial-and channel-wise correlations, which enables both streams to focus more on the complementary informative cues from each other and mitigates the effects of uncertainties and noisy measurements from different modalities. Such a feature rectification tackles varying noises and uncertainties in diverse modalities. It enables better multi-modal feature extraction and interaction. The second module, Feature Fusion Module (FFM), is constructed in two stages, which merges both rectified features from RGB-and X-modality into a single feature for the semantic prediction. Motivated by the large receptive fields obtained via self-attention <ref type="bibr" target="#b24">[25]</ref> of vision transformers, a cross-attention mechanism is devised in the first stage of FFM for realizing a cross-modal global reasoning. In the second stage, a mixed channel embedding is applied to produce enhanced output features. Thereby, our introduced comprehensive interactions lie in multiple levels (see <ref type="figure">Fig. 2(c)</ref>), including channel-and spatialwise rectification from the feature map perspective, as well as cross-attention from the sequence-to-sequence perspective, which are critical for generalization across modality combinations.</p><p>To verify our unification proposal, for the first time, we consider and assess CMX in four multi-modal semantic segmentation tasks. The proposed method achieves state-of-the-art performances on multiple RGB-Depth datasets <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, as well as RGB-Thermal <ref type="bibr" target="#b8">[9]</ref> and RGB-Polarization <ref type="bibr" target="#b10">[11]</ref> benchmarks. In particular, CMX attains top mIoU of 56.9% on NYU Depth V2 (RGB-D) <ref type="bibr" target="#b27">[28]</ref>, 59.7% on MFNet (RGB-T) <ref type="bibr" target="#b8">[9]</ref>, and 92.6% on ZJU-RGB-P <ref type="bibr" target="#b10">[11]</ref> datasets. Our universal approach clearly outperforms specialized architectures ( <ref type="figure">Fig. 3</ref>). Further, on popular challenging benchmarks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b27">[28]</ref>, CMX leads to ?5.0% mIoU gains compared to RGB-only segmentation. Aside from these combinations of dense image data, we study dense-sparse fusion using dense RGB-and sparse event-based data (see <ref type="figure">Fig. 1</ref>), which encodes changes of intensity of each pixel asynchronously and thereby is constructive to capturing motion information <ref type="bibr" target="#b11">[12]</ref>. However, an RGB-Event parsing benchmark is not available in the community. Tackling this shortage, we establish an RGB-Event semantic segmentation benchmark based on the EventScape dataset <ref type="bibr" target="#b32">[33]</ref>, where our RGB-X model further shines, setting the new state-of-the-art among &gt;10 benchmarked models. Meanwhile, the proposed CMX solution is proved to be effective for different CNN-and transformer backbone architectures. Moreover, our comprehensive set of investigations on representations of polarization-and event-based data indicates the path to follow and the sweet spot for reaching robust multi-modal semantic segmentation, trumping original representation methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p><p>On a glance, we deliver the following contributions: ? For the first time, explore RGB-X semantic segmentation in four multi-modal sensing data combinations. ? Rethink cross-modal fusion from a generalization perspective and consider that comprehensive cross-modal interactions should be provided to generalize across diverse modalities. ? Propose an RGB-X semantic segmentation framework CMX with cross-modal feature rectification and feature fusion modules, intertwining cross-attention and mixed channel embedding for enhanced global reasoning with transformers. ? Investigate representations of polarimetric-and event data, indicating the optimal path to follow for reaching robust multi-modal semantic segmentation. ? Establish an RGB-Event semantic segmentation benchmark to assess dense-sparse data fusion. Our general method achieves state-of-the-art performances on eight datasets, covering five RGB-D benchmarks, as well as RGB-T, RGB-P, and RGB-E sensing data combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer-driven Semantic Segmentation</head><p>Since Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b33">[34]</ref> address dense semantic segmentation in an end-to-end per-pixel classification a) Input Fusion b) Feature Fusion c) Interactive Fusion feature-wise feature-wise sequence-wise <ref type="figure">Fig. 2</ref>: Comparison of different fusion methods. a) Input fusion merges the input with modality-specific operations <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. b) Feature fusion uses a global feature-wise strategy via channel attention <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. c) In our cross-modal fusion framework for RGB-X semantic segmentation with transformers, comprehensive interactions are considered and provided, including channel-and spatial-wise cross-modal feature rectification from the feature map perspective, as well as cross-attention from the sequence-to-sequence perspective.  <ref type="bibr" target="#b27">[28]</ref>), is less effective on RGB-T or RGB-E tasks. CMX, for the first time, outperforms state-of-the-art specialized methods on four studied segmentation tasks on multiple datasets. fashion, this field has progressed exponentially. A main cluster of subsequent approaches augments the FCN structure by enlarging receptive fields which are vital for accurate segmentation <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Pyramid-, strip-, and atrous spatial pyramid pooling are designed to harvest multi-scale feature representations <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Inspired by the non-local block <ref type="bibr" target="#b39">[40]</ref>, self-attention in transformers <ref type="bibr" target="#b24">[25]</ref> has been used to establish long-range dependencies by DANet <ref type="bibr" target="#b25">[26]</ref> and OCNet <ref type="bibr" target="#b40">[41]</ref>. Attaching importance to efficiency, CCNet <ref type="bibr" target="#b41">[42]</ref> puts forward criss-cross attention, whereas CANet <ref type="bibr" target="#b42">[43]</ref> exploits a covariance matrix when encoding the dependencies. CTNet <ref type="bibr" target="#b43">[44]</ref> jointly explores spatial-and semantic dependency with mutual communications. Additional FCNbased methods improve dense semantic segmentation by refining context priors <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, enhancing boundary cues <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, and appending various attention modules <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref> to boost the performance. Recently, SETR <ref type="bibr" target="#b26">[27]</ref> and Segmenter <ref type="bibr" target="#b59">[60]</ref> directly adopt vision transformers <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> as the backbone, which capture global context from very early layers. SegFormer <ref type="bibr" target="#b60">[61]</ref>, PVT <ref type="bibr" target="#b61">[62]</ref>, Swin <ref type="bibr" target="#b23">[24]</ref>, and UniFormer <ref type="bibr" target="#b62">[63]</ref> create hierarchical structures to make use of multi-resolution features. Leveraging the advance of DETR <ref type="bibr" target="#b63">[64]</ref>, MaX-DeepLab <ref type="bibr" target="#b64">[65]</ref> and MaskFormer <ref type="bibr" target="#b65">[66]</ref> view image segmentation from the perspective of mask classification. Following this trend, various architectures of dense prediction transformers <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref> and semantic segmentation transformers <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref> emerge in the field. While these approaches have achieved high segmentation performance, most of them focus on using RGB images and suffer when RGB images cannot provide sufficient information in real-world scenes, e.g., under low-illumination conditions or in high-dynamic areas. In this work, we tackle multi-modal semantic segmentation to take advantage of complementary information offered by other modalities such as depth, thermal, polarization, and event-based data for boosting RGB segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-modal Semantic Segmentation</head><p>While previous works reach high performance on standard RGBbased semantic segmentation benchmarks, in challenging realworld conditions, it is desirable to involve multi-modality sensing for a reliable and comprehensive scene understanding <ref type="bibr" target="#b12">[13]</ref>. RGB-Depth <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref> and RGB-Thermal <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref> semantic segmentation are broadly investigated. Polarimetric optical cues <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref> and event-driven priors <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref> are often intertwined for robust perception under adverse conditions. In automated driving, optical flow <ref type="bibr" target="#b90">[91]</ref> and LiDAR data <ref type="bibr" target="#b91">[92]</ref> are also incorporated for enhanced semantic road scene understanding. However, most of these works only address a single sensing-modality combination scenario. In this work, we aim for a general and flexible approach, which is generalizable to a diverse mix of multi-modal combinations.</p><p>For multi-modal semantic segmentation, there are two dominant strategies. The first mainstream paradigm models crossmodal complementary information into layer-or operator designs <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref>. For example, a shape-aware convolution layer is introduced in <ref type="bibr" target="#b14">[15]</ref> for embedding depth features, while a spatial-guided convolution operator is proposed in <ref type="bibr" target="#b15">[16]</ref> to adapt receptive fields according to geometric patterns for RGB-D semantic segmentation. Additional methods dynamically exchange channels and shift pixels to enable implicit cross-modal feature fusion <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref>. While these works verify that multimodal features can be learned within a shared network, they are carefully designed for some specific modality combinations, in particular RGB-D semantic segmentation, which are hard to be applied into other modalities. Moreover, there are multi-task frameworks <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref> that facilitate inter-task feature propagation for RGB-D scene understanding, but they rely on supervision from other tasks for joint learning.</p><p>The second paradigm dedicates to developing fusion schemes to bridge two parallel modality streams. ACNet <ref type="bibr" target="#b6">[7]</ref> proposes attention complementary modules to exploit high-quality informative features for RGB-D semantic segmentation based on channel attention <ref type="bibr" target="#b101">[102]</ref>, whereas ABMDRNet <ref type="bibr" target="#b9">[10]</ref> suggests to reduce the modality differences between RGB-and thermal features before selectively extracting discriminative cues for RGB-T fusion. For RGB-P segmentation, Xiang et al. <ref type="bibr" target="#b10">[11]</ref> connect RGB-and polarization branches via efficient channel attention bridges. For RGB-E parsing, Zhang et al. <ref type="bibr" target="#b11">[12]</ref> explore sparse-to-dense and denseto-sparse fusion flows, verifying the importance of event-driven dynamic context for accident scene segmentation. In this research, we also advocate this paradigm but unlike previous works, we address RGB-X semantic segmentation with a unified framework, for generalizing to diverse sensing modality combinations.</p><p>While some previous works use a simple global channelwise interaction strategy, it does not work well across different sensing data. For example, ACNet <ref type="bibr" target="#b6">[7]</ref> and SA-Gate <ref type="bibr" target="#b7">[8]</ref>, designed for RGB-D segmentation, perform less satisfactorily in RGB-T scene parsing <ref type="bibr" target="#b9">[10]</ref>. In contrast, we hypothesize that comprehensive cross-modal interactions should be provided for RGB-X semantic segmentation with various supplements and uncertainties, to fully unleash the potential of cross-modal complementary features. Besides, most of the previous works still adopt CNNs as the backbone without considering that long-range dependency information could be informative for cross-modal fusion. We put forward a framework with transformers which considers global dependencies already in its architecture design. Concurrent works also use transformers for multi-modal action recognition <ref type="bibr" target="#b102">[103]</ref>, object detection <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b105">[106]</ref>, and end-to-end self-driving <ref type="bibr" target="#b106">[107]</ref>. Differing from existing works, we perform fusion on different levels with cross-modal feature rectification and cross-attentional exchanging for enhanced dense semantic prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED FRAMEWORK: CMX</head><p>We first introduce the overview of our proposed CMX framework for RGB-X semantic segmentation in Sec. 3.1. In Sec. 3.2 and Sec. 3.3, we elaborate the proposed CM-FFM and FFM modules for cross-modal feature rectification and fusion. In Sec. 3.4, we detail the used multi-modal data representations for investigating and reaching robust RGB-X semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>We present CMX, a cross-modal fusion framework for RGB-X semantic segmentation. The overview of CMX is shown in <ref type="figure" target="#fig_0">Fig. 4</ref>. We use two parallel backbones to extract features from RGB-and X-modal inputs, which can be RGB-depth, -thermal, -polarization, -event data, etc. Multi-modal data often contain a great amount of noises in different modalities, e.g., low-quality depth regions caused by limited effective distance measuring ranges <ref type="bibr" target="#b7">[8]</ref> and uncertainties resulted from various event representations <ref type="bibr" target="#b11">[12]</ref>.</p><p>While features from different modalities have their specific noisy measurements, the feature of another modality has the potential for rectifying and calibrating this noisy information. Therefore, we design a Cross-Modal Feature Rectification Module (CM-FRM) to rectify features coming from both modalities. They will be assembled between two adjacent stages of backbones. In this way, both rectified features will be sent to the next stage to further deepen and improve the feature extraction. Furthermore, we design a two-stage Feature Fusion Module (FFM) to fuse features belonging to the same level into a single feature map. Then, a decoder is used to transform the feature maps of different levels into the final semantic map. With the aim to harvest diverse supplements and tackle varying uncertainties, our introduced comprehensive interactions lie in multiple levels, including channeland spatial-wise rectification from the feature map perspective in CM-FRM, and cross-attention from the sequence-to-sequence perspective in FFM, which are crucial for generalizing across sensing data combinations and reaching robust RGB-X semantic segmentation. The backbones and the decoder can be replaced by other common methods. In Sec. 3.2 and Sec. 3.3, we detail the design of CM-FRM and FFM, respectively. In the following descriptions, we use X to refer to the supplementary modality, which can be depth-, thermal-, polarization-, event data, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-Modal Feature Rectification</head><p>As analyzed above, the information originating from different sensing modalities are usually complementary <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> but contain noisy measurements. The noisy information can be filtered and calibrated by using features coming from another modality. To this purpose, we propose a novel Cross-Modal Feature Rectification Module (CM-FRM) to perform feature rectification between parallel streams at each stage during feature extraction. For tackling various noises and uncertainties in diverse modalities, CM-FRM processes the input features in two different dimensions, including channel-wise and spatial-wise feature rectification, which together offers a holistic calibration, enabling better multi-modal feature extraction and interaction. Channel-wise feature rectification. We embed both modalities' features RGB in ? R H?W?C and X in ? R H?W?C along the spatial axis into two attention vectors W C RGB ? R C and W C X ? R C . Different from previous channel-wise attention methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b107">[108]</ref>, which use global average pooling to obtain the attention vector, we apply both global max pooling and global average pooling to RGB in and X in along the channel dimension to remain more information. We concatenate the four resulted vectors, having Y ? R 4C . Then, an MLP is applied, followed with a sigmoid function to obtain the weights W C ? R 2C from Y, which will be split into W C RGB and W C X : where ? denotes the sigmoid function. The channel-wise rectification is then operated as:</p><formula xml:id="formula_0">W C RGB , W C X = F split ? F mlp (Y) ,<label>(1)</label></formula><formula xml:id="formula_1">RGB C rec = W C X X in , X C rec = W C RGB RGB in ,<label>(2)</label></formula><p>where denotes channel-wise multiplication. Spatial-wise feature rectification. As the aforementioned channel-wise feature rectification module concentrates on learning global weights for a global calibration, we further introduce a spatial-wise feature rectification for calibrating local information. The two modalities' inputs RGB in and X in will be concatenated and embedded into two spatial weight maps: W S RGB ? R H?W and W S X ? R H?W . The embedding operation has two 1?1 convolution layers assembled with a RELU function. Afterwards, a sigmoid function is applied to obtain the embedded feature map F ? R H?W?2 , which is further split into two weight maps. The process to obtain the spatial weight maps is formulated as:</p><formula xml:id="formula_2">F = Conv 1?1 RELU Conv 1?1 (RGB in X in ) , (3) W S RGB , W S X = F split ?(F) .<label>(4)</label></formula><p>Similar to the channel-wise rectification, the spatial-wise rectification is formulated as:</p><formula xml:id="formula_3">RGB S rec = W S X * X in , X S rec = W S RGB * RGB in ,<label>(5)</label></formula><p>where * denotes spatial-wise multiplication. The whole rectified feature for both modalities RGB out and X out is organized as:</p><formula xml:id="formula_4">RGB out = RGB in + ? C RGB C rec + ? S RGB S rec , X out = X in + ? C X C rec + ? S X S rec .<label>(6)</label></formula><p>? C and ? S are two hyperparameters and we set them both as 0.5. RGB out and X out are the rectified features after the comprehensive calibration, which will be sent into the next stage for feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Fusion</head><p>After obtaining the feature maps at each layer, we build a twostage Feature Fusion Module (FFM) to enhance the information interaction and merge the features from two modalities into a single feature map. As shown in <ref type="figure" target="#fig_0">Fig. 4</ref>(c), in stage 1, the two branches are still maintained, and a cross-attention mechanism is designed to globally exchange information between the two branches. Then, we concatenate the outputs of these two branches. In stage 2, this concatenated feature is transformed into the original size via a mixed channel embedding. Information exchange stage. At this stage, the two modalities' features will exchange their information via a symmetric dual-path structure. For brevity, we take the X-modal path for illustration. We first flatten the input feature with size R H?W ?C to R N ?C , where N =H?W . Afterwards, a linear embedding is used to generate two vectors with the same size R N ?Ci , which we call residual vector X res and interactive vector X inter . We further put forward an efficient cross-attention mechanism applied to these two interactive vectors from different modal paths, which will carry out sufficient information exchange across modalities. This offers complementary interactions from the sequence-to-sequence perspective beyond the rectification-based interactions from the feature map perspective in CM-FRM. Our cross-attention mechanism for enhancing cross-modal feature fusion is based on the traditional self-attention <ref type="bibr" target="#b24">[25]</ref>. The original self-attention operation encodes the input vectors into Query Q, Key K, and Value V. The global attention map is calculated via a matrix multiplication QK T , which has a size of R N ?N and causes a high memory occupation. In contrast, <ref type="bibr" target="#b108">[109]</ref> uses a global context vector G = K T V with a size R C head ?C head and the attention result is calculated by QG. We flexibly adopt the reformulation and develop our multi-head cross-attention based on this efficient self-attention mechanism. Specifically, the interactive vectors will be embedded into Key K and Value V for each head, and both sizes of them are R N ?C head . The output is obtained by multiplying the interactive vector and the context vector from the other modality path, namely a crossattention process, and it is depicted in the following equations:</p><formula xml:id="formula_5">G RGB = K T RGB V RGB , G X = K T X V X ,<label>(7)</label></formula><formula xml:id="formula_6">U RGB = X inter RGB Softmax(G X ), U X = X inter X Softmax(G RGB ).<label>(8)</label></formula><p>Note that G denotes the global context vector, while U indicates the attended result. To realize the attention from different representation subspaces, we remain the multi-head mechanism, where the number of heads matches the transformer backbone. Then, the attended result vector U and the residual vector X res are concatenated. Finally, we apply a second linear embedding and resize the feature to R H?W ?C . Fusion stage. In the second stage of FFM, precisely the fusion stage, we use a simple channel embedding to merge the two paths' features, which is realized via 1?1 convolution layers. Further, we consider that during such a channel-wise fusion, the information of surrounding areas should also be exploited for robust RGB-X segmentation. Thereby, inspired by Mix-FFN in <ref type="bibr" target="#b60">[61]</ref> and ConvMLP <ref type="bibr" target="#b109">[110]</ref>, we add one more depth-wise convolution layer DW Conv 3?3 to realize a skip-connected structure. In this way, the merged features with the size R H?W ?2C are fused into the final output with the size of R H?W ?C for feature decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-modal Data Representations</head><p>To investigate and reach robust RGB-X semantic segmentation, the multi-modal data representations are critical. We study with four modality combinations, including RGB-Depth, RGB-Thermal, RGB-Polarization, and RGB-Event semantic segmentation. Now we elaborate the multi-modal data representations we use for inputting to our symmetric dual-path CMX architecture. RGB-Depth. Depth images naturally offer range, position, and contour information. The fusion of RGB information and depth information is promising to better separate objects with indistinguishable colors and textures at different spatial locations. We encode the raw depth images into HHA images as the depth modality input. This algorithm is proposed by <ref type="bibr" target="#b110">[111]</ref>, which has been proven to have a better use of depth information. Precisely, HHA offers a representation of geometric properties at each pixel, including horizontal disparity, height above ground, and the angle the pixel's local surface makes with the inferred gravity direction, emphasizing complementary discontinuities of RGB images. RGB-Thermal. At night or in places with insufficient light, objects and backgrounds have similar color information and are difficult to distinguish. Thermal images provide infrared characteristics of objects, which are potential to improve objects with thermal properties such as people. We directly use the infrared thermal image and copy the single-channel thermal image input 3 times to match the backbone input. RGB-Polarization. High-reflectivity objects such as glasses and cars have RGB information that are easily confused with surroundings. Polarization cameras record the optical polarimetric information when polarized reflection occurs, which offer complementary information serviceable for better segmenting and understanding scenes with specular surfaces. The polarization sensor equipped with a polarization mask layer with four different polarization directions <ref type="bibr" target="#b10">[11]</ref> and thereby each captured image set consists of four pixel-aligned images at different polarization angles</p><formula xml:id="formula_7">[I 0 ? , I 45 ? , I 90 ? , I 135 ? ],</formula><p>where I angle denotes the image recorded at the corresponding angle. We investigate representations of polarization data, including the Degree of Linear Polarization (DoLP ) and the Angle of Linear Polarization (AoLP ), which are key polarimetric properties characterizing light polarization patterns <ref type="bibr" target="#b10">[11]</ref>. They are derived by Stokes vectors S = [S 0 , S 1 , S 2 , S 3 ] that describe the polarization state of light. Precisely, S 0 represents the total light intensity, S 1 and S 2 denote the ratio of 0 ? and 45 ? linear polarization over its perpendicular polarized portion, and S 3 stands for the circular polarization power which is not involved in our work. The Stokes vectors S 0 , S 1 , S 2 can be calculated from image intensity measurements</p><formula xml:id="formula_8">[I 0 ? , I 45 ? , I 90 ? , I 135 ? ] via: S 0 = I 0 ? + I 90 ? = I 45 ? + I 135 ? , S 1 = I 0 ? ? I 90 ? , S 2 = I 45 ? ? I 135 ? .<label>(9)</label></formula><p>Then, DoLP and AoLP are formally computed as:</p><formula xml:id="formula_9">DoLP = S 2 1 + S 2 2 S 0 ,<label>(10)</label></formula><formula xml:id="formula_10">AoLP = 1 2 arctan S 1 S 2 .<label>(11)</label></formula><p>In our experiments, we further study with monochromatic and trichromatic polarization cues, coupled with RGB images in multi-modal RGB-P semantic segmentation. For monochromatic representation used in previous works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b111">[112]</ref>, we obtain it from monochromatic intensity measurements and convert it to 3-channel input by copying the single-channel information. For trichromatic polarization representation in either DoLP or AoLP , we compute separately for their respective RGB channels. RGB-Event. Event data provide multiple advantages such high dynamic range, high temporal resolution, and not being influenced by motion blur <ref type="bibr" target="#b88">[89]</ref>, which are critical in dynamic scenes with a great amount of motion information such as road-driving environments <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b89">[90]</ref>. Due to the sparse nature of event data, the raw events require a pre-processing to obtain a representation to achieve multi-modal fusion. Events are typically converted into a tensor with the same spatial dimension as the corresponding RGB image. For example, a set of raw events in a time window ?T =t N ?t 1 is embedded into a voxel grid with spatial dimensions H?W and time bins B, where t 1 and t N are the startand the end time stamp of the raw events. For example, event data in <ref type="bibr" target="#b32">[33]</ref> is directly converted to B=3. In this work, events are first embedded into a voxel grid with a higher time resolution, which we set the upscale size of the event bin as 6. Then, every 6 adjacent panels are superimposed to obtain a tensor with fine-grained event embedding. A comparison between the direct representation in <ref type="bibr" target="#b32">[33]</ref> and our event representation is visualized in <ref type="figure" target="#fig_1">Fig. 5</ref>, in which our event representation is more fine-grained in each event panel. Apart from B=3, we further investigate different settings of event time bin B={1, 5, 10, 15, 20, 30} in our method for reaching robust RGB-E semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT DATASETS AND SETUPS</head><p>In this section, we describe the multi-modal datasets and implementation details of our model in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We RGB-E EventScape dataset. While event-based data offer various benefits for perception in dynamic scenes <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b89">[90]</ref>, a largescale multi-modal RGB-Event semantic segmentation benchmark is not available in the field. Addressing this gap, we create an RGB-E multi-modal semantic segmentation benchmark 1 based on the EventScape dataset <ref type="bibr" target="#b32">[33]</ref>, which is originally designed for depth estimation. To maintain data diversity from the video sequences of the EventScape dataset generated from the CARLA simulator <ref type="bibr" target="#b113">[114]</ref>, we select one frame from every 30 frames. Thus, there are 4077 and 749 images in the training-and evaluation set, respectively. The images have a 512?256 resolution and are annotated with 12 semantic classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>During training on all datasets, data augmentation is performed by random flipping and scaling with random scales [0.5, 1.75].</p><p>We take Mix Transformer encoder (MiT) pretrained on Ima-geNet <ref type="bibr" target="#b114">[115]</ref> as the backbone and MLP-decoder with an embedding dimension of 512 unless specified, both introduced in Seg-Former <ref type="bibr" target="#b60">[61]</ref>. We select AdamW optimizer <ref type="bibr" target="#b115">[116]</ref> with weight decay 0.01. The original learning rate is set as 6e ?5 and we employ a poly learning rate schedule. We use cross-entropy as the loss function. When reporting multi-scale testing results on NYU Depth V2 and SUN RGB-D, we use multiple scales ({0.75, 1, 1.25}) with horizontal flipping. We use mean Intersection over Union (mIoU) averaged across semantic classes as the primary evaluation metric to measure the segmentation performance. More specific settings for different datasets are described in detail in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT RESULTS AND ANALYSES</head><p>In this section, we present experimental results for verifying the effectiveness of our proposed CMX approach for RGB-X semantic segmentation. In Sec. 5.1, we show the performances of our model on multiple indoor and outdoor RGB-Depth benchmarks, compared with state-of-the-art methods. In Sec. 5.2, we analyze 1. https://paperswithcode.com/sota/semantic-segmentation-on-eventscape  the RGB-Thermal segmentation performance for robust daytimeand nighttime semantic perception. In Sec. 5.3 and Sec. 5.4, we study the generalization of CMX to RGB-Polarization and RGB-Event modality combinations and representations of these multi-modal data. We conduct a comprehensive variety of ablation studies in Sec. 5.5 to confirm the effects of different components in our solution. Finally, we perform efficiency-and qualitative analysis in Sec. 5.6 and Sec. 5.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on RGB-Depth Datasets</head><p>We first assess the performance of our method on different RGB-D semantic segmentation datasets. NYU Depth V2. The results on the NYU Depth V2 dataset are shown in <ref type="table" target="#tab_2">Table 1</ref>. The performances of our model using different sizes of backbones are reported. It can be easily seen that our approach achieves leading scores. The proposed method with SegFormer-B2 already exceeds previous methods, attaining 54.4% in mIoU. Our CMX models based on SegFormer-B4 and -B5 further dramatically improve the mIoU to 56.3% and 56.9%, clearly standing out in front of all state-of-the-art approaches. The best CMX model even reaches superior results than recent strong pretraining-based methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b100">[101]</ref> like Omnivore <ref type="bibr" target="#b20">[21]</ref> that uses images, videos, and single-view 3D data for supervision. SUN-RGBD. The segmentation results of CMX on the SUN-RGBD dataset are shown in <ref type="table" target="#tab_3">Table 2</ref>. We experiment by training and testing backbones of two different sizes and our method achieves leading performances. Our interactive cross-modal fusion   approach exceeds previous works that either use input fusion depicted in <ref type="figure">Fig. 2(a)</ref> including SGNet <ref type="bibr" target="#b15">[16]</ref> and ShapeConv <ref type="bibr" target="#b14">[15]</ref>, or build on feature fusion illustrated in <ref type="figure">Fig. 2(b)</ref> including ACNet <ref type="bibr" target="#b6">[7]</ref> and SA-Gate <ref type="bibr" target="#b7">[8]</ref>. In particular, with SegFormer-B4 and -B5, our proposed method elevates the state-of-the-art to more than 52.0% in mIoU. CMX is also better than multi-task methods like PAP <ref type="bibr" target="#b96">[97]</ref> and TET <ref type="bibr" target="#b99">[100]</ref>. As the model with SegFormer-B5 comes with much larger computational complexity and relatively small improvements, which will be assessed in Sec. 5.6, in the following experiments we mainly use SegFormer-B2 and -B4 backbones.</p><p>Stanford2D3D. As shown in <ref type="table" target="#tab_4">Table 3</ref>, on the large-scale Stan-ford2D3D dataset, our proposed solution also achieves stateof-the-art mIoU scores. CMX based on SegFormer-B2 already surpasses the previous best ShapeConv <ref type="bibr" target="#b14">[15]</ref> based on ResNet-101 <ref type="bibr" target="#b123">[124]</ref> in mIoU and our model based on SegFormer-B4 further improves the accuracy values, reaching mIoU of 62.1%. The results demonstrate the effectiveness and learning capacity of our approach on such a large RGB-D dataset. ScanNetV2. We have also tested our CMX model with SegFormer-B2 on the ScanNetV2 benchmark. As shown in <ref type="table" target="#tab_5">Table 4</ref>, RGB-D methods generally obtain better performance than RGB-only methods. It can be clearly seen that CMX outperforms RGB methods and achieves the top mIoU of 61.3% among the RGB-D methods. On the ScanNetV2 leaderboard, while there are methods like BPNet <ref type="bibr" target="#b97">[98]</ref> that reaches a higher score, they rely on 3D supervision from point clouds to perform joint 2D-and 3D reasoning. In contrast, our method attains a competitively accurate performance by using purely 2D data and effectively leveraging the complementary information inside RGB-D modalities.</p><p>Cityscapes. The previous results are reported on indoor RGB-D datasets. To study the generalizability to outdoor scenes, we assess the effectiveness of CMX on Cityscapes, a well-known semantic segmentation dataset of RGB-D road-driving environments. As shown in <ref type="table" target="#tab_6">Table 5</ref>, we compare the performances of our models against state-of-the-art RGB and RGB-D methods. Compared with SegFormer-B2 (RGB), our RGB-D approach elevates the accuracy by 0.6% in mIoU. Our approach based on SegFormer-B4 achieves a state-of-the-art score of 82.6% among the competitive models, outstripping all existing RGB-D methods on Cityscapes by more than 0.4% in absolute mIoU values, verifying that CMX generalizes well to street scene understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on RGB-Thermal Dataset</head><p>We then investigate the generalizability of our approach to another modality combination scenario, precisely, RGB-Thermal semantic segmentation on the MFNet dataset <ref type="bibr" target="#b8">[9]</ref>.</p><p>Comparison with the state-of-the-art. In <ref type="table" target="#tab_7">Table 6</ref>, we compare our method against RGB-only models and multi-modal methods using RGB-T inputs. As unfolded, ACNet <ref type="bibr" target="#b6">[7]</ref> and SA-Gate <ref type="bibr" target="#b7">[8]</ref>, designed for RGB-D segmentation, perform less satisfactorily on RGB-T data, as they only use a global channel-wise fusion strategy and thereby fail to fully exploit the complementary cues. Depth-aware CNN <ref type="bibr" target="#b13">[14]</ref>, an input fusion method with modalityspecific operator design, also does not yield high performance. In contrast, the proposed CMX strategy, enabling comprehensive interactions from various perspectives, generalizes smoothly in RGB-T semantic segmentation. It can be seen that our method based on SegFormer-B2 achieves mIoU of 58.2%, clearly outperforming the previous best RGB-T methods ABMDRNet <ref type="bibr" target="#b9">[10]</ref>, FEANet <ref type="bibr" target="#b16">[17]</ref>, and GMNet <ref type="bibr" target="#b18">[19]</ref>. Our CMX solution based on SegFormer-B4 further elevates state-of-the-art mIoU to 59.7%, widening the accuracy gap in contrast to existing methods. Moreover, it is worth pointing out that the improvements brought by our RGB-X approach compared with the RGB baselines are compelling, i.e., +5.0% and +4.9% in mIoU for SegFormer-B2 and -B4 backbones, respectively. Our approach overall achieves top scores on car, person, bike, curve, car stop, and bump. For person with infrared properties, our approach enjoys more than +11.0% gain in IoU, confirming the effectiveness of CMX in harvesting complementary cross-modal information.</p><p>Day and night performances. Following <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b83">[84]</ref>, we assess day-and night segmentation results of our models compared against existing methods. For daytime scenes, both RGB-and thermal images provide beneficial meaningful information, and our approach increases IoU by 2.7%?3.1% compared with RGBonly baselines. At the nighttime, RGB segmentation often suffers from poor lighting conditions, and it even carries much noisy information in the RGB data. Yet, our CMX rectifies the noisy images and exploits supplementary features from thermal data, dramatically improving the performances by &gt;7.0% in mIoU at night and enhancing the robustness of semantic scene understanding in unfavorable environments with adverse illuminations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on RGB-Polarization Dataset</head><p>We now extend the study to RGB-P semantic segmentation on the ZJU-RGB-P dataset <ref type="bibr" target="#b10">[11]</ref>.</p><p>Comparison with the state-of-the-art. <ref type="table" target="#tab_9">Table 8</ref> shows per-class accuracy of our approach compared against RGB-only <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b129">[130]</ref> and RGB-Polarization fusion methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b111">[112]</ref>. Our approach largely elevates state-of-the-art performance, outperforming the previous best RGB-P method <ref type="bibr" target="#b10">[11]</ref> by &gt;6.0% in mIoU. We observe that the accuracy improvement on pedestrian is significant thanks to the capacity of the transformer backbone and our crossmodal fusion mechanisms. The IoU improvements compared to the results using only RGB modality (SegFormer-B2 <ref type="bibr" target="#b60">[61]</ref>) are clear on classes with polarimetric characteristics such as glass (&gt;8.0%) and car (&gt;2.5%), further evidencing the generalizability of our cross-modal fusion solution in bridging RGB-P streams.</p><p>Polarization data representations. We study polarimetric data representations and the results displayed in <ref type="table" target="#tab_9">Table 8</ref> indicate that AoLP and DoLP representations both carry effective polarization information beneficial for semantic scene understanding, which is consistent with the finding in <ref type="bibr" target="#b10">[11]</ref>. Besides, trichromatic representations are consistently better than monochromatic representations used in previous RGB-P segmentation works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b111">[112]</ref>. This is expected as the trichromatic representation provides more detailed information, which should be leveraged to fully unlock the potential of trichromatic polarization cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on RGB-Event Dataset</head><p>We take a step further to assess the generalizability of our proposed approach for dense-sparse data fusion.</p><p>Comparison with the state-of-the-art. In <ref type="table" target="#tab_10">Table 9</ref>, we benchmark more than 10 semantic segmentation methods. We compare our models against RGB-only methods covering CNNbased SwiftNet <ref type="bibr" target="#b129">[130]</ref>, Fast-SCNN <ref type="bibr" target="#b140">[141]</ref>, CGNet <ref type="bibr" target="#b141">[142]</ref>, and DeepLabV3+ <ref type="bibr" target="#b142">[143]</ref>, as well as transformer-based Swin <ref type="bibr" target="#b23">[24]</ref>, SegFormer <ref type="bibr" target="#b60">[61]</ref>, and Trans4Trans <ref type="bibr" target="#b2">[3]</ref>. We also include multimodal methods, spanning RFNet <ref type="bibr" target="#b0">[1]</ref> designed for road-driving scene segmentation and ISSAFE <ref type="bibr" target="#b11">[12]</ref>, the only known RGB-Event method designed for traffic accident scene segmentation, as well as SA-Gate <ref type="bibr" target="#b7">[8]</ref>, a state-of-the-art RGB-D segmentation method. While efficiently attaining accuracy improvements over SwiftNet, RFNet and ISSAFE overall only achieve rather unsatisfactory mIoU scores. SA-Gate, crafted for RGB-D data, is also less effective in the RGB-E combination scenario. Compared with existing methods, our models improve segmentation performance by mixing RGB-Event features, as it can be clearly seen in <ref type="table" target="#tab_10">Table 9</ref> and <ref type="figure" target="#fig_3">Fig. 6</ref>. Our model using MiT-B4 as the backbone reaches 64.28% in mIoU, towering over all other methods and setting the state-of-the-art on the RGB-E benchmark. This further verifies the versatility of our solution for different multi-modal combinations. Moreover, we experiment with DeepLabV3+ <ref type="bibr" target="#b142">[143]</ref> and Swin transformer with UperNet <ref type="bibr" target="#b143">[144]</ref> in dual-branch structures by using their multi-stage features with CMX. The results show that our RGB-X solution consistently improves the scores of RGB-only models, confirming that our method is not strictly tied to a concrete backbone architecture, but can be easily deployed with other CNN-and transformer models, which helps to yield effective models for multi-modal semantic segmentation. <ref type="figure" target="#fig_3">Fig. 6</ref> depicts per-class accuracy comparison between the RGB baseline and our RGB-Event model with   SegFormer-B2. With event data, the foreground objects are more accurately parsed by our RGB-E model, such as vehicle (+2.1%), pedestrian (+11.7%), and traffic light (+7.0%). Event data representations. In <ref type="table" target="#tab_2">Table 10</ref>, we further study with different settings of event time bin B={1, 3, 5, 10, 15, 20, 30} with our CMX fusion model based on SegFormer B2. Compared with the original event representation <ref type="bibr" target="#b32">[33]</ref>, our method achieves consistent improvements (visualized in <ref type="figure" target="#fig_4">Fig. 7</ref>) on different settings of event time bins, such as +1.63% of mIoU when B=30. In particular, it helps our CMX to obtain the highest mIoU of 61.90% in the setting of B=3. In B=1, embedding all events in a single time bin leads to dragging behind images of moving objects and being sub-optimal for feature fusion. In higher time bins, events produced in a short interval are dispersed to more bins, resulting in insufficient events in a single bin. These corroborate relevant observations in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b89">[90]</ref> and that the event representation B=3 is an effective time bin setting for RGB-E semantic segmentation with CMX. We note that the models in <ref type="table" target="#tab_2">Table 10</ref> are trained on an A100 GPU, and CMX produces a slightly different pixel accuracy score compared to that of ours (SegFormer-B2) in <ref type="table" target="#tab_10">Table 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation Study</head><p>We perform a series of ablation studies to explore how different parts of our architecture affect the segmentation. We use depth information encoded into HHA as the complementary modality here. We take MiT-B2 as the backbone with the MLP decoder in our ablation studies, unless specified. The semantic segmentation performance is evaluated on the NYU Depth V2 dataset.</p><p>Effectiveness of CM-FRM and FFM. We design CM-FRM and FFM to rectify and merge features coming from the RGBand X-modality branches. We take out these two modules from the architecture respectively, where the results are shown in <ref type="table" target="#tab_2">Table 11</ref>. If CM-FRM is ablated, the features will be extracted independently in their own branches, and for FFM we simply average the two features for semantic prediction. Compared with the baseline, using only CM-FRM improves mIoU by 2.5%, using only FFM improves mIoU by 1.2%, and together CM-FRM and FFM improve the semantic segmentation performance by 3.8%. Ablation with CM-FRM and FFM variants. We further experiment with variants of the CM-FRM and FFM modules. As shown in <ref type="table" target="#tab_2">Table 12</ref>, channel only denotes using channel-wise rectification only (? C =0 and ? S =1), and spatial only means using spatialwise rectification only (? C =0 and ? S =1). It can be seen that replacing the proposed CM-FRM by either channel only or spatial only causes a sub-optimal accuracy, further confirming the efficacy of combining the two dimensions' rectification for holistic feature calibration, which is crucial for robust multi-modal segmentation. In our channel-wise calibration, we use both global average pooling and global max pooling to remain more information. The results in <ref type="table" target="#tab_2">Table 12</ref> show the effects of using only global average pooling (avg. p.) and using only global max pooling (max. p.). Both of them are less effective than our complete CM-FRM, which offers a more comprehensive rectification. Previous ablation studies support the design of CM-FRM. To understand the capability of FFM, we here test with two variants. As shown in <ref type="table" target="#tab_2">Table 12</ref>, stage 2 only means there is no information exchange before the mixed channel embedding, whereas self attn denotes that the context vectors will not be exchanged in the stage 1 of FFM. The proposed modules productively rectify and fuse the features in different levels. Specifically, the experiments with the FFM variants illustrate the effectiveness of our cross-attention  <ref type="bibr" target="#b11">12</ref>: Ablation with different CM-FRM/FFM variants on NYU Depth V2 test set. channel only denotes using channelwise rectification only and spatial only denotes using spatial-wise rectification only. avg.p. denotes using average pooling only and max.p. means using max pooling only in CM-FRM. stage 2 only means there is no information exchange before mixed channel embedding, while self attn denotes that the context vectors are not exchanged in stage 1 of FFM. design for information exchange. In contrast to the complete FFM, the variant using only self-attention is less constructive, and it does not bring clear benefits compared to the variant with only stage 2. These indicate the importance of fusion from the sequence-tosequence perspective, which is not considered in previous works.</p><p>Overall, the ablation shows that our interactive strategy, providing comprehensive interactions, is effective for cross-modal fusion.</p><p>Ablation of the supplementary modality. Previous works have shown that multi-modal segmentation has a better performance than single-modal RGB segmentation <ref type="bibr" target="#b6">[7]</ref>. We carry out experiments to certify and the results are shown in <ref type="table" target="#tab_2">Table 13</ref>. Note that here, the MLP decoder is not used, in order to focus on studying the influence of feature extraction from different supplementary modalities. First, we use only a single MiT-B2 backbone which uses only RGB information. Then, due to our approach using two parallel backbones, to facilitate a fair comparison we keep a dualpath architecture with both RGB inputs, which results slightly better than a single backbone. Additionally, we replace the second modality with random noise, which obtains even better results than two RGB inputs. This means that even pure noise information may help the model to identify noisy information in the RGB branch.</p><p>The model learns to focus on relevant features and thus gains robustness. It may also help prevent over-fitting during the learning process. However, when using depth information, we have observed obvious improvements compared to these three settings, which further proves that the fusion of RGB and depth information brings clearly better predictions. Encoding depth images using the HHA representation further increases the scores. The overall gain of 5.3% in mIoU, compared with the RGB-only baseline, is also compelling, which is similar to that in RGB-T semantic segmentation, demonstrating the effectiveness of our proposed method for rectifying and fusing cross-modal information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Efficiency Analysis</head><p>In <ref type="table" target="#tab_2">Table 14</ref>, we present computational complexity results of our models. Compared with the previous state-of-the-art method SA-Gate <ref type="bibr" target="#b7">[8]</ref> on the NYU Depth V2 dataset using ResNet50, our model with SegFormer-B2 has similar #Params but significantly lower FLOPs. Our CMX model with SegFormer-B4 greatly elevates the mIoU score to 56.0%, further widening the accuracy gap with moderate model complexity. With SegFormer-B5, mIoU further increases to 56.8%, but it also comes with larger complexity. For efficiency-critical applications, the CMX solution with SegFormer-B2 or -B4 would be preferred to enable both accurate and efficient multi-modal semantic scene perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Qualitative Analysis</head><p>In our final study, we showcase various examples of representative qualitative results to deepen the analysis. Segmentation results. We visualize four segmentation results across different modality combinations with the backbone of SegFormer-B2 in <ref type="figure" target="#fig_6">Fig. 8</ref>. The proposed method can harvest complementary information from the second modality and thereby yields more accuracy segmentation results. For RGB-D semantic segmentation in an indoor scene from the NYU Depth V2 dataset <ref type="bibr" target="#b27">[28]</ref>, CMX leverages geometric information and correctly identifies the bed while the RGB-only model wrongly classifies it as a sofa. For RGB-T segmentation, while the baseline yields very coarse segmentation under the low illumination conditions at night, much clearer boundary distinctions are made by our method between the persons and unlabeled background with infrared information acquired in the thermal modality and fused via our solution. For RGB-P segmentation, specular glass areas are more precisely parsed, the cars which also contain polarization cues are completely and smoothly segmented with delineated borders, and the pedestrian detection also shows beneficial effects. Moreover, for dense-sparse fusion of RGB-Event data, our method generalizes well and enhances the segmentation of moving objects. Overall, the qualitative examination backs up that our general approach is suitable for a diverse mix of multi-modal sensing combinations for robust semantic scene understanding. We further view the outdoor RGB-D semantic segmentation results on the Cityscapes dataset <ref type="bibr" target="#b31">[32]</ref> based on the backbone of SegFormer-B4. We show the results of the baseline RGB-only model and our RGB-X approach, in particular the difference maps w.r.t. the segmentation ground truth. As displayed in <ref type="figure">Fig. 9</ref>, in spite of the noisy depth measurements, the HHA image encoding depth information still benefits the segmentation via our model that is able to rectify and fuse cross-modal complementary features. It can be seen that our approach has better pixel accuracy scores on a wide variety of driving scene elements such as fence, rider, sidewalk, and road in the positive group, which are highlighted in green boxes. However, the shadows and weak illumination conditions are still challenging for both models and make the depth cues less effective. For example, depth information in the regions of sidewalk in the negative group, may be less informative for fusion, as highlighted in the red boxes.</p><p>Feature analysis. To understand the key module for feature rectification, we visualize the input-and rectified features of CM-FRM in layer 1, and their difference map, as shown in <ref type="figure" target="#fig_7">Fig. 10</ref>. It can be seen that the feature maps are enhanced in both streams after the cross-modal calibration. The RGB-stream delivers texture information to the supplement modality, while the supplement modality further improves the boundary and emphasizes complementary discontinuities of RGB features. In the RGB-D segmentation scenario, the RGB-feature difference map shows that the ground area is better spotlighted, thanks to the HHA image encoding depth information, which provides geometric cues such as height above ground, beneficial for higher-level semantic prediction of groundrelated classes. In the RGB-T nighttime scene parsing cases, the pedestrians are hard to be seen in the RGB images. But the RGBfeature difference map clearly highlights the pedestrians thanks to the supplementary thermal modality with infrared imaging. These indicate that the complementary features have been infused into the RGB-stream. The RGB features have been rectified to better focus on informative ones and capture such complementary discontinuities towards accurate semantic understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>To revitalize multi-modal pixel-wise semantic scene understanding, we investigate RGB-X semantic segmentation and propose CMX, a universal vision-transformer-based cross-modal fusion architecture, which is generalizable to a diverse mix of sensing data combinations. We put forward a Cross-Modal Feature Rectification Module (CM-FRM) and a Feature Fusion Module (FFM) for facilitating interactions towards accurate RGB-X semantic segmentation. CM-FRM conducts channel-and spatialwise rectification, rendering comprehensive feature calibration. FFM intertwines cross-attention and mixed channel embedding for enhanced global information exchange. To further assess the generalizability of CMX to dense-sparse data fusion, we establish an RGB-Event semantic segmentation benchmark. We study effective representations of polarimetric-and event data, indicating the optimal path to follow for reaching robust multi-modal semantic segmentation. The proposed model sets the new state-of-the-art on eight benchmarks, spanning five RGB-D datasets, as well as RGB-Thermal, RGB-Polarization, and RGB-Event combinations.</p><p>In the future, we aim to extend CMX to a three-path architecture for combing three or more types of sensor data. Another possible research direction is to investigate multi-modal pretraining for learning complementary synergies between modalities that are beneficial for downstream semantic segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>Baseline difference map HHA Our difference map GT <ref type="figure">Fig. 9</ref>: Visualization of semantic segmentation results for the RGB-only baseline and our RGB-X approach, both of which are based on SegFormer-B4. "Acc" is short for pixel accuracy of the segmentation result. From left to right: RGB image, baseline difference map w.r.t. the ground truth, HHA image encoding depth information, our difference map, and segmentation ground truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A MORE IMPLEMENTATION DETAILS</head><p>We implement our experiments with PyTorch. We employ a poly learning rate schedule with a factor of 0.9 and an initial learning rate of 6e ?5 . The number of warm-up epochs is 10. We now describe specific implementation details for different datasets. NYU Depth V2 dataset. We train our model with the MiT-B2 backbone on four 2080Ti GPUs, models with MiT-B4 and MiT-B5 backbones on three 3090 GPUs. The number of training epochs is set as 500. We take the whole image with the size 640?480 for training and inference. We use a batch size of 8 for the MiT-B2 backbone and 6 for MiT-B4 and MiT-B5. SUN-RGBD dataset. The models are trained with a batch-size of 4 per GPU. During training, the images are randomly cropped to 480?480. The model based on MiT-B2 is trained on two V100 GPUs for 200 epochs. The models based on MiT-B4 and MiT-B5 are trained on eight V100 GPUs, 250 epochs for MiT-B4 and 300 epochs for MiT-B5.</p><p>Stanford2D3D dataset. The model is trained on four 2080Ti GPUs. The number of training epochs here is set as 32. We resize the input images to 480?480. We use a batch size of 12 for the MiT-B2 backbone and 8 for MiT-B4.</p><p>ScanNetV2 dataset. The model is trained on four 2080Ti GPUs. The number of training epochs here is set as 100. We resize the input RGB images to 640?480. We use a batch size of 12 for the MiT-B2 backbone.</p><p>Cityscapes dataset. The model is trained on eight A100 GPUs for 500 epochs. The batch size is set as 8. The images are randomly cropped into 1024?1024 for training and inference is performed on the full resolution with a sliding window of 512?512. The embedding dimension of the MiT-B4 backbone and MLP-decoder is set as 768.</p><p>RGB-T MFNet dataset. The model is trained on four 2080Ti GPUs. We use the original image size of 640 ? 480 for training and inference. Batch size is set to 8 for the MiT-B2 backbone and we train for 500 epochs. Consistent with the batch size of 8, the model based on MiT-B4 is trained on four A100 GPUs, which requires a larger memory. RGB-P ZJU dataset. The model is trained on four 2080Ti GPUs. We resize the image from 1224?1024 to 612?512. The number of training epochs is set as 400. We use a batch size of 8 for the MiT-B2 backbone and 4 for MiT-B4.</p><p>RGB-E EventScape dataset. The proposed model is trained with a batch size of 4 and the original resolution of 512?256 on a single 1080Ti GPU. The number of training epochs is set as 100.</p><p>The embedding dimension of the MiT-B4 backbone and MLPdecoder is set as 768.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B FAILURE CASE ANALYSES</head><p>In addition to the representative qualitative results and analyses that sufficiently show the effectiveness of CMX, we further present failure case analysis to help gather a more comprehensive understanding of RGB-X semantic segmentation. In <ref type="figure" target="#fig_8">Fig. 11</ref>, we show a set of failure cases in different sensing modality combination scenarios. The first rows shows that for the RGB-D semantic segmentation in a highly composite indoor scene with extremely densely arranged objects, the parsing results are still less visually satisfactory. In the second row of a nighttime scene, the guardrails are mis-classified by the RGB-X method as color cone, despite our model delivering more complete and consistent segmentation than the RGB-only model and having better segmentation of person with thermal properties. This illustrates that at night, the perception of some remote objects is still challenging in RGB-T semantic segmentation and it should be noted for safety-critical applications like automated driving. In the third row, the RGB-P model might be misguided by the polarized background area in an occluded situation, and yields less accurate parsing results, indicating that polarization, as a strong prior for segmentation of specular surfaces like glass and car regions, should be carefully leveraged in unconstrained scenes with a lot of occlusions. In the last row, the fences are partially detected as vehicles in the RGB-E segmentation result, but our model still yields more correctly identified pixels than the RGBonly model by harvesting complementary cues from event data. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 :</head><label>4</label><figDesc>a) Overview of CMX for RGB-X semantic segmentation. The inputs are an RGB image and another modality data (e.g., depth, thermal, polarization, or event). b) Detailed architecture of Cross-Modal Feature Rectification Module (CM-FRM), with colored arrows to represent information flows of the two modalities. c) Detailed architecture of two-stage Feature Fusion Module (FFM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison between event representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Per-class accuracy comparison in IoU between RGB and RGB-Event semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Effect of event representations and time bins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Visualization of semantic segmentation results for RGB-only and RGB-X approaches. We use SegFormer-B2 for RGB segmentation and the proposed approach with the same backbone MiT-B2 and MLP-Decoder for RGB-X segmentation. From top to bottom: RGB-Depth, RGB-Thermal, RGB-Polarization (AoLP), and RGB-Event semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>Visualization of the feature extracted in layer 1 and the rectified feature, and their difference map. ACKNOWLEDGMENTS This work was supported in part by the Federal Ministry of Labor and Social Affairs (BMAS) through the AccessibleMaps project under Grant 01KM151112, in part by the University of Excellence through the "KIT Future Fields" project, in part by the Helmholtz Association Initiative and Networking Fund on the HAICORE@KIT partition, and in part by Hangzhou SurImage Technology Company Ltd.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :</head><label>11</label><figDesc>Visualization of failure cases. We use SegFormer-B2 for RGB segmentation and the proposed approach with the same backbone MiT-B2 and MLP-Decoder for RGB-X segmentation. From top to bottom: RGB-Depth, RGB-Thermal, RGB-Polarization (AoLP), and RGB-Event semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>NYU Depth V2 dataset<ref type="bibr" target="#b27">[28]</ref> contains 1449 RGB-D images with the size 640?480, divided into 795 training images and 654 testing images with annotations on 40 semantic categories. SUN-RGBD dataset. SUN-RGBD dataset<ref type="bibr" target="#b28">[29]</ref> consists of 10335 RGB-D images, including 5285 for training and 5050 for testing, annotated in 37 classes. Following<ref type="bibr" target="#b7">[8]</ref>,<ref type="bibr" target="#b112">[113]</ref>, we randomly crop and resize the input to 480?480. During training, the RGB images are re-scaled to 640?480 to keep the same size as the depth images. During testing, the semantic predictions are resized to the full resolution of 1296?968.Cityscapes dataset. Cityscapes<ref type="bibr" target="#b31">[32]</ref> is an outdoor RGB-D dataset that focuses on semantic understanding of urban road-driving street scenes. It is divided into 2975/500/1525 images in the training/validation/testing splits, both with finely annotated dense labels on 19 classes. The image scenes cover 50 different cities with a full resolution of 2048?1024. RGB-T MFNet dataset. MFNet<ref type="bibr" target="#b8">[9]</ref> is a multi-spectral RGB-Thermal image dataset, which consists of 1569 images annotated in 8 classes at the resolution of 640?480. 820 images are captured during the day and the other 749 are collected at night. The training set contains 50% of the daytime-and 50% of the nighttime images, while the validation-and test set respectively contains 25% of the daytime-and 25% of the nighttime images.</figDesc><table /><note>use multiple RGB-Depth semantic segmentation datases, as well as datasets of RGB-Thermal, RGB-Polarization, RGB-Event modality combinations for verifying our proposed CMX model. NYU Depth V2 dataset.Stanford2D3D dataset. Stanford2D3D dataset [30] is comprised of 70496 RGB-D images with 13 object categories. Follow- ing [14], [15], areas 1, 2, 3, 4, and 6 are used for training and area 5 is for testing. The input image is resized to 480?480. ScanNetV2 dataset. ScanNetV2 [31] is a large-scale benchmark for indoor holistic scene understanding. For its 2D semantic label benchmark, it provides 19466 RGB-D samples for training, 5436 for validation, and 2135 frames for testing. The semantic annota- tions are created on 20 classes. The RGB images are captured at a resolution of 1296?968 and depth at 640?480 pixels.RGB-P ZJU dataset. ZJU-RGB-P [11] is an RGB-Polarization dataset collected by an integrated multi-modal vision sensor designed for automated driving [20] on complex campus street scenes. It is composed of 344 images for training and 50 images for evaluation, both labeled with 8 semantic classes at the pixel level. The input image is resized to 612?512.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Results on NYU Depth V2.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell><cell>Pixel Acc. (%)</cell></row><row><cell>3DGNN [117]</cell><cell>43.1</cell><cell>-</cell></row><row><cell>Kong et al. [118]</cell><cell>44.5</cell><cell>72.1</cell></row><row><cell>LS-DeconvNet [119]</cell><cell>45.9</cell><cell>71.9</cell></row><row><cell>CFN [120]</cell><cell>47.7</cell><cell>-</cell></row><row><cell>ACNet [7]</cell><cell>48.3</cell><cell>-</cell></row><row><cell>RDF-101 [121]</cell><cell>49.1</cell><cell>75.6</cell></row><row><cell>SGNet [16]</cell><cell>51.1</cell><cell>76.8</cell></row><row><cell>ShapeConv [15]</cell><cell>51.3</cell><cell>76.4</cell></row><row><cell>NANet [113]</cell><cell>52.3</cell><cell>77.9</cell></row><row><cell>SA-Gate [8]</cell><cell>52.4</cell><cell>77.9</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>54.1</cell><cell>78.7</cell></row><row><cell>CMX (SegFormer-B2)  *</cell><cell>54.4</cell><cell>79.9</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>56.0</cell><cell>79.6</cell></row><row><cell>CMX (SegFormer-B4)  *</cell><cell>56.3</cell><cell>79.9</cell></row><row><cell>CMX (SegFormer-B5)</cell><cell>56.8</cell><cell>79.9</cell></row><row><cell>CMX (SegFormer-B5)  *</cell><cell>56.9</cell><cell>80.1</cell></row></table><note>* denotes multi-scale test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Results on SUN-RGBD.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell><cell>Pixel Acc. (%)</cell></row><row><cell>3DGNN [117]</cell><cell>45.9</cell><cell>-</cell></row><row><cell>RDF-152 [121]</cell><cell>47.7</cell><cell>81.5</cell></row><row><cell>CFN [120]</cell><cell>48.1</cell><cell>-</cell></row><row><cell>D-CNN [14]</cell><cell>42.0</cell><cell>-</cell></row><row><cell>ACNet [7]</cell><cell>48.1</cell><cell>-</cell></row><row><cell>TCD [122]</cell><cell>49.5</cell><cell>83.1</cell></row><row><cell>SGNet [16]</cell><cell>48.6</cell><cell>82.0</cell></row><row><cell>SA-Gate [8]</cell><cell>49.4</cell><cell>82.5</cell></row><row><cell>NANet [113]</cell><cell>48.8</cell><cell>82.3</cell></row><row><cell>ShapeConv [15]</cell><cell>48.6</cell><cell>82.2</cell></row><row><cell>CMX (SegFormer-B2)  *</cell><cell>49.7</cell><cell>82.8</cell></row><row><cell>CMX (SegFormer-B4)  *</cell><cell>52.1</cell><cell>83.5</cell></row><row><cell>CMX (SegFormer-B5)  *</cell><cell>52.4</cell><cell>83.8</cell></row></table><note>* denotes multi-scale test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Results on the Stanford2D3D dataset<ref type="bibr" target="#b29">[30]</ref>.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell><cell>Pixel Acc. (%)</cell></row><row><cell>Depth-aware CNN [14]</cell><cell>39.5</cell><cell>65.4</cell></row><row><cell>MMAF-Net-152 [123]</cell><cell>52.9</cell><cell>76.5</cell></row><row><cell>ShapeConv-101 [15]</cell><cell>60.6</cell><cell>82.7</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>61.2</cell><cell>82.3</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>62.1</cell><cell>82.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>Results on the ScanNetV2 test set<ref type="bibr" target="#b30">[31]</ref>. Results are obtained from the ScanNetV2 benchmark leaderboard.</figDesc><table><row><cell>Method</cell><cell>Modal</cell><cell>mIoU (%)</cell></row><row><cell>PSPNet [38]</cell><cell>RGB</cell><cell>47.5</cell></row><row><cell>AdapNet++ [125]</cell><cell>RGB</cell><cell>50.3</cell></row><row><cell>3DMV (2d-proj) [126]</cell><cell>RGB-D</cell><cell>49.8</cell></row><row><cell>FuseNet [127]</cell><cell>RGB-D</cell><cell>53.5</cell></row><row><cell>SSMA [125]</cell><cell>RGB-D</cell><cell>57.7</cell></row><row><cell>GRBNet [80]</cell><cell>RGB-D</cell><cell>59.2</cell></row><row><cell>MCA-Net [128]</cell><cell>RGB-D</cell><cell>59.5</cell></row><row><cell>DMMF [129]</cell><cell>RGB-D</cell><cell>59.7</cell></row><row><cell>CMX (Ours)</cell><cell>RGB-D</cell><cell>61.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 :</head><label>5</label><figDesc>Results on the Cityscapes val set in full resolution.</figDesc><table><row><cell>Method</cell><cell>Modal</cell><cell>Backbone</cell><cell>mIoU (%)</cell></row><row><cell>SwiftNet [130]</cell><cell>RGB</cell><cell>ResNet-18</cell><cell>70.4</cell></row><row><cell>ESANet [2]</cell><cell>RGB</cell><cell>ResNet-50</cell><cell>79.2</cell></row><row><cell>GSCNN [52]</cell><cell>RGB</cell><cell>WideResNet-38</cell><cell>80.8</cell></row><row><cell>CCNet [42]</cell><cell>RGB</cell><cell>ResNet-101</cell><cell>81.3</cell></row><row><cell>DANet [26]</cell><cell>RGB</cell><cell>ResNet-101</cell><cell>81.5</cell></row><row><cell>ACFNet [131]</cell><cell>RGB</cell><cell>ResNet-101</cell><cell>81.5</cell></row><row><cell>SegFormer [61]</cell><cell>RGB</cell><cell>MiT-B2</cell><cell>81.0</cell></row><row><cell>SegFormer [61]</cell><cell>RGB</cell><cell>MiT-B4</cell><cell>82.3</cell></row><row><cell>RFNet [1]</cell><cell>RGB-D</cell><cell>ResNet-18</cell><cell>72.5</cell></row><row><cell>PADNet [132]</cell><cell>RGB-D</cell><cell>ResNet-50</cell><cell>76.1</cell></row><row><cell>Kong et al. [118]</cell><cell>RGB-D</cell><cell>ResNet-101</cell><cell>79.1</cell></row><row><cell>ESANet [2]</cell><cell>RGB-D</cell><cell>ResNet-50</cell><cell>80.0</cell></row><row><cell>SA-Gate [8]</cell><cell>RGB-D</cell><cell>ResNet-50</cell><cell>80.7</cell></row><row><cell>SA-Gate [8]</cell><cell>RGB-D</cell><cell>ResNet-101</cell><cell>81.7</cell></row><row><cell>AsymFusion [96]</cell><cell>RGB-D</cell><cell>Xception65</cell><cell>82.1</cell></row><row><cell>SSMA [125]</cell><cell>RGB-D</cell><cell>ResNet-50</cell><cell>82.2</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>RGB-D</cell><cell>MiT-B2</cell><cell>81.6</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>RGB-D</cell><cell>MiT-B4</cell><cell>82.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>Per-class results on the MFNet dataset<ref type="bibr" target="#b8">[9]</ref> for RGB-Thermal semantic segmentation.</figDesc><table><row><cell>Method</cell><cell>Modal</cell><cell>Unlabeled</cell><cell>Car</cell><cell cols="7">Person Bike Curve Car Stop Guardrail Color Cone Bump</cell><cell>mIoU</cell></row><row><cell>ERFNet [133]</cell><cell>RGB</cell><cell>96.7</cell><cell>67.1</cell><cell>56.2</cell><cell>34.3</cell><cell>30.6</cell><cell>9.4</cell><cell>0.0</cell><cell>0.1</cell><cell>30.5</cell><cell>36.1</cell></row><row><cell>DANet [26]</cell><cell>RGB</cell><cell>96.3</cell><cell>71.3</cell><cell>48.1</cell><cell>51.8</cell><cell>30.2</cell><cell>18.2</cell><cell>0.7</cell><cell>30.3</cell><cell>18.8</cell><cell>41.3</cell></row><row><cell>SegNet [5]</cell><cell>RGB</cell><cell>96.7</cell><cell>65.3</cell><cell>55.7</cell><cell>51.1</cell><cell>38.4</cell><cell>10.0</cell><cell>0.0</cell><cell>12.0</cell><cell>51.5</cell><cell>42.3</cell></row><row><cell>CCNet [42]</cell><cell>RGB</cell><cell>97.7</cell><cell>79.5</cell><cell>52.7</cell><cell>56.2</cell><cell>32.2</cell><cell>29.0</cell><cell>1.2</cell><cell>41.0</cell><cell>0.2</cell><cell>43.3</cell></row><row><cell>UNet [134]</cell><cell>RGB</cell><cell>96.9</cell><cell>66.2</cell><cell>60.5</cell><cell>46.2</cell><cell>41.6</cell><cell>17.9</cell><cell>1.8</cell><cell>30.6</cell><cell>44.2</cell><cell>45.1</cell></row><row><cell>PSPNet [38]</cell><cell>RGB</cell><cell>96.8</cell><cell>74.8</cell><cell>61.3</cell><cell>50.2</cell><cell>38.4</cell><cell>15.8</cell><cell>0.0</cell><cell>33.2</cell><cell>44.4</cell><cell>46.1</cell></row><row><cell>APCNet [135]</cell><cell>RGB</cell><cell>97.4</cell><cell>83.0</cell><cell>51.6</cell><cell>58.7</cell><cell>27.0</cell><cell>30.3</cell><cell>11.8</cell><cell>35.6</cell><cell>45.6</cell><cell>49.0</cell></row><row><cell>DUC [136]</cell><cell>RGB</cell><cell>97.7</cell><cell>82.5</cell><cell>69.4</cell><cell>58.9</cell><cell>40.1</cell><cell>20.9</cell><cell>3.4</cell><cell>42.1</cell><cell>40.9</cell><cell>50.7</cell></row><row><cell>HRNet [6]</cell><cell>RGB</cell><cell>98.0</cell><cell>86.9</cell><cell>67.3</cell><cell>59.2</cell><cell>35.3</cell><cell>23.1</cell><cell>1.7</cell><cell>46.6</cell><cell>47.3</cell><cell>51.7</cell></row><row><cell>SegFormer-B2 [61]</cell><cell>RGB</cell><cell>97.9</cell><cell>87.4</cell><cell>62.8</cell><cell>63.2</cell><cell>31.7</cell><cell>25.6</cell><cell>9.8</cell><cell>50.9</cell><cell>49.6</cell><cell>53.2</cell></row><row><cell>SegFormer-B4 [61]</cell><cell>RGB</cell><cell>98.0</cell><cell>88.9</cell><cell>64.0</cell><cell>62.8</cell><cell>38.1</cell><cell>25.9</cell><cell>6.9</cell><cell>50.8</cell><cell>57.7</cell><cell>54.8</cell></row><row><cell>MFNet [9]</cell><cell>RGB-T</cell><cell>96.9</cell><cell>65.9</cell><cell>58.9</cell><cell>42.9</cell><cell>29.9</cell><cell>9.9</cell><cell>0.0</cell><cell>25.2</cell><cell>27.7</cell><cell>39.7</cell></row><row><cell>SA-Gate [8]</cell><cell>RGB-T</cell><cell>96.8</cell><cell>73.8</cell><cell>59.2</cell><cell>51.3</cell><cell>38.4</cell><cell>19.3</cell><cell>0.0</cell><cell>24.5</cell><cell>48.8</cell><cell>45.8</cell></row><row><cell>Depth-aware CNN [14]</cell><cell>RGB-T</cell><cell>96.9</cell><cell>77.0</cell><cell>53.4</cell><cell>56.5</cell><cell>30.9</cell><cell>29.3</cell><cell>8.5</cell><cell>30.1</cell><cell>32.3</cell><cell>46.1</cell></row><row><cell>ACNet [7]</cell><cell>RGB-T</cell><cell>96.7</cell><cell>79.4</cell><cell>64.7</cell><cell>52.7</cell><cell>32.9</cell><cell>28.4</cell><cell>0.8</cell><cell>16.9</cell><cell>44.4</cell><cell>46.3</cell></row><row><cell>PSTNet [137]</cell><cell>RGB-T</cell><cell>97.0</cell><cell>76.8</cell><cell>52.6</cell><cell>55.3</cell><cell>29.6</cell><cell>25.1</cell><cell>15.1</cell><cell>39.4</cell><cell>45.0</cell><cell>48.4</cell></row><row><cell>RTFNet [82]</cell><cell>RGB-T</cell><cell>98.5</cell><cell>87.4</cell><cell>70.3</cell><cell>62.7</cell><cell>45.3</cell><cell>29.8</cell><cell>0.0</cell><cell>29.1</cell><cell>55.7</cell><cell>53.2</cell></row><row><cell>FuseSeg [84]</cell><cell>RGB-T</cell><cell>97.6</cell><cell>87.9</cell><cell>71.7</cell><cell>64.6</cell><cell>44.8</cell><cell>22.7</cell><cell>6.4</cell><cell>46.9</cell><cell>47.9</cell><cell>54.5</cell></row><row><cell>AFNet [85]</cell><cell>RGB-T</cell><cell>98.0</cell><cell>86.0</cell><cell>67.4</cell><cell>62.0</cell><cell>43.0</cell><cell>28.9</cell><cell>4.6</cell><cell>44.9</cell><cell>56.6</cell><cell>54.6</cell></row><row><cell>ABMDRNet [10]</cell><cell>RGB-T</cell><cell>98.6</cell><cell>84.8</cell><cell>69.6</cell><cell>60.3</cell><cell>45.1</cell><cell>33.1</cell><cell>5.1</cell><cell>47.4</cell><cell>50.0</cell><cell>54.8</cell></row><row><cell>FEANet [17]</cell><cell>RGB-T</cell><cell>98.3</cell><cell>87.8</cell><cell>71.1</cell><cell>61.1</cell><cell>46.5</cell><cell>22.1</cell><cell>6.6</cell><cell>55.3</cell><cell>48.9</cell><cell>55.3</cell></row><row><cell>GMNet [19]</cell><cell>RGB-T</cell><cell>97.5</cell><cell>86.5</cell><cell>73.1</cell><cell>61.7</cell><cell>44.0</cell><cell>42.3</cell><cell>14.5</cell><cell>48.7</cell><cell>47.4</cell><cell>57.3</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>RGB-T</cell><cell>98.3</cell><cell>89.4</cell><cell>74.8</cell><cell>64.7</cell><cell>47.3</cell><cell>30.1</cell><cell>8.1</cell><cell>52.4</cell><cell>59.4</cell><cell>58.2</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>RGB-T</cell><cell>98.3</cell><cell>90.1</cell><cell>75.2</cell><cell>64.5</cell><cell>50.2</cell><cell>35.3</cell><cell>8.5</cell><cell>54.2</cell><cell>60.6</cell><cell>59.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc>Segmentation results on daytime-and nighttime images on the MFNet dataset<ref type="bibr" target="#b8">[9]</ref>.</figDesc><table><row><cell>Method</cell><cell>Modal</cell><cell cols="2">Daytime mIoU mAcc</cell><cell cols="2">Nighttime mIoU mAcc</cell></row><row><cell>FRRN [138]</cell><cell>RGB</cell><cell>40.0</cell><cell>45.1</cell><cell>37.3</cell><cell>41.6</cell></row><row><cell>DFN [139]</cell><cell>RGB</cell><cell>38.0</cell><cell>42.4</cell><cell>42.3</cell><cell>46.2</cell></row><row><cell>BiSeNet [140]</cell><cell>RGB</cell><cell>44.8</cell><cell>52.9</cell><cell>47.7</cell><cell>53.1</cell></row><row><cell>SegFormer-B2 [61]</cell><cell>RGB</cell><cell>48.6</cell><cell>60.9</cell><cell>49.2</cell><cell>64.2</cell></row><row><cell>SegFormer-B4 [61]</cell><cell>RGB</cell><cell>49.4</cell><cell>65.0</cell><cell>52.4</cell><cell>61.4</cell></row><row><cell>MFNet [9]</cell><cell>RGB-T</cell><cell>36.1</cell><cell>42.6</cell><cell>36.8</cell><cell>41.4</cell></row><row><cell>FuseNet [127]</cell><cell>RGB-T</cell><cell>41.0</cell><cell>49.5</cell><cell>43.9</cell><cell>48.9</cell></row><row><cell>RTFNet [82]</cell><cell>RGB-T</cell><cell>45.8</cell><cell>60.0</cell><cell>54.8</cell><cell>60.7</cell></row><row><cell>FuseSeg [84]</cell><cell>RGB-T</cell><cell>47.8</cell><cell>62.1</cell><cell>54.6</cell><cell>67.3</cell></row><row><cell>GMNet [19]</cell><cell>RGB-T</cell><cell>49.0</cell><cell>71.0</cell><cell>57.7</cell><cell>71.3</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>RGB-T</cell><cell>51.3</cell><cell>70.2</cell><cell>57.8</cell><cell>67.4</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>RGB-T</cell><cell>52.5</cell><cell>69.5</cell><cell>59.4</cell><cell>69.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>Per-class results on the ZJU-RGB-P dataset<ref type="bibr" target="#b10">[11]</ref> for RGB-Polarization semantic segmentation.</figDesc><table><row><cell>Method</cell><cell>Modal</cell><cell cols="2">Building Glass</cell><cell>Car</cell><cell cols="2">Road Vegetation</cell><cell>Sky</cell><cell cols="2">Pedestrian Bicycle</cell><cell>mIoU</cell></row><row><cell>SwiftNet [130]</cell><cell>RGB</cell><cell>83.0</cell><cell>73.4</cell><cell>91.6</cell><cell>96.7</cell><cell>94.5</cell><cell>84.7</cell><cell>36.1</cell><cell>82.5</cell><cell>80.3</cell></row><row><cell>EAFNet [11]</cell><cell>RGB-P</cell><cell>87.0</cell><cell>79.3</cell><cell>93.6</cell><cell>97.4</cell><cell>95.3</cell><cell>87.1</cell><cell>60.4</cell><cell>85.6</cell><cell>85.7</cell></row><row><cell>NLFNet [112]</cell><cell>RGB-P</cell><cell>85.4</cell><cell>77.1</cell><cell>93.5</cell><cell>97.7</cell><cell>93.2</cell><cell>85.9</cell><cell>56.9</cell><cell>85.5</cell><cell>84.4</cell></row><row><cell>SegFormer-B2 [61]</cell><cell>RGB</cell><cell>90.6</cell><cell>79.0</cell><cell>92.8</cell><cell>96.6</cell><cell>96.2</cell><cell>89.6</cell><cell>82.9</cell><cell>89.3</cell><cell>89.6</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>RGB-AoLP (Monochromatic)</cell><cell>91.9</cell><cell>87.0</cell><cell>95.6</cell><cell>98.2</cell><cell>96.7</cell><cell>89.0</cell><cell>84.9</cell><cell>92.0</cell><cell>91.8</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>RGB-AoLP (Trichromatic)</cell><cell>91.5</cell><cell>87.3</cell><cell>95.8</cell><cell>98.2</cell><cell>96.6</cell><cell>89.3</cell><cell>85.6</cell><cell>91.9</cell><cell>92.0</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>RGB-AoLP (Monochromatic)</cell><cell>91.8</cell><cell>88.8</cell><cell>96.3</cell><cell>98.3</cell><cell>96.7</cell><cell>89.1</cell><cell>86.3</cell><cell>92.3</cell><cell>92.4</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>RGB-AoLP (Trichromatic)</cell><cell>91.6</cell><cell>88.8</cell><cell>96.3</cell><cell>98.3</cell><cell>96.8</cell><cell>89.7</cell><cell>86.2</cell><cell>92.8</cell><cell>92.6</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>RGB-DoLP (Monochromatic)</cell><cell>91.4</cell><cell>87.6</cell><cell>96.0</cell><cell>98.2</cell><cell>96.6</cell><cell>89.1</cell><cell>87.1</cell><cell>92.3</cell><cell>92.1</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>RGB-DoLP (Trichromatic)</cell><cell>91.8</cell><cell>87.8</cell><cell>96.1</cell><cell>98.2</cell><cell>96.7</cell><cell>89.4</cell><cell>86.1</cell><cell>91.8</cell><cell>92.2</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>RGB-DoLP (Monochromatic)</cell><cell>91.8</cell><cell>88.6</cell><cell>96.3</cell><cell>98.3</cell><cell>96.7</cell><cell>89.4</cell><cell>86.0</cell><cell>92.1</cell><cell>92.4</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>RGB-DoLP (Trichromatic)</cell><cell>91.6</cell><cell>88.6</cell><cell>96.3</cell><cell>98.3</cell><cell>96.7</cell><cell>89.5</cell><cell>86.4</cell><cell>92.2</cell><cell>92.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 :</head><label>9</label><figDesc>Results for RGB-Event semantic segmentation.</figDesc><table><row><cell>Method</cell><cell cols="4">Modal Backbone mIoU (%) Pixel Acc. (%)</cell></row><row><cell>SwiftNet [130]</cell><cell cols="2">RGB ResNet-18</cell><cell>36.67</cell><cell>83.46</cell></row><row><cell>Fast-SCNN [141]</cell><cell cols="2">RGB Fast-SCNN</cell><cell>44.27</cell><cell>87.10</cell></row><row><cell>CGNet [142]</cell><cell>RGB</cell><cell>M3N21</cell><cell>44.75</cell><cell>87.13</cell></row><row><cell>Trans4Trans [3]</cell><cell>RGB</cell><cell>PVT-B2</cell><cell>51.86</cell><cell>89.03</cell></row><row><cell>Swin-s [24]</cell><cell>RGB</cell><cell>Swin-s</cell><cell>52.49</cell><cell>88.78</cell></row><row><cell>Swin-b [24]</cell><cell>RGB</cell><cell>Swin-b</cell><cell>53.31</cell><cell>89.21</cell></row><row><cell>DeepLabV3+ [143]</cell><cell cols="2">RGB ResNet-101</cell><cell>53.65</cell><cell>89.92</cell></row><row><cell>SegFormer-B2 [61]</cell><cell>RGB</cell><cell>MiT-B2</cell><cell>58.69</cell><cell>91.21</cell></row><row><cell>SegFormer-B4 [61]</cell><cell>RGB</cell><cell>MiT-B4</cell><cell>59.86</cell><cell>91.61</cell></row><row><cell>RFNet [1]</cell><cell cols="2">RGB-E ResNet-18</cell><cell>41.34</cell><cell>86.25</cell></row><row><cell>ISSAFE [12]</cell><cell cols="2">RGB-E ResNet-18</cell><cell>43.61</cell><cell>86.83</cell></row><row><cell>SA-Gate [8]</cell><cell cols="2">RGB-E ResNet-101</cell><cell>53.94</cell><cell>90.03</cell></row><row><cell cols="3">CMX (DeepLabV3+) RGB-E ResNet-101</cell><cell>54.91</cell><cell>89.67</cell></row><row><cell>CMX (Swin-s)</cell><cell cols="2">RGB-E Swin-s</cell><cell>60.86</cell><cell>91.25</cell></row><row><cell>CMX (Swin-b)</cell><cell cols="2">RGB-E Swin-b</cell><cell>61.21</cell><cell>91.61</cell></row><row><cell cols="3">CMX (SegFormer-B2) RGB-E MiT-B2</cell><cell>61.90</cell><cell>91.88</cell></row><row><cell cols="3">CMX (SegFormer-B4) RGB-E MiT-B4</cell><cell>64.28</cell><cell>92.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 10 :</head><label>10</label><figDesc>Comparison of event representations and time bins.</figDesc><table><row><cell>Time bins</cell><cell cols="2">mIoU (%) Pixel Acc. (%)</cell></row><row><cell cols="3">Original representation [33]</cell></row><row><cell>B=1</cell><cell>61.30</cell><cell>91.75</cell></row><row><cell>B=3</cell><cell>61.28</cell><cell>91.64</cell></row><row><cell>B=5</cell><cell>60.32</cell><cell>91.38</cell></row><row><cell>B=10</cell><cell>60.48</cell><cell>91.64</cell></row><row><cell>B=15</cell><cell>60.04</cell><cell>91.44</cell></row><row><cell>B=20</cell><cell>60.93</cell><cell>91.74</cell></row><row><cell>B=30</cell><cell>59.60</cell><cell>91.15</cell></row><row><cell></cell><cell>Ours</cell><cell></cell></row><row><cell>B=1</cell><cell>61.47</cell><cell>91.92</cell></row><row><cell>B=3</cell><cell>61.90</cell><cell>91.86</cell></row><row><cell>B=5</cell><cell>61.23</cell><cell>91.92</cell></row><row><cell>B=10</cell><cell>61.33</cell><cell>91.78</cell></row><row><cell>B=15</cell><cell>60.60</cell><cell>91.76</cell></row><row><cell>B=20</cell><cell>61.62</cell><cell>91.93</cell></row><row><cell>B=30</cell><cell>61.23</cell><cell>91.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 11 :</head><label>11</label><figDesc>Ablation for CM-FRM/FFM on NYU Depth V2 test set.</figDesc><table><row><cell cols="2">Feature Rectify Feature Fusion</cell><cell cols="2">mIoU (%) Pixel Acc. (%)</cell></row><row><cell>No</cell><cell>Avg.</cell><cell>50.3</cell><cell>76.8</cell></row><row><cell>CM-FRM</cell><cell>Avg.</cell><cell>52.8</cell><cell>78.0</cell></row><row><cell>No</cell><cell>FFM</cell><cell>51.5</cell><cell>77.1</cell></row><row><cell>CM-FRM</cell><cell>FFM</cell><cell>54.1</cell><cell>78.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 13 :</head><label>13</label><figDesc>Ablation for the second modality on NYU Depth V2 test set.</figDesc><table><row><cell>Modalities</cell><cell cols="2">mIoU (%) Pixel Acc. (%)</cell></row><row><cell>RGB</cell><cell>46.7</cell><cell>73.8</cell></row><row><cell>RGB + RGB</cell><cell>47.2</cell><cell>74.1</cell></row><row><cell>RGB + Noise</cell><cell>47.7</cell><cell>74.5</cell></row><row><cell>RGB + Raw depth</cell><cell>51.1</cell><cell>75.7</cell></row><row><cell>RGB + HHA</cell><cell>52.0</cell><cell>77.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 14 :</head><label>14</label><figDesc>Efficiency results. FLOPs are estimated for inputs of RGB (480?640?3) and HHA (480?640?3).</figDesc><table><row><cell>Method</cell><cell cols="3">Params/M FLOPs/G mIoU (%)</cell></row><row><cell>SA-Gate [8] (ResNet50)</cell><cell>63.4</cell><cell>204.9</cell><cell>50.4</cell></row><row><cell>CMX (SegFormer-B2)</cell><cell>66.6</cell><cell>67.6</cell><cell>54.1</cell></row><row><cell>CMX (SegFormer-B4)</cell><cell>139.9</cell><cell>134.3</cell><cell>56.0</cell></row><row><cell>CMX (SegFormer-B5)</cell><cell>181.1</cell><cell>167.8</cell><cell>56.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time fusion network for RGB-D semantic segmentation incorporating unexpected obstacle detection for road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RA-L</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5558" to="5565" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Efficient RGB-D semantic segmentation for indoor scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Trans4Trans: Efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ACNet: Attention based network to exploit complementary features for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bi-directional cross-modality feature propagation with separation-andaggregation gate for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MFNet: Towards real-time semantic segmentation for autonomous vehicles with multi-spectral scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ABM-DRNet: Adaptive-weighted bi-directional modality difference reduction network for RGB-T semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Polarization-driven semantic segmentation via efficient attention-bridged fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4802" to="4820" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ISSAFE: Improving semantic segmentation in accidents by fusing event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion for semantic image segmentation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidib?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>M?riaudeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page">104042</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth-aware CNN for RGB-D segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ShapeConv: Shape-aware convolutional layer for indoor RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial information guided convolution for real-time RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2313" to="2324" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FEANet: Feature-enhanced attention network for RGB-thermal real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zig-zag network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2642" to="2655" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GMNet: Gradedfeature multilabel-learning network for RGB-thermal urban scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="7790" to="7802" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A multimodal vision sensor for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Omnivore: A single model for many visual modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Combining events and frames using recurrent asynchronous multimodal networks for monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?egg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hidalgo-Carri?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RA-L</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2822" to="2829" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large kernel matters-Improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DenseASPP for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Strip pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">OCNet: Object context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2375" to="2398" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Covariance attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lasang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1805" to="1818" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">CTNet: Context-based tandem network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mining contextual information beyond image for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ISNet: Integrate image-level and semantic-level context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Exploring cross-image pixel contrast for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gated-SCNN: Gated shape CNNs for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via decoupled body and edge supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">PSANet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Expectationmaximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cars can&apos;t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">SegFormer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">UniFormer: Unifying convolution and self-attention for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09450</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">MaX-DeepLab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">P2T: Pyramid pooling transformer for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12011</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">HRFormer: High-resolution transformer for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">MPViT: Multi-path vision transformer for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Se-Mask: Semantically masked transformers for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12782</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Semantic segmentation by early region proxy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Lawin transformer: Improving semantic segmentation transformer with multi-scale representations via large window attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01615</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">StructToken : Rethinking semantic segmentation with structural prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12612</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Dynamic focus-aware positional queries for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01244</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">SCN: Switchable context network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1120" to="1131" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Variational contextdeformable ConvNets for indoor scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep feature selection-and-fusion for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Gated-residual block for semantic segmentation using RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">CANet: Co-attention network for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">108468</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">RTFNet: RGB-thermal fusion network for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RA-L</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2576" to="2583" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Segmenting objects in day and night: Edge-conditioned CNN for thermal image semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3069" to="3082" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">FuseSeg: Semantic segmentation of urban scenes based on RGB and thermal data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ASE</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1000" to="1011" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Attention fusion network for multi-spectral semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="179" to="184" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Outdoor scenes pixel-wise semantic segmentation using polarimetry and fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blanchon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Seulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crombez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidib?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>VISAPP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deep polarization cues for transparent object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Taamazyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Glass segmentation using intensity and spectral polarization cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">EV-SegNet: Semantic segmentation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Exploring event-driven dynamic context for accident scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2606" to="2622" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Optical flow augmented semantic segmentation networks for automated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krizek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Helw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>VISAPP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Perceptionaware multi-sensor fusion for 3D LiDAR semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Malleable 2.5D convolution: Learning receptive fields along the depth-axis for RGB-D scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Depth-adapted CNN for RGB-D cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Allibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning deep multimodal feature representation with asymmetric multi-layer fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Bidirectional projection network for cross dimension scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Channel exchanging networks for multimodal and multitask dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02252</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Tube-embedded transformer for pixel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">TMM</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Multi-MAE: Multi-modal multi-task masked autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mizrahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01678</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Trear: Transformerbased RGB-D egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCDS</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="246" to="252" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">MDETR-modulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">TriTransNet: RGB-D salient object detection with a triplet transformer embedding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">SwinNet: Swin transformer drives edge-aware RGB-D and RGB-T salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">TCSVT</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">SCA-CNN: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Efficient attention: Attention with linear complexities,&quot; in WACV</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">ConvMLP: hierarchical convolutional MLPs for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04454</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">NLFNet: Non-local fusion towards generalized multimodal semantic segmentation across RGB-depth, polarization, and thermal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<editor>ROBIO</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Non-local aggregation for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="658" to="662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">3D graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Locality-sensitive deconvolution networks with gated fusion for RGB-D indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">RDFNet: RGB-D multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Two-stage cascaded decoder for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1115" to="1119" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Multi-modal attention-based fusion model for semantic segmentation of RGB-depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fooladgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11691</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Self-supervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1239" to="1285" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">3DMV: Joint 3D-multi-view prediction for 3D semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">FuseNet: Incorporating depth into semantic segmentation via fusion-based CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Multilevel cross-aware RGBD indoor semantic segmentation for bionic binocular robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-MRB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="382" to="390" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">RGB-D semantic segmentation and label-oriented voxelgrid fusion for accurate 3D semantic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="197" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">In defense of pretrained ImageNet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">ACFNet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">PAD-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized ConvNet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Adaptive pyramid context network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">PST900: RGB-thermal calibration, dataset and segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">BiSeNet: Bilateral segmentation network for real-time semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Fast-SCNN: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P K</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">CGNet: A light-weight context guided network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1169" to="1179" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
