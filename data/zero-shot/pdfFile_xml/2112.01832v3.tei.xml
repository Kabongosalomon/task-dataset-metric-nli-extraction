<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MoE Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">AIMC Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aozhu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MoE Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">AIMC Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MoE Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">AIMC Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MoE Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">AIMC Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Computer and Information Engineering</orgName>
								<orgName type="institution">Zhejiang Gongshang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MoE Key Lab of DEKE</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">AIMC Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Text-to-video retrieval, video/text feature fusion</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we revisit feature fusion, an old-fashioned topic, in the new context of text-to-video retrieval. Different from previous research that considers feature fusion only at one end, let it be video or text, we aim for feature fusion for both ends within a unified framework. We hypothesize that optimizing the convex combination of the features is preferred to modeling their correlations by computationally heavy multihead self attention. We propose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature fusion at both early and late stages and at both video and text ends, making it a powerful method for exploiting diverse (off-the-shelf) features. The interpretability of LAFF can be used for feature selection. Extensive experiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and TRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text-to-video retrieval is to retrieve videos w.r.t. to an ad-hoc textual query from many unlabeled videos. Both video and text have to be embedded into one or more cross-modal common spaces for text-to-video matching. The stateof-the-art tackles the task in different approaches, including novel networks for query representation learning <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b64">65]</ref>, multi-modal Transformers for video representation learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>, hybrid space learning for interpretable cross-modal matching <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60]</ref>, and more recently CLIP2Video <ref type="bibr" target="#b16">[17]</ref> that learns text and video representations in an end-to-end manner. Differently, we look into feature fusion, an important yet largely underexplored topic for text-to-video retrieval.</p><p>Given video/text samples represented by diverse features, feature fusion aims to answer a basic research question of what is the optimal way to combine these features? By optimal we mean the fusion shall maximize the retrieval performance. Meanwhile, the fusion process shall be explainable to interpret the importance of the individual features. As the use of each feature introduces extra computational and storage overheads, the explainability is crucial for the fusion process to be selective to balance the performance and the cost.</p><p>Feature fusion is not new by itself. In fact, the topic has been extensively studied in varied contexts such as multimedia content analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50]</ref> and multimodal or multi-view image classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b63">64]</ref>. These earlier efforts focus on combining hand-crafted features, because such kinds of features are known to be domain-specific, suffering from the semantic gap problem <ref type="bibr" target="#b48">[49]</ref>, and thus insufficient for content representation when used alone. While current deep learning features are already more powerful than their predecessors, no single feature appears to rule all. Dark knowledge about objects and scenes is better carried in pre-trained 2D convolutional neural networks (2D-CNNs) <ref type="bibr" target="#b47">[48]</ref>, while 3D-CNNs are more suited for representing actions and motions <ref type="bibr" target="#b19">[20]</ref>. For text-to-video retrieval, there are some initial efforts on combining diverse deep video features, e.g. JE <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, CE <ref type="bibr" target="#b34">[35]</ref> and MMT <ref type="bibr" target="#b18">[19]</ref>, whilst W2VV++ <ref type="bibr" target="#b27">[28]</ref> and SEA <ref type="bibr" target="#b30">[31]</ref> show the potential of combining different text features for better query representation. The recent CLIP series <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref>, due to their end-to-end learning paradigm, actually lacks the ability of exploiting existing features. Therefore, even in the era of deep learning, the need for feature fusion remains strong.</p><p>Concerning approaches to feature fusion, vector concatenation is commonly used when combining features at an early stage <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>. As for late fusion, multiple feature-specific common spaces are learned in parallel, with the resultant similarities combined either by averaging <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref>, empirical weighting <ref type="bibr" target="#b40">[41]</ref> or by Mixture of Experts (MoE) ensembles <ref type="bibr" target="#b34">[35]</ref>. As the number of features grows, vector concatenation suffers from the curse of dimensionality, while constructing common spaces per feature lacks inter-feature interactions. Moreover, the prior works focus either on the video end or on the text end. To the best of our knowledge, no attempt is made to develop a unified learning-based approach that works for both ends in the context of text-to-video retrieval, see Tab. 1.</p><p>One might consider feature fusion by Multi-head Self-Attention (MHSA), the cornerstone of Transformers <ref type="bibr" target="#b55">[56]</ref>. As <ref type="figure" target="#fig_0">Fig. 1(a)</ref> shows, MHSA transforms a specific feature by blending it with information from all other features, with the blending weights produced by a self-attention mechanism termed QKV. Note that the module was initially developed for NLP tasks, for which exploiting element-wise correlations is crucial for resolving semantic ambiguity. However, as video features extracted by distinct 2D-CNNs and 3D-CNNs are meant for describing the video content from different aspects, we conjecture that optimizing their combination is preferred to modeling their correlations. Moreover, the selfattention in MHSA, computed by Softmax(( QK T ? dv )V ), depends largely on interfeature correlations. It thus tends to have a group effect that features related to each other will be more attended. Consequently, the related yet relatively weak features will be over-emphasized. Hence, despite its high prevalence in varied contexts, we consider MHSA suboptimal for the current task.</p><p>We propose in this paper a much simplified feature fusion block, termed Lightweight Attention Feature Fusion (LAFF), see <ref type="figure" target="#fig_0">Fig. 1</ref>  convex manner in a specific LAFF block, with the combination weights learned to optimize cross-modal text-to-video matching. Performing fusion at the feature level, LAFF can thus be viewed as an early fusion method. Meanwhile, with the multi-head trick as used in MHSA, multiple LAFFs can be deployed within a single network, with their resultant similarities combined in a late fusion manner. The ability to perform feature fusion at both early and late stages and at both video and text ends makes LAFF a powerful method for exploiting diverse, multi-level (off-the-shelf) features for text-to-video retrieval. In sum, our main contributions are as follows:</p><p>? We are the first to study both video-end and text-end feature fusion for text-tovideo retrieval. Given the increasing availability of deep vision/language models for feature extraction, this paper presents an effective mean to harness such dark knowledge for tackling the task.</p><p>? We propose LAFF, a lightweight feature fusion block, capable of performing fusion at both early and late stages. Compared to MHSA, LAFF is much more compact yet more effective. Its attentional weights can also be used for selecting fewer features, with the retrieval performance mostly preserved.</p><p>? Experiments on five benchmarks, i.e. MSR-VTT, MSVD, TGIF, VATEX and TRECVID AVS 2016-2020, show that the LAFF-based video retrieval model ( <ref type="figure">Fig. 2)</ref> compares favorably against the state-of-the-art, resulting in a strong baseline for text-to-video retrieval. Code is available at GitHub 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Feature fusion for text-to-video retrieval. Previous methods on feature fusion focus either on the video end or on the text end, see Tab. 1. For videoend feature fusion, earlier works often simply use vector concatenation to merge multiple features in advance to cross-modal representation learning <ref type="bibr" target="#b13">[14]</ref>. JE <ref type="bibr" target="#b41">[42]</ref> and its journal extension <ref type="bibr" target="#b40">[41]</ref> have made an initial attempt to combine diverse video features by late fusion, where multiple feature-specific common spaces are learned. Multi-space similarities are either averaged <ref type="bibr" target="#b41">[42]</ref> or linearly combined with empirical weights <ref type="bibr" target="#b40">[41]</ref>. CE <ref type="bibr" target="#b34">[35]</ref> also uses late fusion, but resorts to a learning based method, i.e. Mixture of Experts (MoE), to determine the fusion weights on the fly. MMT <ref type="bibr" target="#b18">[19]</ref> improves over CE by first using a multi-modal Transformer to aggregate features from the frame level to the video level, and later using MoE for combining multi-space similarities. JE, CE and MMT all use a single text feature, so these methods leave text-end feature fusion untouched.</p><p>As for text-end feature fusion, W2VV++ <ref type="bibr" target="#b27">[28]</ref> takes an early fusion approach, combining the output of three text encoders, i.e. bag-of-words, word2vec and GRU, by vector concatenation. By contrast, SEA <ref type="bibr" target="#b30">[31]</ref> opts for late fusion, first building a common space per text feature and then averaging the similarities computed within the individual spaces. The more recent TEACHTEXT <ref type="bibr" target="#b10">[11]</ref> first trains multiple CEs with different text encoders, and later combines these models by late fusion with MoE-predicted weights. <ref type="table">Table 1</ref>: Taxonomy of feature fusion methods for text-to-video retrieval. Note that feature fusion is conceptually different from multi-level feature learning, e.g. JPoSE <ref type="bibr" target="#b58">[59]</ref>, PIE-Net <ref type="bibr" target="#b51">[52]</ref> and Dual Encoding <ref type="bibr" target="#b14">[15]</ref>, where new features are first computed at varied levels from a single feature input and combined later. So research in that line is excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Feature Attentional feature fusion in other contexts. LAFF is conceptually different from context gating modules <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>, which aim to re-weight each dimension of a given feature vector, and thus produce weight per dimension. By contrast, as LAFF is to combine multiple features, it produces weight per feature vector. LAFF is technically inspired by attention-based multiple instance learning (MIL) <ref type="bibr" target="#b21">[22]</ref>, wherein there is a need of aggregating multiple instance-level features into a case-level feature. However, the text-to-video retrieval task differs from MIL in the following two aspects, making the attention-based MIL not directly applicable in the new context. First, instances in a MIL setting are of the same modality, e.g. patches taken from the same image <ref type="bibr" target="#b21">[22]</ref> or images from an image array <ref type="bibr" target="#b26">[27]</ref>, so the instance-level features are homogeneous and directly comparable. By contrast, the video or text features to be fused are obtained by distinct feature extractors with varied feature dimensions and are thus incompatible. Second, MIL is typically exploited in the context of a classification task, so the case-level feature is used as input to a classification layer. By contrast, our fused features are meant for cross-modal matching. Hence, a feature fusion block at one end shall be used in pair with a fusion block at the other end. The technical novelty of LAFF lies in its overall design that effectively answers the unique challenges in video/text feature fusion for text-to-video retrieval. While combining homogeneous features have been studied in other contexts <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b57">58]</ref>, our paper fills the gap between diverse feature fusion and text-to-video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A New Baseline</head><p>We propose trainable feature fusion for both video and text ends. Specifically, suppose we have a specific video x represented by a set of k 1 video-level features, {f v,1 (x), . . . , f v,k1 (x)}, and a specific textual query q represented by a set of k 2 sentence-level features {f t,1 (q), . . . , f t,k2 (q)}. We shall construct two feature fusion blocks to encode respectively the video and the query into their d-dimensional cross-modal embeddings e(x) and e(q). Their semantic similarity s(x, q) is measured in terms of the two embeddings accordingly, i.e.</p><formula xml:id="formula_0">? ? ? e(x) := f usion v ({f v,1 (x), . . . , f v,k1 (x)}), e(q) := f usion t ({f t,1 (q), .</formula><p>. . , f t,k2 (q)}), s(x, q) := similarity(e(x), e(q)).</p><p>(1)</p><p>As such, text-to-video retrieval for the given query q is achieved by sorting all videos in a test collection in light of their s(x, q) in descending order. In what follows, we describe the proposed LAFF as a unified implementation of the fusion blocks in Eq. (1), followed by its detailed usage for text-to-video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The LAFF Block</head><p>Without loss of generality, we are provided with a diverse set of k different features {f 1 , . . . , f k }, sized as d 1 , . . . , d k , respectively. As the features are obtained by distinct extractors and thus incompatible, we shall use a feature transformation layer to rectify the diverse features to be of the same length. To convert the i-th feature to a new d-dimensional feature, we use </p><formula xml:id="formula_1">f ? i = ?(Linear di?d (f i )),<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Conceptual diagram of using paired LAFF blocks for text-tovideo retrieval. Given a specific video x, we employ multiple (pre-trained) feature extractors to obtain a set of</p><formula xml:id="formula_2">k 1 video-level features {f v,1 (x), . . . , f v,k1 (x)}.</formula><p>In a similar manner we extract from a query sentence q a set of k 2 sentencelevel features {f t,1 (q), . . . , f t,k2 (q)}. Each pair of LAFFs determines a common space. In particular, a specific pair of LAFFs, indexed by i, aggregates the video (sentence) features into a d-dimensional cross-modal feature e i (x) (e i (q)), and consequently computes the video-text similarity s i (x, q) per space. The sum of the similarities from all h common spaces, i.e.</p><formula xml:id="formula_3">h i=1 s i (x, q), is used for retrieval.</formula><p>where ? is a nonlinear activation function. As the output of the non-linear activation in LAFF is to calculate the cosine similarity, we use tanh in this work 5 . The notation of Linear di?d indicates a fully connected layer with an input of size d i and an output of size of d. Each input feature has its own Linear, optional when d i equals to d.</p><p>Although the transformed features {f ? i } are now comparable, they are not equally important for representing the video/text content. We thus consider a weighted fusion, i.e.f</p><formula xml:id="formula_4">= k i a i f ? i ,<label>(3)</label></formula><p>with weights {a 1 , . . . , a k } computed by a lightweight attentional layer as follow,</p><formula xml:id="formula_5">{a 1 , . . . , a k } = sof tmax(Linear d?1 ({f ? 1 , . . . , f ? k })).<label>(4)</label></formula><p>As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, the Attention-free feature fusion block is a special case of LAFF when enforcing the weights in Eq. (3) to be uniform, i.e. a i = 1 k . Compared to Attention-free, LAFF has d more parameters to learn, see Tab. 2.</p><p>Such a small amount of extra parameters turn out to be important for improving the effectiveness of feature fusion, as our ablation study will show. Compared with MHSA, LAFF has much fewer trainable parameters and is thus more dataefficient. Furthermore, as the attentional weights of LAFF are directly used for a convex combination of the features, LAFF is more interpretable than MHSA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Paired LAFFs for Text-to-Video Retrieval</head><p>Network Architecture We now detail the usage of LAFF for text-to-video retrieval. A straightforward solution is to substitute LAFF for the f usion functions in Eq. (1). As such, we have a single configuration of how the video/text features are combined. However, due to the high complexity of the video and text contents, we hypothesize that the single configuration is suboptimal for cross-modal representation and matching. Borrowing the multi-head idea of MHSA, we consider multi-head LAFF. In particular, we deploy h pairs of LAFFs, where each pair of LAFFs jointly determine a latent common space for video-text matching. In particular, a specific pair of LAFFs, denoted as &lt; LAF F v,i , LAF F t,i &gt;, aggregates the video/text features into a d-dimensional cross-modal embedding vector e i (x)/e i (q), i.e.</p><formula xml:id="formula_6">? ? ? e i (x) = LAF F v,i (x) e i (q) = LAF F t,i (q) s i (x, q) = similarity(e i (x), e i (q))<label>(5)</label></formula><p>where similarity is the widely used cosine similarity. Accordingly, we compute the final video-text similarity as the mean of the h individual similarities,</p><formula xml:id="formula_7">s(x, q) = 1 h h i=1 s i (x, q).<label>(6)</label></formula><p>The overall architecture is illustrated in <ref type="figure">Fig. 2</ref>. In order to make the amount of trainable parameters invariant with respect to h, we set d = d0 h , where d 0 is a constant empirically set to 2,048. As such, the multi-head version of LAFF is not an ensemble. We use h = 8, unless otherwise stated.</p><p>LAFF for multi-level feature fusion. So far we presume the features to be fused are already at the video level. In fact, for its high flexibility, LAFF can be extended with ease to a multi-level variant to deal with the situation wherein different frame-level and video-level features coexist. <ref type="figure" target="#fig_2">Fig. 3</ref> shows this variant, which we term LAFF-ml. LAFF-ml works in a bottom-up manner, where a set of specific frame-level features are aggregated via a specific LAFF block to produce a video-level feature. Suppose there are two different frame-level features, e.g. clip and rx101. Each will have its own LAFF block. The (resultant) different video features are then fused via a video-level LAFF block.  Network Training Following the good practice of the previous work, we adopt as our base loss function the triplet ranking loss with hard-negative mining <ref type="bibr" target="#b15">[16]</ref>. For a specific sentence q in a given training batch, let x + and x ? be videos relevant and irrelevant w.r.t. q, and x * ? be the hard negative that violates the ranking constraint the most. We have</p><formula xml:id="formula_8">x * ? = argmax x? (s(x ? , q) ? s(x + , q)) loss(q) = max(0, ? + s(x * ? , q) ? s(x + , q)),<label>(7)</label></formula><p>where ? is a positive hyper-parameter controlling the margin of the ranking loss. As <ref type="bibr" target="#b30">[31]</ref> has documented, when training a cross-modal network that produces multiple similarities, combining losses per similarity gives better results than using a single loss with the combined similarity. Hence, we follow this strategy, computing loss i (q), namely the loss in the i-th space by substituting s i for s in Eq. <ref type="bibr" target="#b6">(7)</ref>. The network is trained to minimize a combined loss h i=1 loss i (q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In order to evaluate the effectiveness of LAFF, we conduct a series of experiments. An ablation study is performed on MSR-VTT, a de facto benchmark, to evaluate LAFF in multiple aspects. We then compare our LAFF-based retrieval model with the state-of-the-art on MSR-VTT and three other popular benchmarks including MSVD, TGIF and VATEX. In order to assess our retrieval method on a much larger collection, a post-competition evaluation is conducted on the TRECVID AVS benchmark series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Common Setups</head><p>Implementation Details. Eight video features and five text features are used, see Tab. 3. The margin ? in the loss is set to 0.2 according to VSE++ <ref type="bibr" target="#b15">[16]</ref>. We perform SGD based training, with a mini-batch size of 128 and RMSProp as the optimizer. The learning rate is initially set to 10 ?4 , decayed by a factor of 0.99 per epoch. Following <ref type="bibr" target="#b22">[23]</ref>, we half the learning rate if the validation performance does not increase in three consecutive epochs. Early stop occurs when no validation performance increase is achieved in ten consecutive epochs. The dropout rate of the Linear layers is set to 0.2. All experiments were done with PyTorch (1.7.1) <ref type="bibr" target="#b44">[45]</ref> on an Nvidia GEFORCE GTX 2080Ti GPU.</p><p>Evaluation Criteria. We report three standard rank-based metrics: Recall at Rank N (R@N, N=1, 5, 10), Median rank (Med r), and mean Average Precision (mAP) for assessing the overall ranking quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Our ablation study is conducted on MSR-VTT <ref type="bibr" target="#b62">[63]</ref>, which has 10k videos in total, each associated with 20 captions. We adopt the official data split: 6,513 videos for training, 497 videos for validation and the remaining 2,990 images for test. In order to distinguish this data split from other customized splits, e.g. JSFusion <ref type="bibr" target="#b66">[67]</ref>, we term the split MV-test3k. On Combining Diverse Video/Text Features. We investigate how LAFF responds when diverse video/text features are gradually added. For the ease of lateral comparison, we include as baselines the following two models: W2VV++ <ref type="bibr" target="#b27">[28]</ref>, which simply uses vector concatenation, and SEA <ref type="bibr" target="#b30">[31]</ref> which learns crossmodal similarities per text feature.  Given the many video and text features investigated in this work, a complete enumeration of video-text feature combinations is impractical. We choose to reduce the computation by only varying the features at one end, with features at the other end fixed. <ref type="figure" target="#fig_4">Fig. 4(a)</ref> shows the performance curves of W2VV++, SEA and LAFF w.r.t. text features, with {rx101, re152 } as their common video features. The performance of all three models improves at the earlier steps when few features are fused. There is a noticeable drop in the performance curve of W2VV++ when bert is included. LAFF is more effective and more stable. Similar results can be observed from 4(b), which shows the performance curves of the three models w.r.t. video features. The above results justify the effectiveness of LAFF for combining diverse video/text features.</p><p>Comparing Feature Fusion Blocks. We compare the three feature fusion blocks by replacing LAFF in <ref type="figure">Fig. 2</ref> with MHSA and Attention-free, respectively. For a more fair comparison, we also apply the multi-loss trick on MHSA by optimizing losses for different heads, denoted as MHSA(multi-loss). Moreover, we include as a baseline method that uses the simple feature concatenation strategy, as previously adopted in W2VV++ <ref type="bibr" target="#b27">[28]</ref>. The performance of text-to-video retrieval with specific feature fusion blocks is reported in Tab. 4. LAFF performs the best, followed by Attention-free, the concatenation baseline and MHSA. Attention-free, while being extremely simple, is more effective than MHSA for combining the increasing amounts of text features, with its mAP increases from 0.264, 0.321 to 0.326. The superior performance of LAFF against Attention-free (0.358 versus 0.326) justifies the necessity of the attentional layer. LAFF Weights for Model Interpretability and Feature Selection. <ref type="figure" target="#fig_5">Fig. 5</ref> visualizes the LAFF weights of videos and their associated captions selected from the MV-test3k test set. We observe that 3D-CNN features receive more weight when the video content contains more motions, see <ref type="figure" target="#fig_5">Fig. 5(b)</ref>. For each feature, its weight averaged over samples reflects its contribution to the retrieval performance. The weights of text features in descending order are clip (64.3%), bow (15.7%), gru (9.5%), w2v (6.5%), bert (4.0%). For video feaetures, the order is clip (38.0%), x3d (16.8%), ircsn (13.3%), tf (10.9%), rx101 (7.0%), wsl (6.6%) , c3d (5.1%), re152 (1.4%). We re-train our model with the top-3 ranked video / text features. Compared to the full setup (mAP of 0.358), the reduced model obtains mAP of 0.353, meaning a relatively small performance loss of 1.4%. Hence, the LAFF weights are helpful for feature selection.</p><p>Combined Loss versus Single Loss. As Tab. 5 shows, LAFF trained with the combined loss produces a relative improvement of over 10% in terms of mAP, when compared to its single-loss counterpart.</p><p>Hillary Clinton gives a speech on race.</p><p>A father and daughter are performing a dance routine.</p><p>Two male commentators discussing a football game.</p><p>People are doing martial arts kicks.</p><p>A girl does a cartwheel on her front lawn.</p><p>Two male commentators discussing a football game.   The Effect of the Number of Common Spaces. Concerning the number of common spaces h, we try different values, i.e. {1, 2, 4, 8, 16}. As shown in Tab. 6, the performance improves as h increases, with the peak performance reached at h = 8. While using a larger h is beneficial, the relatively small gap between LAFF(h=1) and LAFF(h=8) suggests that the good performance of LAFF-based video retrieval is largely contributed by the LAFF block rather than the multi-space similarity.</p><p>To reveal how different are the embedding spaces to each other, we compute the Jaccard index between the top-5 video retrieval results of the individual spaces w.r.t. a specific query caption. The inter-space Jaccard index is lower than 0.5, suggesting sufficient divergence. Nevertheless, whether videos/captions have been separated along different axes needs further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with SOTA on Video Description Datasets</head><p>Datasets. We further include MSVD <ref type="bibr" target="#b8">[9]</ref>, TGIF <ref type="bibr" target="#b31">[32]</ref> and VATEX <ref type="bibr" target="#b56">[57]</ref>. For MSVD and TGIF, we follow their official data splits. For VATEX, we follow the data split as used in HGR <ref type="bibr" target="#b9">[10]</ref>. As for MSR-VTT, in addition to the official MV-test3k split, we also report performance on another popular data split <ref type="bibr" target="#b66">[67]</ref>, with 9k videos for training and 1k for test. We term this split MV-test1k .</p><p>Baselines. We compare with the SOTA that uses the same data splits as we have mentioned. In particular, the following published models are included: JE <ref type="bibr" target="#b40">[41]</ref>, W2VV++ <ref type="bibr" target="#b27">[28]</ref>, CE <ref type="bibr" target="#b34">[35]</ref>, TCE <ref type="bibr" target="#b64">[65]</ref>, HGR <ref type="bibr" target="#b9">[10]</ref>, SEA <ref type="bibr" target="#b30">[31]</ref>, MMT <ref type="bibr" target="#b18">[19]</ref>, DE <ref type="bibr" target="#b14">[15]</ref>, SSB <ref type="bibr" target="#b45">[46]</ref>, SSML <ref type="bibr" target="#b0">[1]</ref>, CLIP <ref type="bibr" target="#b46">[47]</ref>, CILP-FRL <ref type="bibr" target="#b7">[8]</ref>, and CLIP2Video (twotower version 6 ) <ref type="bibr" target="#b16">[17]</ref>. In addition, we finetune CLIP per dataset, termed as CLIP-FT. The video/text feature extracted by CLIP-FT is denoted as clip-ft.</p><p>Note that the video/text features used in the above models vary. For a headto-head comparison, we re-train LAFF, LAFF-ml, JE, W2V++, SEA and MMT using the same set of selected features 7 . Other baselines are not re-run, as they cannot handle the diverse video/text features without proper re-engineering. Since the fusion weights in JE have to be manually specified, we tried with three choices, i.e. JE with uniform weights, JE (0.8 for clip-ft) which assigns a much larger weight of 0.8 to the best clip-ft feature (the remains features have equal weights) and JE (0.9 for clip-ft). When comparing with CLIP2Video, we let LAFF include the global video/text features extracted by CLIP2Video to see whether LAFF can flexibly harness new and more powerful features.</p><p>Results. The performance of the different models on the multiple benchmarks is summarized in Tab. 7. Note that due to the inclusion of the better clip-ft feature, the performance is better than that reported in the ablation study. The baselines (JE, W2VV++ , SEA and MMT) get even worse results than using a single feature (clip-ft). The result suggests that one cannot take for granted that adding better features will yield better performance, and an intellectual design of feature fusion is needed. The proposed LAFF consistently performs the best on all the test sets. LAFF-ml outperforms LAFF, which shows that flexible use of LAFF in multiple levels can further improve performance. Comparison with arXiv SOTA CLIP2Video <ref type="bibr" target="#b16">[17]</ref> n.a n.a n.a 44.5 71.3 80.6 44.7 74.8 83.7 n.a n.a n.a 54.8 89.1 95.1 LAFF n.a n.a n.a 45.8 71.5 82.0 45.4 75.5 84.1 n.a n.a n.a 58.3 91.7 96.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with SOTA on TRECVID AVS 2016-2020</head><p>Setup. The test collection for TRECVID AVS 2016-2018 (TV16/TV17/TV18) is IACC.3 <ref type="bibr" target="#b43">[44]</ref> with 335,944 video clips. The test collection for TRECVID AVS 2019-2020 (TV19/TV20) is V3C1 <ref type="bibr" target="#b5">[6]</ref> with 1,082,659 video clips. We use the official metric, i.e. inferred Average Precision (infAP) <ref type="bibr" target="#b65">[66]</ref>.</p><p>Baselines. Due to the prominent performance of CLIP-FT and CLIP2Video as shown in section 4.3, we again compare with the two models. Since the top-3 ranked solutions of the AVS evaluation naturally reflect the state-of-the-art, we include them as well. CLIP2Video was trained on the MV-test1k split. So for a fair comparison, we train LAFF and CLIP-FT on the same split. Results. As shown in Tab. 8, LAFF performs the best on TV16-TV19. Note that the top performer of TV20 was trained on the joint set of MSR-VTT, TGIF and VATEX. Re-training LAFF on this larger dataset results in infAP of 0.358, marginally better than the top performer. We also conduct a case study on TV20, which shows that LAFF outperforms the CLIP series for action related queries with a large margin, see supplementary materials. We attribute this result to the fact that LAFF integrates 3D-CNN features (ircsn, c3d and tf ), which were designed to capture action and motion information in the video content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>For video retrieval by text, we propose LAFF, an extremely simple feature fusion block. LAFF is more effective than Multi-head Self-Attention, yet with much fewer parameters. Moreover, the attentional weights produced by LAFF can be used to explain the contribution of the individual video/text features for crossmodal matching. Consequently, the weights can be used for feature selection for building a more compact video retrieval model. Our LAFF-based video retrieval model surpasses the state-of-the-art on MSR-VTT, MSVD, TGIF, VATEX and TRECVID AVS 2016-2020. Given the increasing availability of (deep) video/text features, we believe our work opens up a promising avenue for further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Three distinct blocks for feature fusion: (a) Multi-Head Self-Attention (MHSA), (b) Attention-free, and (c) our proposed LAFF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>LAFF-ml for multi-level feature fusion. Frame-level LAFF is applied per feature, e.g. clip or rx101. The outputs of the frame-level LAFF blocks are later combined (with other video-level features, e.g. x3d ) by a video-level LAFF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Performance curves of three distinct models, i.e. W2VV++, SEA and LAFF, w.r.t. (a) text feature fusion, with {rx101,re152 } as video features, and (b) video feature fusion, with {bow,w2v,gru} as text features. LAFF is both effective and stable for fusing diverse features. Data: MV-test3k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of LAFF weights per feature, with samples from the MV-test3k test set. Green, brown, and blue mean text features, 2D video features and 3D video features, respectively. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(c). LAFF is generic, working for both video and text ends. Video/text features are combined in a</figDesc><table><row><cell cols="3">Attention ?? 0905</cell><cell></cell><cell cols="2">Linear</cell><cell>?</cell><cell cols="6">FC Attention ?? 0905 Attention ?? 0905</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Linear</cell><cell>?</cell><cell>?</cell><cell>FC</cell><cell>FC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">?</cell><cell>?</cell><cell>?</cell></row><row><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell>1 ?</cell><cell></cell><cell></cell><cell>1 ?</cell><cell>1</cell><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell>1 ?</cell><cell>1 ?</cell><cell>1</cell></row><row><cell>Linear</cell><cell>?</cell><cell>Linear</cell><cell>Linear</cell><cell>?</cell><cell>Linear</cell><cell></cell><cell cols="2">Linear Linear</cell><cell>? ?</cell><cell cols="3">Linear Linear</cell><cell>Linear</cell><cell cols="2">Linear</cell><cell>?</cell><cell>? Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>?</cell><cell>Linear</cell></row><row><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell>1 ?</cell><cell></cell><cell></cell><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell></cell><cell cols="2">1 ?</cell><cell>1 ?</cell><cell>1 ?</cell><cell>1 ?</cell></row><row><cell>tanh</cell><cell></cell><cell>tanh</cell><cell>tanh</cell><cell></cell><cell>tanh</cell><cell></cell><cell cols="2">tanh tanh</cell><cell></cell><cell cols="3">tanh tanh</cell><cell>tanh</cell><cell></cell><cell>tanh</cell><cell></cell><cell>tanh</cell><cell>tanh</cell><cell>tanh</cell><cell>tanh</cell></row><row><cell>1 ?</cell><cell cols="2">1 ?</cell><cell>1 ?</cell><cell cols="2">1 ?</cell><cell></cell><cell>1 ?</cell><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell>1 ?</cell><cell></cell><cell>1 ?</cell><cell></cell><cell cols="2">1 ?</cell><cell>1 ?</cell><cell>1 ?</cell><cell>1 ?</cell><cell>1 ?</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell>Q</cell><cell></cell><cell>K</cell><cell>V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q</cell><cell></cell><cell></cell><cell></cell><cell>K</cell><cell>V</cell></row><row><cell></cell><cell></cell><cell cols="2">Linear</cell><cell>Linear</cell><cell cols="2">Linear</cell><cell></cell><cell></cell><cell cols="2">Linear</cell><cell></cell><cell></cell><cell cols="2">Linear</cell><cell cols="2">Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell cols="4">Scaled Dot-Product Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? 1</cell><cell></cell><cell></cell><cell cols="5">Scaled Dot-Product Attention ? 1</cell><cell>? 1</cell></row><row><cell></cell><cell>Mean Pooling</cell><cell></cell><cell></cell><cell>Concat ?</cell><cell></cell><cell>??</cell><cell></cell><cell cols="3">Softmax Mean Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Concat ? Softmax</cell><cell>??</cell><cell>Softmax</cell></row><row><cell></cell><cell>1 ?</cell><cell></cell><cell></cell><cell>? ?</cell><cell>*</cell><cell></cell><cell></cell><cell></cell><cell>1 ?</cell><cell>? 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">? 1</cell><cell>? ?</cell><cell>*</cell><cell>? 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Linear</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Linear</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Mean Pooling 1 ?</cell><cell>1 ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) MHSA</cell><cell></cell><cell cols="6">(b) Attention-free</cell><cell cols="5">(c) LAFF</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Complexity analysis of feature fusion blocks, with D indicating the overall dimension of the input features and d as the dimension of the output features. FLOPs are computed based on input features of shape 8 ? 2048.</figDesc><table><row><cell cols="2">Feature fusion block Parameters</cell><cell>FLOPs (M)</cell></row><row><cell>MHSA</cell><cell>D ? d + 4 ? d 2</cell><cell>94.90</cell></row><row><cell>Attention-free</cell><cell>D ? d</cell><cell>27.78</cell></row><row><cell>LAFF</cell><cell>D ? d + d</cell><cell>27.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Video/text features used in ablation study. Video-level features are obtained by mean pooling over frames or segments, unless otherwise stated. Mean pooling over hidden vectors of GRU trained from scratch<ref type="bibr" target="#b13">[14]</ref>.bert 768 The base version of BERT, pre-trained on BooksCorpus and English Wikipedia<ref type="bibr" target="#b12">[13]</ref>. clip 512 The same CLIP as used to extract video features</figDesc><table><row><cell cols="3">Feature Dim. Short description</cell></row><row><cell cols="2">Video features:</cell><cell></cell></row><row><cell>rx101</cell><cell cols="2">2,048 ResNeXt-101 trained on the full set of ImageNet [39].</cell></row><row><cell>re152</cell><cell cols="2">2,048 ResNet-152 from the MXNet model zoo.</cell></row><row><cell>wsl</cell><cell>2,048</cell><cell>ResNeXt-101 pre-trained by weakly supervised learning on 940 million public images, followed by fine-tuning on ImageNet1k [37].</cell></row><row><cell>clip</cell><cell cols="2">512 CLIP (ViT-B/32) pre-trained on web images and corpus by contrastive learning [48].</cell></row><row><cell>c3d</cell><cell cols="2">2,048 C3D trained on Kinetics400 [53].</cell></row><row><cell>ircsn</cell><cell cols="2">2,048 irCSN-152 which trained by weakly supervised learning on IG-65M [20].</cell></row><row><cell>tf</cell><cell cols="2">768 TimeSformer trained on HowTo100M [7].</cell></row><row><cell>x3d</cell><cell cols="2">2,048 X3D trained on Kinetics400 [18].</cell></row><row><cell cols="2">Text features:</cell><cell></cell></row><row><cell>bow</cell><cell>m</cell><cell>m-dimensional Bag-of-words feature, with m being 7,675 (MSR-VTT), 2,916 (MSVD), 3,980 (TGIF), or 10,312 (VATEX)</cell></row><row><cell>w2v</cell><cell cols="2">500 Word2Vec trained on Flickr tags [14].</cell></row><row><cell>gru</cell><cell>1,024</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparing feature fusion blocks. The simple feature concatenation used by W2VV++ is taken as a baseline. Numbers in parentheses are relative improvements against this baseline. Video features: all. Data: MV-test3k.</figDesc><table><row><cell>Text features</cell><cell>Fusion block</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell><cell>Medr</cell><cell>mAP</cell></row><row><cell></cell><cell>Baseline</cell><cell>14.0</cell><cell>35.9</cell><cell>47.7</cell><cell>12</cell><cell>0.249</cell></row><row><cell></cell><cell>MHSA</cell><cell>11.7</cell><cell>31.9</cell><cell>43.4</cell><cell>15</cell><cell>0.219 (12.0%?)</cell></row><row><cell>bow, w2v, gru</cell><cell>MHSA(mulit-loss)</cell><cell>11.1</cell><cell>30.1</cell><cell>41.1</cell><cell>18</cell><cell>0.207 (16.8%?)</cell></row><row><cell></cell><cell>Attention-free</cell><cell>15.4</cell><cell>37.8</cell><cell>49.7</cell><cell>11</cell><cell>0.264 (6.0%?)</cell></row><row><cell></cell><cell>LAFF</cell><cell>16.0</cell><cell>39.5</cell><cell>51.4</cell><cell>10</cell><cell>0.276 (10.8%?)</cell></row><row><cell></cell><cell>Baseline</cell><cell>19.2</cell><cell>43.5</cell><cell>55.3</cell><cell>8</cell><cell>0.310</cell></row><row><cell></cell><cell>MHSA</cell><cell>18.8</cell><cell>43.0</cell><cell>54.6</cell><cell>8</cell><cell>0.305 (1.6%?)</cell></row><row><cell>bow, w2v, gru, clip</cell><cell>MHSA(mulit-loss)</cell><cell>18.7</cell><cell>43.1</cell><cell>54.5</cell><cell>8</cell><cell>0.305 (1.6%?)</cell></row><row><cell></cell><cell>Attention-free</cell><cell>20.5</cell><cell>44.8</cell><cell>56.2</cell><cell>7</cell><cell>0.321 (3.5%?)</cell></row><row><cell></cell><cell>LAFF</cell><cell>23.7</cell><cell>49.1</cell><cell>60.6</cell><cell>6</cell><cell>0.358 (15.5%?)</cell></row><row><cell></cell><cell>Baseline</cell><cell>14.5</cell><cell>35.0</cell><cell>46.1</cell><cell>13</cell><cell>0.247</cell></row><row><cell></cell><cell>MHSA</cell><cell>17.9</cell><cell>41.6</cell><cell>53.3</cell><cell>9</cell><cell>0.294 (19.0%?)</cell></row><row><cell>bow, w2v, gru, clip, bert</cell><cell>MHSA (mulit-loss)</cell><cell>19.0</cell><cell>43.4</cell><cell>54.9</cell><cell>8</cell><cell>0.306 (23.9%?)</cell></row><row><cell></cell><cell>Attention-free</cell><cell>20.9</cell><cell>45.3</cell><cell>56.9</cell><cell>7</cell><cell>0.326 (32.0%?)</cell></row><row><cell></cell><cell>LAFF</cell><cell>23.8</cell><cell>49.0</cell><cell>60.3</cell><cell>6</cell><cell>0.358 (44.9%?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Combined loss versus single loss. Video features: all.</figDesc><table><row><cell>Text features</cell><cell>Loss</cell><cell cols="2">R1 R5 R10 Med r mAP</cell></row><row><cell>bow, w2v, gru</cell><cell cols="2">Single Combined 16.0 39.5 51.4 14.1 36.2 47.8</cell><cell>12 0.250 10 0.276 (10.4%?)</cell></row><row><cell>bow, w2v, gru, clip</cell><cell cols="2">Single Combined 23.7 49.1 60.6 20.6 45.0 56.5</cell><cell>7 0.324 6 0.358 (10.5%?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Effect of the number of common spaces h. Features: all.</figDesc><table><row><cell>h</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell><cell>Med r</cell><cell>mAP</cell></row><row><cell>1</cell><cell>23.1</cell><cell>48.3</cell><cell>59.9</cell><cell>6</cell><cell>0.352</cell></row><row><cell>2</cell><cell>23.1</cell><cell>48.3</cell><cell>60.1</cell><cell>6</cell><cell>0.352</cell></row><row><cell>4</cell><cell>23.5</cell><cell>49.1</cell><cell>60.4</cell><cell>6</cell><cell>0.356</cell></row><row><cell>8</cell><cell>23.7</cell><cell>49.1</cell><cell>60.6</cell><cell>6</cell><cell>0.358</cell></row><row><cell>16</cell><cell>23.5</cell><cell>48.8</cell><cell>60.4</cell><cell>6</cell><cell>0.356</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison with the state-of-the-art on four benchmark datasets, i.e. MSR-VTT (MV-test3k/MV-test1k), MSVD, TGIF and VATEX. Baselines which use video/text features different from ours and thus not directly comparable are provided in the supplement. The same video and text feature as ours JE [41] (uniform weights) 21.2 46.5 58.4 36.0 65.9 76.4 35.9 71.0 81.8 18.7 37.5 47.1 50.2 88.7 95.4 JE (0.8 for clip-ft) 26.1 51.7 63.3 41.2 73.2 82.5 39.4 69.9 79.4 21.7 41.3 50.9 54.1 89.0 95.0 JE (0.9 for clip-ft) 25.9 51.4 63.0 40.9 72.7 82.1 38.8 69.7 78.9 21.3 40.9 50.3 53.5 88.3 94.6 W2VV++ [28] 23.0 49.0 60.7 39.4 68.1 78.1 37.8 71.0 81.6 22.0 42.8 52.7 55.8 91.2 96.0 SEA [31] 19.9 44.3 56.5 37.2 67.1 78.3 34.5 68.8 80.5 16.4 33.6 42.5 52.4 90.2 95.9 MMT [19] 24.9 50.5 62.0 39.5 68.3 78.3 40.6 72.0 81.7 22.1 42.2 51.7 54.4 89.2 95.0 LAFF 28.0 53.8 64.9 42.2 70.7 81.2 45.2 75.8 84.3 24.1 44.7 54.3 57.7 91.3 95.9 LAFF-ml 29.1 54.9 65.8 42.6 71.8 81.0 45.4 76.0 84.6 24.5 45.0 54.5 59.1 91.7 96.3</figDesc><table><row><cell>Model</cell><cell>MV-test3k</cell><cell>MV-test1k</cell><cell>MSVD</cell><cell>TGIF</cell><cell>VATEX</cell></row><row><cell></cell><cell cols="5">R1 R5 R10 R1 R5 R10 R1 R5 R10 R1 R5 R10 R1 R5 R10</cell></row><row><cell>CLIP-FT (this paper )</cell><cell cols="5">27.7 53.0 64.2 39.7 67.8 78.4 44.6 74.7 84.1 21.5 40.6 49.9 53.3 87.5 94.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>State-of-the-art performance on TRECVID AVS 2016-2020.</figDesc><table><row><cell>Model</cell><cell>TV16</cell><cell>TV17</cell><cell>TV18</cell><cell>TV19</cell><cell>TV20</cell><cell>MEAN</cell></row><row><cell>Rank 1</cell><cell>0.054 [24]</cell><cell>0.206 [51]</cell><cell>0.121 [26]</cell><cell>0.163 [62]</cell><cell>0.354 [68]</cell><cell>n.a</cell></row><row><cell>Rank 2</cell><cell>0.051 [38]</cell><cell>0.159 [54]</cell><cell>0.087 [21]</cell><cell>0.160 [29]</cell><cell>0.269 [30]</cell><cell>n.a</cell></row><row><cell>Rank 3</cell><cell>0.040 [33]</cell><cell>0.120 [43]</cell><cell>0.082 [5]</cell><cell>0.123 [55]</cell><cell>0.229 [61]</cell><cell>n.a</cell></row><row><cell>CLIP2Video</cell><cell>0.176</cell><cell>0.229</cell><cell>0.114</cell><cell>0.176</cell><cell>0.207</cell><cell>0.180</cell></row><row><cell>CLIP-FT</cell><cell>0.191</cell><cell>0.215</cell><cell>0.105</cell><cell>0.147</cell><cell>0.203</cell><cell>0.172</cell></row><row><cell>LAFF</cell><cell>0.211</cell><cell>0.285</cell><cell>0.137</cell><cell>0.192</cell><cell>0.265</cell><cell>0.218</cell></row><row><cell>LAFF-ml</cell><cell>0.222</cell><cell>0.290</cell><cell>0.147</cell><cell>0.181</cell><cell>0.245</cell><cell>0.217</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/ruc-aimc-lab/laff</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Other non-linear activations such as ReLU and sigmoid will make each dimension non-negative, constrain the feature space be in the first quadrant, and consequently put a lower boundary of 0 on the cosine similarity. As such, the similarity will be less discriminative than the tanh counterpart.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We prefer the two-tower version to the single-tower version, as the latter has to compute video and text embeddings online, making it not scalable for real applications.<ref type="bibr" target="#b6">7</ref> Video features: clip-ft, x3d, ircsn and tf. Text features: clip-ft, bow, w2v and gru.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Noise estimation using density estimation for self-supervised multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal fusion for multimedia analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El Saddik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="345" to="379" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">NTU ROSE lab at TRECVID 2018: Ad-hoc video search and video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bastan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">V3C1 dataset: An evaluation of content characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Berns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beecks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICMR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What matters for ad-hoc video search? a large-scale evaluation on TRECVID</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV Workshop on ViRal</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TEACHTEXT: Crossmodal generalized distillation for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting visual features from text for image and video caption retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3377" to="3388" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dual encoding for video retrieval by text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">VSE++: improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11097</idno>
		<title level="m">CLIP2Video: Mastering video-text retrieval via image clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Large-scale weakly-supervised pretraining for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Informe-dia@TRECVID 2018: Ad-hoc video search with discrete and continuous representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vaibhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Renoust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klinkigt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hiroke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><forename type="middle">A</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<idno>NII-HITACHI-UIT at TRECVID 2016</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>MMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Renmin University of China and Zhejiang Gongshang University at TRECVID 2018: Deep Cross-Modal Embeddings for Video-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep multiple instance learning with spatial attention for rop case classification, instance selection and abnormality localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">W2VV++: Fully deep learning for ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACMMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Renmin University of China and Zhejiang Gongshang University at TRECVID 2019: Learn to search and describe videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Renmin University of China at TRECVID 2020: Sentence Encoder Assembly for Ad-hoc Video Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SEA: Sentence encoder assembly for video retrieval by textual queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="4351" to="4362" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<title level="m">TGIF: A new dataset and benchmark on animated gif description</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Informedia @ TRECVID 2016</title>
		<imprint>
			<publisher>TRECVID</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Attention distillation for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<title level="m">CLIP4Clip: An empirical study of clip for end to end video clip retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ITI-CERTH participation in TRECVID</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Markatopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moumtzidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mironidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kaltsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ioannidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Symeonidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Avgerinakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andreadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gialampoukidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Briassouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shuffled ImageNet banks for video event detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Joint embeddings with multimodal cues for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICMR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Vireo @ TRECVID 2017: Video-to-text, ad-hoc video search and video hyperlinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Creating a web-scale video collection for research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lanagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Workshop on Web-scale Multimedia Corpus</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A straightforward framework for video retrieval using CLIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>MCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multimodal video indexing: A review of the state-ofthe-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="35" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">University of Amsterdam and Renmin university at TRECVID 2017: Searching video, detecting events and describing video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Koelma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Polysemous visual-semantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Waseda Meisei at TRECVID 2017: Ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ueki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Waseda Meisei SoftBank at TRECVID 2019: Ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Ueki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>An Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">VATEX: A largescale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Fine-grained action retrieval through multiple parts-of-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Interpretable embedding for ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACMMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Vireo@ TRECVID 2020 ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Hybrid sequence encoder for text based video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multi-view correlated feature learning by uncovering shared component</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Tree-augmented cross-modal encoding for complex-query video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>SIGIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Estimating average precision with incomplete and imperfect judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">RUC AIM3 at TRECVID 2020: Ad-hoc video search &amp; video to text description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TRECVID</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
