<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Image Recognition by Predicting Parameters from Activations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
							<email>siyuan.qiao@jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
							<email>cxliu@jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
							<email>wei.shen@t.shu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<email>alan.yuille@jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Image Recognition by Predicting Parameters from Activations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we are interested in the few-shot learning problem. In particular, we focus on a challenging scenario where the number of categories is large and the number of examples per novel category is very limited, e.g. 1, 2, or 3. Motivated by the close relationship between the parameters and the activations in a neural network associated with the same category, we propose a novel method that can adapt a pre-trained neural network to novel categories by directly predicting the parameters from the activations. Zero training is required in adaptation to novel categories, and fast inference is realized by a single forward pass. We evaluate our method by doing few-shot image recognition on the Im-ageNet dataset, which achieves the state-of-the-art classification accuracy on novel categories by a significant margin while keeping comparable performance on the large-scale categories. We also test our method on the MiniImageNet dataset and it strongly outperforms the previous state-ofthe-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed rapid advances in deep learning <ref type="bibr" target="#b18">[19]</ref>, with a particular example being visual recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref> on large-scale image datasets, e.g., Im-ageNet <ref type="bibr" target="#b25">[26]</ref>. Despite their great performances on benchmark datasets, the machines exhibit clear difference with people in the way they learn concepts. Deep learning methods typically require huge amounts of supervised training data per concept, and the learning process could take days using specialized hardware, i.e. GPUs. In contrast, children are known to be able to learn novel visual concepts almost effortlessly with a few examples after they have accumulated enough past knowledge <ref type="bibr" target="#b1">[2]</ref>. This phenomenon motivates computer vision research on the problem of few-shot learning, i.e., the task to learn novel concepts from only a few examples for each category <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Formally, in the few-shot learning problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>, we are provided with a large-scale set D large with categories C large and a few-shot set D few with categories C few that do not overlap with C large . D large has sufficient training samples for each category whereas D few has only a few examples (&lt; 6 in this paper). The goal is to achieve good classification performances, either on D few or on both D few and D large . We argue that a good classifier should have the following properties: (1) It achieves reasonable performance on C few . <ref type="bibr" target="#b1">(2)</ref> Adapting to C few does not degrade the performance on C large significantly (if any). (3) It is fast in inference and adapts to few-shot categories with little or zero training, i.e., an efficient lifelong learning system <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Both parametric and non-parametric methods have been proposed for the few-shot learning problem. However, due to the limited number of samples in D few and the imbalance between D large and D few , parametric models usually fail to learn well from the training samples <ref type="bibr" target="#b22">[23]</ref>. On the other hand, many non-parametric approaches such as nearest neighbors can adapt to the novel concepts easily without severely forgetting the original classes. But this requires careful designs of the distance metrics <ref type="bibr" target="#b0">[1]</ref>, which can be difficult and sometimes empirical. To remedy this, some previous work instead adapts feature representation to the metrics by using siamese networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>. As we will show later through experiments, these methods do not fully satisfy the properties mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?? ?? ??</head><p>?? SoftMax Activations Pug Jay Hen Snail Corgi <ref type="figure">Figure 1</ref>: Illustration of pre-training on D large (black) and few-shot novel category adaptation to D few (green). The green circles are the novel categories, and the green lines represent the unknown parameters for categories in C few .</p><p>In this paper, we present an approach that meets the desired properties well. Our method starts with a pre-trained deep neural network on D large . The final classification layers Figure 2: Our motivation: t-SNE <ref type="bibr" target="#b21">[22]</ref> results on the average activations? y of each category before the fully connected layer of a 50-layer ResNet <ref type="bibr" target="#b9">[10]</ref> pre-trained on D large from ImageNet <ref type="bibr" target="#b25">[26]</ref> (left) and the parameters w y of each category in the last fully connected layer (right). Each point represents a category. Highlighted points with the same color and shape correspond to the same category. Circles are mammals, triangles are birds, diamonds are buses, and squares are home appliances.</p><p>(the fully connected layer and the softmax layer) are shown in <ref type="figure">Figure 1</ref>. We use w y ? R n to denote the parameters for category y in the fully connected layer, and use a(x) ? R n to denote the activations before the fully connected layer of an image x. Training on D large is standard; the real challenge is how to re-parameterize the last fully connected layer to include the novel categories under the few-shot constraints, i.e., for each category in C few we have only a few examples. Our proposed method addresses this challenge by directly predicting the parameters w y (in the fully connected layer) using the activations belonging to that category, i.e. A y = {a(x)|x ? D large ? D few , Y (x) = y}, where Y (?) denotes the category of the image.</p><p>This parameter predictor stems from the tight relationship between the parameters and activations. Intuitively in the last fully connected layer, we want w y ? a y to be large, for all a y ? A y . Let? y ? R n be the mean of the activations in A y . Since it is known that the activations of images in the same category are spatially clustered together <ref type="bibr" target="#b4">[5]</ref>, a reasonable choice of w y is to align with? y in order to maximize the inner product, and this argument holds true for all y. To verify this intuition, we use t-SNE <ref type="bibr" target="#b21">[22]</ref> to visualize the neighbor embeddings of the activation statistic? y and the parameters w y for each category of a pre-trained deep neural network, as shown in <ref type="figure">Figure 2</ref>. Comparing them and we observe a high similarity in both the local and the global structures. More importantly, the semantic structures <ref type="bibr" target="#b11">[12]</ref> are also preserved in both activations and parameters, indicating a promising generalizability to unseen categories.</p><p>These results suggest the existence of a categoryagnostic mapping from the activations to the parameters given a good feature extractor a(?). In our work, we parameterize this mapping with a feedforward network that is learned by back-propagation. This mapping, once learned, is used to predict parameters for both C few and C large .</p><p>We evaluate our method on two datasets. The first one is MiniImageNet <ref type="bibr" target="#b27">[28]</ref>, a simplified subset of ImageNet ILSVRC 2015 <ref type="bibr" target="#b25">[26]</ref>, in which C large has 80 categories and C few has 20 categories. Each category has 600 images of size 84 ? 84. This small dataset is the benchmark for natural images that the previous few-shot learning methods are evaluated on. However, this benchmark only reports the performances on D few , and the accuracy is evaluated under 5-way test, i.e., to predict the correct category from only 5 category candidates. In this paper, we will take a step forward by evaluating our method on the full ILSVRC 2015 <ref type="bibr" target="#b25">[26]</ref>, which has 1000 categories. We split the categories into two sets where C large has 900 and C few has the rest 100. The methods will be evaluated under 1000-way test on both D large and D few . This is a setting that is considerably larger than what has been experimented in the few-shot learning before. We compare our method with the previous work and show state-of-the-art performances.</p><p>The rest of the paper is organized as follows: ?2 defines and explains our model, ?3 presents the related work, ?4 shows the experimental results, and ?5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>The key component of our approach is the categoryagnostic parameter predictor ? :? y ? w y <ref type="figure" target="#fig_0">(Figure 3</ref>). More generally, we could allow the input to ? to be a statistic representing the activations of category y. Note that we use the same mapping function for all categories y ? C large , because we believe the activations and the parameters have similar local and global structure in their respective space. Once this mapping has been learned on D large , because of this structure-preserving property, we expect it to generalize to categories in C few .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>??</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Learning Parameter Predictor</head><p>Since our final goal is to do classification, we learn ? from the classification supervision. Specifically, we can learn ? from D large by minimizing the classification loss (with a regularizer ||?||) defined by</p><formula xml:id="formula_0">L(?) = (y,x)?D large ? ? ??? (?y) a(x) + log y ?C large e ? ? y a(x) ? ? ? + ?||?|| (1)</formula><p>Eq. 1 models the parameter prediction for categories y ? C large . However, for the few-shot set C few , each category only has a few activations, whose mean value is the activation itself when each category has only one sample. To model this few-shot setting in the large-scale training on D large , we allow both the individual activations and the mean activation to represent a category. Concretely, let s y ? A y ?? y be a statistic for category y. Let S large denote a statistic set {s 1 , ..., s |Clarge| } with one for each category in C large . We sample activations s y for each category y from A y ?? y with a probability p mean to use? y and 1 ? p mean to sample uniformly from A y . Now, we learn ? to minimize the loss defined by</p><formula xml:id="formula_1">L(?) = (y,x)?D large E S large ? ? ??? (sy) a(x) + log y ?C large e ? s y a(x) ? ? ? + ?||?|| (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Inference</head><p>During inference we include C few , which calls for a statistic set for all categories S = {s 1 , ..., s |C| }, where C = C large ? C few . Each statistic set S can generate a set of parameters {?(s 1 ), ..., ?(s |C| )} that can be used for building a classifier on C. Since we have more than one possible set S from the dataset D = D large ? D few , we can do classification based on all the possible S. Formally, we compute the probability of x being in category y by</p><formula xml:id="formula_2">P (y|x) = e ES [?(sy)a(x)] / ? ? y ?C e ES ?(s y )a(x) ? ?<label>(3)</label></formula><p>However, classifying images with the above equation is time-consuming since it computes the expectations over the entire space of S which is exponentially large. We show in the following that if we assume ? to be a linear mapping, then this expectation can be computed efficiently.</p><p>In the linear case ? is a matrix ?. The predicted parameter for category y is?</p><formula xml:id="formula_3">y = ? ? sy<label>(4)</label></formula><p>The inner product of x before the softmax function for category y is</p><formula xml:id="formula_4">h(sy, a(x)) =?y ? a(x) = ? ? sy ? a(x)<label>(5)</label></formula><p>If a(x) and s y are normalized, then by setting ? as the identity matrix, h(s y , a(x)) is equivalent to the cosine similarity between s y and a(x). Essentially, by learning ?, we are learning a more general similarity metric on the activations a(x) by capturing correlations between different dimensions of the activations. We will show more comparisons between the learned ? and identity matrix in ?4.1.</p><p>Because of the linearity of ?, the probability of x being in category y simplifies to</p><formula xml:id="formula_5">P (y|x) = e a(x)??(E S [sy]) / ? ? y ?C e a(x)??(E S s y ) ? ? = e a(x)???E S [sy ] / ? ? y ?C e a(x)???E S s y ? ? (6)</formula><p>Now E S [s y ] can be pre-computed which is efficient. Adapting to novel categories only requires updating the corresponding E S [s y ]. Although it is ideal to keep the linearity of ? to reduce the amount of computation, introducing nonlinearity could potentially improve the performance. To keep the efficiency, we still push in the expectation and approximate Eq. 3 as in Eq. 6.</p><p>When adding categories y ? C few , the estimate of E S [s y ] may not be reliable since the number of samples is small. Besides, Eq. 2 models the sampling from one-shot and mean activations. Therefore, we take a mixed strategy for parameter prediction, i.e., we use ES[sy] to predict parameters for category y ? C large , but for C few we treat each sample as a newly added category, as shown in <ref type="figure" target="#fig_1">Figure 4a</ref>. For each novel category in C few , we compute the maximal response of the activation of the test image to the parameter set predicted from each activation in the statistic set of the corresponding novel category in C few . We use them as the inputs to the SoftMax layer to compute the probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training Strategy</head><p>The objective of training is to find ? that minimizes Eq. 2. There are many methods to do this. We approach this by using stochastic gradient decent with weight decay and momentum. <ref type="figure" target="#fig_1">Figure 4b</ref> demonstrates the training strategy of the parameter predictor ?. We train ? on D large with categories C large . For each batch of the training data, we sample |C large | statistics s y from A y ?? y to build a statistic set S with one for each category y in C large . Next, we sample a training activation set T from D large with one for each category in C large . In total, we sample 2|C large | activations. The activations in the statistic sets are fed to ? to generate parameters for the fully connected layer. With the predicted parameters for each category in C large , the training activation set then is used to evaluate their effectiveness by classifying the training activations. At last, we compute the classification loss with respect to the ground truth, based on which we calculate the gradients and back-propagate them in the path shown in <ref type="figure" target="#fig_1">Figure 4b</ref>. After the gradient flow passes through ?, we update ? according to the gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Implementation Details</head><p>Full ImageNet Dataset Our major experiments are conducted on ILSVRC 2015 <ref type="bibr" target="#b25">[26]</ref>. ILSVRC 2015 is a largescale image dataset with 1000 categories, each of which has about 1300 images for training, and 50 images for validation. For the purpose of studying both the large-scale and the few-shot settings at the same time, ILSVRC 2015 is split to two sets by the categories. The training data from 900 categories are collected into D large , while the rest 100 categories are gathered as set D few .</p><p>We first train a 50-layer ResNet [10] on D large . We use the outputs of the global average pooling layer as the activation a(x) of an image x. For efficiency, we compute the activation a(x) for each image x before the experiments as well as the mean activations? y . Following the training strategy shown in ?2.3, for each batch, we sample 900 activations as the statistic set and 900 activations as the training activation set. We compute the parameters using the statistic set, and copy the parameters into the fully connected layer. Then, we feed the training activations into the fully connected layer, calculate the loss and back-propagate the gradients. Next, we redirect the gradient flow into ?. Finally, we update ? using stochastic gradient descent. The learning rate is set to 0.001. The weight decay is set to 0.0005 and the momentum is set to 0.9. We train ? on D large for 300 epochs, each of which has 250 batches. p mean is set to 0.9.</p><p>For the parameter predictor, we implement three different ?: ? 1 , ? 2 and ? 2 * . ? 1 is a one-layer fully connected model. ? 2 is defined as a sequential network with two fully connected layers in which each maps from 2048 dimensional features to 2048 dimensional features and the first one is followed by a ReLU non-linearity layer <ref type="bibr" target="#b23">[24]</ref>. The final outputs are normalized to unity in order to speed up training and ensure generalizability. By introducing nonlinearity, we observe slight improvements on the accuracies for both C large and C few . To demonstrate the effect of minimizing Eq. 2 instead of Eq. 1, we train another ? 2 * which has the same architecture with ? 2 but minimizes Eq. 1. As we will show later through experiments, ? 2 * has strong bias towards C large .</p><p>MiniImageNet Dataset For comparison purposes, we also test our method on MiniImageNet dataset <ref type="bibr" target="#b27">[28]</ref>, a simplified subset of ILSVRC 2015. This dataset has 80 categories for D large and 20 categories for D few . Each category has 600 images. Each image is of size 84 ? 84. For the fairness of comparisons, we train two convolutional neural networks to get the activation functions a(?). The first one is the same as that of Matching Network <ref type="bibr" target="#b27">[28]</ref>, and the second one is a wide residual network <ref type="bibr" target="#b30">[31]</ref>. We train the wide residual network WRN-28-10 [31] on D large , following its configuration for CIFAR-100 dataset <ref type="bibr" target="#b13">[14]</ref>. There are some minor modifications to the network architecture as the input size is different. To follow the architecture, the input size is set to 80?80. The images will be rescaled to this size before training and evaluation. There will be 3 times of downsampling rather than 2 times as for CIFAR dataset. The training process follows WRN-28-10 <ref type="bibr" target="#b30">[31]</ref>. We also use the output of the global average pooling layer as the activation a(x) of an image x. For the parameter predictor ?, we train it by following the settings of ? 2 for the full ImageNet dataset except that now the dimension corresponds to the output of the activations of the convolutional neural networks. The two architectures will be detailed in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Large-Scale Image Recognition</head><p>We have witnessed an evolution of image datasets over the last few decades. The sizes of the early datasets are relatively small. Each dataset usually collects images on the order of tens of thousands. Representative datasets include Caltech-101 <ref type="bibr" target="#b6">[7]</ref>, Caltech-256 <ref type="bibr" target="#b8">[9]</ref>, Pascal VOC <ref type="bibr" target="#b5">[6]</ref>, and CIFAR-10/100 <ref type="bibr" target="#b13">[14]</ref>. Nowadays, large-scale datasets are available with millions of detailed image annotations, e.g. ImageNet <ref type="bibr" target="#b25">[26]</ref> and MS COCO <ref type="bibr" target="#b19">[20]</ref>. With datasets of this scale, machine learning methods that have large capacity start to prosper, and the most successful ones are convolutional neural network based <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Few-Shot Image Recognition</head><p>Unlike large-scale image recognition, the research on few-shot learning has received limited attention from the community due to its inherent difficulty, thus is still at an early stage of development. As an early attempt, Fei-Fei et al. proposed a variational Bayesian framework for oneshot image classification <ref type="bibr" target="#b6">[7]</ref>. A method called Hierarchical Bayesian Program Learning <ref type="bibr" target="#b17">[18]</ref> was later proposed to specifically approach the one-shot problem on character recognition by a generative model. On the same character recognition task, Koch et al. developed a siamese convolutional network <ref type="bibr" target="#b12">[13]</ref> to learn the representation from the dataset and modeled the few-shot learning as a verification task. Later, Matching Network <ref type="bibr" target="#b27">[28]</ref> was proposed to approach the few-shot learning task by modeling the problem as a k-way m-shot image retrieval problem using attention and memory models. Following this work, Ravi and Larochelle proposed a LSTM-based meta-learner optimizer <ref type="bibr" target="#b24">[25]</ref>, and Chelsea et al. proposed a model-agnostic meta learning method <ref type="bibr" target="#b7">[8]</ref>. Although they show state-of-theart performances on their few-shot learning tasks, they are not flexible for both large-scale and few-shot learning since k and m are fixed in their architectures. We will compare ours with these methods on their tasks for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Unified Approach</head><p>Learning a metric then using nearest neighbor <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref> is applicable but not necessarily optimal to the unified problem of large-scale and few-shot learning since it is possible to train a better model on the large-scale part of the dataset using the methods in ?3.1. Mao et al. proposed a method called Learning like a Child <ref type="bibr" target="#b22">[23]</ref> specifically for fast novel visual concept learning using hundreds of examples per category while keeping the original performance. However, this method is less effective when the training examples are extremely insufficient, e.g. &lt; 6 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Full ImageNet Classification</head><p>In this section we describe our experiments and compare our approach with other strong baseline methods. As stated in ?1, there are three aspects to consider in evaluating a method: (1) its performance on the few-shot set D few , (2) its performance on the large-scale set D large , and (3) its computation overhead of adding novel categories and the complexity of image inference. In the following paragraphs, we will cover the settings of the baseline methods, compare the performances on the large-scale and the few-shot sets, and discuss their efficiencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>The baseline methods must be applicable to both large-scale and few-shot learning settings. We compare our method with a fine-tuned 50-layer ResNet <ref type="bibr" target="#b9">[10]</ref>, Learning like a Child <ref type="bibr" target="#b22">[23]</ref> with a pre-trained 50-layer ResNet as the starting network, Siamese-Triplet Network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref> using three 50-layer ResNets with shared parameters, and the nearest neighbor using the pre-trained 50-layer ResNet convolutional features. We will elaborate individually on how to train and use them.</p><p>As mentioned in ?2.4, we first train a 900-category classifier on D large . We will build other baseline methods using this classifier as the staring point. For convenience, we denote this classifier as R pt large , where pt stands for "pretrained". Next, we add the novel categories C few to each method. For the 50-layer ResNet, we fine tune R pt large with the newly added images by extending the fully connected layer to generate 1000 classification outputs. Note that we will limit the number of training samples of C few for the few-shot setting. For Learning like a Child, however, we fix the layers before the global average pooling layer, extend the fully connected layer to include 1000 classes, and  <ref type="table">Table 1</ref>: Comparing 1000-way accuracies with feature extractor a(?) pre-trained on D large . For different D few settings, red: the best few-shot accuracy, and blue: the second best.</p><p>only update the parameters for C few in the last classification layer. Since we have the full access to D large , we do not need Baseline Probability Fixation <ref type="bibr" target="#b22">[23]</ref>. The nearest neighbor with cosine distance can be directly used for both tasks given the pre-trained deep features.</p><p>The other method we compare is Siamese-Triplet Network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>. Siamese network is proposed to approach the few-shot learning problem on Omniglot dataset <ref type="bibr" target="#b15">[16]</ref>.</p><p>In our experiments, we find that its variant Triplet Network <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref> is more effective since it learns feature representation from relative distances between positive and negative pairs instead of directly doing binary classification from the feature distance. Therefore, we use the Triplet Network from <ref type="bibr" target="#b20">[21]</ref> on the few-shot learning problem, and upgrade its body net to the pre-trained R pt large . We use cosine distance as the distance metric and fine-tune the Triplet Network. For inference, we use nearest neighbor with cosine distance. We use some techniques to improve the speed, which will be discussed later in the efficiency analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot Accuracies</head><p>We first investigate the few-shot learning setting where we only have several training examples for C few . Specifically, we study the performances of different methods when D few has for each category 1, 2, and 3 samples. It is worth noting that our task is much harder than the previously studied few-shot learning: we are evaluating the top predictions out of 1000 candidate categories, i.e., 1000-way accuracies while previous work is mostly interested in 5-way or 20-way accuracies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>With the pre-trained R pt large , the training samples in D few are like invaders to the activation space for C large . Intuitively, there will be a trade-off between the performances on C large and C few . This is true especially for non-parametric methods. <ref type="table">Table 1</ref> shows the performances on the validation set of ILSVRC 2015 <ref type="bibr" target="#b25">[26]</ref>. The second column is the percentage of data of D large in use, and the third column is the number of samples used for each category in D few . Note that finetuned ResNet <ref type="bibr" target="#b9">[10]</ref> and Learning like a Child <ref type="bibr" target="#b22">[23]</ref> require fine-tuning while others do not.</p><p>Triplet Network is designed to do few-shot image inference by learning feature representations that adapt to the  <ref type="table">Table 2</ref>: Oracle 1000-way accuracies of the feature extractor a(?) pre-trained on D large . chosen distance metric. It has better performance on C few compared with the fine-tuned ResNet and Learning like a Child when the percentage of D large in use is low. However, its accuracies on C large are sacrificed a lot in order to favor few-shot accuracies. We also note that if full category supervision is provided, the activations of training a classifier do better than that of training a Triplet Network. We speculate that this is due to the less supervision of training a Triplet Network which uses losses based on fixed distance preferences. Fine-tuning and Learning like a Child are training based, thus are able to keep the high accuracies on D large , but perform badly on D few which does not have sufficient data for training. Compared with them, our method shows state-of-the-art accuracies on C few without compromising too much the performances on C large . <ref type="table">Table 1</ref> also compares ? 2 and ? 2 * , which are trained to minimize Eq. 2 and Eq. 1, respectively. Since during training ? 2 * only mean activations are sampled, it shows a bias towards C large . However, it still outperforms other baseline methods on C few . In short, modeling using Eq. 2 and Eq. 1 shows a tradeoff between C large and C few .</p><p>Oracles Here we explore the upper bound performance on C few . In this setting we have all the training data for C large and C few in ImageNet. For the fixed feature extractor a(?) pre-trained on D large , we can train a linear classifier on C large and C few , or use nearest neighbor, to see what are the upper bounds of the pre-trained a(?). <ref type="table">Table 2</ref> shows the results. The performances are evaluated on the validation set of ILSVRC 2015 <ref type="bibr" target="#b25">[26]</ref> which has 50 images for each category. The feature extractor pre-trained on D large demonstrates reasonable accuracies on C few which it has never seen during training for both parametric and non-parametric methods.</p><p>Efficiency Analysis We briefly discuss the efficiencies of each method including ours on the adaptation to novel categories and the image inference. The methods are tested on NVIDIA Tesla K40M GPUs. For adapting to novel categories, fine-tuned ResNet and Learning like a Child require re-training the neural networks. For re-training one epoch of the data, fine-tuned ResNet and Learning like a Child both take about 1.8 hours on 4 GPUs. Our method only needs to predict the parameters for the novel categories using ? and add them to the original neural network. This process takes 0.683s using one GPU for adapting the network to 100 novel categories with one example each. Siamese-Triplet Network and nearest neighbor with cosine distance require no operations for adapting to novel categories as they are ready for feature extraction. For image inference, Siamese-Triplet Network and nearest neighbor are very slow since they will look over the entire dataset. Without any optimization, this can take 2.3 hours per image when we use the entire D large . To speed up this process in order to do comparison with ours, we first pre-compute all the features. Then, we use a deep learning framework to accelerate the cosine similarity computation. At the cost of 45GB memory usage and the time for feature pre-computation, we manage to lower the inference time of them to 37.867ms per image. Fine-tuned ResNet, Learning like a Child and our method are very fast since at the inference stage, these three methods are just normal deep neural networks. The inference speed of these methods is about 6.83ms per image on one GPU when the batch size is set to 32. In a word, compared with other methods, our method is fast and efficient in both the novel category adaptation and the image inference.</p><p>Comparing Activation Impacts In this subsection we investigate what ? 1 has learned that helps it perform better than the cosine distance, which is a special solution for onelayer ? by setting ? to the identity matrix 1. We first visualize the matrix ? 1 ij in log scale as shown in the left image of <ref type="figure" target="#fig_2">Figure 5</ref>. Due to the space limit, we only show the upperleft 256?256 submatrix. Not surprisingly, the values on the diagonal dominates the matrix. We observe that along the diagonal, the maximum is 0.976 and the minimum is 0.744, suggesting that different from 1, ? 1 does not use each activation channel equally. We speculate that this is because the pre-trained activation channels have different distributions of magnitudes and different correlations with the classification task. These factors can be learned by the last fully connected layer of R pt large with large amounts of data but are Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-Shot 5-Shot</head><p>Fine-Tuned Baseline 28.86 ? 0.54% 49.79 ? 0.79% Nearest Neighbor 41.08 ? 0.70% 51.04 ? 0.65% Matching Network <ref type="bibr" target="#b27">[28]</ref> 43.56 ? 0.84% 55.31 ? 0.73% Meta-Learner LSTM <ref type="bibr" target="#b24">[25]</ref> 43.44 ? 0.77% 60.60 ? 0.71% MAML <ref type="bibr" target="#b7">[8]</ref> 48.70 ? 1.84% 63.11 ? 0.92% Ours-Simple 54.53 ? 0.40% 67.87 ? 0.20% Ours-WRN 59.60 ? 0.41% 73.74 ? 0.19% <ref type="table">Table 3</ref>: 5-way accuracies on MiniImageNet with 95% confidence interval. Red: the best, and blue: the second best.</p><p>assumed equal for every channel in cosine distance. This motivates us to investigate the impact of each channel of the activation space. For a fixed activation space, we define the impact of its j-th channel on mapping ? by I j (?) = i |? ij |. Similarly, we define the activation impact I j (?) on w pt large which is the parameter matrix of the last fully connected layer of R pt large . For cosine distance, I j (1) = 1, ?j. Intuitively, we are evaluating the impact of each channel of a on the output by adding all the weights connected to it. For w pt large which is trained for the classification task using large-amounts of data, if we normalize I(w pt large ) to unity, the mean of I(w pt large ) over all channel j is 2.13e-2 and the standard deviation is 5.83e-3. w pt large does not use channels equally, either. In fact, ? 1 has a high similarity with w pt large . We show this by comparing the orders of the channels sorted by their impacts. Let top-k(S) find the indexes of the top-k elements of S. We define the top-k similarity of I(?) and I(w pt large ) by OS(?, w pt large , k) = card top-k(I(?)) ? top-k(I(w pt large )) /k <ref type="bibr" target="#b6">(7)</ref> where card is the cardinality of the set. The right image of <ref type="figure" target="#fig_2">Figure 5</ref> plots the two similarities, from which we observe high similarity between ? and w pt large compared to the random order of 1. From this point of view, ? 1 outperforms the cosine distance due to its better usage of the activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MiniImageNet Classification</head><p>In this subsection we compare our method with the previous state-of-the-arts on the MiniImageNet dataset. Unlike ImageNet classification, the task of MiniImageNet is to find the correct category from 5 candidates, each of which has 1 example or 5 examples for reference. The methods are only evaluated on D few , which has 20 categories. For each task, we uniformly sample 5 categories from D few . For each of the category, we randomly select one or five images as the references, depending on the settings, then regard the rest images of the 5 categories as the test images. For each task, we will have an average accuracy over this 5 categories. We repeat the task with different categories and report the mean of the accuracies with the 95% confidence interval. <ref type="table">Table 3</ref> summarizes the few-shot accuracies of our method and the previous state-of-the-arts. For fair comparisons, we implement two convolutional neural networks. The convolutional network of Ours-Simple is the same as that of Matching Network <ref type="bibr" target="#b27">[28]</ref> while Ours-WRN uses WRN-28-10 <ref type="bibr" target="#b30">[31]</ref> as stated in ?2.4. The experimental results demonstrate that our average accuracies are better than the previous state-of-the-arts by a large margin for both the Simple and WRN implementations.</p><p>It is worth noting that the methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref> are not evaluated in the full ImageNet classification task. This is because the architectures of these methods, following the problem formulation of Matching Network <ref type="bibr" target="#b27">[28]</ref>, can only deal with the test tasks that are of the same number of reference categories and images as that of the training tasks, limiting their flexibilities for classification tasks of arbitrary number of categories and reference images. In contrast, our proposed method has no assumptions regarding the number of the reference categories and the images, while achieving good results on both tasks. From this perspective, our methods are better than the previous state-of-the-arts in terms of both the performance and the flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we study a novel problem: can we develop a unified approach that works for both large-scale and fewshot learning. Our motivation is based on the observation that in the final classification layer of a pre-trained neural network, the parameter vector and the activation vector have highly similar structures in space. This motivates us to learn a category-agnostic mapping from activations to parameters. Once this mapping is learned, the parameters for any novel category can be predicted by a simple forward pass, which is significantly more convenient than re-training used in parametric methods or enumeration of training set used in non-parametric approaches.</p><p>We experiment our novel approach on the MiniImageNet dataset and the challenging full ImageNet dataset. The challenges of the few-shot learning on the full ImageNet dataset are from the large number of categories (1000) and the very limited number (&lt; 4) of training samples for C few . On the full ImageNet dataset, we show promising results, achieving state-of-the-art classification accuracy on novel categories by a significant margin while maintaining comparable performance on the large-scale classes. We further visualize and analyze the learned parameter predictor, as well as demonstrate the similarity between the predicted parameters and those of the classification layer in the pre-trained deep neural network in terms of the activation impact. On the small MiniImageNet dataset, we also outperform the previous state-of-the-art methods by a large margin. The experimental results demonstrate the effectiveness of the proposed method for learning a category-agnostic mapping.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Building the fully connected layer by parameter prediction from activation statistics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the novel category adaption (a) and the training strategies for parameter predictor ? (b). (b): red and solid arrows show the feedforward data flow, while blue and dashed arrow shows the backward gradient flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the upper-left 256 ? 256 submatrix of ? 1 in log scale (left) and top-k similarity between ? 1 , 1 and w pt large (right). In the right plotting, red and solid lines are similarities between ? 1 and w pt large , and green and dashed lines are between 1 and w pt large .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Classifier Top-1 Clarge Top-5 Clarge Top-1 Cfew Top-5 Cfew</figDesc><table><row><cell>NN</cell><cell>70.25%</cell><cell>89.98%</cell><cell>52.46%</cell><cell>80.94</cell></row><row><cell>Linear</cell><cell>75.20%</cell><cell>92.38%</cell><cell>60.50%</cell><cell>87.58</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Locally weighted learning for control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lazy learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="75" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How children learn the meanings of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining topics in documents: standing on the shoulders of big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1116" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Topic modeling using topics from many domains, lifelong learning and big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.5" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>7694</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">What makes imagenet good for transfer learning? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1608.08614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>In Masters thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tenenbaum. One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th Annual Meeting of the Cognitive Science Society</title>
		<meeting>the 33th Annual Meeting of the Cognitive Science Society<address><addrLine>CogSci; Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Oneshot learning by inverting a compositional causal process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer of view-manifold learning to similarity perception of novel objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning like a child: Fast novel visual concept learning from sentence descriptions of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sort: Second-order response transform for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1703.06993</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
