<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IQ-Learn: Inverse soft-Q Learning for Imitation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
							<email>divgarg@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuvam</forename><surname>Chakraborty</surname></persName>
							<email>shuvamc@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cundy</surname></persName>
							<email>cundy@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
							<email>tsong@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IQ-Learn: Inverse soft-Q Learning for Imitation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many sequential decision-making problems (e.g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with complex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn't utilize any information involving the environment's dynamics. Many existing methods that exploit dynamics information are difficult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in offline and online imitation learning settings, significantly outperforming existing methods both in the number of required environment interactions and scalability in high-dimensional spaces, often by more than 3x 1 .</p><p>Proposition 3.5. For a fixed Q, argmin ??? J (?, Q) is simply the solution to max entropy RL with rewards r = T ? Q. Thus, using Eq. 1, the argmin policy satisfies ? Q (a|s) = 1 Z s exp(Q(s, a)),</p><p>with normalization factor Z s = a <ref type="figure">exp Q(s, a)</ref>. Thus, the policy minima for a given Q describes a manifold in the Q-policy space <ref type="figure">(Figure 1)</ref>.</p><p>Proposition 3.4 and 3.5 tell us that if we know Q, then the inner optimization problem in terms of policy is trivial, and obtained in a closed form! Thus, we can recover an objective that only requires learning Q: max</p><p>Lastly, we have: Proposition 3.6. Let J * (Q) = J (? Q , Q). Then the new objective J * is concave in Q.</p><p>Thus, this new optimization objective is well-behaved and has a unique maxima Q * that gives the required saddle point as (? Q * , Q * ). policy reward L(?, ?) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i g 4 y F 5 q J x p 0 C c 2 l I l U 5 z G o v C I U I = " &gt; A A A B 9 H i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J U k J J I R Y 9 F L x 4 8 V L A f 0 I S y 2 W z a p Z t s 3 J 0 U S u j v 8 O J B E a / + G G / + G 7 d t D t r 6 Y O D x 3 g w z 8 / x E c A 2 2 / W 2 t r K 6 t b 2 w W t o r b O 7 t 7 + 6 W D w 5 a W q a K s S a W Q q u M T z Q S P W R M 4 C N Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t 0 f a V r 2 T a 7 3 X c O 9 O g w + H s j 1 R z p s = " &gt; A A A B 8 n i c b V B N S w M x E M 3 6 W e t X 1 a O X Y B E q S N m V i h 6 L X j x 4 q G A / Y L u U b D b b h m a T J Z k V S u n P 8 O J B E a / + G m / + G 9 N 2 D 9 r 6 Y O D x 3 g w z 8 8 J U c A O u + + 2 s r K 6 t b 2 w W t o r b O 7 t 7 + 6 W D w 5 Z R m a a s S Z V Q u h M S w w S X r A k c B O u k m p E k F K w d D m + n f v u J a c O V f I R R y o K E 9 C W P O S V g J f + + 0 q W R g n O s z 3 q l s l t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S B 7 e N 8 / t j H 0 k d 8 P 8 D Z 1 O q 7 + c T n 8 = " &gt; A A A C A H i c b V D L S s N A F J 3 U V 6 2 v q A s X b g a L U E F K I h V d F t 2 I q w r 2 A U 0 o k 8 m k H T</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Imitation of an expert has long been recognized as a powerful approach for sequential decisionmaking <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1]</ref>, with applications as diverse as healthcare <ref type="bibr" target="#b38">[39]</ref>, autonomous driving <ref type="bibr" target="#b40">[41]</ref>, and playing complex strategic games <ref type="bibr" target="#b7">[8]</ref>. In the imitation learning (IL) setting, we are given a set of expert trajectories, with the goal of learning a policy which induces behavior similar to the expert's. The learner has no access to the reward, and no explicit knowledge of the dynamics.</p><p>The simple behavioural cloning <ref type="bibr" target="#b33">[34]</ref> approach simply maximizes the probability of the expert's actions under the learned policy, approaching the IL problem as a supervised learning problem. While this can work well in simple environments and with large quantities of data, it ignores the sequential nature of the decision-making problem, and small errors can quickly compound when the learned policy departs from the states observed under the expert. A natural way of introducing the environment dynamics is by framing the IL problem as an Inverse RL (IRL) problem, aiming to learn a reward function under which the expert's trajectory is optimal, and from which the learned imitation policy can be trained <ref type="bibr" target="#b0">[1]</ref>. This framing has inspired several approaches which use rewards either explicitly or implicitly to incorporate dynamics while learning an imitation policy <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b21">22]</ref>. However, these dynamics-aware methods are typically hard to put into practice due to unstable learning which can be sensitive to hyperparameter choice or minor implementation details <ref type="bibr" target="#b20">[21]</ref>.</p><p>In this work, we introduce a dynamics-aware imitation learning method which has stable, nonadversarial training, allowing us to achieve state-of-the-art performance on imitation learning bench- <ref type="table">Table 1</ref>: A comparison of various algorithms for imitation learning. "Convergence Guarantees" refers to if a proof is given that the algorithm converges to the correct policy with sufficient data. We consider an algorithm "directly optimized" if it consists of an optimization algorithm (such as gradient descent) applied to the parameters of a single function Our key insight is that much of the difficulty with previous IL methods arises from the IRL-motivated representation of the IL problem as a min-max problem over reward and policy <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>This introduces a requirement to separately model the reward and policy, and train these two functions jointly, often in an adversarial fashion. Drawing on connections between RL and energy-based models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, we propose learning a single model for the Q-value. The Q-value then implicitly defines both a reward and policy function. This turns a difficult min-max problem over policy and reward functions into a simpler minimization problem over a single function, the Q-value. Since our problem has a one-to-one correspondence with the min-max problem studied in adversarial IL <ref type="bibr" target="#b16">[17]</ref>, we maintain the generality and guarantees of these previous approaches, resulting in a meaningful reward that may be used for inverse reinforcement learning. Furthermore, our method may be used to minimize a variety of statistical divergences between the expert and learned policy. We show that we recover several previously-described approaches as special cases of particular divergences, such as the regularized behavioural cloning of <ref type="bibr" target="#b29">[30]</ref>, and the conservative Q-learning of <ref type="bibr" target="#b22">[23]</ref>.</p><p>In our experiments, we find that our method is performant even with very sparse data -surpassing prior methods using one expert demonstration in the completely offline setting -and can scale to complex image-based tasks like Atari reaching expert performance. Moreover, our learnt rewards are highly predictive of the original environment rewards. Finally, our method is robust to distribution shifts in the environment showing great generalization performance to never seen goals and an ability to act as a meta-learner.</p><p>Concretely, our contributions are as follows:</p><p>? We present a modified Q-learning update rule for imitation learning that can be implemented on top of soft-Q learning or soft actor-critic (SAC) algorithms in fewer than 15 lines of code. ? We introduce a simple framework to minimize a wide range of statistical distances: Integral Probability Metrics (IPMs) and f-divergences, between the expert and learned distributions. ? We empirically show state-of-art results in a variety of imitation learning settings: online and offline IL. On the complex Atari suite, we outperform prior methods by 3-7x while requiring 3x less environment steps. ? We characterize our learnt rewards and show a high positive correlation with the ground-truth rewards, justifying the use of our method for Inverse Reinforcement Learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Preliminaries We consider environments represented as a Markov decision process (MDP), which is defined by a tuple (S, A, p 0 , P, r, ?). S, A represent state and action spaces, p 0 and P(s |s, a) represent the initial state distribution and the dynamics, r(s, a) ? R represents the reward function, and ? ? (0, 1) represents the discount factor. R S?A = {x : S ? A ? R} will denote the set of all functions in the state-action space and R will denote the extended real numbers R ? {?}. Sections 3 and 4 will work with finite state and action spaces S and A, but our algorithms and experiments later in the paper use continuous environments. ? is the set of all stationary stochastic policies that take actions in A given states in S. We work in the ?-discounted infinite horizon setting, and we will use an expectation with respect to a policy ? ? ? to denote an expectation with respect to the trajectory it generates: E ? [r(s, a)] E[ ? t=0 ? t r(s t , a t )], where s 0 ? p 0 , a t ? ?(?|s t ), and s t+1 ? P(?|s t , a t ) for t ? 0. For a policy ? ? ?, we define its occupancy measure ? ? : S ? A ? R as ? ? (s, a) = (1 ? ?)?(a|s) ? t=0 ? t P (s t = s|?). We refer to the expert policy as ? E and its occupancy measure as ? E . In practice, ? E is unknown and we have access to a sampled dataset of demonstrations D. For brevity, we refer to ? ? as ? for a learnt policy in the paper.</p><p>Soft Q-functions For a reward r ? R and ? ? ?, the soft Bellman operator B ? r : R S?A ? R S?A is defined as (B ? r Q)(s, a) = r(s, a) + ?E s ?P(s,a) V ? (s ) with V ? (s) = E a??(?|s) [Q(s, a) ? log ?(a|s)]. The soft Bellman operator is contractive <ref type="bibr" target="#b12">[13]</ref> and defines a unique soft Q-function for r, given as the fixed point solution Q = B ? r Q with Q ? ?.</p><p>Max Entropy Reinforcement Learning For a given reward function r ? R, maximum entropy RL <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref> aims to learn a policy that maximizes the expected cumulative discounted reward along with the entropy in each state: max ??? E ?? [r(s, a)] + H(?), where H(?) E ?? [? log ?(a|s)] is the discounted causal entropy of the policy ?. The optimal policy satisfies <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5]</ref>:</p><formula xml:id="formula_0">? * (a|s) = 1 Z s exp (Q * (s, a)),<label>(1)</label></formula><p>where Z s is the normalization factor given as a exp (Q * (s, a )) and Q * is the optimal soft Qfunction. </p><p>In continuous action spaces, Z s becomes computationally intractable and soft actor-critic methods like SAC <ref type="bibr" target="#b12">[13]</ref> can be used to learn an explicit policy.</p><p>Max Entropy Inverse Reinforcement Learning Given demonstrations sampled using the policy ? E , maximum entropy Inverse RL aims to recover the reward function in a family of functions R ? R S?A that rationalizes the expert behavior by solving the optimization problem:</p><formula xml:id="formula_2">max r?R min ??? E ? E [r(s, a)] ? (E ?? [r(s, a)] + H(?))</formula><p>, where the expected reward of ? E is empirically approximated using a dataset D. It looks for a reward function that assigns high reward to the expert policy and low reward to other ones, while searching for the best policy for the reward function in an inner loop.</p><p>The Inverse RL objective can be generalized in terms of its occupancy measure, and with a convex reward regularizer ? :</p><formula xml:id="formula_3">R S?A ? R [17] max r?R min ??? L(?, r) = E ? E [r(s, a)] ? E ?? [r(s, a)] ? H(?) ? ?(r).<label>(3)</label></formula><p>In general, for a non-restrictive set of reward functions R = R S?A , we can exchange the max-min resulting in an objective that minimizes the statistical distance parameterized by ?, between the expert and the policy <ref type="bibr" target="#b16">[17]</ref> min</p><formula xml:id="formula_4">??? max r?R L(?, r) = min ??? d ? (? ? , ? E ) ? H(?),<label>(4)</label></formula><formula xml:id="formula_5">with d ? ? * (? E ? ? ? ),</formula><p>where ? * is the convex conjugate of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inverse soft Q-learning (IQ-Learn) Framework</head><p>A naive solution to the nested min-max IRL problem in (Eq. 3) involves (1) an outer loop learning rewards and (2) executing RL in an inner loop to find an optimal policy for them. However, we know that this optimal policy can be obtained solely in terms of the soft Q-function (Eq. 1). Interestingly, as we will show later, the rewards can also be represented in terms of only Q (Eq. 2). Together, these observations suggest it might be possible to directly solve the IRL problem by optimizing only over the Q-function, thus reducing the nested min-max problem to a single minimization problem over Q.</p><p>To motivate the search of an imitation learning algorithm that depends only on the Q-function, we characterize the space of Q-functions and policies obtained using Inverse RL. We will study ? ? ?, r ? R and Q-functions Q ? ?, with fully general classes R = ? = R S?A . The full policy class ? is convex, compact with ? E ? ?.</p><p>We start with the analysis developed in <ref type="bibr" target="#b16">[17]</ref>: Proposition 3.1. The regularized IRL objective L(?, r) given by Eq. 3 is convex in the occupancy measure of the policy (? ? ) and concave in the reward function (r), and for a strongly convex regularizer ? has a unique saddle point (? * , r * ).</p><p>To characterize the Q-functions obtained using Inverse RL it is useful to transform the IRL problem over rewards to a problem over Q-functions.</p><p>Define the inverse soft Bellman operator T ? :</p><formula xml:id="formula_6">R S?A ? R S?A as (T ? Q)(s, a) = Q(s, a) ? ?E s ?P(?|s,a) V ? (s ).</formula><p>with V ? (s) = E a??(?|s) [Q(s, a) ? log ?(a|s)] as defined before. Then, T ? inverts the soft Bellman operator B ? to map from Q-functions to rewards. We can get a one-to-one correspondence between r and Q: Lemma 3.2. For a fixed policy ?, the inverse soft Bellman operator T ? is bijective, and for any r ? R, Q = (T ? ) ?1 r is the unique fixed point of the Bellman operator B ? r .</p><p>The proof of this lemma is in Appendix A.2. For a policy ?, we are justified in changing between rewards and the corresponding soft-Q functions using T ? . Thus, we can freely transform functions from the reward-policy space, ? ? R, to the Q-policy space, ? ? ?, giving us the following lemma:</p><formula xml:id="formula_7">Lemma 3.3. Let L(?, r) = E ? E [r(s, a)] ? E ?? [r(s, a)] ? H(?) ? ?(r) and J (?, Q) = E ? E [(T ? Q)(s, a)] ? E ?? [(T ? Q)(s, a)] ? H(?) ? ?(T ? Q)</formula><p>, then for all policies ? ? ?, L(?, r) = J (?, (T ? ) ?1 r) ?r ? R, and J (?, Q) = L(?, T ? Q) ?Q ? ?.</p><p>The proof follows directly from Lemma 3.2. These lemmas allow us to adapt the Inverse RL objective L(?, r) to learning Q through J (?, Q), i.e., working in the Q-policy space.</p><p>We can simplify the new objective J (?, Q) by working with initial states s 0 sampled from the initial state distribution p 0 (s) (Lemma A.2 in Appendix) as follows:</p><formula xml:id="formula_8">J (?, Q) = E (s,a)?? E [Q(s, a) ? ?E s ?P(?|s,a) V ? (s )] ? (1 ? ?)E s0?p0 [V ? (s 0 )] ? ?(T ? Q). (5)</formula><p>where again V ? (s) = E a??(?|s) [Q(s, a) ? log ?(a|s)].</p><p>We are now ready to study J (?, Q), the Inverse RL problem in the Q-policy space. As the regularizer ? depends on both Q and ?, a general analysis over all functions in R S?A becomes too difficult. We restrict ourselves to regularizers induced by a convex function g :</p><formula xml:id="formula_9">R ? R such that ? g (r) = E ? E [g(r(s, a))].<label>(6)</label></formula><p>This allows us to simplify our analysis to the set of all real functions while retaining generality 2 . We further motivate this choice in Section 4. Proposition 3.4. In the Q-policy space, there exists a unique saddle point (? * , Q * ) that optimizes J . i.e. Q * = argmax Q?? min ??? J (?, Q) and ? * = argmin ??? max Q?? J (?, Q). Furthermore, ? * and r * = T ? * Q * are the solution to the Inverse RL objective L(?, r).</p><p>Thus we have, max Q?? min ??? J (?, Q) = max r?R min ??? L(?, r). And the maxima Q * is simply the optimal soft Q-function for the reward r * .</p><p>This tells us, even after transforming to Q-functions we have retained the saddle point property of the original IRL objective and optimizing J (?, Q) recovers this saddle point. In the Q-policy space, we can get an additional property: In Appendix C we expand on our analysis and characterize the behavior for different choices of regularizer ?, while giving proofs of all our propositions. <ref type="figure">Figure 1</ref> summarizes the properties for the IRL objective: there exists an optimal policy manifold depending on Q, allowing optimization along it (using J * ) to converge to the saddle point. We further present analysis of IL methods that learn Q-functions like SQIL <ref type="bibr" target="#b32">[33]</ref> and ValueDICE <ref type="bibr" target="#b21">[22]</ref> and find subtle fallacies affecting their learning.</p><formula xml:id="formula_10">? = 1 Zs exp(Q)</formula><p>Note that although the same analysis holds in the reward-policy space, the optimal policy manifold depends on Q, which isn't trivially known unlike when we work directly in the Q-policy space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>In this section, we develop our inverse soft-Q learning (IQ-Learn) algorithm, such that it recovers the optimal soft Q-function for an MDP from a given expert distribution. We start by learning energy-based models for the policy similar to soft Q-learning and later learn an explicit policy similar to actor-critic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">General Inverse RL Objective</head><p>For designing a practical algorithm using regularizers of the form ? g (from Eq. 6), we define g using a concave function ? :</p><formula xml:id="formula_11">R ? ? R, such that g(x) = x ? ?(x) if x ? R ? +? otherwise with the rewards constrained in R ? .</formula><p>For this choice of ?, the Inverse RL objective L(?, r) takes the form of Eq. 4 with a distance measure:</p><formula xml:id="formula_12">d ? (?, ? E ) = max r?R ? E ? E [?(r(s, a))] ? E ? [r(s, a)],<label>(8)</label></formula><p>This forms a general learning objective that allows the use of a wide-range of statistical distances including Integral Probability Metrics (IPMs) and f-divergences (see Appendix B). <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Choice of Statistical Distances</head><p>While choosing a practical regularizer, it can be useful to obtain certain properties on the reward functions we recover. Some (natural) nice properties are: having rewards bounded in a range, learning smooth functions or enforcing a norm-penalty.</p><p>In fact, we find these properties correspond to the Total Variation distance, the Wasserstein-1 distance and the ? 2 -divergence respectively. The regularizers and the induced statistical distances are summarized in <ref type="table">Table 2</ref>: <ref type="table">Table 2</ref>: Enforced reward property, corresponding regularizer ? and statistical distance (Rmax, K, ? ? R + )</p><formula xml:id="formula_13">Reward Property ? d ? Bound range ? = 0 if |r| ? Rmax and +? otherwise 2Rmax ? TV(?, ?E) Smoothness ? = 0 if r Lip ? K and +? otherwise K ? W1(?, ?E) L2 Penalization ?(r) = ?r 2 1 4? ? ? 2 (?, ?E)</formula><p>We find that these choices of regularizers 4 work very well in our experiments. In Appendix B, we further give a table for the well known f -divergences, the corresponding ? and the learnt reward estimators, along with a result ablation on using different divergences. Compared to ? 2 , we find other f -divergences like Jensen-Shannon result in similar performances but are not as readily interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inverse soft-Q update (Discrete control)</head><p>Optimization along the optimal policy manifold gives the concave objective (Prop 3.6): max</p><formula xml:id="formula_14">Q?? J * (Q) = E ? E [?(Q(s, a) ? ?E s ?P(?|s,a) V * (s ))] ? (1 ? ?)E ?0 [V * (s 0 )],<label>(9)</label></formula><p>with V * (s) = log a exp Q(s, a).</p><p>For each Q, we get a corresponding reward r(s, a) = Q(s, a)??E s ?P(?|s,a) [log a exp Q (s , a )]. This correspondence is unique (Lemma C.3 in Appendix), and every update step can be seen as finding a better reward for IRL.</p><p>Note that estimating V * (s) exactly is only possible in discrete action spaces. Our objective forms a variant of soft-Q learning: to learn the optimal Q-function given an expert distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Inverse soft actor-critic update (Continuous control)</head><p>In continuous action spaces, it might not be possible to exactly obtain the optimal policy ? Q , which forms an energy-based model of the Q-function, and we use an explicit policy ? to approximate ? Q .</p><p>For any policy ?, we have a objective (from Eq. 5):</p><formula xml:id="formula_15">J (?, Q) = E ? E [?(Q ? ?E s ?P(?|s,a) V ? (s ))] ? (1 ? ?)E ?0 [V ? (s 0 )].<label>(10)</label></formula><p>For a fixed Q, soft actor-critic (SAC) update: max ? E s?D,a??(?|s) [Q(s, a) ? log ?(a|s)], brings ? closer to ? Q while always minimizing Eq. 10 (Lemma A.3 in Appendix). Here D is the distribution of previously sampled states, or a replay buffer.</p><p>Thus, we obtain the modified actor-critic update rule to learn Q-functions from the expert distribution:</p><p>1. For a fixed ?, optimize Q by maximizing J (?, Q). 2. For a fixed Q, apply SAC update to optimize ? towards ? Q .</p><p>This differs from ValueDICE <ref type="bibr" target="#b21">[22]</ref>, where the actor is updated adverserially and the objective may not always converge (Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Practical Algorithm</head><p>Algorithm 1 shows our Q-learning and actor-critic variants, with differences with conventional RL algorithms in red (we optimize -J to use gradient descent). We can implement our algorithm IQ-Learn in 15 lines of code on top of standard implementations of (soft) DQN <ref type="bibr" target="#b13">[14]</ref> for discrete control or soft actor-critic (SAC) <ref type="bibr" target="#b12">[13]</ref> for continuous control, with a change on the objective for the Q-function. Default hyperparameters from <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> work well, except for tuning the entropy regularization. Target networks were helpful for continuous control. We elaborate details in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training methodology</head><formula xml:id="formula_16">Corollary A.1.1 states E (s,a)?? [V ? (s) ? ?E s ?P(?|s,a) V ? (s )] = (1 ? ?)E s?p0 [V ? (s)],</formula><p>where ? is any policy's occupancy. We use this to stabilize training instead of using Eq. 9 directly.</p><p>Algorithm 1 Inverse soft Q-Learning (both variants) 1: Initialize Q-function Q ? , and optionally a policy ? ? 2: for step t in {1...N} do 3:</p><p>Train Q-function using objective from Equation <ref type="bibr" target="#b8">9</ref>:</p><formula xml:id="formula_17">?t+1 ? ?t ? ?Q? ? [?J (?)]</formula><p>(Use V * for Q-learning and V ? ? for actor-critic) 4:</p><p>(only with actor-critic) Improve policy ? ? with SAC style actor update:</p><formula xml:id="formula_18">?t+1 ? ?t + ??? ? E s?D,a?? ? (?|s) [Q(s, a) ? log ? ? (a|s)] 5: end for</formula><p>Algorithm 2 Recover policy and reward 1: Given trained Q-function Q ? , and optionally a trained policy ? ? 2: Recover policy ?:</p><p>(Q-learning) ? := 1 Z exp Q ? (actor-critic) ? := ? ? 3: For state s, action a and s ? P(?|s, a) 4: Recover reward r(s, a, s ) = Q ? (s, a) ? ?V ? (s ) Online:</p><p>Instead of directly estimating E p0 [V ? (s 0 )] in our algorithm, we can sample (s, a, s ) from a replay buffer and get a single-sample estimate</p><formula xml:id="formula_19">E (s,a,s )?replay [V ? (s) ? ?V ? (s )].</formula><p>This removes the issue where we are only optimizing Q in the inital states resulting in overfitting of V ? (s 0 ), and improves the stability for convergence in our experiments. We find sampling half from the policy buffer and half from the expert distribution gives the best performances. Note that this is makes our learning online, requiring environment interactions.</p><formula xml:id="formula_20">Offline: Although E p0 [V ? (s 0 )]</formula><p>can be estimated offline we still observe an overfitting issue. Instead of requiring policy samples we use only expert samples to estimate E (s,a,s )?expert [V ? (s) ? ?V ? (s )] to sufficiently approximate the term. This methodology gives us state-of-art results for offline IL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Recovering rewards</head><p>Instead of the conventional reward function r(s, a) on state and action pairs, our algorithm allows recovering rewards for each transition (s, a, s ) using the learnt Q-values as follows:</p><formula xml:id="formula_21">r(s, a, s ) = Q(s, a) ? ?V ? (s )<label>(11)</label></formula><p>Now, E s ?P(?|s,a) [Q(s, a) ? ?V ? (s )] = Q(s, a) ? ?E s ?P(?|s,a) [V ? (s )] = T ? Q(s, a). This is just the reward function r(s, a) we want. So by marginalizing over next-states, our expression correctly recovers the reward over state-actions. Thus, Eq. 11 gives the reward over transitions.</p><p>Our rewards require s which can be sampled from the environment, or by using a dynamics model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation of Statistical Distances</head><p>Implementing TV and W 1 distances is fairly trivial and we give details in Appendix B. For the ? 2 -divergence, we note that it corresponds to ?(x) = x ? 1 4? x 2 . On substituting in Eq. 9, we get</p><formula xml:id="formula_22">max Q?? E ?E [(Q(s, a) ? ?E s ?P(?|s,a) V * (s ))] ? (1 ? ?)E p0 [V * (s 0 )] ? 1 4? E ?E [(Q(s, a) ? ?E s ?P(?|s,a) V * (s )) 2 ]</formula><p>In a fully offline setting, this can be further simplified as (using the offline methodology in Sec 5.1):</p><formula xml:id="formula_23">min Q?? ? E ? E [(Q(s, a) ? V * (s))] + 1 4? E ? E [(Q(s, a) ? ?E s ?P(?|s,a) V * (s )) 2 ]<label>(12)</label></formula><p>This is interestingly the same as the Q-learning objective in CQL <ref type="bibr" target="#b22">[23]</ref>, a state-of-art method for offline RL (using 0 rewards), and shares similarities with regularized behavior cloning <ref type="bibr" target="#b32">[33]</ref>. <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Learning state-only reward functions</head><p>Previous works like AIRL <ref type="bibr" target="#b9">[10]</ref> propose learning rewards that are only function of the state, and claim that these form of reward functions generalize between different MDPs. We find our method can predict state-only rewards by using the policy and expert state-marginals with a modification to Eq. 9:</p><formula xml:id="formula_24">max Q?? J * (Q) = E s?? E (s) [E a??(?|s) [?(Q(s, a) ? ?E s ?P(?|s,a) V * (s ))]] ? (1 ? ?)E p0 [V * (s 0 )],</formula><p>with ? being here a stop gradient of ? Q . Interestingly, our objective no longer depends on the the expert actions ? E and can be used for IL using only observations. For the sake of brevity, we expand on this in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Classical IL: Imitation learning has a long history, with early works using supervised learning to match a policy's actions to those of the expert <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>. A significant advance was made with the formulation of IL as the composition of RL and IRL <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b42">43]</ref>, recovering the expert's policy by inferring the expert's reward function, then finding the policy which maximizes reward under this reward function. These early approaches required a hand-designed featurization of the MDP, limiting their applicability to complex MDPs. In this setting, early approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref> noted a formal equivalence between IRL and IL using an inverse Bellman operator similar to our own.</p><p>Online IL: More recent work aims to leverage the power of modern machine learning approaches to learn good featurizations and extend IL to complex settings. Recent work generally falls into one of two settings: online or offline. In the online setting, the IL algorithm is able to interact with the environment to obtain dynamics information. GAIL <ref type="bibr" target="#b16">[17]</ref> takes the nested RL/IRL formulation of earlier work , optimizing over all reward functions with a convex regularizer. This results in the objective in Eq. <ref type="formula" target="#formula_3">(3)</ref>, with a max-min adversarial problem similar to a GAN <ref type="bibr" target="#b10">[11]</ref>. A variety of further work has built on this adversarial approach <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3]</ref>. A separate line of work aims to simplify the problem in Eq. (3) by using a fixed r or ?. In SQIL <ref type="bibr" target="#b32">[33]</ref>, r is chosen to be the 1-0 indicator on the expert demonstrations, while ASAF <ref type="bibr" target="#b3">[4]</ref> takes the GAN approach and uses a discriminator (with role similar to r) of fixed form, consisting of a ratio of expert and learner densities. AdRIL <ref type="bibr" target="#b37">[38]</ref> is a recent extension of SQIL, additionally assigning decaying negative reward to previous policy rollouts.</p><p>Offline IL: In the offline setting, the learner has no access to the environment. The simple behavioural cloning (BC) <ref type="bibr" target="#b33">[34]</ref> approach is offline, but doesn't use any dynamics information. ValueDICE <ref type="bibr" target="#b21">[22]</ref> is a dynamics-aware offline approach with an objective somewhat similar to ours, motivated from minimization of a variational representation of the KL-divergence between expert and learner policies. ValueDICE requires adversarial optimization to learn the policy and Q-functions, with a biased gradient estimator for training. We show a way to recover a unbiased gradient estimate for the KL-divergence in Appendix C. The O-NAIL algorithm <ref type="bibr" target="#b1">[2]</ref> builds on ValueDICE and combines with a SAC update to obtain a method that is similar to our algorithm described in section 4.4, with the specific choice of reverse KL-divergence as the relevant statistical distance. The EDM method <ref type="bibr" target="#b18">[19]</ref> incorporates dynamics via learning an explicit energy based model for the expert state occupancy, although some theoretical details have been called into question (see <ref type="bibr" target="#b36">[37]</ref> for details). The recent AVRIL approach <ref type="bibr" target="#b5">[6]</ref> uses a variational method to solve a probabilistic formulation of IL, finding a posterior distribution over r and ?. Illustrating the potential benefits of alternative distances for IL, the PWIL <ref type="bibr" target="#b6">[7]</ref> algorithm gives a non-adversarial procedure to minimize the Wasserstein distance between expert and learned occupancies. The approach is specific to the primal form of the W 1 -distance, while our method (when used with the Wasserstein distance) targets the dual form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Setup</head><p>We compare IQ-Learn ("IQ") to prior works on a diverse collection of RL tasks and environmentsranging from low-dimensional control tasks: CartPole, Acrobot, LunarLander -to more challenging continuous control MuJoCo tasks: HalfCheetah, Hopper, Walker and Ant. Furthermore, we test on the visually challenging Atari Suite with high-dimensional image inputs. We compare on offline ILwith no access to the the environment while training, and online IL -with environment access. We show results on W 1 and ? 2 as our statistical distances, as we found them more effective than TV distance. In all cases, we train until convergence and average over multiple seeds. Hyperparameter settings and training details are detailed in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Benchmarks</head><p>Offline IL We compare to the state-of-art IL methods EDM and AVRIL, following the same experimental setting as <ref type="bibr" target="#b5">[6]</ref>. Furthermore, we compare with ValueDICE which also learns Q-functions, albeit with drawbacks such as adversarial optimization. We also experimented with SQIL, but found that it was not competitive in the offline setting. Finally, we utilize BC as an additional IL baseline.</p><p>Online IL We use MuJoCo and Atari environments and compare against state-of-art online IL methods: ValueDICE, SQIL and GAIL. We only show results on ? 2 as W 1 was harder to stabilize on complex environments <ref type="bibr" target="#b5">6</ref>   Offline IL We present results on the three offline control tasks in <ref type="figure" target="#fig_2">Figure 2</ref>. On all tasks, IQ strongly outperforms prior works we compare to in performance and sample efficiency. Using just one expert trajectory, we achieve expert performance on Acrobot and reach near expert on Cartpole. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mujoco Control</head><p>We present our results on the MuJoCo tasks using a single expert demo in <ref type="table" target="#tab_2">Table 3</ref>. IQ achieves expert-level performance in all the tasks while outperforming prior methods like ValueDICE and GAIL. We did not find SQIL competitive in this setting, and skip it for brevity.</p><p>Atari We present our results on Atari using 20 expert demos in <ref type="figure" target="#fig_3">Figure 3</ref>. We reach expert performance on Space Invaders while being near expert on Pong and Breakout. Compared to prior methods like SQIL, IQ obtains 3-7x normalized score 7 and converges in ?300k steps, being 3x faster compared to Q-learning based RL methods that take more than 1M steps to converge. Other popular methods like GAIL and ValueDICE perform near random even with 1M env steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Recovered Rewards</head><p>IQ has the added benefit of recovering rewards and can be used for IRL. On Hopper task, our learned rewards have a Pearson correlation of 0.99 with the true rewards. In <ref type="figure" target="#fig_4">Figure 4</ref>, we visualize our recovered rewards in a simple grid environment. We elaborate details in Appendix D.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Robustness to Distribution Shifts</head><p>We find IQ to be robust to distribution shifts between the expert and policy occupanices, and detail experiments with shift in the initial state distributions as well as goal distributions in Appendix F.</p><p>Overall we find that IQ shows good generalization performance to never seen before goals, and the capability to act as a meta-learner for IL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Outlook</head><p>We present a new principled framework for learning soft-Q functions for IL and recovering the optimal policy and the reward, building on past works in IRL <ref type="bibr" target="#b42">[43]</ref>. Our algorithm IQ-Learn outperforms prior methods with very sparse expert data and scales to complex image-based environments. We also recover rewards highly correlated with actual rewards. It has applications in autonomous driving and complex decision-making, but proper considerations need to be taken into account to ensure safety and reduce uncertainty, before any deployment. Finally, human or expert data can have errors that can propagate. A limitation of our method is that our recovered rewards depend on the environment dynamics, preventing trivial use on reward transfer settings. One direction of future work could be to learn a reward model from the trained soft-Q model to make the rewards explicit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>We thank Kuno Kim and John Schulman for helpful discussions. We also thank Ian Goodfellow as some initial motivations for this work were developed under an internship with him.</p><p>A Appendix A <ref type="figure">Figure 5</ref>: State Rewards Visualization. We visualize the state-only rewards recovered on a continuous control point maze task. The agent (white circle) has to reach the goal (red star) avoiding the barrier on right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Learning with state-only rewards</head><p>For a policy ? ? ?, we define its state-marginal occupancy measure ? ? :</p><formula xml:id="formula_25">S ? R as ? ? (s) = (1 ? ?) ? t=0 ? t P (s t = s|?)</formula><p>. Suppose we are interested in learning rewards that are functions of only the states, then the Inverse-RL objective L from Eq. 3 becomes a function of the state-marginal occupancies: max</p><formula xml:id="formula_26">r?R ? min ??? L s (?, r) = E s?? E (s) [?(r(s))] ? E s??(s) [r(s)] ? H(?).<label>(13)</label></formula><p>Now, we can parameterize the rewards r(s) using state-only value-functions V (s) and remove the dependency on Q(s, a). Then V (s) can be learnt similar to learning Q(s, a) in the main paper, but Q(s, a) remains unknown and the optimal policy cannot be obtained simply as an energy-based model of Q.</p><p>Instead, we develop a new objective that can learn Q while recovering state-only rewards below.</p><p>We expand the original objective L using the expert occupancy:</p><formula xml:id="formula_27">L(?, r) = E s?? E (s) E a?? E (?|s) ?(r(s, a)) ? ?(s)?(a|s) ? E (s)? E (a|s)</formula><p>r(s, a)) ? H(?).</p><p>We see that the action dependency comes in the equation from the fact that we have ?/? E inside. Now, we propose to fix the expression to make it independent of actions by replacing the expert policy ? E with the policy ?. The new objective becomes:</p><formula xml:id="formula_28">L (?, r) = E s?? E (s) E a??(?|s) ?(r(s, a)) ? ?(s) ? E (s) r(s, a) ? H(?).</formula><p>Then for a fixed policy ?, while maximizing over r the constraint we have is that each reward component r(s, a) ? R ? . In a state s, r(s, a) that maximizes the objective will take the same value independent of the action <ref type="bibr" target="#b7">8</ref> . Thus, the expectation over actions can be removed and this recovers Eq. 13.</p><p>Writing the new objective using Q-functions, we get the modification to Eq. 9: max</p><formula xml:id="formula_29">Q?? J * (Q) = E s?? E (s) [E a??(?|s) [?(Q(s, a) ? ?E s ?P (s,a) V * (s ))]] ? (1 ? ?)E p0 [V * (s 0 )],<label>(14)</label></formula><p>with ? set to stop_grad(? Q ) to prevent passing gradients through it.</p><p>This new objective does not depend on the the expert actions ? E and can be used for IL using only observations (ILO). We visualize state-only rewards recovered on a 2D point mass navigation task in <ref type="figure">Fig 5.</ref> Notice that the rewards are not directional and are high on all sides of the target point, indicating they are not dependent on the action. We present additional results in Appendix D and a theoretical guarantee in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proofs for Section 3 and Section 4</head><p>Proof for Lemma 3.2. Let P ? be the (stochastic) transition matrix for the MDP corresponding to a policy ?, such that for any x ? R S?A , P ? x(s, a) = E s ?P(?|s,a),a ??(?|s ) [x(s , a )].</p><p>Let r = T ? Q for any Q ? R S?A . We expand T ? in vector form over S ? A using P ? . Then r = Q ? ?P ? (Q ? log ?). Here, (I ? ?P ? ) is invertible as ?P ? &lt; 1, for ? &lt; 1, and the corresponding Neumann series converges. Thus Q = (I ? ?P ? ) ?1 (r ? log ?) + log ?. So we see that for any r ? R S?A , there exists a unique preimage Q ? R S?A proving that T ? is a bijection.</p><p>Furthermore, on rearranging the vector form, we have Q = r + ?P ? (Q ? log ?). This is just the vector expansion of the soft-bellmann operator B ? r , which has a unique contraction Q for a given r. Thus, Q = (T ? ) ?1 r = B ? r Q for any r ? R S?A . Lemma A.1. Let the initial state distribution be p 0 (s), then for a policy ? and V ? defined as before, we have</p><formula xml:id="formula_30">E (s,a)??? [V ? (s) ? ?E s ?P(?|s,a) V ? (s )] = (1 ? ?)E s?p0 [V ? (s)].</formula><p>Proof. We expand the discounted stationary distribution ? over state-actions and show the series forms a telescopic sum. Let p ? t (s) be the marginal state distribution at time t for a policy ?. Then,</p><formula xml:id="formula_31">E (s,a)??? [V ? (s) ? ?E s ?P(?|s,a) V ? (s )] = (1 ? ?) ? t=0 ? t E s?p ? t ,a??(s) V ? (s) ? ?E s ?P(?|s,a) V ? (s ) = (1 ? ?) ? t=0 ? t E s?p ? t [V ? (s)] ? (1 ? ?) ? t=0 ? t+1 E s?p ? t+1 [V ? (s)] = (1 ? ?)E s?p0 [V ? (s)].</formula><p>Corollary A.1.1. In fact, for any valid occupancy measure ? over state-actions and V ? , it holds that</p><formula xml:id="formula_32">E (s,a)?? [V ? (s) ? ?E s ?P(?|s,a) V ? (s )] = (1 ? ?)E s?p0 [V ? (s)].</formula><p>Proof. This relies on the fact that V ? (s) is a function of only state and doesn't depend on the action. First, for any valid occupancy measure ?, there exists a corresponding unique policy ? ? (a|s) s.t. ? ? generates ? <ref type="bibr" target="#b16">[17]</ref>.</p><p>Let p ? t (s) be the marginal state distribution at timestep t for the policy ? ? . Then,</p><formula xml:id="formula_33">E (s,a)?? [V ? (s) ? ?E s ?P(?|s,a) V ? (s )] = (1 ? ?) ? t=0 ? t E s?p ? t ,a?? ? (s) V ? (s) ? ?E s ?P(?|s,a) V ? (s ) = (1 ? ?) ? t=0 ? t E s?p ? t [V ? (s)] ? (1 ? ?) ? t=0 ? t+1 E s?p ? t+1 [V ? (s )] = (1 ? ?)E s?p ? 0 [V ? (s)]</formula><p>. Now p ? 0 is just the initial state distribution p 0 which is independent of the policy, thus giving our result.</p><formula xml:id="formula_34">Lemma A.2. E ?? [(T ? Q)(s, a)] + H(?) = (1 ? ?)E p0 [V ? (s 0 )], where p 0 (s) is the initial state distribution.</formula><p>Proof. We can show this forms a telescopic series as in <ref type="bibr" target="#b27">[28]</ref> using lemma A.1 to depend only on the initial state distribution: </p><formula xml:id="formula_35">= (1 ? ?)E p0 [V ? (s)].</formula><p>This makes sense as the LHS and RHS both represent the max entropy RL objective, that is to maximize the cumulative sum of rewards or the expected value with respect to a policy for the initial state.</p><p>Lemma A.3. SAC actor update decreases the objective J (?, Q) for the actor-critic update in main paper, wrt ? for a fixed Q.</p><p>Proof.</p><formula xml:id="formula_36">V ? (s) = E a?? [Q(s, a) ? log ?(a|s)] = ?D KL ?(?|s) 1 Z s exp(Q(s, ?) + log(Z s ),</formula><p>where Z s is the normalizing factor a exp Q(s, a)</p><p>Now, for a policy ? the the SAC actor update rule <ref type="bibr" target="#b12">[13]</ref> is arg min</p><formula xml:id="formula_37">? D KL ? 1 Z exp(Q) .</formula><p>Thus, if ? is the policy obtained on applying the SAC actor update to ? , we have V ? (s) &gt; V ? (s). So, as long as ? in J is a monotonically non-decreasing function, this implies J (?, Q) &lt; J (? , Q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Appendix B</head><p>Integral Probability Metric (IPM) An IPM parameterized by F between two distributions P and Q is defined as</p><formula xml:id="formula_38">? F (P, Q) := sup f ?F |E P f (X) ? E Q f (X)| .<label>(15)</label></formula><p>Suppose F is such that f ? F ? ?f ? F. Then,</p><formula xml:id="formula_39">? F (P, Q) = sup f ?F |E P f ? E Q f | = sup f ?F E P f ? E Q f.<label>(16)</label></formula><p>Some IPMs that satisfy this symmetry are: Dudley metric, Wasserstein metric, total variation distance, Maximum Mean Discrepancy (MMD).</p><p>We can see that for ? = I, R ? = F, Eq. 8 reduces to Eq. 16.</p><p>f -divergence The f -divergence between two distributions P and Q is defined using the convex conjugate f * as</p><formula xml:id="formula_40">D f (P Q) = E Q f P Q = sup g:X ?R E P [g(X)] ? E Q [f * (g(X))] .<label>(17)</label></formula><p>Interpreting g = ?r,</p><formula xml:id="formula_41">D f (P Q) = sup r:X ?R E P [?r(X)] ? E Q [f * (?r(X))]<label>(18)</label></formula><p>= sup</p><formula xml:id="formula_42">r:X ?R E Q [?f * (?r)] ? E P [r].<label>(19)</label></formula><p>Thus, for ?(x) = ?f * (?x), R ? = R S?A , Eq. 8 reduces to Eq. 18. </p><formula xml:id="formula_43">Divergence f (t) f * (u) ?(x) r Forward KL ? log t ?1 ? log(?u) 1 + log x ? E ? Reverse KL t log t e (u?1) ?e ?(x+1) ?(1 + log ? ? E ) Squared Hellinger ( ? t ? 1) 2 u 1?u x 1+x ? E ? ? 1 Pearson ? 2 (t ? 1) 2 u + u 2 4 x ? x 2 4 2(1 ? ? ? E ) Total variation 1 2 |t ? 1| u x 1 2 sign (1 ? ? ? E ) Jensen-Shannon ?(t + 1) log( t+1 2 ) + t log t ? log (2 ? e u ) log (2 ? e ?x ) log 1 2 (1 + ? E ? )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Implementation of Statistical Distances</head><p>Total Variation Total variation gives a constraint on reward functions: |r| ? 1 2 . As Q t = ? t=t ? t r(s t , a t ) + ? t H(a t |s t ), we obtain a constraint on Q:</p><formula xml:id="formula_44">|Q| ? 1 1?? (R max + log |A|) = 1 1?? ( 1 2 + log |A|)</formula><p>This can be easily enforced by bounding Q to this range using a tanh activation.</p><p>W 1 Distance For Wasserstein-1 distance, we use gradient penalty <ref type="bibr" target="#b11">[12]</ref> to enforce the Lipschitz constraint, although other techniques like spectral normalization <ref type="bibr" target="#b25">[26]</ref> can also be utilized.</p><formula xml:id="formula_45">? 2 -divergence ? 2 -divergence corresponds to an f -divergence with a choice of f (x) = (x ? 1) 2 .</formula><p>We generalize this to a choice of f (x) = ?(x ? 1) 2 with ? &gt; 0, which scales the original divergence by a constant factor of ?.</p><formula xml:id="formula_46">Then ?(x) = ?f * (?x) = x ? 1 4? x 2 .</formula><p>It corresponds to using a (strong) convex reward regularizer ?(r) = 1 4? r 2 . We test IQ-Learn with different divergences: Jensen-Shannon (JS), Hellinger, KL and ? 2 divergence. We use the LunarLander environment with our offline IL experimental settings and a single expert trajectory. All experiments are repeated over 10 seeds. We show a box-plot of the environment returns for different divergences and find that JS, Hellinger and ? 2 divergence perform similarly, consistent with the findings on different type of GANs <ref type="bibr" target="#b24">[25]</ref>. Here, KL-divergence performs worse and is suboptimal compared to the other divergences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Effect of different Divergences</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Appendix C</head><p>In this section, we expand over our analysis in Section 3 and present proof of properties over the Q-policy space: Propositions 3.4, 3.5, 3.6 in main paper.</p><p>For simplicity, we define a concave function ? : R ? R ? {??} such that g is given as g(x) := x ? ?(x), same as in Section 4 of the main paper. We are interested in regularizers ? induced by g, such that</p><formula xml:id="formula_47">? g (r) = E ? E [g(r(s, a))].<label>(20)</label></formula><p>We simplify the IRL objective (from Eq. 5):</p><formula xml:id="formula_48">J (?, Q) = E ? E [T ? Q] ? (1 ? ?)E p0 [V ? (s 0 )] ? ?(T ? Q) = E ? E [T ? Q] ? (1 ? ?)E ?0 [V ? (s 0 )] ? E ? E [T ? Q ? ?(T ? Q)] = E ? E [?(Q ? ?E s ?P(?|s,a) V ? (s ))] ? (1 ? ?)E ?0 [V ? (s 0 )].</formula><p>Lemma C.1. J (?, ?) is concave for all ? ? ?.</p><p>Proof. Let Q 1 , Q 2 ? ? and suppose ? ? [0, 1]. We rely on the fact that the regularized IRL objective L(?, ?) is concave for all ?. Note that r = T ? Q is an affine transform of Q, given in vector form as</p><formula xml:id="formula_49">r ? log ? = (I ? P ? ) Q. Thus, T ? (?Q 1 + (1 ? ?)Q 2 ) = ?T ? Q 1 + (1 ? ?)T ? Q 2 . J (?, ?Q 1 + (1 ? ?)Q 2 ) = L(?, T ? (?Q 1 + (1 ? ?)Q 2 )) = L(?, ?T ? Q 1 + (1 ? ?)T ? Q 2 ) ? ?L(?, T ? Q 1 ) + (1 ? ?)L(?, T ? Q 2 ) = ?J (?, Q 1 ) + (1 ? ?)J (?, Q 2 ).</formula><p>Thus, J (?, ?) is concave.</p><p>For building up our analysis, we will first prove the saddle point properties of J (?, Q) by adding a monotonicity assumption on ? that it is a non-decreasing function. We then generalize the proof to show that these properties hold for any concave ? in Section C.1. Lemma C.2. For ? g corresponding to a non-decreasing ?, J (?, Q) has a unique minima ? Q = 1 Zs exp(Q) with normalizing factor Z s = a exp Q(s, a).</p><p>Proof. We have,</p><formula xml:id="formula_50">V ? (s) = E a?? [Q(s, a) ? log ?(a|s)] = ?D KL ?(?|s) 1 Z s exp(Q(s, ?) + log(Z s ).</formula><p>For a fixed Q, the KL divergence is strictly convex in ? with minima at ? Q , implying V ? (s) is strictly concave in ? . Similarly, r = T ? Q = Q ? ?E s ?P(?|s,a) V ? (s ) is strictly convex in ? with minima at ? Q . Now, as ? is a non-decreasing function, E ? E [?(T ? Q)] will be minimum at ? Q and will be always non-decreasing as we pull away. Similarly the second term of J , given as</p><formula xml:id="formula_51">?(1 ? ?)E ?0 [V ? (s 0 )]</formula><p>is convex with a minima at ? Q . Thus J (?, Q) &gt; J (? Q , Q), for any ? = ? Q . This is sufficient to establish that J(?, Q) has a unique minima at ? Q . Then T * is bijective.</p><p>Proof. For r = T * Q, we have Q(s, a) = r(s, a) + ?E s ?P(?|s,a) [log a exp Q (s , a )]. This is just the soft Bellman equation (Eq. 2), for which a unique contraction Q * exists satisfying it <ref type="bibr" target="#b13">[14]</ref>. Thus for any r, we have a unique preimage Q * such that r = T * Q * . Hence, T * is a bijection.</p><p>Lemma C.4. We have that T * Q = T ? Q Q. Moreover, for Q ? R S?A , let r = T * Q. Then, the optimal (soft) policy with respect to r satisfies ? * r = ? Q . This notably implies that ? Q = argmax ? E s,a??? [(T * Q)(s, a) ? ln ?(a|s)].</p><p>Proof. The first holds is true by basic properties of the Legendre-Fenchel transform [? , Appx. A].</p><p>Here, Q is the fixed point of the optimal Bellman operator B * for reward r, so ? Q is the optimal policy.</p><p>Lemma C.5. We have that a unique saddle point exists for J (?, Q) implying min Let (? * , r * ) be the unique saddle point for L. We will first solve for the min-max of J .</p><p>As r = T ? Q is an affine transform of Q for a fixed ?, we have ? * = argmin Thus, argmin ? max Q J coincides with the first coordinate of the saddle point for L. Now, we can relate the second coordinates.</p><p>For r * = argmax r L(? * , r), as L satisfies the minimax theorem, we necessarily have that ? * = argmin ? L(?, r * ).</p><p>So, as L(?, r * ) = E s,a?? E [?(r * (s, a))] ? E s,a??? [r * (s, a) ? ln ?(a|s)], this means that ? * is the optimal policy for r * . Write Q * = (T * ) ?1 r * the associated optimal Q-function, we have that ? * = ? Q * . So, using the affine transformation property, we have</p><formula xml:id="formula_53">r * = argmax r L(? * , r) = T ? * argmax Q L(? * , T ? * Q) = T ? Q * argmax Q L(? Q * , T ? Q * Q) = T ? Q * Q ? .</formula><p>where Q ? := argmax Q L(? Q * , T ? Q * Q).</p><p>As r * = T * Q * by definition of Q * , we have T * Q * = T ? Q * Q ? . We also know that</p><formula xml:id="formula_54">T * Q = T ? Q Q, so T * Q * = T ? Q * Q * = T ? Q * Q ? . Composing with (T ? Q * ) ?1 we obtain Q ? = Q * .</formula><p>Overall, with (? * , r * ) the unique saddle point of L, having defined Q * = (T * ) ?1 r * , we have shown min ??? max Q?? J (?, Q) = J (? * , Q * ),</p><p>and ? * = ? Q * . Now, we show the same holds for the max-min of J . We can relate Q * * := argmax Q min ? J (?, Q) and ? Q * * to the saddle point of L(?, r). We have</p><formula xml:id="formula_55">min ? J (?, Q) = min ? L(?, T ? Q) by def. of J = L(? Q , T ? Q Q) by Lemma C.2 = L(? Q , T * Q) by Lemma C.4 = E ? E [?(T * Q)] ? E ?? Q [(T * Q) ? ln ? Q ] by def. of L = min ? E ? E [?(T * Q)] ? E ?? [(T * Q) ? ln ?]</formula><p>by Lemma C.4</p><p>= min ? L(?, T * Q).</p><p>We therefore have that,</p><formula xml:id="formula_56">Q * * = argmax Q min ? J (?, Q) = argmax Q min ? L(?, T * Q).</formula><p>Recalling that (? * , r * ) is the saddle point of L, we have r * = T * Q * * as T * is bijective. However, by definition Q * = (T * ) ?1 r * , which readily implies that Q * * = Q * . In the above section, we made a monotonicity assumption on ? in Lemma C.2. We show that we can relax this assumption and the saddle point properties still hold, although J is not so well-behaved everywhere anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Generalization</head><p>For a fixed ?, the optimizer of the concave problem, max r L(?, r) = E ? E [?(r(s, a))]?E ? [r(s, a)]? H(?) satisfies 9 :</p><formula xml:id="formula_57">? (r)? E ? ? = 0.</formula><p>Thus, ? (r(s, a)) = ?(s, a)/? E (s, a) ? [0, ?). This tells us that there exists a set of rewards R ? , such that ? is non-decreasing on this set. For a concave ?, R ? is just the convex set of reals that are on the left of its maxima.</p><p>Lemma C.6. Define a convex feasibility region on the Q-policy space:</p><formula xml:id="formula_58">F ? = {(?, Q) : r = T ? Q ? S ? A ? R ? }.</formula><p>Then, for a given ?, any optimal Q = argmax Q J (?, Q ) has to lie in F ? .</p><p>Proof. If Q is optimal, then T ? Q maximizes L(?, r), and so it's corresponding r is optimal. For a fixed ?, and any (s, a) ? S ? A, the optimal reward has to satisfy ? (r(s, a)) = ?(s, a)/? E (s, a). Thus, each component of the reward vector lies in R ? . This tells us T ? Q lies in the required region.</p><p>We get two properties in the feasibility region F ? :</p><formula xml:id="formula_59">1. argmax Q J (?, Q) lies in F ? ,</formula><p>2. ? is non-decreasing, so lemma C.2 holds in this region.</p><p>We just need one last lemma to prove the existence of a unique saddle point:</p><p>Lemma C.7. A saddle point exists only at the intersection of two curves: argmax Q J (?, Q) and argmin ? J (?, ?).</p><p>Proof. We parameterize the curves f (?) = argmax Q J (?, Q) and g(Q) = argmin ? J (?, Q). A saddle point has to satisfy min ? max Q J (?, Q) = max Q min ? J (?, Q). This implies, min ? J (?, f (?)) = max Q J (g(Q), Q). This equation can only be satisfied when both the curves intersect.</p><p>Therefore, any saddle point lies at the intersection of the Q-maxima and policy minima curves.</p><p>We have established that within the feasibility region F ? , lemma C.1 and C.2 hold. Thus, there exists a single saddle point in this region. Furthermore, argmax Q J (?, Q) lies in F ? so lemma C.4 tells us there cannot exist any other saddle points outside F ? .</p><p>This completes our proof of the existence of a unique saddle point of J for any concave ?.</p><p>We summarize these properties in <ref type="figure" target="#fig_10">Fig 7.</ref>  For any ?, our soft actor-critic (SAC) policy update (Sec 4.4) minimizes the KL divergence between the current policy ? and ? Q , always pointing towards the the policy minima manifold whereas adversarial policy update relying on the local gradient can diverge away from it (outside the feasibility region). This has the effect, that with sufficient steps, learning with SAC updates is guaranteed to converge to the saddle point, but no such guarantee exists with adversarial policy updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Convergence Guarantee</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Effect of various divergences</head><p>In the Q-policy space, the policy minima manifold ? Q is an energy-based model of Q, and doesn't depend on the choice of regularizer ?.</p><p>Whereas, the Q-maxima manifold is dependent on the choice of regularizer. As the saddle point is formed by the intersection of these two curves (Lemma C.4), we can study how different divergences will affect the saddle point which solves the regularized-IRL problem.</p><p>We have that for a choice of ?, the Q-maxima manifold is given by the condition:</p><formula xml:id="formula_60">? (r)? E ? ? = 0.</formula><p>Thus on the maxima manifold, r = (? ) ?1 (?/? E ). We visualize this in the <ref type="figure">Fig. 9</ref>, we see that different statistical distances correspond to different saddle points. The overall effect is that that at the saddle point ? * remains close to ? E , but may not be exactly equal as the regularization constrains the policy class.</p><p>In general, ? * is the solution to the (transcendental) equation:</p><formula xml:id="formula_61">? (T ? Q)? E ? ? Q = 0,<label>(22)</label></formula><p>where ? Q is the occupancy measure corresponding to ? Q = 1 Z exp Q. For f -divergences, this can be simplified as</p><formula xml:id="formula_62">T ? Q = ?f ? Q ? E .<label>(23)</label></formula><p>For an IPM parametrized by F, ? (x) = 0 and the equation will be maximized on the boundary of F, without a closed form equation. Now, SQIL <ref type="bibr" target="#b32">[33]</ref> uses the reward of the form 1 ? 0 dependent on sampling from the expert or policy distributions. This condition corresponds to a maxima manifold in this space, such that instead of the reward being a function of the ratio density of the expert and the policy, it is stochastically dependent on the sampling. Thus, instead of being fixed, the manifold will shift stochastically with the sampling. This has the corresponding effect of shifting the saddle point and can result in numerical instabilities near convergence, as a unique convegence point does not exist for the SQIL style update.</p><p>Similary, we can analyze ValueDICE <ref type="bibr" target="#b21">[22]</ref>. ValueDICE mimimizes the Reverse-KL divergence between the expert and policy using the Donsker-Varadhan (DV) variational form of Reverse-KL. This corresponds to the maxima manifold with rewards satisfying r = log(? E /?), but suffers from two issues: 1) biased gradient estimates, and 2) adversarial policy updates.</p><p>We have already shown how adverserial policy updates are not optimal, we will now focus on fixing the biasing issue with the Reverse-KL distance.</p><p>First, the DV representation is given as:</p><formula xml:id="formula_63">KL(?, ? E ) = max r?R log E ? E [e ?r(s,a) ] ? E ? [r(s, a)]</formula><p>This corresponds to a ?(x) = log E ? E [e ?x ], even though its outside the class of ? we study, it satisfies all the previous properties we developed (Lemma C.1 -C.4). Now, to unbias the Reverse-KL representation, we propose using the f -divergence representation, with f (t) = t log t ? t + 1. Then the f -divergence for this choice of f is just the Reverse-KL divergence, but it's variational form is: Thus, we can obtain the same Q-maxima manifold to minimize the Reverse-KL distance as Val-ueDICE by using this new representation, while avoiding the biasing issue.</p><formula xml:id="formula_64">max r?R E ? E [?e ?</formula><p>Effect of different forms of Reverse-KL We test IQ-Learn with different variational representations of Reverse-KL: Donsker-Varadhan (DV), Original KL (KL), ours Modified KL (KL-fix).</p><p>We use the LunarLander environment with our offline IL experimental settings and a single expert trajectory. All experiments are repeated over 10 seeds. We show a box-plot of the environment returns for different variational forms and find that our proposed form (KL-fix) and the DV representation perform similarly. The original f-divergence form of KL remains problematic, performing noticeably worse, which may be due to an issue with its corresponding Q-maxima manifold. Compared to DV, our proposed KL variation representation has the advantage of giving unbiased gradient estimates and can be more stable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 A theoretical guarantee for A.1</head><p>Motivated by learning a state-dependent reward function, Section A.1 proposes an alternative objective function for learning Q. Here, we provide a form of monotonic improvement guarantee for an idealized version of the objective in Eq. 14.</p><p>Before this, lets introduce some notations that will be useful, as we'll now work with both state and state-action occupancy measures. Let write ? ? ? S?A an occupancy measure on state-actions, ? ? ? S an occupancy measure on states. For ? ? ? S and ? ? ? S A , write ?? ? ? S?A defined as (??)(s, a) = ? ? (s)?(a|s). Now, let recall Eq. 14: Now, for any initial Q 0 , define for k ? 0</p><formula xml:id="formula_65">J * (Q) = E s?? E [E a?stop_grad(? Q )(?|s) [?((T * Q)(s, a))]]?(1??)E s??0 [log</formula><formula xml:id="formula_66">? k = ? Q k = softmax(Q k ) Q k+1 = argmax Q J * k (Q) .<label>(25)</label></formula><p>Eq. (24) can be see as an optimistic version of Eq. <ref type="bibr" target="#b24">(25)</ref>, in the sense that instead of optimizing to the end each subproblem, we update ? k at each gradient step. It is Eq. (25) that we'll analyse. Before that, we need some assumption: ? is concave and non-decreasing. Write f the convex conjugate of</p><formula xml:id="formula_67">??(?x) (that is f * (x) = ??(?x)), f satisfies f (1) = 0.</formula><p>In other words, we restrict ourselves to (a subclass of) f -divergences, but it should be possible to adapt the analysis to other cases (eg, IPMs). The core result is the following.</p><p>Theorem C.8. Under the previous assumption, the sequence of policies (? k ) k?0 produced by Eq. (25) satisfies a monotonic improvement guarantee, in the sense that for any k ? 0, we have</p><p>Training Setup We test with <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15)</ref> expert trajectories uniformly sampled from a pool of 1000 expert demonstrations. Each algorithm is trained until convergence and tested by performing 300 live rollouts in the simulated environment and recording the average episode rewards. We repeat this over 10 seeds, consequently with different initializations and seen trajectories.</p><p>Implementation All methods use neural networks with the same architecture of 2 hidden layers of 64 units each connected by exponential linear unit (ELU) activation functions.</p><p>We use the original public code implementations of EDM, AVRIL and ValueDICE. Note, ValueDICE is adapted to discrete environments using an actor with Gumbel-softmax distribution output.</p><p>Hyperparameters We use batch size 32 and Q-network learning rate 1e?4 with entropy coefficient 0.01. We found learning rate of 1e ? 4 worked best for IQ-Learn on discrete environments. We also found entropy coefficient values [1e ? 2, 1e ? 3] to be optimal depending on the environment. Here, we don't use target updates as we found them to give no visible improvement and slow down the training. Implementation For Mujoco, with all methods we use critic and actor networks with an MLP architecture with 2 hidden layers and 256 hidden units, keeping settings similar to original SAC <ref type="bibr" target="#b12">[13]</ref>.</p><p>For Atari, with all methods we use a single convolution neural network same as the original DQN architecture <ref type="bibr" target="#b26">[27]</ref>. For IQ-Learn in continuous environments, for SAC policy updates we sample states from both policy and expert distributions. We regularize policy states in addition to expert states to improve the stability of learning Q-values. We use soft target updates and find them helpful for stabilizing the training.</p><p>For BC and GAIL, we use the stable-baselines implementations. For SQIL, we use original public code for Atari environments. For ValueDICE, we use the open-sourced official code.</p><p>Hyperparameters For SAC style learning, we use default settings of critic learning rate 3e ? 4 and policy learning rate values [3e ? 4, 3e ? 5]. We found 3e ? 5 to work well in complex environments and remain stable, although 3e ? 4 can be better with simpler environments (like Half-Cheetah). We use a fixed batch size of 256 and found entropy coefficient 0.01 to work well. We use soft target updates with the default SAC smoothing constant ? = 0.05. For DQN-style learning on Atari, we use Q-network learning rate 1e ? 4 with entropy coefficient 1e ? 4 and batch size 64. We found entropy coefficient values [1e ? 3, 1e ? 4] to work well. We didn't find noticeable improvements with using target updates on Atari (with the exception of Space Invaders, where they stabilize the training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Additional Results</head><p>Mujoco We show additional results on Mujoco obtained using 10 expert trajectories in <ref type="table" target="#tab_5">Table 5</ref>. We find IQ-Learn gets state-of-art performance in all environments and reaches expert-level rewards. Atari Suite. We show detailed performance of IQ-Learn on Atari Suite environments using 20 expert demonstrations in <ref type="table" target="#tab_6">Table 6</ref>. Reward Correlations. We show the Pearson correlation coefficient of our learnt rewards with environment rewards in <ref type="table" target="#tab_7">Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Recovering Rewards</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 12: Hopper correlations</head><p>We show visualizations of our reward correlations on the Hopper environment using 10 expert demonstrations in <ref type="figure" target="#fig_2">Fig 12.</ref> We obtain a Pearson correlation of 0.99 of our recovered episode rewards compared with the original environment rewards, showing that our rewards are almost linear with the actual rewards, and thus can be used for Inverse RL. Note, that to recover rewards with IQ-Learn, we need to sample the current state and the next state.</p><p>We perform similar comparisons on GAIL and SQIL, obtaining Pearson coefficients of 0.90 and 0.72 respectively.</p><p>In the main paper, we also show recovered rewards on a simple grid environment by using sampling based Q-learning with a simple Q-network having two hidden layers. In the section below, we further compare IQ-Learn on a tabular setting.</p><p>Tabular Inverse RL To further validate IQ-Learn as a method for IRL and show we recover correct rewards, we directly compare with the classical Max Entropy IRL <ref type="bibr" target="#b42">[43]</ref> method on a tabular Grid world setting, by using an open-source implementation <ref type="bibr" target="#b9">10</ref> . We implement IQ-Learning as a modification to tabular value iteration. The classical method requires repeated backward and forward passes, to calculate soft-values and action probabilities for a given reward and optimize the rewards respectively. IQ-Learn skips the expensive backward pass and directly optimizes the rewards. We show comparision in <ref type="figure" target="#fig_3">Fig 13,</ref> where we find our method recovers very similar rewards while being more than 3x faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IQ-Learn</head><p>Max entropy IRL <ref type="figure" target="#fig_3">Figure 13</ref>: <ref type="table">Tabular</ref>   We show results for IQ-Learn trained with using only expert observations in <ref type="table" target="#tab_8">Table 8</ref>. We test on CartPole, LunarLander and Hopper environments with 1 and 10 expert demonstrations using online IL settings without any subsampling of trajectories. We find that with one expert demonstration, we get below expert-level rewards, and as expected, our performance suffers compared to with using expert actions. We find using 10 demonstrations is enough to reach expert-level performance in these simple environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Imitation learning with Observations</head><p>Target updates are helpful in stabilizing the training in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Appendix E E.1 Dynamics-Aware Imitation Learning and the Loop MDP</head><p>In this section we illustrate the importance of dynamics-awareness in imitation learning with a toy MDP based on the Loop MDP from <ref type="bibr" target="#b33">[34]</ref>. The MDP is shown in <ref type="figure" target="#fig_4">Fig 14.</ref> The MDP has a fixed length of 100 steps. The key problem for dynamics-unaware algorithms, such as behavioural cloning, is the behaviour in state s 2 . If we happen to use an expert trajectory where the expert never visits state s 2 , then the learned policy will not necessarily have the right behaviour in state s 2 . This is because the objective for behavioural cloning is to match the action probabilities in the expert states, and s 2 is not in the expert states visited. However, the dynamics-aware methods are able to deduce that taking action a 1 in state s 2 will return the imitator to state s 1 . Although this MDP is simple, it illustrates a general advantage of dynamics-aware methods which will hold in many situations. In particular, it will hold for environments where the expert may keep very close to an optimal trajectory, yet it is possible to recover back to that trajectory if a small mistake is made, such as in autonomous lane-keeping in a car.  <ref type="figure" target="#fig_4">Figure 14</ref>: A variant of the Loop MDP from <ref type="bibr" target="#b33">[34]</ref>. Taking actions labelled in green gives 1 reward, while actions in black give reward 0. The MDP is stochastic for action a 1 in state s 0 , which with probability p leads to state s 2 , and with probability 1 ? p leads to state s 1 .</p><p>To substantiate this illustrative case, we implemented this MDP and evaluated a few methods. We use a single expert trajectory which goes from s 0 to s 1 , never going to state s 2 . We set p = 0.5 for this experiment. The results are in <ref type="table">Table 9</ref>, averaged over five random seeds. They are as we expect, with the dynamics-aware methods able to convincingly master the environment and find the optimal policy, while the behavioural cloning approach achieves around 50 reward. This is because it learns the wrong behaviour in state s 2 so gets zero reward in that state in the 50% of the time that taking action a 1 results in a transition to state s 2 . <ref type="table">Table 9</ref>: Results of imitation learning algorithms on the Loop MDP described above. We observe that the dynamics-unaware behavioural cloning baseline performs much worse than the other dynamicsaware methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Episode Reward</head><p>Behavioural Cloning 54 ? 5 SQIL 100 ? 0 IQ (Online, ? 2 ) 100 ? 0 The dynamics are encoded in our learning objective by the discount factor ?, and setting it to zero removes dynamic-awareness in IQ-Learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Ablation on Gamma</head><p>To show how dynamics help with learning, we do an ablation on ? with IQ-Learn. We use the offline IL settings for CartPole environment with one expert trajectory.</p><p>We set ? to 0.99 and 0. The results are visualized in <ref type="figure" target="#fig_18">Fig 15,</ref> we can see that without the dynamics the training is not stable and there is a strong decay in the rewards obtained by the IL agent from the environment. Whereas, when using dynamics, we see that the training is stable and properly converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Appendix F F.1 Generalization over distribution shift</head><p>We show our method can be robust to distribution shifts between the expert and policy and perform additional experiments over two different settings: 1) Initial distribution shift using a modified LunarLander env motivated by <ref type="bibr" target="#b32">[33]</ref> and 2) Goal distribution shift using DeepMind Control Suite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.1 Initial distribution shift</head><p>We experiment with initial shift distribution in the LunarLander-v2 environment similar to <ref type="bibr" target="#b32">[33]</ref>. The agent is typically initialized in a small zone at the middle top of the screen. Instead, we modify the environment to initialize the agent near the top-left corner of the screen. We use experts from the unmodified environment, and test whether the agent can still learn to land the lunar lander while recovering from the initial distribution shift.</p><p>Offline Case: We find in the offline case that the agent cannot learn to recover from the occupancy shift. The lander typically tends to fly off the frame and shows random behavior. This is expected as IQ-learn is not aware of the shift of initial distributions between the agent and the expert, and can't explore the environment to correct the initial state shift to match the occupancy distributions.</p><p>Online Case: In the online case, we find that the agent can sufficiently explore the environment, and learns a behavior of first horizontally moving the lander from the top left to the top center and then successfully imitating the original expert trajectory, receiving an avg. episode reward of 250 with 10 expert demos.</p><p>An extra consideration here is in Eq. 9, where we originally only apply reward regularization to the expert states, but we find applying regularization to both expert and policy states to be beneficial in this case. As it enforces the learning of an implicit reward function that can generalize outside the expert distribution to more arbitrary policy states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.2 Goal Distribution Shift</head><p>We experiment with the reacher_easy task in DeepMind Control Suite. We choose the reacher environment as it is a multi-task environment, where the goal given by the target position changes in every episode randomly. Such environments have been found to be very difficult to solve using IRL <ref type="bibr" target="#b39">[40]</ref> as a large number of expert demos are needed to fully cover the goal distributions, and usually require meta-IRL methods to figure the right task context for a given expert demonstration like PEMIRL <ref type="bibr" target="#b39">[40]</ref>.</p><p>We test with different number of expert demonstrations: <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20)</ref> each with different target positions on the offline and online settings. The average expert performance is ? 990 in this case and we report averaged results over 100 episodes with different targets. Offline Case: In the offline setting, a single demonstration is typically not enough to learn a generalized reward function and leads to a reward that overfits to a particular target position. We quantify the results in <ref type="table" target="#tab_9">Table 10</ref>, with the observation that imitation learning performance improves with the number of expert demos. This can be justified, as more experts with different targets allow learning a reward function that is better generalizable.</p><p>Online Case: In the online setting, our method is able to explore the environment over different episodes and can learn to correct the behavior leading to better performance. In particular, given a sufficient number of expert demos, it can learn to associate what expert behavior to imitate given a particular target and learns a more reward function generalizable over multiple goals. We show quantitative results in <ref type="table" target="#tab_10">Table 11</ref>.</p><p>BC and GAIL on reacher_easy even with 50 experts obtain mean rewards of 325.2 and 440.1 respectively, which is equivalent to what we see using our method with just 5 expert demos! It is surprising to us that our method can learn a reward to figure out what goal state to reach, acting as a meta-learner even when not engineered specifically to do so. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Q</head><label></label><figDesc>* satisfies the soft-Bellman equation: Q * (s, a) = (B * Q * )(s, a) := r(s, a) + ?E s ?P(?|s,a) log a exp(Q * (s , a )) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 =Figure 1 :</head><label>141</label><figDesc>" i h R P G f 7 o 9 A x x B d v e W w q Y H m 2 9 d h I = " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u B o t Q N y W R i i 4 U C m 5 c t m A f 2 I Q w m U 7 a o Z N J m J m I J W T h x l 9 x 4 0 I R t 3 6 E O / / G a Z u F t h 6 4 c D j n X u 6 9 x 4 8 Z l c q y v o 3 C y u r a + k Z x s 7 S 1 v b O 7 Z + 4 f d G S U C E z a O G K R 6 P l I E k Y 5 a S u q G O n F g q D Q Z 6 T r j 6 + n f v e e C E k j f q s m M X F D N O Q 0 o B g p L X l m 2 Y k p v I J O I B B O 7 S y 9 8 2 Q G H f I Q V 1 s n n l m x a t Y M c J n Y O a m A H E 3 P / HI G E U 5 C w h V m S M q + b c X K T Z F Q F D O S l Z x E k h j h M R q S v q Y c h U S 6 6 e y J D B 5 r Z Q C D S O j i C s 7 U 3 x M p C q W c h L 7 u D J E a y U V v K v 7 n 9 R M V X L g p 5 X G i C M f z R U H C o I r g N B E 4 o I J g x S a a I C y o v h X i E d J 5 K J 1 b S Y d g L 7 6 8 T D q n N b t e O 2 v V K 4 3 L P I 4 i K I M j U A U 2 O A c N c A O a o A 0 w e A T P 4 B W 8 G U / G i / F u f M x b C 0 Y + c w j + w P j 8 A S B B l x 0 = &lt; / l a t e x i t &gt; J ? ( Q )&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B / F 7 G S C Y 3 8 1 g F w b u q o C B P C M g S B A = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R a h u i g z U t F l 0 Y 2 4 a s E + o B 1 L J k 3 b 0 E x m S D K F M v R P 3 L h Q x K 1 / 4 s 6 / M d P O Q l s P B A 7 n 3 M s 9 O X 7 E m d K O 8 2 3 l 1 t Y 3 N r f y 2 4 W d 3 b 3 9 A / v w q K n C W B L a I C E P Z d v H i n I m a E M z z W k 7 k h Q H P q c t f 3 y X + q 0 J l Y q F 4 l F P I + o F e C j Y g B G s j d S z 7 W 6 A 9 Y h g n j z M n i 5 K 9 f O e X X T K z h x o l b g Z K U K G W s / + 6 v Z D E g d U a M K x U h 3 X i b S X Y K k Z 4 X R W 6 M a K R p i M 8 Z B 2 D B U 4 o M p L 5 s l n 6 M w o f T Q I p X l C o 7 n 6 e y P B g V L T w D e T a U 6 1 7 K X i f 1 4 n 1 o M b L 2 E i i j U V Z H F o E H O k Q 5 T W g P p M U q L 5 1 B B M J D N Z E R l h i Y k 2 Z R V M C e 7 y l 1 d J 8 7 L s V s p X 9 U q x e p v V k Y c T O I U S u H A N V b i H G j S A w A S e 4 R X e r M R 6 s d 6 t j 8 V o z s p 2 j u E P r M 8 f j h e S 9 Q = = &lt; / l a t e x i t &gt; Properties of IRL objective in reward-policy space and Q-policy space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Offline IL results. We plot the average environment returns vs the number of expert trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Atari Results. We show the returns vs the number of env steps (averaged over 5 seeds).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Reward Visualization. We use a discrete GridWorld environment with 5 possible actions: up, down, left, right, stay. Agent starts in a random state. (With 30 expert demos)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>E</head><label></label><figDesc>?? [Q(s, a) ? ?E s ?P(?|s,a) V ? (s )] + H(?) = E ?? [Q(s, a) ? ?E s ?P(?|s,a) V ? (s ) + H(?(a|s)] = E ?? [Q(s, a) ? log ?(a|s) ? ?E s ?P(?|s,a) V ? (s )] = E ?? [V ? (s) ? ?E s ?P(?|s,a) V ? (s )]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Divergence ablation. We show environment returns for different divergences on LunarLander.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Lemma C. 3 .</head><label>3</label><figDesc>Define T * : R S?A ? R S?A such that (T * Q) (s, a) = Q(s, a) ? ?E s ?P(?|s,a) [log a exp Q (s , a )].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>??? max r?R L(?, r) = argmin ??? max Q?? L(?, T ? Q) = argmin ??? max Q?? J (?, Q)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>, Q ? ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 P + Y q 3 v i F r F o v s J o E M / k j y 4 X X i c = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S L U I i W R i h 6 L X j y 2 Y D + g S c t m u 2 m X b p J l d y O U 0 L / h x Y M i X v 0 z 3 v w 3 b t s c t P X B w O O 9 G W b m + Y I z p W 3 7 2 1 p b 3 9 j c 2 s 7 t 5 H f 3 9 g 8 O C 0 f H L R U n k t A m i X k s O z 5 W l L O I N j X T n H a E p D j 0 O W 3 7 4 / u Z 3 3 6 i U r E 4 e t Q T Q b 0 Q D y M W M I K 1 k d y S K 1 i v f I k a v f J F v 1 C 0 K / Y c a J U 4 G S l C h n q / 8 O U O Y p K E N N K E Y 6 W 6 j i 2 0 l 2 K p G e F 0 m n c T R Q U m Y z y k X U M j H F L l p f O b p + j c K A M U x N J U p N F c / T 2 R 4 l C p S e i b z h D r k V r 2 Z u J / X j f R w a 2 X s k g k m k Z k s S h I O N I x m g W A B k x S o v n E E E w k M 7 c i M s I S E 2 1 i y p s Q n O W X V 0 n r q u J U K 9 e N a r F 2 l 8 W R g 1 M 4 g x I 4 c A M 1 e I A 6 N I G A g G d 4 h T c r s V 6 s d + t j 0 b p m Z T M n 8 A f W 5 w + 2 H J A t &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " C o 3 / Z / b k t u O L d 4 d 4 C G l d C 6 7 / t v s = " &gt; A A A C B H i c b V D L S s N A F J 3 U V 6 2 v q M t u B o t Q N y W R i t 0 I B T c u W 7 A P b E K Y T C f t 0 M k k z E z E E r J w 4 6 + 4 c a G I W z / C n X / j t M 1 C W w 9 c O J x z L / f e 4 8 e M S m V Z 3 0 Z h b X 1 j c 6 u 4 X d r Z 3 d s / M A + P u j J K B C Y d H L F I 9 H 0 k C a O c d B R V j P R j Q V D o M 9 L z J 9 c z v 3 d P h K Q R v 1 X T m L g h G n E a U I y U l j y z 7 M Q U X k E n E A i n d p b e e T K D D n m I q + 0 z z 6 x Y N W s O u E r s n F R A j p Z n f j n D C C c h 4 Q o z J O X A t m L l p k g o i h n J S k 4 i S Y z w B I 3 I Q F O O Q i L d d P 5 E B k + 1 M o R B J H R x B e f q 7 4 k U h V J O Q 1 9 3 h k i N 5 b I 3 E / / z B o k K G m 5 K e Z w o w v F i U Z A w q C I 4 S w Q O q S B Y s a k m C A u q b 4 V 4 j H Q e S u d W 0 i H Y y y + v k u 5 5 z a 7 X L t r 1 S r O R x 1 E E Z X A C q s A G l 6 A J b k A L d A A G j + A Z v I I 3 4 8 l 4 M d 6 N j 0 V r w c h n j s E f G J 8 / H w 2 X G Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " I d O t c q + e h b 8 N u 7 i E + e T H x s c p c d A = " &gt; A A A B 8 n i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I R Z d F Q V x W s A + Y D i W T Z t r Q T D I k G a E M / Q w 3 L h R x 6 9 e 4 8 2 / M t L P Q 1 g O B w z n 3 k n N P m H C m j e t + O 6 W 1 9 Y 3 N r f J 2 Z W d 3 b / + g e n j U 0 T J V h L a J 5 F L 1 Q q w p Z 4 K 2 D T O c 9 h J F c R x y 2 g 0 n t 7 n f f a J K M y k e z T S h Q Y x H g k W M Y G M l v x 9 j M y a Y Z 3 e z Q b X m 1 t 0 5 0 C r x C l K D A q 1 B 9 a s / l C S N q T C E Y 6 1 9 z 0 1 M k G F l G O F 0 V u m n m i a Y T P C I + p Y K H F M d Z P P I M 3 R m l S G K p L J P G D R X f 2 9 k O N Z 6 G o d 2 M o + o l 7 1 c / M / z U x N d B x k T S W q o I I u P o p Q j I 1 F + P x o y R Y n h U 0 s w U c x m R W S M F S b G t l S x J X j L J 6 + S z k X d a 9 Q v H x q 1 5 k 1 R R x l O 4 B T O w Y M r a M I 9 t K A N B C Q 8 w y u 8 O c Z 5 c d 6 d j 8 V o y S l 2 j u E P n M 8 f e n + R Z A = = &lt; / l a t e x i t &gt; Feasibility region in Q-policy space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Policy learning. Comparison of SAC vs adverserial policy update outside the feasibility region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>1 &lt; 2 &lt;Figure 9 :</head><label>129</label><figDesc>, Q ? ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l 1 p V m l c m R 4 0 L p 6 z d u R V s S I A P Z B Q = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h F i m J V P R Y 8 O K x B f s B T V o 2 2 0 2 7 d J M s u x u h h P 4 N L x 4 U 8 e q f 8 e a / c d v m o K 0 P B h 7 v z T A z z x e c K W 3 b 3 1 Z u Y 3 N r e y e / W 9 j b P z g 8 K h 6 f t F W c S E J b J O a x 7 P p Y U c 4 i 2 t J M c 9 o V k u L Q 5 7 T j T + 7 n f u e J S s X i 6 F F P B f V C P I p Y w A j W R n L L r m D 9 y h V q 9 i u X g 2 L J r t o L o H X i Z K Q E G R q D 4 p c 7 j E k S 0 k g T j p X q O b b Q X o q l Z o T T W c F N F B W Y T P C I 9 g y N c E i V l y 5 u n q E L o w x R E E t T k U Y L 9 f d E i k O l p q F v O k O s x 2 r V m 4 v / e b 1 E B 3 d e y i K R a B q R 5 a I g 4 U j H a B 4 A G j J J i e Z T Q z C R z N y K y B h L T L S J q W B C c F Z f X i f t 6 6 p T q 9 4 0 a 6 V 6 P Y s j D 2 d w D m V w 4 B b q 8 A A N a A E B A c / w C m 9 W Y r 1 Y 7 9 b H s j V n Z T O n 8 A f W 5 w + 1 g p A r &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " a a k T g P R d 6 J W 3 F R L J L j C M a K X e 0 L E = " &gt; A A A B 7 X i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m V F j 0 W R f B Y w X 5 A u 5 R s m m 1 j s 8 m S Z I W y 9 D 9 4 8 a C I V / + P N / + N 2 X Y P 2 v p g 4 P H e D D P z g p g z b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j t p a J I r R F J J e q G 2 B N O R O 0 Z Z j h t B s r i q O A 0 0 4 w u c n 8 z h N V m k n x Y K Y x 9 S M 8 E i x k B B s r t f s x G 9 y W B u W K W 3 X n Q K v E y 0 k F c j Q H 5 a / + U J I k o s I Q j r X u e W 5 s / B Q r w w i n s 1 I / 0 T T G Z I J H t G e p w B H V f j q / d o b O r D J E o V S 2 h E F z 9 f d E i i O t p 1 F g O y N s x n r Z y 8 T / v F 5 i w i s / Z S J O D B V k s S h M O D I S Z a + j I V O U G D 6 1 B B P F 7 K 2 I j L H C x N i A s h C 8 5 Z d X S f u i 6 t W q 9 f t a p X G d x 1 G E E z i F c / D g E h p w B 0 1 o A Y F H e I Z X e H O k 8 + K 8 O x + L 1 o K T z x z D H z i f P 8 1 8 j q E = &lt; / l a t e x i t &gt; W l a t e x i t s h a 1 _ b a s e 6 4 = " f k 4 4 w N m 1 h D u F T 2 S 7 Z 0 J H l I t G W e 8 = " &gt; A A A B 6 n i c d V B N S w M x E J 3 1 s 9 a v q k c v w S J 4 K p t S b Q s e C l 4 8V r Q f 0 C 4 l m 2 b b 0 G x 2 S b J C W f o T v H h Q x K u / y J v / x m x b Q U U f D D z e m 2 F m n h 8 L r o 3 r f j g r q 2 v r G 5 u 5 r f z 2 z u 7 e f u H g s K 2 j R F H W o p G I V N c n m g k u W c t w I 1 g 3 V o y E v m A d f 3 K V + Z 1 7 p j S P 5 J 2 Z x s w L y U j y g F N i r H T b G e B B o e i W X N f F G K O M 4 O q F a 0 m 9 X i v j G s K Z Z V G E J Z q D w n t / G N E k Z N J Q Q b T u Y T c 2 X k q U 4 V S w W b 6 f a B Y T O i E j 1 r N U k p B p L 5 2 f O k O n V h m i I F K 2 p E F z 9 f t E S k K t p 6 F v O 0 N i x vq 3 l 4 l / e b 3 E B D U v 5 T J O D J N 0 s S h I B D I R y v 5 G Q 6 4 Y N W J q C a G K 2 1 s R H R N F q L H p 5 G 0 I X 5 + i / 0 m 7 X M K V 0 v l N p d i 4 X M a R g 2 M 4 g T P A U I U G X E M T W k B h B A / w B M + O c B 6 d F + d 1 0 b r i L G e O 4 A e c t 0 8 n K o 2 3 &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " A K z c m N G + 1 k k D r o c J Y n z g F k T l w 8 s = " &gt; A A A B 7 X i c d V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K k m p t g U X B T c u K 9 g H t G P J p J k 2 N p M M S U Y o p f / g x o U i b v 0 f d / 6 N m b a C i h 6 4 c D j n X u 6 9 J 4g F N x a h D y + z s r q 2 v p H d z G 1 t 7 + z u 5 f c P W k Y l m r I m V U L p T k A M E 1 y y p u V W s E 6 s G Y k C w d r B + D L 1 2 / d M G 6 7 k j Z 3 E z I / I U P K Q U 2 K d 1 O r R E b 8 t 9 f M F V E Q I Y Y x h S n D l H D l S q 1 V L u A p x a j k U w B K N f v 6 9 N 1 A 0 i Z i 0 V B B j u h j F 1 p 8 S b T k V b J b r J Y b F h I 7 J k H U d l S R i x p / O r 5 3 B E 6 c M Y K i 0 K 2 n h X P 0 + M S W R M Z M o c J 0 R s S P z 2 0 v F v 7 x u Y s O q P + U y T i y T d L E o T A S 0 C q a v w w H X j F o x c Y R Q z d 2 t k I 6 I J t S 6 g H I u h K 9 P 4 f + k V S r i c v H s u l y o X y z j y I I j c A x O A Q Y V U A d X o A G a g I I 7 8 A C e w L O n v E f v x X t d t G a 8 5 c w h + A H v 7 R N 1 H Y 8 O &lt; / l a t e x i t &gt;JS&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q a 5 d K J A 3 p y k Y B u 2 N e w f y W 5 I M Y J 8 = " &gt; A A A B 6 X i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h Z 0 Q T Q I e A l 7 E U 3 z k A c k S Z i e z y Z D Z 2 W V m V g g h f + D F g y J e / S N v / o 2 z S Q Q V L W g o q r r p 7 v J j w b V x 3 Q 9 n a X l l d W 0 9 s 5 H d 3 N r e 2 c 3 t 7 T d 1 l C j K G j Q S k W r 7 R D P B J W s Y b g R r x 4 q R 0 B e s 5 Y 8 u U r 9 1 z 5 T m k b w z 4 5 h 5 I R l I H n B K j J V u r m 5 7 u b x b c F 0 X Y 4 x S g s t n r i X V a q W I K w i n l k U e F q j 3 c u / d f k S T k E l D B d G 6 g 9 3 Y e B O i D K e C T b P d R L O Y 0 B E Z s I 6 l k o R M e 5 P Z p V N 0 b J U + C i J l S x o 0 U 7 9 P T E i o 9 T j 0 b W d I z F D / 9 l L x L 6 + T m K D i T b i M E 8 M k n S 8 K E o F M h N K 3 U Z 8 r R o 0 Y W 0 K o 4 v Z W R I d E E W p s O F k b w t e n 6 H / S L B Z w q X B 6 X c r X z h d x Z O A Q j u A E M J S h B p d Q h w Z Q C O A B n u D Z G T m P z o v z O m 9 d c h Y z B / A D z t s n k U q N Y w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " r L B k O N X t O 2 z m 4 T x Q I d W p J o W + K o 4 = " &gt; A A A C B H i c b V B N S 8 N A E N 3 U r 1 q / o h 5 7 W S x C v Z R E K n o R C l 4 8 t m B b s Q l l s 9 2 0 S z e b Z X c j l p C D F / + K F w + K e P V H e P P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F w h G l X a c b 6 u w s r q 2 v l H c L G 1 t 7 + z u 2 f s H H R U n E p M 2 j l k s b w O k C K O c t D X V j N w K S V A U M N I N x l d T v 3 t P p K I x v 9 E T Q f w I D T k N K U b a S H 2 7 7 A k K L 6 E X S o R T N 0 v v + i q D H n k Q 1 d Z J 3 6 4 4 N W c G u E z c n F R A j m b f / v I G M U 4 i w j V m S K m e 6 w j t p 0 h q i h n J S l 6 i i E B 4 j I a k Z y h H E V F + O n s i g 8 d G G c A w l q a 4 h j P 1 9 0 S K I q U m U W A 6 I 6 R H a t G b i v 9 5 v U S H F 3 5 K u U g 0 4 X i + K E w Y 1 D G c J g I H V B K s 2 c Q Q h C U 1 t 0 I 8 Q i Y P b X I r m R D c x Z e X S e e 0 5 t Z r Z 6 1 6 p d H I 4 y i C M j g C V e C C c 9 A A 1 6 A J 2 g C D R / A M X s G b 9 W S 9 W O / W x 7 y 1 Y O U z h + A P r M 8 f I X W X I Q = = &lt; / l a t e x i t &gt; Saddle points. Effect of regularizer ? on the saddle point. (not to scale)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>r(s,a) ] ? E ? [r(s, a)] ? 1 and corresponds to ?(x) = ?e ?x with rewards r = log(? E /?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Reverse-KL ablation. We show environment returns for different variational forms of Reverse-KL on LunarLander.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>a</head><label></label><figDesc>exp Q(s, a)]. (24) Now, we'll consider a more conservative version of this objective function. For a given policy ? k , define J * k (Q) = E s?? E [E a?? k (?|s) [?((T * Q)(s, a))]] ? (1 ? ?)E s??0 [log a exp Q(s, a)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Grid Rewards. We recover similar rewards as Max entropy IRL (Ziebert et al.) while avoiding an expensive backward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 :</head><label>15</label><figDesc>Ablation on Gamma</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. Using target updates stabilizes the Q-learning on MuJoCo. For brevity, further online IL results are shown in Appendix D.</figDesc><table><row><cell cols="4">7.3 Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Reward</cell><cell>500 400 300 200 100</cell><cell>2</cell><cell>4 Number of Expert Trajectories 6 8 10 12 14 Acrobot Expert Random IQ W1 (Ours) IQ 2 (Ours) AVRIL BC EDM vDICE</cell><cell>Reward</cell><cell>500 0 100 200 300 400</cell><cell>2</cell><cell>Cartpole 6 8 10 12 14 Number of Expert Trajectories 4 Expert Random IQ W1 (Ours) IQ 2 (Ours) AVRIL BC EDM vDICE</cell><cell>Reward</cell><cell>300 200 100 0 100 200</cell><cell>2</cell><cell>LunarLander 6 8 Number of Expert Trajectories 4 10 12 14 Expert Random IQ W1 (Ours) IQ 2 (Ours) AVRIL BC EDM vDICE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mujoco Results. We show our performance on MuJoCo</figDesc><table><row><cell cols="3">control tasks using a single expert trajectory.</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell cols="4">GAIL DAC ValueDICE IQ (Ours) Expert</cell></row><row><cell>Hopper</cell><cell>3252.5 3305.1</cell><cell>3312.1</cell><cell>3546.4</cell><cell>3532.7</cell></row><row><cell cols="2">Half-Cheetah 3080.0 4080.6</cell><cell>3835.6</cell><cell>5076.6</cell><cell>5098.3</cell></row><row><cell>Walker</cell><cell>4013.7 4107.9</cell><cell>3842.6</cell><cell>5134.0</cell><cell>5274.5</cell></row><row><cell>Ant</cell><cell>2299.1 1437.5</cell><cell>1806.3</cell><cell>4362.9</cell><cell>4700.0</cell></row><row><cell>Humanoid</cell><cell>232.6 380.5</cell><cell>644.5</cell><cell>5227.1</cell><cell>5312.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>List of divergence functions, convex conjugates, ? and optimal reward estimators</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Mujoco Results. We show our performance on MuJoCo control tasks using 10 expert trajectories.</figDesc><table><row><cell>Task</cell><cell>Random</cell><cell>BC</cell><cell>GAIL</cell><cell>ValueDICE</cell><cell>IQ (Ours)</cell><cell>Expert</cell></row><row><cell>Hopper</cell><cell>14 ? 8</cell><cell>1345 ? 422</cell><cell>3322 ? 510</cell><cell cols="2">3399 ? 651 3529 ? 15</cell><cell>3533 ? 39</cell></row><row><cell cols="2">Half-Cheetah ?282 ? 80</cell><cell>2701 ? 950</cell><cell cols="3">4280 ? 1002 4840 ? 132 5154 ? 82</cell><cell>5098 ? 62</cell></row><row><cell>Walker</cell><cell>1 ? 5</cell><cell>3730 ? 1440</cell><cell>4417 ? 420</cell><cell cols="2">4384 ? 345 5212 ? 85</cell><cell>5274 ? 53</cell></row><row><cell>Ant</cell><cell>?70 ? 111</cell><cell>2272 ? 472</cell><cell>3997 ? 312</cell><cell cols="2">4507 ? 265 4683 ? 67</cell><cell>4700 ? 80</cell></row><row><cell>Humanoid</cell><cell>123 ? 35</cell><cell>2057 ? 843</cell><cell>372 ? 51</cell><cell cols="2">2001 ? 524 5288 ? 73</cell><cell>5313 ? 210</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on Atari Suite. We show our results on Atari Suite tasks using 20 expert demonstrations.</figDesc><table><row><cell>Env</cell><cell>IQ (Ours)</cell><cell>Expert</cell></row><row><cell>Pong</cell><cell>19 ? 2</cell><cell>21 ? 0</cell></row><row><cell>Breakout</cell><cell>320 ? 72</cell><cell>376 ? 34</cell></row><row><cell>Space Invaders</cell><cell>807 ? 102</cell><cell>823 ? 272</cell></row><row><cell>BeamRider</cell><cell>3025 ? 845</cell><cell>4295 ? 1173</cell></row><row><cell>Seaquest</cell><cell>2349 ? 342</cell><cell>2393 ? 291</cell></row><row><cell>Qbert</cell><cell cols="2">12940 ? 2026 11496 ? 1988</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Reward Correlations. We show pearson correlations between our learnt reward and the env rewards.</figDesc><table><row><cell>Env</cell><cell cols="4">Reward correlation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cartpole</cell><cell></cell><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LunarLander</cell><cell></cell><cell></cell><cell>0.92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hopper</cell><cell></cell><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Half-Cheetah</cell><cell></cell><cell></cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pong</cell><cell></cell><cell></cell><cell>0.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Do we overfit? Compared to ValueDICE, we</cell><cell></cell><cell>5000</cell><cell></cell><cell></cell><cell cols="2">Half-Cheetah</cell><cell></cell><cell></cell></row><row><cell>don't observe overfitting using IQ-Learn with</cell><cell></cell><cell>4000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the number of update steps. We show a compar-ision on Half-Cheetah environment using one</cell><cell>Reward</cell><cell>2000 3000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>expert trajectory in Fig 11. ValueDICE begins to overfit around 100k update steps, whereas IQ-Learn converges to expert rewards and remains stable.</cell><cell></cell><cell>0 1000</cell><cell>0k</cell><cell>50k</cell><cell>100k</cell><cell>150k Update steps</cell><cell>200k</cell><cell>250k</cell><cell>Expert IQ (Ours) vDICE 300k</cell></row><row><cell></cell><cell cols="9">Figure 11: Half-Cheetah overfitting comparision</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Results on ILO. We show evironment returns using 1 and 10 expert demonstrations.</figDesc><table><row><cell>Env</cell><cell>1 demo</cell><cell>10 demos</cell></row><row><cell>CartPole LunarLander Hopper</cell><cell cols="2">452 ? 50 20 ? 102 2507 ? 345 3465 ? 51 485 ? 25 220 ? 69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Offline. We show evironment returns vs number of experts on reacher_easy for offline case.</figDesc><table><row><cell cols="2">Num Experts Rewards</cell></row><row><cell>1</cell><cell>105.4</cell></row><row><cell>5</cell><cell>120.1</cell></row><row><cell>10</cell><cell>210.6</cell></row><row><cell>20</cell><cell>325.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Online. We show evironment returns vs number of experts on reacher_easy for online case.</figDesc><table><row><cell cols="2">Num Experts Rewards</cell></row><row><cell>1</cell><cell>271.3</cell></row><row><cell>5</cell><cell>485.1</cell></row><row><cell>10</cell><cell>545.0</cell></row><row><cell>20</cell><cell>734.9</cell></row><row><cell>50</cell><cell>926.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our implementation is available at https://github.com/Div99/IQ-Learn. arXiv:2106.12142v4 [cs.LG] 3 Nov 2022</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Averaging over the expert occupancy allows ? to adjust to arbitrary experts and accommodate multimodality.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We recover IPMs when using identity ? and restricted reward family R.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The additional scalar terms scale the entropy regularization strength and can be ignored in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The simplification to get Eq. (12) is not applicable in the online IL setting where our method differs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">? 2 and W1 can be used together to still have a convex regularization and is more stable.<ref type="bibr" target="#b6">7</ref> Normalized rewards are obtained by setting random behavior to 0 and expert one to 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The objective and the reward constraints remain same along each action dimension and a symmetry argument holds.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">A concave function may not be differentiable everywhere and in general, we get a condition on the subdifferential of ?: ?/?E ? ??(r).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/yrlu/irl-imitation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Funding Transparency</head><p>This research was supported in part by NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024) and FLI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<title level="m">Apprenticeship learning via inverse reinforcement learning. International conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Non-adversarial imitation learning and its connections to adversarial methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Arenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03525</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Model-based adversarial imitation learning. stat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1050</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarial soft advantage fitting: Imitation learning without policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonseok</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Infinite time horizon maximum causal entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bambos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">53rd IEEE Conference on Decision and Control</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4911" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scalable bayesian inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Primal wasserstein imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?onard</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021-Ninth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mastering the real-time strategy game starcraft ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepmind</forename><surname>G Alphastar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inverse optimal control with linearly-solvable mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnamurthy</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning robust rewards with adverserial inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkHywl-A-.1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reinforcement learning with deep energy-based policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A robot controller using learning by imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Symposium on Intelligent Robotic Systems, LIFTA-IMAG</title>
		<meeting>2nd Int. Symposium on Intelligent Robotic Systems, LIFTA-IMAG<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inverse reinforcement learning with simultaneous estimation of rewards and dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Gindele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Model-free irl using maximum likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinamra</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bikramjit</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Strictly batch imitation learning by energy-based distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Bica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS), 2020</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch, off-policy and model-free apprenticeship learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Workshop on Reinforcement Learning (EWRL)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Kumar Krishna Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imitation learning via off-policy distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hyg-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Conservative q-learning for offline reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.04779" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srivatsan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<title level="m">Truly batch apprenticeship learning with deep successor features. International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Are gans created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno>abs/1802.05957</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning. ArXiv, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5602</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dualdice: Efficient estimation of off-policy stationary distribution corrections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlam</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosted and reward-regularized classification for apprenticeship learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Autonomous agents and multi-agent systems (AAMAS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bridging the gap between imitation learning and inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1814" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rl baselines3 zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonin</forename><surname>Raffin</surname></persName>
		</author>
		<ptr target="https://github.com/DLR-RM/rl-baselines3-zoo" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sqil: Imitation learning via reinforcement learning with sparse rewards. arXiv: Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siddharth Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient reductions for imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to fly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Sammut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Kedzier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Michie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Machine Learning</title>
		<meeting>the Ninth Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On general minimax theorems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Sion</surname></persName>
		</author>
		<idno>doi: pjm/1103040253</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokul</forename><surname>Swamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiban</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei Steven</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02063</idno>
		<title level="m">Critique of Strictly Batch Imitation Learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Of moments and matching: Trade-offs and treatments in imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokul</forename><surname>Swamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiban</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03236</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial cooperative imitation learning for dynamic treatment regimes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1785" to="1795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-inverse reinforcement learning with probabilistic context variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Exploring imitation learning for autonomous driving with feedback synthesizer and differentiable rasterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01882</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Modeling purposeful adaptive behavior with the principle of maximum causal entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
