<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SOFT DIFFUSION SCORE MATCHING FOR GENERAL CORRUPTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Daras</surname></persName>
							<email>giannisdaras@utexas.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">T</forename><surname>Austin</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
							<email>htalebi@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
							<email>dimakis@austin.utexas.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
							<email>milanfar@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SOFT DIFFUSION SCORE MATCHING FOR GENERAL CORRUPTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for CelebA. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, that after corruption, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art FID score 1.85 on CelebA-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion. * The work was done during an internship at Google.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Score-based models <ref type="bibr" target="#b49">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b51">Song et al., 2021b)</ref> and Denoising Diffusion Probabilistic Models (DDPMs) <ref type="bibr" target="#b47">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b14">Ho et al., 2020;</ref><ref type="bibr" target="#b48">Song et al., 2021a)</ref> are two powerful classes of generative models that produce samples by inverting a diffusion process. These two classes have been unified under a single framework <ref type="bibr" target="#b51">(Song et al., 2021b)</ref> and are widely known as diffusion models. Diffusion modeling has found great success in a wide range of applications <ref type="bibr" target="#b7">(Croitoru et al., 2022;</ref><ref type="bibr" target="#b55">Yang et al., 2022)</ref>, including image <ref type="bibr" target="#b38">Ramesh et al., 2022;</ref><ref type="bibr" target="#b41">Rombach et al., 2022;</ref>, audio <ref type="bibr" target="#b29">(Kong et al., 2021;</ref><ref type="bibr" target="#b39">Richter et al., 2022;</ref><ref type="bibr" target="#b46">Serr? et al., 2022)</ref>, video generation , as well as solving inverse problems <ref type="bibr" target="#b9">(Daras et al., 2022;</ref><ref type="bibr" target="#b23">Kadkhodaie &amp; Simoncelli, 2021;</ref><ref type="bibr" target="#b26">Kawar et al., 2022;</ref><ref type="bibr" target="#b20">Jalal et al., 2021;</ref><ref type="bibr" target="#b44">Saharia et al., 2022b;</ref><ref type="bibr" target="#b30">Laumont et al., 2022;</ref><ref type="bibr" target="#b53">Whang et al., 2022;</ref>. <ref type="bibr" target="#b24">Karras et al. (2022)</ref> analyze the design space of diffusion models. The authors identify three stages: i) the noise scheduling, ii) the network parametrization (each one leads to a different loss function), iii) the sampling algorithm. We argue that there is one more important step: choosing how to corrupt. Typically, the diffusion is additive noise of different magnitudes (and sometimes input rescalings). There have been a few recent attempts to use different corruptions <ref type="bibr" target="#b10">(Deasy et al., 2021;</ref><ref type="bibr" target="#b18">Hoogeboom et al., 2022a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b2">Avrahami et al., 2022;</ref><ref type="bibr" target="#b35">Nachmani et al., 2021;</ref><ref type="bibr" target="#b21">Johnson et al., 2021;</ref><ref type="bibr" target="#b31">Lee et al., 2022;</ref>, but the results are usually inferior to diffusion with additive noise. Also, a common framework on how to properly design general corruption processes is missing.</p><p>We present such a principled framework for learning to invert a general class of corruption processes. We propose a new objective called Soft Score Matching that provably learns the score for any regular <ref type="figure">Figure 1</ref>: Top two rows: Demonstration of our generalized diffusion method. Instead of corrupting by only adding noise, we propose a framework to provably learn the score function to reverse any linear diffusion (left: blur and noise, right: masking and noise). Our (blur and noise) models achieve state-of-the-art FID score 1.85 on CelebA-64. Uncurated samples shown in the last three rows. linear corruption process. Soft Score Matching incorporates the filtering process in the network and trains the model to predict a clean image that after corruption matches the diffused observation.</p><p>Our theoretical results show that Soft Score Matching learns the score (i.e. likelihood gradients) for corruption processes that satisfy a regularity condition that we identify: the diffusion must transform any image into any other image with nonzero likelihood. Using our method and Gaussian Blur paired with little noise as the diffusion mechanism, we achieve state-of-the-art FID on CelebA (FID 1.85). We also show that our corruption process leads to generative models that are faster compared to vanilla Gaussian denoising diffusion.</p><p>Our contributions: a) We propose a learning objective that: i) provably learns the score for a wide family of regular diffusion processes and ii) enables learning under limited randomness in the diffusion. b) We present a principled way to select the intermediate distributions. Our method minimizes the Wasserstein distance along the path from the initial to the final distribution. c) We propose a novel sampling method that we call Momentum Sampler: our sampler uses a convex combination of corruptions at different diffusion levels and is inspired by momentum methods in optimization. d) We train models on CelebA and CIFAR-10. Our trained models on CelebA achieve a new state-of-the-art FID score of 1.85 for linear diffusion models while being significantly faster compared to models trained with vanilla Gaussian denoising diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Diffusion models are generative models that produce samples by inverting a corruption process. The corruption level is typically indexed by a time t, with t = 0 corresponding to clean and t = 1 to fully corrupted images. The diffusion process can be discrete or continuous. The two general classes of diffusion models are Score-Based Models <ref type="bibr" target="#b49">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b51">Song et al., 2021b)</ref> and Denoising Diffusion Probabilistic Models (DDPMs) <ref type="bibr" target="#b47">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b14">Ho et al., 2020)</ref>.</p><p>The typical diffusion in score-based modeling is additive noise of increasing magnitude. The perturbation kernel at time t is: q t (x t |x 0 ) = N (x t |? = x 0 , ? = ? 2 t I), where x 0 ? q 0 is a clean image. Score models are trained with the Denoising Score Matching (DSM) objective:</p><formula xml:id="formula_0">min ? E t?U [0,1] w t E (x0,xt)?q0(x0)qt(xt|x0) ||s ? (x t |t) ? ? xt log q t (x t |x 0 )|| 2 ,<label>(1)</label></formula><p>where w t scales differently the weights of the inner objectives. If we train for each noise level t independently, given enough data and model capacity, the network is guaranteed to recover the gradient of the log likelihood <ref type="bibr" target="#b52">(Vincent, 2011)</ref>, known as the score function. In other words, the model s ? (x t |t) is trained such that: s ? (x t |t) ? ? xt log q t (x t ). In practice, we use parameter sharing and conditioning on time t to learn all the scores. Once the model is trained, we start from a sample of the final distribution, q 1 , and then use the learned score to gradually denoise it <ref type="bibr" target="#b49">(Song &amp; Ermon, 2019;</ref>. The final variance ? 2 1 is selected to be very large such that the distribution q 1 is approximately Gaussian, i.e. the signal to noise ratio tends to 0.</p><p>DDPMs corrupt by rescaling the input images and by adding noise. The corruption can be modelled with a Markov chain with perturbation kernel q t (x t |x t??t ) = N (x t |? = ? 1 ? ? t x t??t , ? = ? t I). Typically, ? 1 = 1 and hence q 1 = N (0, I). DDPMs are also trained with the DSM objective which is derived by minimizing an evidence lower bound (ELBO) <ref type="bibr" target="#b14">(Ho et al., 2020)</ref>.</p><p>In their seminal work, <ref type="bibr" target="#b51">Song et al. (2021b)</ref> observe that the diffusions of both Score-Based models and DDPMs can be expressed as solutions of Stochastic Differential Equations (SDEs) of the form:</p><formula xml:id="formula_1">dx = f (x, t)dt + g(t)dw,<label>(2)</label></formula><p>where w is the standard Wiener process. Particularly, Score-Based models use: f (x, t) = 0, g(t) = d? 2 t dt and DDPMs use:</p><formula xml:id="formula_2">f (x, t) = ? 1 2 ? t x, g(t) = ? ? t .</formula><p>As explained earlier, for Score-Based models we need large noise at the end for the final distribution to approximate a normal distribution. Hence, the corresponding SDE is named Variance Exploding (VE) SDE <ref type="bibr" target="#b51">(Song et al., 2021b)</ref>. DDPMs usually have a final distribution of unit variance and hence their SDE is known as the Variance Preserving (VP) SDE <ref type="bibr" target="#b51">(Song et al., 2021b)</ref>. <ref type="bibr" target="#b51">Song et al. (2021b)</ref> also propose another SDE with bounded variance, the subVP-SDE, that experimentally yields better likelihoods.</p><p>For both Score-Based models and DDPMs, Eq. (2) is known as the Forward SDE. This SDE is reversible <ref type="bibr" target="#b0">Anderson (1982)</ref> and the Reverse SDE is given below:</p><formula xml:id="formula_3">dx = f (x, t) ? g 2 (t)? x log q t (x) dt + g(t)dw,<label>(3)</label></formula><p>wherew is the standard Wiener process when the time flows in the reverse direction. Typically, ? x log q t (x) is approximated by s ? (x t |t) and samples are generated by solving the Reverse SDE <ref type="bibr" target="#b51">(Song et al., 2021b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Our framework for training diffusion models with more general corruptions includes three components: i) the training objective, ii) the sampling, iii) the scheduling of the corruption mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRAINING OBJECTIVE</head><p>We study corruption processes of the form:</p><formula xml:id="formula_4">x t = C t x 0 + s t ? t ,<label>(4)</label></formula><p>where C t : R n ? R n is a deterministic linear operator, ? t is a Wiener process, and s t is a nonnegative scalar controlling the noise level at time t, and x 0 ? q 0 (x). We further denote with ? 2 t the variance of the noise at level t and we assume that it is a non-decreasing function of t. Unless stated otherwise, we assume that time is continuous and runs from t = 0 to t = 1. Additionally, we assume that at t = 0, we have C 0 = I n?n and ? 0 = 0, i.e. t = 0 corresponds to natural images. We also assume that recovering x 0 from x t is harder as t gets larger (i.e., entropy of q t (x 0 | x t ) increases with t). Eq. (4) defines a general class of diffusion processes, that includes (as special cases) the VE, VP and subVP SDEs used in <ref type="bibr" target="#b51">Song et al. (2021b)</ref>. Our diffusion is the sum of a deterministic linear corruption of x 0 and a stochastic part that progressively adds noise. For any corruption process of this family, we are interested in learning the scores, i.e. ? xt log q t (x t ) for all t.</p><p>For the vanilla Gaussian denoising diffusion, the celebrated result of <ref type="bibr" target="#b52">Vincent (2011)</ref> shows that we only need access to the gradient of the conditional log-likelihood, ? xt log q t (x t |x 0 ), in order to learn the score, ? xt log q t (x t ). By revisiting the proof of <ref type="bibr" target="#b52">Vincent (2011)</ref>, we find that this is actually true for a wide set of corruption processes, as long as some mild technical conditions are satisfied. In fact, the following general Theorem holds:</p><p>Theorem 3.1. Let q 0 , q t be two distributions in R n . Assume that all conditional distributions, q t (x t |x 0 ), are fully supported and differentiable in R n . Let:</p><formula xml:id="formula_5">J 1 (?) = 1 2 E xt?qt ||s ? (x t ) ? ? xt log q t (x t )|| 2 ,<label>(5)</label></formula><formula xml:id="formula_6">J 2 (?) = 1 2 E (x0,xt)?q0(x0)qt(xt|x0) ||s ? (x t ) ? ? xt log q t (x t |x 0 )|| 2 .<label>(6)</label></formula><p>Then, there is a universal constant C (that does not depend on ?) such that: J 1 (?) = J 2 (?) + C.</p><p>Theorem 3.1 implies that minimizing the second function is equivalent to minimizing the first one. The second function is nothing else than the DSM objective. Our main observation is that noise is not always necessary for learning the score using the DSM objective. A necessary condition is that the corruption process gives non-zero probability to all x t for any image x 0 . This is easily achieved by adding Gaussian noise, but this is not the only option. The proof of this Theorem is deferred in the Appendix and it is following the calculations of <ref type="bibr" target="#b52">Vincent (2011)</ref>.</p><p>Network parametrization. For the class of diffusion processes given by Eq. (4), we have that: q t (x t |x 0 ) = N x t ; ? = C t x 0 , ? = ? 2 t I and hence the objective becomes:</p><formula xml:id="formula_7">L(t) = 1 2 E (x0,xt)?q0(x0)qt(xt|x0) s ? (x t |t) ? C t x 0 ? x t ? 2 t 2 .<label>(7)</label></formula><p>As shown, the objective of the model is to predict the (normalized) difference, C t x 0 ? x t , which is actually the noise, ? t . We argue that even though this objective is theoretically grounded, in many cases, it would not work in practice because we would need infinite samples to actually learn the vector-field ? xt log q t (x t ) in a way that would allow sampling.</p><p>Assume that the corruption process is blurring (at different levels) paired with additive noise of small magnitude. The objective written in Eq. (7) learns the distributions of blurry (and slightly noisy images) by just removing noise. Hence, in practice we might only learn these distributions locally (around the blurry images) and hence we might not be able to reduce the blurriness. This point might be better understood after we present our Sampling Method in Section 3.2.</p><p>To account for this problem, we propose a network reparametrization which leverages that we know the linear corruption mechanism, C t . Specifically, we propose the following parametrization:</p><formula xml:id="formula_8">s ? (x t |t) = C t h ? (x t |t) ? x t ? 2 t .<label>(8)</label></formula><p>Crucially, the network incorporates the corruption process. The loss becomes:</p><formula xml:id="formula_9">L(t) = 1 2 E (x0,xt)?q0(x0)qt(xt|x0) 1 ? 4 t ||C t (h ? (x t |t) ? x 0 )|| 2 .<label>(9)</label></formula><p>When C t is a blurring matrix, this loss function is the MSE between the blurred prediction of h ? and the blurred clean image. Finally, as observed in previous works <ref type="bibr" target="#b49">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b14">Ho et al., 2020;</ref><ref type="bibr" target="#b24">Karras et al., 2022)</ref>, the optimization landscape becomes smoother when we are predicting the residual, instead of the clean image directly. This corresponds to the additional reparametrization:</p><formula xml:id="formula_10">h ? (x t |t) = ? ? (x t |t) + x t ,<label>(10)</label></formula><p>which leads to the final form of our loss function:</p><formula xml:id="formula_11">L(t) = 1 2 E (x0,xt)?q0(x0)qt(xt|x0) 1 ? 4 t ||C t (? ? (x t |t) ? r t )|| 2 ,<label>(11)</label></formula><p>where r t is the residual with respect to the clean image, i.e. r t = x 0 ? x t . Following prior work, we use a single network conditioned on time t that is optimized for all L(t). Hence, the total loss is:</p><formula xml:id="formula_12">L = E t?U [0,1] w(t) E (x0,xt)?q0(x0)qt(xt|x0) ||C t (? ? (x t |t) ? r t )|| 2 ,<label>(12)</label></formula><p>where the weights are usually chosen to be 1 or 1/? 2 t <ref type="bibr" target="#b24">(Karras et al., 2022;</ref><ref type="bibr" target="#b28">Kingma et al., 2021)</ref>. We call our training objective Soft Score Matching. The name is inspired from "soft filtering" a term used in photography to denote an image filter that removes fine details (e.g., blur, fading, etc). As in the Denoising Score Matching, the network is essentially trained to predict the residual to the clean image, but in our case, the loss is in the filtered space. When there is no filtering matrix, i.e. C t = I, we recover the DSM objective used in <ref type="bibr" target="#b49">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b51">Song et al., 2021b)</ref>.</p><p>Comparison with objectives used in other works. Previous <ref type="bibr" target="#b1">(Anonymous, 2022)</ref> or concurrent <ref type="bibr" target="#b3">(Bansal et al., 2022;</ref><ref type="bibr" target="#b40">Rissanen et al., 2022;</ref><ref type="bibr" target="#b17">Hoogeboom &amp; Salimans, 2022</ref>) works that consider different degradations than Gaussian Diffusion, use the heuristic objective of predicting the clean image, i.e. they minimize: ||? ? (x t |t) ? r t ||. This is actually an upper-bound on our loss,</p><formula xml:id="formula_13">i.e. ||C t (? ? (x t |t) ? r t ) || ? ||C t ||||? ? (x t |t) ? r t ||.</formula><p>Since the spectral norm, ||C t ||, is fixed, one can optimize for the upper-bound by minimizing ||? ? (x t |t) ? r t ||. Instead, Soft Score Matching optimizes directly for learning the score. Experimentally, Soft Score Matching outperforms (ours FID: 1.85, theirs: 5.91), under the exact same setting, this simple baseline (see Experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SAMPLING</head><p>Algorithm 1 Naive Sampler</p><formula xml:id="formula_14">Require: p1, ? ? , Ct, ?t, ?t x1 ? p1(x) for t = 1 to 0 with step ??t d? x0 = ? ? (xt|t) + xt ?t ? N (0, I) xt??t ? Ct??tx0 + ?t??t?t end for return x0</formula><p>Naive Sampler. Once the model is trained, we need a way to generate samples. The simplest idea is take advantage of the fact that our trained model, ? ? (x t |t), can be used to get an estimate of the clean image,x 0 . Hence, whenever we want to move from a corruption level t to a corruption level t ? ?t, we can feed the image x t to the model to get an estimate of the clean image,x 0 , and then corrupt back to level t ? ?t. This idea is summarized in Algorithm 1.</p><p>Momentum Sampler. As we will show in the Experiments section, the naive sampler we presented above leads to generated images that lack diversity. We propose an simple, yet novel, alternative method for sampling from the general linear diffusion model presented in Eq. (4). Our method is inspired by the continuous formulation of diffusion models that is introduced in <ref type="bibr" target="#b51">Song et al. (2021b)</ref>.</p><p>To develop our idea, we start with the toy setting where the dataset contains one single image, ? ? R n . Then the corruption of Eq. (4) can be seen as the solution to the following SDE:</p><formula xml:id="formula_15">dx t =? t ?dt + d(? 2 t ) dt dw,<label>(13)</label></formula><p>where w is the standard Wiener process. This is a special case of the It? SDE: dx = f (x, t)dt + g(t)dw, that appears in <ref type="bibr" target="#b51">Song et al. (2021b)</ref>. Particularly, f (x, t) =? t ? and g(t) = d(? 2 t ) dt . We note that crucially we treat ? as a constant and hence f (x, t) does not depend on previous values of x. Under this setting, this SDE describes a diffusion process that is reversible Anderson <ref type="bibr">(1982)</ref>. The reverse is also a diffusion process and it is given below:</p><formula xml:id="formula_16">dx t = ? t ? ? d(? 2 t ) dt ? xt log q t (x t ) dt + d(? 2 t ) dt dw.<label>(14)</label></formula><p>wherew is a standard Wiener process when time flows backwards from t = 1 to t = 0. In practice, to solve Eq. <ref type="formula" target="#formula_0">(14)</ref>, we would discretize the SDE (i.e., apply Euler-Maruyama, and approximate the function derivatives with finite differences). We use step size ?t &gt; 0,</p><formula xml:id="formula_17">x t ? x t??t = C t ? C t??t ?t ? ? ? 2 t ? ? 2 t??t ?t ? xt log q t (x t ) ?t + ? 2 t ? ? 2 t??t ?t ? ?t? ?? x t??t ? x t = (C t??t ? C t )? ? (? 2 t??t ? ? 2 t )? xt log q t (x t ) + ? 2 t ? ? 2 t??t ?,<label>(15)</label></formula><p>where ? ? N (0, I). So far we assumed that the dataset has one single image ? while ? is actually sampled from a distribution, q 0 . For each ? ? q 0 , there is a different SDE describing the corruption process. The key idea is that at each timestep t, we use the trained network to predict C t ? and then we solve the associated reverse SDE. Remember from Eq. (9) that the network h ? was trained such that h ? (x t |t) ? C t x 0 . Hence, we can use our trained network to estimate at each time t the filtered version, C t ?, of the image ? that it is to be generated. For the estimation of ? xt log q t (x t ) we also use our model that provably learns the score according to Theorem 3.1. Putting everything together:</p><formula xml:id="formula_18">?x t = x t??t ? x t = (16) = (C t??t ? C t ) ? ? (x t |t) + x t ? ? 2 t??t ?? 2 t ? 2 t C t ? ? (x t |t) + x t ? x t + ? 2 t ? ? 2 t??t ?. x 0</formula><p>Our sampler is summarized in Algorithm 2. It is interesting to understand what this sampler is doing. Essentially, there are two updates: one for deblurring and one for denoising. At the core of this update equation, is the prediction of the clean image,x 0 . Once the clean image is predicted, we blur it back to two different corruption levels, t and t ? ?t. The deblurring gradient is the residual between the blurred image at level t ? ?t and the blurred image at level t.</p><p>Algorithm 2 Momentum Sampler</p><formula xml:id="formula_19">Require: p1, ? ? , Ct, ?t, ?t x1 ? p1(x) for t = 1 to 0 with step ??t d? x0 = ? ? (xt|t) + xt Coarse prediction of the clean image. yt ? Ctx0 Coarse prediction of filtered image at t. ?t ? N (0, I) t ??t ? xt Estimate of noise at t. zt??t ? xt ? (? 2 t??t ?? 2 t ) ? 2 t? t + ? 2 t ? ? 2 t??t ?t Filtered image at t with noise at t ? ?t. yt??t ? Ct??tx0 Coarse prediction of filtered image at t ? ?t. xt??t ? zt??t + (?t??t ??t) Filtered image at t ? ?t with noise at t ? ?t. end for return x0</formula><p>Interestingly, the denoising update is the same as the one used in typical score-based models (that only use additive noise). In fact, if there is no blur (C t = I), our sampler becomes exactly the sampler used for the Variance Exploding (VE) SDE in <ref type="bibr" target="#b51">Song et al. (2021b)</ref>.</p><p>We call our sampler Momentum Sampler because we can think of it as a generalization of the update of the Naive Sampler, where there is a momentum term. To understand this better, we look at the setting where there is no noise. Then, the update rule of the Momentum Sampler is:</p><formula xml:id="formula_20">?x t = C t??tx0 ? C tx0 .<label>(17)</label></formula><p>As seen, the first term is what the Naive Sampler would use to update the image at level t and the second term is what the Naive Sampler would use to update the image at level t ? ?t. If these two directions are aligned, then the gradient ?x t is small. Hence, there is a notion of momentum, analogous to how the term is used in classical optimization.</p><p>Probability Flow Momentum Sampler. The update-rule of our Momentum Sampler was derived by looking at the first-order discretization of the backward SDE associated with our corruption, given in Eq. <ref type="formula" target="#formula_0">(14)</ref>. Similarly to <ref type="bibr" target="#b51">Song et al. (2021b)</ref>, we can also consider the Ordinary Differential Equation (ODE) associated with this SDE:</p><formula xml:id="formula_21">dx t = ? t ? ? 1 2 d(? 2 t ) dt ? xt log q t (x t ) dt.<label>(18)</label></formula><p>Using the same derivations as we did before, we can approximate? t ?, ? xt log q t (x t ) with our trained neural network and get a deterministic version of the Momentum Sampler. We name this sampler Probability Flow Momentum Sampler, following the naming convention of <ref type="bibr" target="#b51">Song et al. (2021b)</ref>. We detail our derivations in Section B of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SCHEDULING</head><p>The last piece of our framework is how to choose the corruption levels, i.e. the scheduling of the diffusion. For example, for Gaussian Blur, the scheduling decides how much time is spent in the diffusion in the very blurry, somewhat blurry and almost no blurry regimes. We provide a principled way of choosing the corruption levels for arbitrary corruption processes.</p><p>Let D 0 the distribution of real images and D 1 be a known distribution that we know how to sample from, e.g. a Normal Distribution. In the design phase of score-based modeling, the goal is to choose a set of intermediate distributions, {D t }, that smoothly transform images from D 0 to samples from the distribution D 1 . Let ? = {? 1 , ? 2 , ..., ? k , ...} be the space of diffusion parameters, i.e. each ? i corresponds to a distribution D ?i . In the case of blur for example, ? i controls how much we blur the image. Let also M : X ? X ? R be a metric that measures distances between distributions, e.g. M might be the Wasserstein Distance of distributions with support X .</p><p>We construct a weighted graph G with the nodes being the distributions and the weights given by:</p><formula xml:id="formula_22">w D ?i , D ?j = M D ?i , D ?j , if M D ?i , D ?j ? , ?, otherwise.<label>(19)</label></formula><p>For a fixed , we choose the distributions that minimize the cost of the path between D 0 and D 1 . The parameter expresses the power of the best neural network we can train to reverse one step of the diffusion. If = ?, then for any metric M , the shortest path is to go directly from D 0 to D 1 . However, it is impossible to denoise in one step a super noisy image. Hence, we need to go through many intermediate steps, which is forced by setting to a smaller value. As we increase the number of the candidate distributions we are getting closer to finding the geodesic between D 0 and D 1 . However, the computational cost of the method increases since we need to estimate all the pairwise distances M(D ?i , D ?j ). In practice, we use a relatively small number of candidate distributions, e.g. T = 256 and once the path is found, we do linear interpolation to extend to the continuous case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our method in CelebA-64 and CIFAR-10. We show that by just changing the corruption mechanism (and using our framework for scheduling, learning and sampling) we significantly improve the FID score and reduce the sampling time. We use the architecture and the training hyperparameters from <ref type="bibr" target="#b51">Song et al. (2021b)</ref> (full details can be found in the Appendix).</p><p>For most of our experiments, we use Gaussian Blur as our primary corruption mechanism. To illustrate that our method works more generally, we also show results with masking which is discussed separately later. Consistent with the description of our method, our deterministic corruptions are also paired with additive low magnitude noise. This is required by our theoretical results, otherwise the conditional log-likelihood, log q(x t |x 0 ) would be undefined. We also find the addition of noise beneficial in practice (see Appendix F.1.1 for ablation studies on the role of noise).</p><p>Scheduling. We use the standard geometric scheduling for the noise <ref type="bibr" target="#b49">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b51">Song et al., 2021b</ref>) and use the methodology described in Section 3.3 to select the blur levels. We use the Wasserstein distance as the metric M to measure how close are the different distributions. To clearly illustrate the switch to a different corruption, our diffusion has an initial stage that increases the noise (with no blur) and then we fix the noise (to a small value, e.g. ? = 0.1) and change (using our scheduling framework) the amount of blur. Our diffusion spends less than 20% of the total time in the initial stage that increases the noise. We ablate those choices in Section F.1 of the Appendix.</p><p>One important property of our framework is that the scheduling is dataset specific. Intuitively, the way we corrupt should depend on the nature of the data we are modelling. The interested reader can find the found schedulings for each dataset in <ref type="figure">Figure 6</ref> of the Appendix.</p><p>Results. We train our networks on CelebA-64 and CIFAR-10 using the found schedulings and the training objective of Eq. (12). We start by showing uncurated samples of our models in <ref type="figure" target="#fig_0">Figure 2</ref>. The generated images have high diversity and fidelity in both datasets. We compare the FID <ref type="bibr" target="#b13">(Heusel et al., 2017)</ref> obtained by our method on CelebA with many natural baselines that use any of the VE, VP or subVP SDEs. Specifically, we compare against DDPM <ref type="bibr" target="#b14">(Ho et al., 2020</ref>) that uses the VP SDE, DDIM <ref type="bibr" target="#b48">(Song et al., 2021a</ref>) that uses the same model but with a different sampler, DDPM++  that is the state-of-the-art model for VP-SDE and the NCSN++ models <ref type="bibr" target="#b51">(Song et al., 2021b)</ref> trained with the VE and subVP-SDEs. For a fair comparison, we only use reported numbers in published papers for the baselines and we do not rerun them ourselves. Our model achieves state-of-the-art FID score, 1.85, in CelebA, outperforming all the other methods. The results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. For CIFAR-10, we obtain FID score 3.86 with our Probability Flow Momentum Sampler and 3.91 with our Momentum Sampler. This FID score is competitive, yet not state-of-the-art. Specifically, we outperform the NCSN++ (VE SDE) baseline with Reverse Diffusion (similar to our SDE sampler -achieves FID 4.79) and with Probability Flow Ode sampling (theirs FID: 10.54). However, it is known <ref type="bibr" target="#b24">(Karras et al., 2022</ref>) that diffusion models with VP SDEs usually do better in CIFAR-10, e.g. DDPMs achieves FID 3.17.</p><p>Our method is superior in sampling time, for both CIFAR-10 and CelebA. <ref type="figure" target="#fig_1">Figure 3</ref> shows how FID changes based on the Number of Function Evaluations (NFEs). Our method requires significant less steps to achieve the same or better quality than NCSN++ (VE SDE) <ref type="bibr" target="#b51">(Song et al., 2021b)</ref>, using the same architecture and training hyperparameters (FID values taken from <ref type="bibr" target="#b33">(Ma et al., 2022)</ref>).</p><p>Model FID DDPM (VP SDE) <ref type="bibr" target="#b14">(Ho et al., 2020)</ref> 3.26 DDIM (VP SDE) <ref type="bibr" target="#b48">(Song et al., 2021a)</ref> 3.51 DDPM++ (VP SDE)  1.90 NCSN++ (subVP-SDE) <ref type="bibr" target="#b51">(Song et al., 2021b)</ref> 3.95 NCSN++ (VE SDE) <ref type="bibr" target="#b51">(Song et al., 2021b)</ref> 3.25 Ours (VE SDE + Blur)</p><p>1.85  Ablation Study for Sampling. For all the results we presented so far, we used the Momentum Sampler that we introduced in Section 3.2 and Algorithm 2. In this section, we ablate the choice of the sampler. Specifically, we compare with the intuitive Naive Sampler described in Algorithm 1. We show that the choice of the sampler has a dramatic effect in the quality and the diversity of the generated images. Results are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. The images from the Naive Sampler seem repetitive and lack details. This is reflected in the poor FID score. The Momentum Sampler leads to images with greater variety and detail, dramatically improving FID from 27.82 to 1.85.  Training Objective. Concurrent works <ref type="bibr" target="#b17">(Hoogeboom &amp; Salimans, 2022;</ref><ref type="bibr" target="#b3">Bansal et al., 2022</ref>) that also consider different corruption processes have used as their loss a simple objective: given an input image, they predict the residual to the clean. As explained in Section 3.1, this is equivalent to optimizing for an upper-bound of the Soft Score Matching objective (which is minimizing the error to the score-function, see Theorem 3.1). We show that if we use our exact pipeline (model, scheduling, sampler, training hyperparameters and so on) and we replace our Soft Score matching Loss with the loss introduced in <ref type="bibr" target="#b3">Bansal et al. (2022)</ref>, FID score increases from 1.85 to 5.91. This experiment shows the effectiveness of the Soft Score Matching objective.</p><p>Ablation Studies for Scheduling. We perform extensive ablations on the scheduling to understand the role and importance of noise in the framework and whether our proposed scheme outperforms other natural baselines. The results are detailed in section F.1 of the Appendix. Our main findings are as follows: i) the Momentum Sampler works even if noise and blur are changing simultaneously (i.e. for schedulings with non-fixed noise), ii) lowering the maximum value of noise leads to important performance degradation -for very small noise the method completely fails and iii) our found scheduling outperforms significantly (baseline FID: 8.35, ours: 1.85) a natural baseline that sets blur parameters such that MSE between the corrupted and the clean image decays in the same rate for Gaussian Denoising and Soft Diffusion. Masking Diffusion Models. To show the generality of our framework, we also train models with (discrete) masking diffusion paired with noise. <ref type="figure" target="#fig_0">Figure 1 (top 2 rows)</ref> shows the forward and the (learned) reverse process for blur on the left and masking on the right. We train the model with our Soft Score Matching objective on CelebA. Unconditional samples from the model trained with masking can be found in <ref type="figure" target="#fig_0">Figure 12</ref> of the Appendix. In <ref type="figure" target="#fig_4">Figure 5</ref>, we show the predictions of our two trained models (blur and masking) for the conditional mean, E[x 0 |x t ], at different times of the diffusion. Soft Score Matching trains the model to make a prediction that matches the real images in the filtered space. Hence, given masked images the masking model is incentived to predict right only the observed noisy region. As diffusion time t becomes smaller (cleaner images), the observed region grows and the model predicts bigger windows. Although it is interesting that we can train Masking Diffusion models, there are several limitations compared to Blur Diffusion (and even Gaussian Diffusion). We observe that these models are very slow to sample from: with 1024 sampling steps, FID is 30.92 while with 4096, FID improves to 12.37.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>We showed that by just changing the corruption mechanism, we observe important computational benefits. Reducing the number of function evaluations for sample generation with diffusion models is an active area of research. Jolicoeur-Martineau et al. <ref type="formula" target="#formula_0">(2021)</ref>; ; <ref type="bibr" target="#b48">Song et al. (2021a)</ref> propose more efficient samplers for diffusion models. <ref type="bibr" target="#b41">Rombach et al. (2022)</ref>; <ref type="bibr" target="#b9">Daras et al. (2022)</ref> train diffusion models on low-dimensional latent spaces. <ref type="bibr" target="#b54">Xiao et al. (2022)</ref> combine diffusion models with Generative Adversarial Networks (GANs) to allow for bigger denoising steps. ; <ref type="bibr" target="#b15">Ho et al. (2022a)</ref> generate high-resolution images progressively, from coarse to fine quality. <ref type="bibr" target="#b45">Salimans &amp; Ho (2022)</ref> train progressively a student network that mimics the teacher diffusion model with fewer sampling steps. We note that all these works are orthogonal to ours and therefore can be used in combination with our framework for even faster sampling.</p><p>On scheduling, there is closely related work by <ref type="bibr" target="#b4">Bao et al. (2022)</ref>. The authors find a closed form solution (w.r.t. the score function) for the variance of the reverse SDE for Gaussian Diffusion. Then, they select a noise scheduling that minimizes the KL along the path from the initial to the final distribution. In our work, we use Wasserstein distances and consider general corruption processes for which it is unclear whether such a closed form solution exists. Instead, we estimate the distances in a data-driven way. This allows us to schedule arbitrary diffusion processes in a principled way.</p><p>There is significant recent <ref type="bibr" target="#b1">(Anonymous, 2022)</ref> and concurrent work <ref type="bibr" target="#b40">(Rissanen et al., 2022;</ref><ref type="bibr" target="#b3">Bansal et al., 2022;</ref><ref type="bibr" target="#b17">Hoogeboom &amp; Salimans, 2022</ref>) that proposes diffusion with other degradations. These concurrent works have significant differences since they use different loss functions and sampling mechanisms. Soft Score Matching experimentally outperforms (under the exact same setting) the loss functions used in the concurrent works (ours FID: 1.85, theirs: 5.91). Our (blur) models obtain state-of-the-art FID for CelebA (FID 1.85). For CIFAR10, we outperform (FID: 3.86) Gaussian diffusion with Variance Exploding SDE <ref type="bibr" target="#b51">(Song et al., 2021b)</ref>. <ref type="bibr" target="#b17">Hoogeboom &amp; Salimans (2022)</ref> also use blurring (but with Variance Preserving SDE) to further push the CIFAR-10 performance to FID 3.17. These advancements show that there are multiple diffusions with promising potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>We presented a framework to train and sample from diffusion models that reverse general corruption processes. We showed that by changing the corruption process, we can get significant sample quality improvements and computational benefits. This work opens several future research directions. For example, it is possible to optimize or learn the corruption process for solving a specific type of inverse problem. It is also worth exploring if mixing different corruptions (blur, noise, masking, etc.) improves performance or sampling quality. Finally, it is important to understand the role of noise, from both a theoretical and practical standpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 PROOFS Theorem 3.1. Let q 0 , q t be two distributions in R n . Assume that all the conditional distributions, q t (x t |x 0 ), are supported and differentiable in R n . Let:</p><formula xml:id="formula_23">J 1 (?) = 1 2 E xt?qt ||s ? (x t ) ? ? xt log q t (x t )|| 2 ,<label>(20)</label></formula><formula xml:id="formula_24">J 2 (?) = 1 2 E (x0,xt)?q0(x0)qt(xt|x0) ||s ? (x t ) ? ? xt log q t (x t |x 0 )|| 2 .<label>(21)</label></formula><p>Then, there is a universal constant C (that does not depend on ?) such that: J 1 (?) = J 2 (?) + C.</p><p>The proof of this Theorem is following the calculations of <ref type="bibr" target="#b52">Vincent (2011)</ref>.</p><p>Proof of Theorem 3.1.</p><formula xml:id="formula_25">J 1 (?) = 1 2 E xt?qt ||s ? (x t )|| 2 ? 2s ? (x t ) T ? xt log q t (x t ) + ||?x t log q t (x t )|| 2 (22) = 1 2 E xt?qt ||s ? (x t )|| 2 ? E xt?qt s ? (x t ) T ? xt log q t (x t ) + C 1 .<label>(23)</label></formula><p>Similarly,</p><formula xml:id="formula_26">J 2 (?) = 1 2 E xt?qt ||s ? (x t )|| 2 ? E (x0,xt)?q0( x0)qt(xt|x0) s ? (x t ) T ? xt log q t (x t |x 0 ) + C 2 .<label>(24)</label></formula><p>It suffices to show that:</p><formula xml:id="formula_27">E xt?qt s ? (x t ) T ? xt log q t (x t ) = E (x0,xt)?q0(x0)qt(xt|x0) s ? (x t ) T ? xt log q t (x t |x 0 ) . (25)</formula><p>We start with the second term.</p><formula xml:id="formula_28">E (x0,xt)?q0(x0)qt(xt|x0) s ? (x t ) T ? xt log q t (x t |x 0 ) = x0 xt q 0 (x 0 )q t (x t |x 0 )s ? (x t ) T ? xt log q t (x t |x 0 )dx t dx 0 (26) = x0 xt s T ? (x t ) (q 0 (x 0 )q t (x t |x 0 )? xt log q t (x t |x 0 )) dx t dx 0 (27) = x0 xt s T ? (x t ) q 0 (x 0 )q t (x t |x 0 ) 1 q t (x t |x 0 ) ? xt q t (x t |x 0 ) dx t dx 0 (28) = x0 xt s T ? (x t ) (q 0 (x 0 )? xt q t (x t |x 0 )) dx t dx 0 (29) = xt x0 s T ? (x t ) (q 0 (x 0 )? xt q t (x t |x 0 )) dx 0 dx t (30) = xt s T ? (x t ) x0 q 0 (x 0 )? xt q t (x t |x 0 )dx 0 dx t (31) = xt s T ? (x t ) x0 ? xt (q 0 (x 0 )q t (x t |x 0 )) dx 0 dx t (32) = xt s T ? (x t ) ? xt x0 q 0 (x 0 )q t (x t |x 0 )dx 0 dx t (33) = xt s T ? (x t )? xt q t (x t )dx t (34) = xt q t (x t )s T ? (x t )? xt log q t (x t )dx t (35) = E xt?qt(xt) s T ? (x t )? xt log q t (x t ) .<label>(36)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROBABILITY FLOW ODE</head><p>In the main text, we derived our Momentum Sampler by analyzing the Backward SDE associated with our corruption process. Inspired by the works of <ref type="bibr" target="#b51">Song et al. (2021b)</ref>; <ref type="bibr" target="#b34">Maoutsa et al. (2020)</ref>, we also consider deterministic sampling that is derived by looking at the ODE that describes our diffusion. Particularly, the ODE:</p><formula xml:id="formula_29">dx t = f (x t , t) ? 1 2 g 2 (t)? xt log q t (x t ) dt,<label>(37)</label></formula><p>has the same marginal distributions <ref type="bibr" target="#b0">(Anderson, 1982;</ref><ref type="bibr" target="#b34">Maoutsa et al., 2020;</ref><ref type="bibr" target="#b51">Song et al., 2021b;</ref><ref type="bibr" target="#b5">Chen et al., 2018)</ref> with the Backward SDE:</p><formula xml:id="formula_30">dx t = f (x t , t) ? g 2 (t)? xt log q t (x t ) dt + g(t)dw.<label>(38)</label></formula><p>For our case, Eq. (37) becomes:</p><formula xml:id="formula_31">dx t = ? t ? ? 1 2 d(? 2 t ) dt ? xt log q t (x t ) dt.<label>(39)</label></formula><p>The first-order discretization of this ODE is given below:</p><formula xml:id="formula_32">x t??t ? x t = (C t??t ? C t )? ? (? 2 t??t ? ? 2 t ) 2 ? xt log q t (x t ).<label>(40)</label></formula><p>We estimate C t ? and ? xt log q t (x t ) with our neural network and we get the Neural ODE <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>:</p><formula xml:id="formula_33">?x t = x t??t ? x t = (41) = (C t??t ? C t ) ? ? (x t |t) + x t ? 1 2 ? 2 t??t ? ? 2 t ? 2 t C t ? ? (x t |t) + x t ? x t ,</formula><p>which is the update rule of our Probability Flow ODE Momentum Sampler.</p><p>We note that our simple discretization is not the only way to solve the ODE of Eq. (37). We can use more sophisticated, e.g. see <ref type="bibr" target="#b12">Dormand &amp; Prince (1980)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C SCHEDULINGS</head><p>We use our framework to select the blur levels in an unsupervised way. For the estimation of Wasserstein distances, we use the ott-jax <ref type="bibr" target="#b8">(Cuturi et al., 2022)</ref> software package. We start with 256 different blur levels and we tune such that the shortest path contains 32 distributions. We then use linear interpolation to extend to the continuous case. Full experimental details can be found in the Appendix. These choices seem to work well in practice, but further optimization could be made in future work. <ref type="figure">Figure 6</ref> shows the found schedulings for the CelebA and the CIFAR-10 datasets. Notice that the scheduling is slightly different between the two datasets -the diffusion depends on the nature of the data we are trying to model. We underline that the parameters for the blur are selected without any supervision, by solving the optimization problem we defined in Section 3.3.  <ref type="figure">Figure 6</ref>: Diffusion scheduling for CelebA-64 and CIFAR-10. The blur corruption levels are selected without supervision to minimize the sum of the Wasserstein distances between consecutive distributions. Notice that the scheduling is slightly different between the two datasets -the diffusion depends on the nature of the data we are trying to model. The support of the Gaussian blur kernel was set to 65 ? 65 and 161 ? 161 for CIFAR-10 and CelebA-64 datasets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D TRAINING DETAILS</head><p>Hyperparameters. For our trainings, we use Adam optimizer with learning rate 2e ? 4, ? 1 = 0.9, ? 2 = 0.999, = 1e ? 8. We additionally use gradient clipping for gradient norms bigger than 1.</p><p>For the learning rate scheduling, we use 5000 steps of linear warmup. We use batch size 128 and we train for 1 ? 2M iterations (based on observed FID performance).</p><p>Blur parameters. For the blurring operator, we use Gaussian blur with fixed kernel size and we vary the variance of the kernel. For CelebA-64, we keep the kernel half size fixed to 80 and we vary the standard deviation from 0.01 to 23. For CIFAR-10, we keep the kernel half size fixed to 32 and we vary the standard deviation from 0.01 to 18. For both datasets, we implement blur with zero-padding. We chose the final blur level such that the final distribution is easy to sample from. In both cases, the final distribution becomes noise on top of (almost) a single color.</p><p>Architecture. We use the architecture of <ref type="bibr" target="#b51">Song et al. (2021b)</ref> without any changes.</p><p>Training Objective For all our experiments, we scale the loss at level t with w(t) = 1/? 2 t as in <ref type="bibr" target="#b49">Song &amp; Ermon (2019;</ref>; <ref type="bibr" target="#b51">Song et al. (2021b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compute and Training Time</head><p>We train our models on 16 v2-TPUs. Our blur models on CelebA had an average speed of 6 iterations per second. For CIFAR-10, the average speed was 11 iterations per second We note that there is no overhead over the NCSN++ paper other than projecting to the measurements space, which can be done very efficiently for both blur and masking.</p><p>Evaluation We keep one checkpoint every 10000 steps and we keep the best model among the kept checkpoints based on the obtained FID score. We use 50000 samples to evaluate the FID, as it is typically done in prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E LIMITATIONS AND THINGS THAT DID NOT WORK</head><p>Our method has several limitations. First, it requires the diffusion operator to be known. This is not always the case, e.g. see <ref type="bibr" target="#b37">Peebles et al. (2022)</ref> where diffusion is applied to checkpoints of different models. Another limitation is that our framework does not offer any guidance on which diffusion operators are actually more or less useful for learning the data distribution. Particularly, we already showed that blurring is a much more powerful diffusion mechanism than masking, in the sense that it leads to better FID scores and faster generations. It is yet to be seen whether blurring is going to be outperformed by some other corruption method. Our method also only concerns linear diffusions (however, the extension to non-linear is relatively straightforward).</p><p>On the theoretical side, our method has also some shortcomings. First, it only intuitively explains why the reparametrization to the Denoising Score Matching is needed. Second, since our method is based on the Denoising Score Matching, it only works when the conditional log-likelihood is defined everywhere. There are distributions for which such condition is not satisfied, but still, can be learned (to some extent) with heuristic methods <ref type="bibr" target="#b3">(Bansal et al., 2022)</ref>.</p><p>On the practical side, we believe that our objective, Soft Score Matching, sometimes leads to slower sampling compared to the simpler objective of predicting the clean image. For example, for masking, since the model is only penalized in the observed region, there is no incentive in expanding this region. Hence, to achieve smooth transition between different masking levels we need to run many steps.</p><p>Experimentally, we tried using our framework with even less stochasticity, but it did not work, e.g. see 7. It would be interesting to understand better what is causing the failure and also what is the proper amount of randomness required at each diffusion step. F ADDITIONAL RESULTS F.1 SCHEDULING ABLATIONS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.1 ABLATION STUDIES FOR NOISE</head><p>In the experiments of the main paper, our diffusion involves an initial stage where only noise is added. Then noise is fixed and the images are getting corrupted by the deterministic operator (e.g. blur or masking). In this section, we show two ablations regarding the noise.</p><p>Magnitude of noise. In this ablation study, we still keep the noise fixed for a significant part of the diffusion, but we ablate the magnitude of the noise. Specifically, we attempt to study to what extent noise is needed in order to learn to reverse corruption processes with Soft Score Matching. We train two additional models on CelebA-64 where we attempt to decrease the maximum noise to a lower value. The corruptions for both models involves an initial stage where noise grows geometrically rate from the initial value (0.01) to the maximum value. Then, all the models keep the noise fixed at their maximum value for the rest of the diffusion. We use the following maximum values: i) ? max = 0.1 (model used in the paper), ii) ? max = 0.05, and iii) ? max = 0.025. Unconditional samples from  the two ablation models are shown in <ref type="figure" target="#fig_7">Figure 7</ref>. As shown, both models fail to produce realistic samples. The quality of samples deteriorates significantly as the noise decreases -the samples from the ablation models are significantly worse than the ones produced by our state-of-the-art model (see <ref type="figure" target="#fig_0">Figure 2</ref> (right)). We want to underline that this is not a conclusive study. It might be the case that with different hyperparameters one can make Soft Score Matching work with lower values of noise. For example, we might need to tune the weights w(t) since for the ablations (and the state-of-the-art model), we use w(t) = 1/? 2 t (as in <ref type="bibr" target="#b49">Song &amp; Ermon (2019;</ref>; <ref type="bibr" target="#b51">Song et al. (2021b)</ref>) which might be causing instabilities for low values of noise .</p><p>Noise changing throughout the diffusion. We also train a model where noise and blur are changing simultaneously throughout the diffusion. This is a sanity check to verify that our framework (learning and sampling) still works when the model needs to deblur and denoise at the same time. For the noise scheduling, we simply use geometric scheduling from 0.1 to 0.01. We keep the blur parameters the same with the state-of-the-art model, i.e. we use the blur parameters shown in <ref type="figure">Figure 6</ref>. This model achieves a competitive FID score, 4.31. This score could probably be further improved (by jointly selecting the blur and the noise scheduling with our framework), but this is beyond the scope of this ablation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.2 ABLATION STUDY FOR BLUR</head><p>For all our experiments so far, we chose the blur corruption levels based on the scheduling method we described in Section 3.3. We show the benefits of our approach by comparing to a natural baseline for selecting the diffusion levels. For this natural baseline, we use the scheduling of Variance Exploding (VE) as guidance. Specifically, we choose the blur parameters such that the MSE between the corrupted image and the clean image decays with the same rate for the Gaussian Denoising Diffusion and our Soft Diffusion (blur and low magnitude noise). Formally, let {q t } 1 t=0 be the (noisy) distributions used in <ref type="bibr" target="#b51">Song et al. (2021b)</ref> for the Variance Exploding (VE) SDE and let {q t } 1 t=0 the blurry (and noisy) distributions we want to select. At level t, we choose the blur parameters such that:</p><formula xml:id="formula_34">E (x0,xt)?qt(xt|x0)q0(x0) ||x 0 ? x t || 2 E (x0,x1)?q1(x1|x0)q0(x0) [||x 0 ? x 1 || 2 ] = E (x0,xt)?q t (xt|x0)q0(x0) ||x 0 ? x t || 2 E (x0,x1)?q 1 (x1|x0)q0(x0) [||x 0 ? x 1 || 2 ] .<label>(42)</label></formula><p>We retrain on CelebA using this natural baseline method for selecting the diffusion parameters. For a fair comparison, we keep the architecture and all the hyperparameters the same and we only ablate the scheduling of the blur. We measure FID for both the trained model with the baseline scheduling and we observe it increases from 1.85 to 8.35. Apart from this large deterioration in performance, the baseline model obtains its best FID score after 2000 steps, while with our scheduling we only need 280 steps to obtain the best FID. This experiment shows that the choice of scheduling is really important for the model performance but for also the computational requirements of the sampling.</p><p>F.2 CIFAR-10 SPEED PLOT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 NEAREST SAMPLES IN TRAINING DATA</head><p>To verify that our model does not simply memorize the training dataset, we present generated images from our model and their nearest neighbor (L2 pixel distance) from the dataset. The results are shown in <ref type="figure" target="#fig_9">Figure 9</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Uncurated samples from our trained models on CIFAR-10 (left) and CelebA (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>FID versus NFEs (CelebA-64).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Sampler (uncurated). FID: 27.82. (b) Momentum Sampler (uncurated). FID: 1.85.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Effect of sampling method on the quality of the generated samples. The images from the Naive Sampler (4a) seem repetitive and lack details. Momentum Sampler (4b) dramatically improves the sampling quality and the FID score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Conditional means E[x0|xt] predictions of our blur/masking models, at different diffusion times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Images generated with model trained with ?max = 0.025 (b) Images generated with model trained with ?max = 0.05</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Ablation study for the magnitude of noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Demonstration of how FID changes based on Number of Function Evaluations (NFEs) for CIFAR-10 for our (blur) model on CIFAR-10. Our model offers significant performance benefits for low number of function evaluations. (a) Generated images. (b) Nearest neighbors from dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Generated images and nearest neighbors (L2 pixel distance) from the training dataset. As shown, the model produces new samples and does not simply memorize the training dataset.F.4 UNCURATED SAMPLESFigures 10 and 11show more uncurated samples from our trained models on CelebA and CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>More uncurated samples from our blur model trained on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Uncurated samples from our masking model trained on CelebA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>FID results on CelebA-64.</figDesc><table><row><cell></cell><cell>128 512</cell><cell>CelebA</cell><cell>ncsn++ ours</cell></row><row><cell>FID</cell><cell>32</cell><cell></cell></row><row><cell></cell><cell>8</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell cols="2">200 Number of Function Evaluations (NFEs) 400 600 800 1000</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was done during an internship at Google Research. This research has been partially supported by NSF Grants CCF 1763702, 1934932, AF 1901281, 2008710, 2019844  the NSF IFML 2019844 award as well as research gifts by Western Digital, WNCG and MLL, computing resources from TACC and the Archie Straiton Fellowship.</p><p>The authors would like to thank Constantinos Daskalakis, Yuval Dagan, Sergey Ioffe and Jos? Lezama for useful discussions. We would also like to thank Vikram Voleti and Simon Welker for useful comments on an earlier version of our preprint.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion models in space and time via the discretized heat equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BrlGyp4uDbc" />
	</analytic>
	<monogr>
		<title level="m">Submitted to ICLR Workshop on Deep Generative Models for Highly Structured Data</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blended diffusion for text-driven editing of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Avrahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="18208" to="18218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Borgnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Min</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furong</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.09392</idno>
		<title level="m">Cold Diffusion: Inverting arbitrary image transforms without noise</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.06503</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving diffusion models for inverse problems using manifold constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongsu</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohoon</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong Chul</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00941</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Florinel-Alin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hondru</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.04747</idno>
		<title level="m">Radu Tudor Ionescu, and Mubarak Shah. Diffusion Models in Vision: A Survey</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laetitia</forename><surname>Meng-Papaxanthos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Bunne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Teboul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12324</idno>
		<title level="m">Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Score-guided intermediate layer optimization: Fast langevin mixing for inverse problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexandros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantinos</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daskalakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.09104</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Heavy-tailed denoising score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Deasy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Simidjievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09788</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A family of embedded runge-kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">and Tim Salimans. Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="47" to="48" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03458</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">Video diffusion models</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.05557</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">Blurring diffusion models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rianne van den Berg, and Tim Salimans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmijn</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Autoregressive diffusion models</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Equivariant diffusion for molecule generation in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">Garcia</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="8867" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust compressed sensing mri with deep generative priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Arvinte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexandros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14938" to="14954" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Austin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07675</idno>
		<title level="m">Rianne van den Berg, and Daniel Tarlow. Beyond in-place corruption: Insertion and deletion in denoising probabilistic models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gotta go fast when generating data with score-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Kachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14080</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic solutions for linear inverse problems using the prior implicit in a denoiser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Kadkhodaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13242" to="13254" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Elucidating the design space of diffusionbased generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00364</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Snips: Solving noisy inverse problems stochastically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahjat</forename><surname>Kawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Vaksman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21757" to="21769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Denoising diffusion restoration models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahjat</forename><surname>Kawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11201" to="11228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Variational diffusion models. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21696" to="21707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian imaging using plug &amp; play priors: when langevin meets tweedie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Laumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><forename type="middle">De</forename><surname>Bortoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Pereyra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="701" to="737" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Progressive deblurring of diffusion models for coarse-to-fine image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong Chul</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.11192</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pseudo numerical methods for diffusion models on manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09778</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Accelerating scorebased generative models for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04029</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interacting particle solutions of fokkerplanck equations through gradient-log-density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitra</forename><surname>Maoutsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">802</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Denoising diffusion gamma models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05948</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichol</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning to learn with generative models of neural network checkpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with clip latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Speech enhancement and dereverberation with diffusion-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Welker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marie</forename><surname>Lemercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bunlong</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Gerkmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.05830</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Generative modelling with inverse heat dissipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Severi Rissanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.13397</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohoon</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01864</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">Pyramidal denoising diffusion probabilistic models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; S Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
	</analytic>
	<monogr>
		<title level="j">Burcu Karagol Ayan</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Seyed Kamyar Seyed Ghasemipour</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=TIdIXIpzhoI" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Universal speech enhancement with score-based diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Oguz Araz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scaini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03065</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12438" to="12448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deblurring via stochastic refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexandros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16293" to="16303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tackling the generative learning trilemma with denoising diffusion GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenda</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.00796</idno>
		<title level="m">Ming-Hsuan Yang, and Bin Cui. Diffusion models: A comprehensive survey of methods and applications</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.01170</idno>
		<title level="m">First hitting diffusion models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
