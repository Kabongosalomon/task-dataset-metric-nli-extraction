<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">R-GCN: The R Could Stand for Random</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vic</forename><surname>Degraeve</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Vandewiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Femke</forename><surname>Ongenae</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofie</forename><forename type="middle">Van</forename><surname>Hoecke</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IDLab</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">R-GCN: The R Could Stand for Random</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Representation Learning ? Knowledge Graph Embeddings ? Graph Convolutional Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The inception of the Relational Graph Convolutional Network (r-gcn) marked a milestone in the Semantic Web domain as a widely cited method that generalises end-to-end hierarchical representation learning to Knowledge Graphs (KGs). r-gcns generate representations for nodes of interest by repeatedly aggregating parameterised, relation-specific transformations of their neighbours. However, in this paper, we argue that the the r-gcn's main contribution lies in this "message passing" paradigm, rather than the learned weights. To this end, we introduce the "Random Relational Graph Convolutional Network" (rrgcn), which leaves all parameters untrained and thus constructs node embeddings by aggregating randomly transformed random representations from neighbours, i.e., with no learned parameters. We empirically show that rr-gcns can compete with fully trained r-gcns in both node classification and link prediction settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Graphs (KGs) are the ideal data structure to represent both expert knowledge and observational data, and can be used to reveal new insights about the modelled domain. The latter becomes even more true thanks to today's hybrid Machine Learning (ML) methodologies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> where semantically enriched data is combined with a statistical method's ability to learn. As these models typically operate on Euclidean data, the integration of KGs into their decision making processes involves a non-trivial transformation from information represented as a variable number of nodes and edges to fixed size numerical vectors, i.e., graph embeddings. Relational Graph Convolutional Networks (r-gcns) embed semantic information about an entity contained in a KG by iteratively updating node representations. First, the previous representations for a node's neighbours are collected and passed through a learned, relation-specific, transformation. These transformed representations are referred to as "messages". Second, an aggregation of the collected messages and the node's own state yields the updated node representation <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this work, we ablate the parameterisation of the r-gcn's message passing step by randomly initialising and freezing the relation-specific transformations. We evaluate these "Random Relational Graph Convolutional Networks" (rr-gcns) on nine node classification datasets, and empirically show that these random transformations produce embeddings for node classification that are on par with -and sometimes even outperform -those produced by r-gcns. We also compare the embeddings produced by our rr-gcns to an end-to-end trained rgcn for a link prediction task and obtain very similar results.</p><p>The remainder of the paper is structured as follows. In Section 2, we summarise our main contributions to the field of KG embedding. Section 3 provides the necessary background in concepts that are fundamental to our rr-gcn. Next, in Section 4, we discuss our technique formally. We then compare rr-gcn to the original r-gcn and discuss the implications of these results in Section 5. Finally, we provide some closing remarks in Section 7 and discuss future work in 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Contributions</head><p>This paper is the first to evaluate fully random r-gcns for KG embeddings. We show that random transformations can capture a surprising amount of information. On the one hand, this makes our rr-gcns an interesting baseline model when developing trained embedding methods. On the other hand, the effectiveness of these random transformations illustrates that, for KGs, the rgcn's message passing and aggregation paradigm is more significant than the actual parameters, which have to be obtained through an expensive training procedure. This opens up avenues for further research on more efficient, and more powerful, message passing parameretisations for KGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In this section, we touch upon the concepts that are fundamental to our rr-gcn approach: (i) we briefly outline prior research directions in Knowledge Graph Embedding (KGE); (ii) we provide the necessary theory of r-gcns; and (iii) we discuss the concept of learning from random transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation Learning for KGs</head><p>Techniques to embed substructures in KGs can be categorised into four groups. A first category compromises of techniques that extract generic properties from neighbourhoods of substructures of interest. These can either be feature-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> or exploit similarities with other substructures of interest, i.e., kernel functions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. A second category consists of (algebraic) embedding spaces learned using tensor factorisation or through negative sampling <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. A third category adapts existing natural language processing (NLP) techniques, such as Word2Vec <ref type="bibr" target="#b12">[13]</ref>, to graph structures <ref type="bibr" target="#b13">[14]</ref>. A fourth and final category contains the message passing architectures that are trained end-to-end to aggregate relevant information around the substructures of interest <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. r-gcns belong to this final category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">R-GCN</head><p>Graph Neural Networks (gnns) allow for graph ML by learning how to update a node's representation based on its neighbours. These gnns work analogously to classical neural networks: they start from initial node features which are passed through multiple gnn layers to refine and abstract the representations by mixing in information from one additional hop with every layer.</p><p>A single gnn layer operates in three steps: every node (1) generates a "message" based on its current representation and sends it along its outgoing edges, (2) aggregates incoming messages, and (3) updates its representation based on the aggregated messages and its own previous representation.</p><p>The definitions of the message, aggregation and update functions differentiate several subtypes of gnns. The most prominent of these subtypes, the Graph Convolutional Network (gcn) <ref type="bibr" target="#b18">[19]</ref>, uses a learned transformation matrix as its update function, aggregates by averaging messages (including the node's own message) and passes the aggregations through an activation function to yield new node representations (see <ref type="bibr">Equation 1)</ref>.</p><formula xml:id="formula_0">h (l+1) i = ? j?N (i)?i 1 |N (i)| + 1 W (l) h (l) j<label>(1)</label></formula><p>Here, ? denotes the activation function used, h (l) i is the representation for node i at layer l, N (i) is the set of neighbours for node i, and W (l) is the learned transformation matrix in layer l. This formulation assumes unweighted edges.</p><p>r-gcns extend gcns to support typed edges. As KGs are multi-relational, this extension is particularly interesting for hybrid ML. They are almost identical to standard gcns but learn a different transformation matrix W (l) r for every relation r ? R. A separate transformation W (l) 0 takes care of self-loops (see <ref type="bibr">Equation 2</ref>). Note that R contains two copies of every relation in the original KG, as r-gcns also take outgoing (i.e. inverse) edges into account, with different learned transformations.</p><formula xml:id="formula_1">h (l+1) i = ? W (l) 0 h (l) i + r?R j?Nr(i) 1 |N r (i)| W (l) r h (l) j<label>(2)</label></formula><p>Learning separate transformations per relation results in extra parameters and might not work well for sparse types. Weight sharing between relations has been proposed to reduce overfitting and the number of required parameters <ref type="bibr" target="#b14">[15]</ref>. Using a basis decomposition, a layer's parameters are reduced to a fixed set of base parameter matrices V i (Equation 3).</p><formula xml:id="formula_2">W (l) r = B b=1 a (l) rb V (l) b<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning from Random Representations</head><p>Our approach is inspired by an algorithm from the time series classification domain, rocket <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. This algorithm applies a large number of random convolutions and aggregations to the raw time series input to extract features with state-of-the-art predictive performance. These random transformations do not have to be learned and can be applied very quickly, making rocket more scalable than other time series classification techniques. Two types of aggregations are used within rocket: (i) max-pooling, which just retrieves the maximum value of a certain convolutional filter when slided across the time series, and (ii) the proportion of positive values (ppv), which, for every filter, captures the proportion of the input time series for which the output of the convolution operation is positive. We will discuss the latter aggregation method in more depth in the context of graphs in Section 4.2.</p><p>The idea of freezing, or not training, layers in neural networks is not novel either. Reservoir Computing <ref type="bibr" target="#b21">[22]</ref> is a paradigm in recurrent neural networks where inputs are first passed through any black-box non-linear system called a "reservoir", which could be an untrained neural network. As such, only the final layer that maps the learned representations to the target output is trained. The paradigm of random modelling often introduces a trade-off between efficiency and effectiveness. As many random, albeit less effective, transformations can often be applied very efficiently, they allow to scale to higher numbers. An example of a technique where quantity matters over quality is ExtraTrees <ref type="bibr" target="#b22">[23]</ref>, where a large number of decision trees with random splits yield good results.</p><p>In a blog post, detailing the inner workings of (non-relational) gcns [24], Thomas Kipf hinted at the discriminative power of representations resulting from untrained transformations, but this hypothesis was never formally evaluated, let alone for KGs. In a recent paper <ref type="bibr" target="#b24">[25]</ref>, random transformations were evaluated in the context of link prediction in a neuromorphic computing setting. In this research, the authors backpropagated through random weights to train initial node embeddings. Other research demonstrated that keeping the initial node features random and frozen, and only training the message passing transformations results in good performance for unirelational graphs <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. For KGs, it is even possible to deterministically generate meaningful node features, avoiding the need to learn initial embeddings in the first place <ref type="bibr" target="#b27">[28]</ref>. As will be explained in the remainder of this paper, we are the first to explore fully random networks for KGs, that keep both the initial representations and the transformations frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we elaborate on the modifications we made to the original r-gcn algorithm. We also present a new information aggregation function for graphs: ppv. Finally, we compare memory requirements for trained and random r-gcns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Modifying the R-GCN Layer</head><p>Our rr-gcn layers use the same message paradigm as r-gcns (see <ref type="table">Equation 2</ref>). The most important difference is that we randomly initialise all transformation matrices using the Glorot uniform initialisation function <ref type="bibr" target="#b28">[29]</ref> and then keep them fixed to freeze the network. Basis decomposition is no longer required and thus not used, as overfitting cannot occur since no training is involved.</p><p>In <ref type="bibr" target="#b14">[15]</ref>, the node features that are fed to the first r-gcn layer are one-hot encoded. Using one-hot node features effectively assigns a separate initial embedding matrix W (0) r per relation type in the first message passing layer. This is indeed desirable for r-gcns, since the output node representation dimensionality is usually very small (10 to 16 in the original paper <ref type="bibr" target="#b14">[15]</ref>). Starting with small, randomly initialised, node representations limits the level of detail in the initial node characterisations and leads to worse performance for some datasets <ref type="bibr" target="#b29">[30]</ref>. As our rr-gcns use much larger node representations, we can feed our first message passing layer random features h (0) i ? R d , with d the embedding size, -which we also do not train -and save them as a single seed. In our experiments, this did not impact performance compared to one-hot encoded inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Proportion of Positive Values (PPV)</head><p>The performance of rocket's random feature extraction for time series depends strongly on their use of ppv <ref type="bibr" target="#b19">[20]</ref> pooling, as is also apparent from their second version which abandons max-pooling and uses only ppv features <ref type="bibr" target="#b20">[21]</ref>. Inspired by their success, we adapted a variant of ppv to graphs that aims to encapsulate additional information about a node's neighbourhood. Given a matrix of node representations H, we define ppv as the proportion of strictly positive values in a 1-hop neighborhood per representation dimension (see <ref type="bibr">Equation 4</ref>).</p><formula xml:id="formula_3">P = ppv(H, N ) = j?N (i) 1 |N (i)| 1[h j &gt; 0]<label>(4)</label></formula><p>Here, h i is the representation for the i th node (i th row in H) and 1[?] is the indicator function. The resulting matrix P , with the same dimensionality as H, houses additional node features that capture information about the feature diversity in every node's neighborhood. The ppv function does not differentiate neighbours based on relation types as it is applied as a "post-processing" step to representations that are the result of relation-specific transformations.  that entities can also be "literals" and have associated values. Both r-gcns and the proposed rr-gcns, however, regard literals as ordinary nodes. Algorithm 1 illustrates the process of generating these embeddings in pseudocode. For ease of notation we assume that triples are accessible through N r (i), which contains the neighbours connected to an entity v i through a relation r ? R. We also assume that the relations in R are encoded as integers starting at one. Apart from the KG's characterisation, our method takes an embedding size e, a seed s, and a number of layers n as input. Based on the seed s, random initial embeddings are sampled from a normal distribution with variance chosen such that the sum of a node's embedding is a standard normal random variable. We chose this initialisation strategy over Glorot as the range of Glorot-initialised matrices gets smaller with both the number of rows and columns. This makes sense for transformation matrices, but not for embeddings. The seed s is then reused to generate a list of additional random seeds, one for every transformation matrix W i . This list of seeds is given as an argument to the rrgcn-conv function, listed in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Putting it all Together</head><formula xml:id="formula_4">Given a KG G = (V, E, R) with entities v i ? V, edges (v i , r, v j ) ? E</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: rr-gcn Message Passing</head><p>Inputs: list of seeds S, embedding size e, node representations H Output: Matrix ? R |V|?e 1 Function rrgcn-conv(S, e, H):</p><formula xml:id="formula_5">2 X ? Oe?e 3 foreach r ? R do 4 Wr ? random-uniform(e ? e, Sr) 5 xi ? hi + j?Nr (i) 1 |Nr (i)| Wrh (l) j 6 W0 ? glorot-uniform(e ? e, S0) 7 xi ? hi + W0h (l) i 8 return X</formula><p>The rrgcn-conv function implements the r-gcn's message passing equation (see Equation 2) with optimised memory usage. Relation-specific (and selfloop) contributions are iteratively added to the zero-initialised (O e?e ) output representation matrix X, generating the required transformations on-the-fly from the given seeds. Note that x i and h i stand for the i th row of X and H respectively, and that the computations for these rows are done all at once using efficient matrix multiplications; we only use the row-wise notation for clarity.</p><p>After the first message passing round, the obtained representations are passed to a ReLU non-linearity ( + ) and stored in H. These hidden representations are then used to calculate the one-hop ppv features P as described in Section 4.2. The ReLU activation function is not used for ppv representations as they are positive by definition and the result of a non-linear operation.</p><p>Using the updated hidden node features, the previous steps are repeated n ? 1 times. There are two noteworthy details here: (i) we use the same seeds for every convolution, and (ii) we convolute the ppv features independently from the regular representations (but with the same seeds). In our experiments, we noticed that using different transforms in every layer was not necessary -and sometimes even hurt performance -for rr-gcns.</p><p>After the random message passing layers, the concatenation ( ) of H and P along the horizontal axis yields the final node embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Memory Usage</head><p>Trained r-gcns need to store their parameters in (GPU) memory. Since relationspecific transformations are typically quite small, this is dominated by the initial node representations. Especially when using one-hot encodings as initial representations, this memory load can become quite significant. Additionally, in the backward pass, gradients for all weights (and for optimisers such as Adam <ref type="bibr" target="#b30">[31]</ref>, even gradient moments) have to be stored as well, which at least doubles the parameter memory requirements. For a KG with nodes V, an r-gcn with B bases and an initial representation size e thus needs to be able to keep at least two B ? |V| ? e-dimensional float-tensors (4 bytes per element) in memory. For DBLP (a dataset with 4,470,778 nodes, <ref type="table">Table 2</ref>) and a 40-base r-gcn with a 16-dimensional embedding size this results in at least 22.89GB of memory.</p><p>An even bigger source of memory usage for trained r-gcns, is the storage of intermediate activations. Even if per-relation contributions are iteratively summed to an accumulator as in Algorithm 2, by default, all individual contributions accross all layers are still kept in memory during training, as they are required for backpropagation. This results in a |V| ? e l -dimensional float-tensor for every relation r ? R and the layer's global output, for every layer l, with e l the output dimensionality for layer l. For DBLP (136 relations with inverses) and a single r-gcn layer with a 16-dimensional output, e.g., this results in 39.20GB. Activation checkpointing <ref type="bibr" target="#b31">[32]</ref> could be used to avoid storing intermediate activations by recalculating them during the backward pass, trading in memory for compute, but no current implementations use this approach.</p><p>Since rr-gcns do not need a backward pass, we do not need to keep any gradients or intermediate activations in memory. Moreover, because the initial node embeddings and transformation matrices are random, we also do not keep them in memory; storing the seeds is sufficient to recreate the necessary tensors when they are needed. The peak memory usage for an rr-gcn with embedding size e is dominated by (1) the previous-layer node representations, (2) the previous-layer ppv features, (3) the accumulator matrix X (Algorithm 2) and (4) the intermediate results for a single relation type; four |V| ? e-dimensional float-matrices, which is comparable to the memory complexity of an r-gcn's forward pass during inference. rr-gcns clearly need much less memory than their trained counterparts for a given embedding size. However, as these embeddings result from combinations of randomly transformed random representations, to capture useful information, a larger embedding size is needed compared to rgcns, which are trained to select only the useful features for the downstream task. As such, rr-gcns are not necessarily more memory efficient than r-gcns, depending on how many random features are necessary for the downstream task. For DBLP and an r-gcn with embedding size 512, e.g., the resulting peak memory usage is 36.62GB. It is important to note that, once trained, r-gcn inference actually requires less memory because of the more compact representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, we present empirical results that show how our rr-gcn method matches -and in some cases exceeds -the performance of end-to-end trained networks for the two main KG machine learning tasks: node classification and link prediction, which we first briefly introduce along with the datasets used for evaluation. We conclude the section with a visual exploration of the resulting embedding spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Node Classification</head><p>For node classification, we are provided with two subsets of vertices V tr ? V and V te ? V with corresponding labels y tr and y te . The goal of this task is to construct a model or hypothesis h(.) based on V tr , y tr , and the graph's edges E that minimises a loss function L(.) and generalises well to the unseen labeled vertices in V te : arg min h L(y te , h Vtr,ytr (V te )) (</p><p>A first collection of datasets we use to compare rr-gcns and r-gcns are those published by Ristoski et al. <ref type="bibr" target="#b32">[33]</ref>. These four datasets have varying sizes, but most of them are of a rather small scale. We include them because they were used to evaluate node classification in the original r-gcn paper <ref type="bibr" target="#b14">[15]</ref>. <ref type="table">Table 1</ref> summarises some important properties of the four datasets.</p><p>In addition, we compare both techniques on "kgbench" <ref type="bibr" target="#b33">[34]</ref>, which contains five larger scale KGs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Link Prediction</head><p>The goal of link prediction is to infer true triples that are not yet captured in a KG. To that end, the KG's known triples are split in two subsets E tr ? E and E te ? E and a model or hypothesis h(.) is constructed. This hypothesis, or "scoring function" is trained to assign a high output to the true edges E tr ? E and a low output for corrupted, false edges E tr,c ? E. The trained hypothesis should then be able to recover the withheld edges E te by assigning them high scores, and low scores to all false edges, i.e., it should minimise a loss function L(.):</p><formula xml:id="formula_7">arg min h L(h Etr (E te ), h Etr (E tr ? E te ))<label>(6)</label></formula><p>We evaluate our rr-gcn's link prediction performance only on the FB15k-237 dataset, as the other datasets used for evaluation in the original r-gcn work <ref type="bibr" target="#b14">[15]</ref> have all since been found to contain significant leakage <ref type="bibr" target="#b10">[11]</ref>. <ref type="table" target="#tab_2">Table 3</ref> summarises some important properties of this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation</head><p>We implemented our rr-gcn layer and embedder in PyG <ref type="bibr" target="#b34">[35]</ref>, an extension of the popular deep learning framework PyTorch <ref type="bibr" target="#b35">[36]</ref> that facilitates the implementation of message passing networks. PyG provides parallel execution of node representation updates. In the remainder of this section, we discuss the evaluation setup for the node classification and link prediction tasks, and introduce our methodology for a rudimentary qualitatitve analysis of the embeddings.</p><p>Node Classification A first step in our evaluation procedure is to reduce the size of the KG by excluding vertices that are further than n hops away from any of the training (V tr ) or testing (V te ) vertices, as no information from more than n hops away can be propagated to the nodes of interest with n message passing layers. Once the size of the KG is reduced, we apply n layers of our rr-gcn to create embeddings of size e. The constants n and e are tuneable hyper-parameters. Once the embeddings are generated, they are provided as input to a gradient boosting classifier. In this study, we used CatBoost <ref type="bibr" target="#b36">[37]</ref>.</p><p>For the small-scale benchmark KGs, the hyper-parameters n and e were tuned using a grid search with stratified 5-fold cross-validation, with a different rr-gcn seed in every fold. For the larger-scale KGs, we evaluated an rr-gcn five times with different seeds on the provided validation set to tune the hyperparameters. We tuned n to take a value in {1, 2, 3, 4, 5} and e to take a value in {256, 384, 512, 768, 1024} and chose optimal values based on log-loss, ignoring configurations that resulted in more than 24GB of GPU memory (which corresponds to the current most high-end consumer GPU, the RTX 3090). An important hyperparameter for CatBoost is the number of boosting iterations. During validation, we determined this quantity using "early stopping". For evaluation runs on a given dataset's test set, we chose the maximum number of iterations required for that dataset's validation runs with optimal hyperparameters. We reproduced r-gcn results from Schlichtkrull et al. <ref type="bibr" target="#b14">[15]</ref> using an external implementation <ref type="bibr" target="#b29">[30]</ref>, as the original code uses deprecated libraries. For the small-scale KGs, we used the hyper-parameter configuration reported in their study <ref type="bibr" target="#b14">[15]</ref>, but we additionally used early stopping with 10 epochs patience using a validation set held out from the training data to determine the optimal number of layers embedding size   <ref type="table">Table 5</ref>. The average accuracy and standard error of our 10 measurements. number of epochs. For the larger-scale KGs, we use the hyper-parameters as reported in the "kgbench" paper <ref type="bibr" target="#b33">[34]</ref>. All "kgbench" measurements were performed on CPU, to accommodate the high memory requirements for backpropagation. Only for the DBLP dataset, for which no results were reported in the original paper due to high memory requirements, we had to use an embedding size of 10 (as opposed to 16) with 40 base functions to make it fit in our 64GB of CPU RAM. We used an early stopping mechanism, with 10 epochs patience, using the provided validation sets.</p><p>We evaluated the r-gcn and rr-gcn setups ten times with different seeds and measured the according test accuracies. We report results for both the rrgcn as described in Algorithm 1 (rr-gcn-ppv) and a version that does not include the ppv features (rr-gcn). The mean accuracy results, and their corresponding standard errors are provided in <ref type="table">Table 5</ref>.</p><p>Link Prediction As in the original work on r-gcns <ref type="bibr" target="#b14">[15]</ref>, we generated false triples using "negative sampling", where the head or tail of each true triple in  <ref type="table">Table 6</ref>. Link prediction results for FB15k-237 of r-gcn and our proposed rr-gcn.</p><p>E tr is randomly corrupted, i.e., replaced by another entitiy. We trained a scoring function (or "decoder") using the binary cross-entropy loss to score true triples higher than negatives. Whereas r-gcns use the bilinear DistMult decoder <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10]</ref> for link prediction, we preceded DistMult by a small three-layer neural network with hidden and output size 2048, to allow the decoder to transform our taskagnostic embeddings into a space in which the bilinearity holds. As trained r-gcns require much larger hidden sizes for link prediction than for node classification (500 instead of 16 in the original work), our random unsupervised embeddings need to be very large to capture a similar amount of information. As such, we used an rr-gcn-ppv with embedding size 32,000. Using principal component analysis (after feature normalisation), we reduced the representation dimensionality to <ref type="bibr" target="#b7">8,</ref><ref type="bibr">192</ref> before feeding them to a three-layer preprocessing neural network that was trained jointly with the 2048-dimensional DistMult decoder. We determined the number of epochs to train the decoder using the provided validation set.</p><p>Since we used the same final decoding assumption, and training r-gcns for link prediction can take several days <ref type="bibr" target="#b29">[30]</ref>, for the end-to-end model, we reused the results reported in <ref type="bibr" target="#b14">[15]</ref>. The filtered mean reciprocal rank (FMRR) and the hits at 1, 3 and 10 are listed <ref type="table">Table 6</ref>.</p><p>Qualitative Evaluation We perform a qualitative evaluation of our produced rr-gcn embeddings by visualising rr-gcn representations for countries in a small subset of DBpedia <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> to gauge if the resulting embedding space makes semantic sense. <ref type="figure" target="#fig_2">Figure 1</ref> shows a plot of t-SNE-transformed <ref type="bibr" target="#b39">[40]</ref> embeddings generated by a 5-layer rr-gcn-ppv with embedding size 512. <ref type="table">Table 5</ref> shows that, for most small-scale datasets, r-gcns score better. However, the differences are not that drastic considering one method is trained end-toend and another is unsupervised and based on random transformations. ppv features, which measure the "representation diversity" around every node, seem to have a positive impact for most datasets. With ppv, our random rr-gcns even score significantly better for the MUTAG dataset. Only for BGS, ppv seemingly detoriates performance. However, the difference between rr-gcn-ppv and rrgcn is statistically insignificant (Mann-Whitney test with cutoff 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>For the larger datasets, the average performance of r-gcns and rr-gcns is even more similar, with most datasets statistically significantly favouring rr-  gcn-ppv over a trained model. The difference for MDGENRE, the only "kgbench" dataset where r-gcns score better on average, is statistically insignificant.</p><p>The KGs on which rr-gcn performs significantly worse than r-gcn often contain many low-degree nodes (including literals, which usually have only one neighbour) close to the nodes to be classified. We hypothesise that these lowdegree nodes add noise to our representations, which can overpower the useful signal in some datasets. As r-gcns can learn initial node embeddings and per relation transformations, they can learn to ignore unnecessary information, or at least make the useful signal more influential in the final representations. To test this hypothesis, we apply r-gcn and rr-gcn-ppv to filtered versions of AIFB, BGS and AM. In these filtered KGs, low-degree nodes (degree ? 5) are removed. <ref type="table" target="#tab_5">Table 7</ref>, removing low-degree nodes seems to close the gap in average performance for most datasets: the performance for r-gcn stays roughly the same, while rr-gcn results improve. The performance differences to the trained counterparts for AIFB and BGS even become statistically insignificant. For AM, however, low-degree node removal seems to have little to no effect. As we suspect that, for this dataset, the r-gcn learns to ignore a subset of relation types, we inspect the mean absolute values of a trained r-gcn's learned per-relation transformation matrices (true and added inverse relation types are averaged), averaged over all layers. Indeed, <ref type="figure" target="#fig_3">Figure 2</ref> illustrates that there are quite a few relation types that a trained r-gcn learns to attenuate. If we take this "relation importance" information into account by removing (in addition to low-degree nodes) all relation types that have less than 60% of the maximum "importance", our rr-gcn obtains an accuracy of 91.31 ? 0.24%, which is even statistically significantly better than trained r-gcns. <ref type="table">Table 6</ref> illustrates that link prediction performance for random and trained r-gcns is similar across all evaluation metrics. Which indicates that, even when r-gcns need large hidden sizes to perform downstream tasks (500 in this case), random message passing can capture a comparable amount of signal, without supervision. As only the decoder needs to be trained, the rr-gcn encoder needs to run only once. This results in a significant speedup compared to end-to-end trained r-gcns, which have to perform both a forward and a backward pass trough the entire network for every training step. Consequently, the rr-gcn link prediction model trains in just under two hours, while r-gcns for this task can take days to converge <ref type="bibr" target="#b29">[30]</ref>. <ref type="figure" target="#fig_2">Figure 1</ref> shows that rr-gcn embeddings preserve semantic relationships between entities: there is a quite clear separation between continents, neighbouring countries are generally close to each other, and some countries with intertwined geopolitical histories even form small clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Inspired by the success of random non-linear transformations in the time series domain, we set out to evaluate random convolutions (rr-gcn) on KGs. In this exploratory study, empirical results show that random transformations usually match or exceed the performance of end-to-end trained r-gcns. Our experiments indicate that a KG's structure alone, which is the only thing our rr-gcns can capture, contains enough semantic information as is, and that when r-gcns do perform better, they mostly learn to ignore certain parts of that structure.</p><p>Aside from surprisingly good performance, rr-gcns exhibit other interesting properties: (i) embeddings can easily be generated for a small subset of the graph for testing purposes (whereas r-gcns need to process the entire graph to train their weights), and (ii), as training through backpropagation is not necessary, they can be used to test the influence of new e.g. aggregation functions (such as ppv) before considering differentiable variants. This opens up many new interesting avenues for further research, both in improving the rr-gcns themselves and KG embedding in general, which we discuss subsequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future Work</head><p>Apart from being used as a baseline that any trained method should be able to beat, we envision that rr-gcns might be helpful as a tool for improving and/or developing other embedding techniques. By further investigating in which situations or for which substructures random information aggregations are insufficient, trained algorithms could be altered to better target problematic computations. Hybrid networks with both trained and random weights could offer the best of both worlds.</p><p>In this work we explored information aggregation using random transformation matrices. Other strategies, e.g. the integration of algebraic embedding priors into message passing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>, could be randomised as well.</p><p>As it gives a measure of the representation diversity around a node and thus captures more of the KG's structure, ppv seems like promising addition to the random message passing toolbox. However, the independent computation of ppv and regular features is somewhat arbitrary. Future work could research other options such as calculating ppv features only after the last message passing layer, or even outright replacing the averaging used in r-gcns and using ppv as the aggregation. Moreover, differentiable alternatives to ppv could be researched to improve trained r-gcns. Future research might reveal new features that capture other aspects of graph, to further improve performance.</p><p>Even though for some datasets (e.g. DBLP), rr-gcns require less memory than their trained counterparts for comparable performance, the memory requirements are still substantial for graphs with many nodes. Future research could trade in some of these memory requirements for compute by e.g. aggregating many small node embeddings.</p><p>Supplemental Material Statement: Source code for the rr-gcn embedder and all experiments is attached with the submission on EasyChair and, if accepted, will be published on GitHub.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 H 3 S 4 H 5 P 6 repeat n -1 times 7 H 8 P 9 P</head><label>23456789</label><figDesc>? normal(|V| ? e, s, ? 2 = 1 e ) ? random-list(s, |R| + 1) ? (rrgcn-conv(S, e, H)) + ? ppv(H, N ) ? (rrgcn-conv(S, e, H)) + ? rrgcn-conv(S, e, P ) ? ppv(P, N ) 10 return [h p]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>t-SNE representation of the embeddings for countries in a subset of DBpedia produced by rr-gcn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Mean absolute weights for per-relation transformations learned by a two-layer r-gcn for the AM dataset, relative to the maximum per-relation mean absolute weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and relation types r ? R, we construct Euclidean vectors for every entity. These vectors should encapsulate each entity's semantics, as contained in the triples of the KG, such that they can be used as features for machine learning models. NoteAlgorithm 1: Generating node embeddings using rr-gcn layers Inputs: relations R, per-relation node neighbours N , embedding size e, number of layers n, seed s, nodes V Output: Matrix ? R |V|?e 1 Function embed-rrgcn-ppv(e, s, n):</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Table 1 .Table 2 .</head><label>212</label><figDesc>summarises some important properties of the five Dataset statistics for the small-scale KGs from Ristoski et al. Dataset statistics for the larger-scale "kgbench" KGs datasets. As opposed to the smaller-scale benchmark KGs, these contain a large number of training and testing entities. Moreover, separate validation sets are provided for hyper-parameter tuning.</figDesc><table><row><cell></cell><cell>Statistic</cell><cell>AIFB</cell><cell>MUTAG</cell><cell>BGS</cell><cell>AM</cell></row><row><cell></cell><cell>Entities</cell><cell>8,285</cell><cell>23,644</cell><cell>333,845</cell><cell cols="2">1,666,764</cell></row><row><cell></cell><cell>Relations</cell><cell>45</cell><cell>23</cell><cell>103</cell><cell>133</cell></row><row><cell></cell><cell>Edges</cell><cell>29,043</cell><cell>74,227</cell><cell>916,199</cell><cell cols="2">5,988,321</cell></row><row><cell></cell><cell>Train Entities</cell><cell>141</cell><cell>272</cell><cell>117</cell><cell>802</cell></row><row><cell></cell><cell>Val. Entities</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>Test Entities</cell><cell>37</cell><cell>68</cell><cell>29</cell><cell>198</cell></row><row><cell></cell><cell>Classes</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>11</cell></row><row><cell></cell><cell>Mean Degree</cell><cell>7.82</cell><cell>6.27</cell><cell>11.89</cell><cell>13.74</cell></row><row><cell></cell><cell>Max. Degree</cell><cell>1,281</cell><cell>6,783</cell><cell>83,024</cell><cell>73,447</cell></row><row><cell>Statistic</cell><cell cols="6">AMPLUS DMG777K DMGFULL MDGENRE DBLP</cell></row><row><cell>Entities</cell><cell>1,153,679</cell><cell>341,270</cell><cell>842,550</cell><cell cols="2">349,344</cell><cell>4,470,778</cell></row><row><cell>Relations</cell><cell>33</cell><cell>60</cell><cell>62</cell><cell>154</cell><cell></cell><cell>68</cell></row><row><cell>Edges</cell><cell>2,521,046</cell><cell>777,124</cell><cell>1,850,451</cell><cell cols="2">1,252,247</cell><cell>21,985,048</cell></row><row><cell cols="2">Train Entities 13,423</cell><cell>5,394</cell><cell>23,566</cell><cell>3,846</cell><cell></cell><cell>26,535</cell></row><row><cell cols="2">Val. Entities 20,000</cell><cell>1,001</cell><cell>10,001</cell><cell>1,006</cell><cell></cell><cell>10,000</cell></row><row><cell cols="2">Test Entities 20,000</cell><cell>2,001</cell><cell>20,001</cell><cell>3,005</cell><cell></cell><cell>20,000</cell></row><row><cell>Classes</cell><cell>8</cell><cell>5</cell><cell>14</cell><cell>12</cell><cell></cell><cell>2</cell></row><row><cell cols="2">Mean Degree 4.37</cell><cell>4.53</cell><cell>4.47</cell><cell>7.17</cell><cell></cell><cell>9.83</cell></row><row><cell cols="2">Max. Degree 154,828</cell><cell>65,576</cell><cell>121,217</cell><cell>57,363</cell><cell></cell><cell>3,364,084</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Dataset statistics for FB15k-237</figDesc><table><row><cell>Statistic</cell><cell>FB15k-237</cell></row><row><cell>Entities</cell><cell>14,541</cell></row><row><cell>Relations</cell><cell>237</cell></row><row><cell cols="2">Train Triples 272,115</cell></row><row><cell cols="2">Val. Triples 17,535</cell></row><row><cell cols="2">Test Triples 20,466</cell></row><row><cell cols="2">Mean Degree 37.52</cell></row><row><cell cols="2">Max. Degree 7614</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Optimal hyperparameters per dataset, for rr-gcns that use ppv features and rr-gcns that leave them out ? 0.45 83.33 ? 1.37 86.11 ? 0.93 AM 88.99 ? 0.39 81.67 ? 0.57 84.65 ? 0.62 BGS 86.21 ? 0.89 80.00 ? 2.34 78.97 ? 2.44 MUTAG 72.50 ? 0.91 70.00 ? 0.83 79.41 ? 0.58 AMPLUS 83.81 ? 0.13 76.85 ? 0.06 84.54 ? 0.08 DBLP 68.51 ? 0.99 70.18 ? 0.11 70.61 ? 0.07 DMG777K 62.51 ? 0.38 61.40 ? 0.32 63.97 ? 0.26 DMGFULL 57.52 ? 0.19 60.50 ? 0.26 63.38 ? 0.17 MDGENRE 67.33 ? 0.19 65.09 ? 0.10 67.15 ? 0.08</figDesc><table><row><cell>Dataset</cell><cell>r-gcn</cell><cell>rr-gcn</cell><cell>rr-gcn-ppv</cell></row><row><cell>AIFB</cell><cell>96.11</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>? 0.45 86.21 ? 0.89 88.13 ? 0.41 rr-gcn-ppv-cut 95.83 ? 0.62 84.14 ? 1.38 84.80 ? 0.23 The average accuracies and standard error of our 10 measurements with degree cutting.</figDesc><table><row><cell>Model</cell><cell>AIFB</cell><cell>BGS</cell><cell>AM</cell></row><row><cell>r-gcn</cell><cell cols="3">96.11 ? 0.45 86.21 ? 0.89 88.99 ? 0.39</cell></row><row><cell>rr-gcn-ppv</cell><cell cols="3">86.11 ? 0.93 78.97 ? 2.44 84.65 ? 0.62</cell></row><row><cell>r-gcn-cut</cell><cell>95.56</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic web in data mining and knowledge discovery: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Knowledge Graphs for Explainable Artificial Intelligence: Foundations, Applications and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Palmonari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IOS Press</publisher>
			<biblScope unit="page" from="49" to="72" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note>Knowledge graph embeddings and explainable ai</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Ink: knowledge graph embeddings for node classification. Data Mining and Knowledge Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Steenwinckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Vandewiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Weyns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terencio</forename><surname>Agozzino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><forename type="middle">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Femke</forename><surname>Ongenae</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>page in production</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mindwalc: mining interpretable, discriminative walks for classification of nodes in a knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Vandewiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Steenwinckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><forename type="middle">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Femke</forename><surname>Ongenae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Informatics and Decision Making</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast approximation of the weisfeiler-lehman graph kernel for rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerben Kd De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="606" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10281</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rdf2vec: Rdf graph embeddings for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="498" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengchen</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Janowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transgcn</surname></persName>
		</author>
		<title level="m">Proceedings of the 10th International Conference on Knowledge Capture</title>
		<meeting>the 10th International Conference on Knowledge Capture</meeting>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01195</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Composition-based multi-relational graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03082</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rocket: exceptionally fast and accurate time series classification using random convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1454" to="1495" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Minirocket: A very fast (almost) deterministic transform for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="248" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An overview of reservoir computing: Theory, applications and implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Campenhout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-01" />
			<biblScope unit="page" from="471" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extremely randomized trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">How powerful are graph convolutional networks?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning through structure: towards deep neuromorphic knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Victor Caceres Chian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Runkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The surprising power of graph neural networks with random node initialization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?smaililkan</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Random features strengthen graph neural networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Nodepiece: Compositional and parameter-efficient representations of large knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno>abs/2106.12144</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Relational graph convolutional networks: A closer look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiviyan</forename><surname>Thanapalasingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lucas Van Berkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>arxiv:1412.6980Comment</idno>
	</analytic>
	<monogr>
		<title level="m">the 3rd International Conference for Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Published as a conference paper at</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A collection of benchmark datasets for systematic evaluations of machine learning on the semantic web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerben Klaas Dirk De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="186" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">kgbench: A collection of knowledge graph datasets for evaluating relational and multimodal machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xander</forename><surname>Wilcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucas Van Berkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with Py-Torch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Catboost: gradient boosting with categorical features support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Veronika Dorogush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasily</forename><surname>Ershov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Gulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11363</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dbpedia -a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Patrick Van Kleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Vandewiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Steenwinckel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>dbpedia] country information</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
