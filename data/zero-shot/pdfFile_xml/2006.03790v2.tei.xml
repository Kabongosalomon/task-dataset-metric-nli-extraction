<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals Measurement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research AI</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Fromm</surname></persName>
							<email>jwfromm@octoml.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shwetak</forename><surname>Patel</surname></persName>
							<email>shwetak@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research AI</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
							<email>damcduff@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">OctoML</orgName>
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals Measurement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Telehealth and remote health monitoring have become increasingly important during the SARS-CoV-2 pandemic and it is widely expected that this will have a lasting impact on healthcare practices. These tools can help reduce the risk of exposing patients and medical staff to infection, make healthcare services more accessible, and allow providers to see more patients. However, objective measurement of vital signs is challenging without direct contact with a patient. We present a videobased and on-device optical cardiopulmonary vital sign measurement approach. It leverages a novel multi-task temporal shift convolutional attention network (MTTS-CAN) and enables real-time cardiovascular and respiratory measurements on mobile platforms. We evaluate our system on an Advanced RISC Machine (ARM) CPU and achieve state-of-the-art accuracy while running at over 150 frames per second which enables real-time applications. Systematic experimentation on large benchmark datasets reveals that our approach leads to substantial (20%-50%) reductions in error and generalizes well across datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The SARS-CoV-2 (COVID-19) pandemic is transforming the face of healthcare around the world <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. One example of this is the sharp increase (by more than 10x) in the number of medical appointments held via telehealth platforms because of the increased pressures on healthcare systems, the desire to protect healthcare workers and restrictions on travel <ref type="bibr" target="#b1">[2]</ref>. Telehealth includes the use of telecommunication tools, such as phone calls and messaging, and online health portals that allow patients to communicate with their providers. The Center for Disease Control and Prevention is recommending the "use of telehealth strategies when feasible to provide high-quality patient care and reduce the risk of COVID-19 transmission in healthcare settings" <ref type="bibr" target="#b0">1</ref> . Performing primary care visits from a patient's home reduces the risk of exposing people to infections, increases the efficiency of visits and facilitates care for people in remote locations or who are unable to travel. These are longstanding arguments for telehealth and will still be valid after the end of the current pandemic. Healthcare systems are likely to maintain a high number of telehealth appointments in the future <ref type="bibr" target="#b2">[3]</ref>.</p><p>However, despite the longstanding promise of telehealth, it is difficult to provide a similar level of care on a video call as during an in-person visit. The physician can diagnose a patient based on visual observations and self-reported symptoms; however, in most cases they cannot objectively assess the patient's physiological state. This means that physicians have to make decisions (e.g., recommending a trip to the emergency department) without important data. In the case of COVID-19, there are severe cardiopulmonary (heart and lung related) symptoms that are difficult to evaluate remotely. The symptoms seen in patients have drawn links to acute respiratory distress syndrome <ref type="bibr" target="#b3">[4]</ref>, myocardial injury, and chronic damage to the cardiovascular system. Experts suggest that particular attention should be given to cardiovascular protection during treatment <ref type="bibr" target="#b4">[5]</ref>. The development of more accurate and efficient non-contact cardiopulmonary measurement technology would give remote physicians access to the data to make more informed decisions. Beyond telehealth, the same technology could impact passive health monitoring, improving the standard of care for infants in neonatal intensive care units <ref type="bibr" target="#b5">[6]</ref>.</p><p>Cameras can be used to measure physiological signals, including heart and respiration rates, and blood oxygenation levels <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, based on facial videos <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Non-contact cardiopulmonary measurement involves capturing subtle changes in light reflected from the body caused by physiological processes. Imaging methods can be used to measure volumetric changes of blood in the surface of the skin cause changes in light absorption (? volume of hemoglobin = ? light absorption). This in turns affects the amount of visible light reflected from the skin, which is the source of the photoplethysmogram (PPG). The mechanical force of blood pumping around the body also causes subtle motions and these are the source of the ballistocardiogram (BCG). These color and motion changes in the video help us extract the pulse signal and heart rate frequency. The PPG and BCG signals provide complementary information to one another and also contain information about breathing due to respiratory sinus arrhythmia <ref type="bibr" target="#b11">[12]</ref>. Respiratory signals can also be recovered from motion-based analyses of the head and torso as the subjects breathes in and out <ref type="bibr" target="#b12">[13]</ref>.</p><p>Computer vision for remote cardiopulmonary measurement is a growing field; however, there is room for improvement in the existing methods. First, accuracy of measurements is critical to avoid false alarms or misdiagnoses. The US Federal Drug Administration (FDA) mandates that testing of a new device for cardiac monitoring should show "substantial equivalence" in accuracy with a legal predicate device (e.g., a contact sensor) <ref type="bibr" target="#b1">2</ref> . This standard has not been obtained in non-contact approaches. Second, designing models that run on-device helps reduce the need for high-bandwidth Internet connections making telehealth more practical and accessible. Moreover, camera-based cardiopulmonary measurement is a highly privacy sensitive application. These data are personally identifiable, combining videos of a patient's face with sensitive physiological signals. Therefore, streaming and uploading data to the cloud to perform analysis is not ideal. Finally, the ability to run at a high frame rates enables opportunistic sensing (e.g., obtaining measurements each time you look at your phone) and helps capture waveform dynamics that could be used to detect arterial fibrillation <ref type="bibr" target="#b13">[14]</ref>, hypertension <ref type="bibr" target="#b14">[15]</ref>, and heart rate variability <ref type="bibr" target="#b15">[16]</ref> where high-frame rates (at least 100Hz) are a requirement to yield precise measurements.</p><p>We propose a novel multi-task temporal shift convolutional attention network (MTTS-CAN) to address the challenges of privacy, portability, and precision in contactless cardiopulmonary measurement. Our end-to-end MTTS-CAN leverages temporal shift modules to perform efficient temporal modeling and remove various sources of noise without any additional computational overhead. An attention module improves signal source separation, and a multi-task mechanism shares the intermediate representations between pulse and respiration to jointly estimate both simultaneously. By combining these three techniques, our proposed network can run on an ARM CPU and achieve the state-of-the-art accuracy and inference speed.</p><p>To summarize, the contributions of this paper are to 1) present an accurate and efficient approach to perform on-device real-time spatial-temporal modeling of vitals signal, 2) evaluate our system and show state-of-the-art performance on two large public datasets, 3) provide an implementation of core tensor operations required for MTTS-CAN using a modern deep learning compiler and an on-device latency evaluation across different architectures showing MTTS-CAN can run at more than 150 frame per second. Our code, models, and video figures are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Camera-based Physiological Measurement: Early work established that the blood volume pulse can be extracted by analysing skin pixel intensity changes over time <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. These methods are grounded by optical models (e.g., the Lambert-Beer law (LBL) and Shafer's dichromatic reflection Starting from previous work that presented a 2D-CAN <ref type="bibr" target="#b30">[31]</ref>, we introduce a fully 3D-CAN, a 2D-3D Hybrid CAN in which the appearance branch takes a single frame, and our proposed temporal shift CAN. Each of these models can be applied in a single or multi-task manner.</p><p>model (DRM)) that provide a framework for modeling how light interacts with the skin. However, traditional signal processing techniques are quite sensitive to noise from other sources in video data, including head motions and illumination changes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. To help address these issues, some approaches incorporate prior knowledge about the physical properties of the patient's skin <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Although effective, these handcrafted signal processing pipelines make it difficult to capture the complexity of the spatial and temporal dynamics of physiological signals in video. Neural network based approaches have been successfully applied using the BVP or respiration as the target signal <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, but these methods still struggle with effectively combining spatial and temporal information while maintaining a low computational budget. More recently, researchers have investigated on-device remote camera-based heart rate variability measurement using facial videos from smartphone cameras <ref type="bibr" target="#b21">[22]</ref>. However, their proposed architecture requires approximately 200ms per frame inference, which is insufficient for real-time performance, and was not evaluated on public datasets.</p><p>Efficient Temporal Models: Yu et al. <ref type="bibr" target="#b19">[20]</ref> have shown that applying 3D convolutional neural networks (CNNs) significantly improves performance and achieves better accuracy compared to using a combination of 2D CNNs and recurrent neural networks. The benefit of 3D CNNs implies that incorporating temporal data in all layers of the model is necessary for high accuracy. However, direct temporal modeling with 3D CNNs requires dramatically more compute and parameters than 2D based models. In addition to reducing computational cost, there are several reasons that it is highly desirable to be able to have efficient non-contact physiological measurement models that run on-device. Temporal Shift Modules <ref type="bibr" target="#b22">[23]</ref> provide a clever mechanism that can be used to replace 3D CNNs without reducing accuracy and requiring only the computational budget of a 2D CNN. This is achieved by shifting the tensor along the temporal dimension, facilitating information exchange across multiple frames. TSM has been evaluated on the tasks of video recognition and video object detection and achieved superior performance in both latency and accuracy. Xiao et al. <ref type="bibr" target="#b23">[24]</ref> used pretrained TSM-based residual networks as a backbone followed by two attention modules for reasoning about human-object interactions. The differences between this aforementioned work and ours is they applied attention modules as the head followed by pretrained TSM-based residual feature maps while our work applies two attention modules to the intermediate feature maps generated from regular 2D CNNs with TSM.</p><p>Machine Learning and COVID-19: Researchers have explored the use of machine learning from various perspectives to help combat COVID-19 <ref type="bibr" target="#b24">[25]</ref>. Recent studies have shown that applying convolutional neural networks to CT scans can help extract meaningful radiological features for COVID-19 diagnosis and facilitate automatic pulmonary CT screening as well as cough monitoring <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Researchers have also looked at the correlation between resting heart rate generated from wearable sensors and COVID-19 related symptoms and behaviors at population scale <ref type="bibr" target="#b29">[30]</ref>.</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Optical Model</head><p>For our optical basis we start with Shafer's Dichromatic Reflection Model (DRM), as in prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9]</ref>. Specifically, we aim to capture both spatial and temporal changes and the relationship 1 <ref type="figure">Figure 2</ref>: We present a multi-task temporal shift convolutional attention network for camera-based physiological measurement.</p><p>between multiple physiological processes. Let us start with the RGB values captured by the cameras as given by:</p><formula xml:id="formula_0">C C C k (t) = I(t) ? (v v v s (t) + v v v d (t)) + v v v n (t)<label>(1)</label></formula><p>where I(t) is the luminance intensity level, modulated by the specular reflection v v v s (t) and the diffuse reflection v v v d (t). The quantization noise of the camera sensor is captured by v v v n (t). Following <ref type="bibr" target="#b17">[18]</ref> we can decompose</p><formula xml:id="formula_1">I(t), v v v s (t) and v v v d (t) into stationary and time-dependent parts: v v v d (t) = u u u d ? d 0 + u u u p ? p(t)<label>(2)</label></formula><p>where u u u d is the unit color vector of the skin-tissue; d 0 is the stationary reflection strength; u u u p is the relative pulsatile strengths caused by hemoglobin and melanin absorption;</p><formula xml:id="formula_2">p(t) represents the physiological changes. v v v s (t) = u u u s ? (s 0 + ?(m(t), p(t)))<label>(3)</label></formula><p>where u u u s denotes the unit color vector of the light source spectrum; s 0 and ?(m(t), p(t)) denote the stationary and varying parts of specular reflections; m(t) denotes all the non-physiological variations such as flickering of the light source, head rotation, and facial expressions.</p><formula xml:id="formula_3">I(t) = I 0 ? (1 + ?(m(t), p(t)))<label>(4)</label></formula><p>where I 0 is the stationary part of the luminance intensity, and I 0 ? ?(m(t), p(t)) is the intensity variation observed by the camera. As in <ref type="bibr" target="#b8">[9]</ref> we can disregard products of time-varying components as they are relatively small:</p><formula xml:id="formula_4">C C C k (t) ? u u u c ? I 0 ? c 0 + u u u c ? I 0 ? c 0 ? ?(m(t), p(t))+ u u u s ? I 0 ? ?(m(t), p(t)) + u u u p ? I 0 ? p(t) + v v v n (t) (5)</formula><p>However, unlike in previous work which modeled pulse and respiration signals as independent <ref type="bibr" target="#b30">[31]</ref>, we leverage the fact that p(t) actually captures a complex combination of both pulse and respiration information. Specifically, both the specular and diffuse reflections are influenced by related physiological processes. Respiratory sinus arrhythmias (RSA) are rhythmical fluctuations in heart periods at the respiration frequency <ref type="bibr" target="#b31">[32]</ref>. Furthermore, the respiration and pulse signals both cause outward motions of the body in the form of chest and head motions. We can say that the physiological process p(t) is a complex combination of both the blood volume pulse, b(t), and the respiration wave, r(t). Thus, p(t) = ?(b(t), r(t)) and the following equation gives a more accurate representation of the underlying process:</p><formula xml:id="formula_5">C C C k (t) ? u u u c ? I 0 ? c 0 + u u u c ? I 0 ? c 0 ? ?(m(t), ?(b(t), r(t)))+ u u u s ? I 0 ? ?(m(t), ?(b(t), r(t))) + u u u p ? I 0 ? p(t) + v v v n (t) (6)</formula><p>Since b(t) and r(t) are so closely intertwined, a temporal multi-task learning approach would seem optimal for this problem and at very least could leverage redundancies between the two signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture</head><p>Efficient Spatial-Temporal Modeling: To achieve state-of-the-art performance in on-device optical cardiopulmonary measurement, an architecture should have the ability to: 1) efficiently learn spatial features that map raw RGB values to latent representations corresponding to the pulse and respiratory signals as well as temporal features that offset various sources of noise (e.g., head motion, ambient illumination changes, etc.), 2) learn the relationships between associated physiological processes, 3) work in real-time to support various telehealth deployments. Our solution is a novel temporal shift convolutional attention architecture ( <ref type="figure" target="#fig_0">Fig. 1D</ref>) which we systematically compare to its variants ( <ref type="figure" target="#fig_0">Fig.  1A</ref>-C) to illustrate its benefits.</p><p>Because of the strong performance shown in prior work <ref type="bibr" target="#b8">[9]</ref>, our architecture leverages a two-branch structure with a spatial attention module ( <ref type="figure" target="#fig_0">Fig. 1A)</ref>. One branch is used for motion modeling, and the other branch for extracting meaningful spatial (i.e., facial) features. However, it fails to capture temporal dependencies beyond consecutive frames and thus is still vulnerable to many sources of noise. Perhaps the simplest way to introduce a strong temporal dependency is a 3D-CAN that leverages 3D convolutions to model temporal relationships ( <ref type="figure" target="#fig_0">Fig. 1B)</ref> which is similar to the model used in <ref type="bibr" target="#b19">[20]</ref> but adds an attention module. However, since 3D convolutions incur quadratic computational cost compared to 2D convolutions, it is not feasible to achieve real-time on-device performance using a primarily 3D architecture. Therefore, we present a Hybrid-CAN architecture that is more computationally efficient than a purely 3D model. Hybrid-CAN combines a 2D-CAN and a 3D-CAN to maintain temporal modeling while leveraging more efficient 2D convolutions where possible.</p><p>Since spatial position changes between adjacent frames are subtle, using 3D convolutions in the appearance branch is unnecessary. As <ref type="figure" target="#fig_0">Fig. 1C</ref> illustrates, the input of the appearance branch is a single frame generated by averaging N (window size) adjacent frames. Although Hybrid-CAN reduces computational cost significantly, the computational overhead from 3D convolutions in the motion branch is still not tolerable if we want to achieve real-time inference on low-end mobile platforms (i.e., ideally at least 60 FPS).</p><p>Therefore, we introduce TS-CAN to remove the 3D convolution operations from the architecture entirely while preserving spatial-temporal modeling. TS-CAN has two major additional components: the temporal shift module (TSM) <ref type="bibr" target="#b22">[23]</ref> and the attention module. TSM performs tensor shifting before the tensor is fed into the convolutional layers as visualized in <ref type="figure">Fig.2</ref>. More specifically, TSM splits the input tensor into three chunks across the channel dimension. Then, it shifts the first chunk to the left by one place (advancing time by one frame) and shifts the second chunk to the right by one place (delaying time by one frame). Both shifting operations are along the temporal axis, and the third chunk remains unchanged. It is worth noting that tensor shifting does not add any additional parameters to the network, but does enable information exchange among neighbouring frames. We used TSM in the motion branch to mimic the effects of 3D convolution, while the appearance branch in the TS-CAN is the same as Hybrid-CAN and only takes a single averaged frame. By doing so, the model not only significantly reduces computational time by only calculating the attention mask once, but also captures most of the pixels that contain skin and reduces camera quantization error.</p><p>Attention on Temporal Shift: Given there are already many different sources of noise described in the previous section, naively shifting an input tensor in time will introduce extra temporal information to our representation. It is then important that we pay attention to the pixels with physiological signals or risk amplifying noise. Therefore, we propose inserting an attention module in TSM to minimize the negative effects introduced by tensor shifting as well as to enable the network to focus on the target signals. The spatial and temporal distribution of physiological signals are not uniform on human skin. Soft-attention masks can assign higher weights to certain shifted pixels with stronger signals in intermediate representations from the convolutional operations. More concretely, our attention modules are the bridges between the appearance branch and the motion branch (See <ref type="figure">Fig. 2</ref>). Softmax attention masks are generated via 1 ? 1 convolutions before pooling layers. The attention mask is calculated as in <ref type="bibr">Equation 7</ref> where k is the index of a layer, ? k is the 1 ? 1 convolution and followed by a sigmoid activate function ?(?). l 1 normalization was applied to soften the extreme values in the mask to make sure the network avoided pixel anomalies. Finally, we perform an element-wise product to the corresponding representation X k from the motion branch.</p><formula xml:id="formula_6">X k H k W k ? ?(? k X k ? + b k ) 2 ?(? k X k ? + b k ) 1<label>(7)</label></formula><p>Multi-Task TS-CAN: We now have an efficient on-device architecture to predict physiological signals in real-time. However, we still have two independent networks, one for estimating the blood volume pulse and another for the respiration signals. Thus, the computational cost is doubled while preventing the possibility for information sharing across these related physiological processes. As we know that pulse and respiration are linked, we propose a multi-task variant of our network (see <ref type="figure">Fig. 2</ref>). This shrinks the computational budget by approximately 50% and the tasks of estimating BVP and respiration can share an intermediate representation. The loss function of this multi-task TS-CAN (MTTS-CAN) is described in Eqn. 8 where b(t) is the gold-standard BVP waveform and r(t) is gold-standard respiration waveform. b(t)' and r(t)' are the respective predictions from the model.</p><formula xml:id="formula_7">L = 1 T T t=1 |b(t) ? b(t) | + ? 1 T T t=1 |r(t) ? r(t) |<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare our methods to four approaches for pulse measurement: POS <ref type="bibr" target="#b17">[18]</ref>, CHROM <ref type="bibr" target="#b16">[17]</ref>, ICA <ref type="bibr" target="#b11">[12]</ref>, 2D-CAN <ref type="bibr" target="#b8">[9]</ref> and two for respiration measurement: 2D-CAN and ARM <ref type="bibr" target="#b12">[13]</ref>. Other than DeepPhys <ref type="bibr" target="#b8">[9]</ref>, we are not aware of other methods that work for both pulse and respiration measurement. We run our experiments using the following datasets:</p><p>AFRL <ref type="bibr" target="#b32">[33]</ref>: 300 videos of 25 participants (17 males) recorded at 658x492 resolution and 120 fps (down-sampled to 30 fps for our experiments). Fingertip reflectance photoplethysmograms(PPG) were used to record ground-truth signals for training our network and electrocardiograms(ECG) were recorded for evaluating performance (this is the medical gold-standard). Each participant was recorded six times with increasing head motion in each task. The participants were asked to sit still for the first two tasks and perform three motion tasks rotating their head about the vertical axis with an angular velocity of 10 degrees/second, 20 degrees/second, 30 degrees/second, respectively. In the last task, participants were asked to orient their head randomly once every second to one of nine predefined locations. The six recording were repeated twice in front of two backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMSE-HR [34]</head><p>: 102 videos of 40 participants were recorded at 1040x1392 resolution and 25 fps during spontaneous emotion elicitation experiments. The gold standard contact signal was measured via a Biopac2 MP150 system 3 which provided pulse rate at 1000 fps and was updated after each heart beat. These videos feature smaller but more spontaneous motions than those in the AFRL dataset including facial expressions. Respiration measurements were not provided.</p><p>Experiment Details: At a high-level all our proposed networks share a similar two-branch architecture. Each branch has four convolutional layers. There is an averaging pooling layer and dropout layer placed after the second and fourth convolutional layers as shown in <ref type="figure">Fig. 2</ref>. Different architectures in <ref type="figure" target="#fig_0">Fig. 1</ref> require different convolutional operations (e.g., 3D-CAN requires 3D CNNs). To preprocess the input of the appearance branch, we downsample each frame c(t) to 36?36, which balances maintaining spatial resolution quality and suppressing camera noise <ref type="bibr" target="#b34">[35]</ref>. For the motion branch, we calculate normalized frames using every two adjacent frames as (c(t + 1) ? c(t))/(c(t) + c(t + 1)).</p><p>The normalized frames are less vulnerable to changes in brightness and skin appearance compared to the raw frames c(t) and reduce the chance of over-fitting to certain datasets.</p><p>Our system is implemented in TensorFlow <ref type="bibr" target="#b35">[36]</ref>. We trained our proposed MTTS-CAN architectures using the Adadelta optimizer <ref type="bibr" target="#b36">[37]</ref> with a learning rate of 1.0, batch size of 32, kernel size of 3?3, pooling size of 2?2, and dropout rates of 0.25 and 0.5. The final model was chosen after the training converged (12 epochs on the respiration task and 24 epochs on the pulse task). We implemented 2D-CAN, 3D-CAN and Hybrid-CAN as baselines to compare against our proposed architectures. For the 3D and Hybrid models the training schema is similar to TS-CAN, but we use a kernel size of 3?3?3 and a pooling size of 2?2?2. We used a window size of 10 frames in all temporal models to provide a fair comparison for our proposed architectures. We picked ? = 0.5 for the multi-tasking loss function in the MTTS-CAN to force estimations of pulse and respiration treated equally (pulse and respiration signals were both normalized in amplitude). To calculate the performance metrics, we post-processed the outputs of all methods in the same way using a 2nd-order Butterworth filter (cut-off frequencies of 0.75 and 2.5 Hz for HR and 0.08 and 0.5 Hz for BR). For the AFRL data, we divided the dataset into 30-second windows with no overlap. For the MMSE-HR dataset we used a  window size equal to the number of frames in each video. We then computed four standard metrics for each window: mean absolute error (MAE), root mean squared error (RMSE) and correlation (?) in heart/breathing rate estimations and the corresponding BVP/respiration signal-to-noise ratio (SNR) <ref type="bibr" target="#b16">[17]</ref>. Details of the calculation for these metrics, training code, architecture and the trained models are available in the supplementary material.</p><p>On-Device Evaluation: Our proposed architectures were deployed on an open-source embedded system called Firefly-RK3399 4 for latency evaluation. This embedded system has two large Cortex-A72 cores and four small Cortex-A53 cores. Although RK3399 also has a mobile Mali GPU, we focus our evaluation on CPU such that our proposed end-to-end architecture can be generalized to any ARM based mobile platform and IoT device. In this work, we extend a deep learning compiler stack -TVM <ref type="bibr" target="#b37">[38]</ref> to support the core temporal shift operation required for TS-CAN. TVM takes a high-level description of a function and generates highly optimized low-level code for a targeted device. More specifically, our TVM-based on-device system first converts a TensorFlow graph to a Relay graph <ref type="bibr" target="#b38">[39]</ref> and complies the code to Firefly-RK3399 using LLVM. We take advantage of TVM's scheduling primitives to generate efficient low-level LLVM code that accelerates expensive operations such as 2D and 3D convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Comparison with the State-of-the-Art: For the AFRL dataset all 25 participants were randomly divided into five folds of five participants each (same folds as in <ref type="bibr" target="#b8">[9]</ref>). The learning models were trained and tested via five-fold cross-validation using data from all tasks. The evaluation metrics are averaged over five folds and shown in <ref type="table" target="#tab_0">Table 1</ref>. All of our proposed models outperform the 2D-CAN and other baselines. Hybrid-CAN and 3D-CAN achieve similar accuracy, reducing MAE by 50% on pulse and 20% on respiration measurement. The hybrid model has lower computational cost and is therefore preferable. TS-CAN also surpasses the 2D-CAN by more than 43% on pulse and 20% on respiration measurement. We also evaluated a multi-tasking version of TS-CAN and Hybrid-CAN, and call them MTTS-CAN and MT-Hybrid-CAN respectively. We observe that there is no accuracy benefit from the multi-tasking model variants relative to the single task versions because the network must use almost all the same parameters for both tasks. However, the MT models require half the computation and half as many parameters compared to running pulse and respiration models separately which is a considerable benefit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Dataset Generalization:</head><p>To test whether our model can generalize to videos with a different resolution, background, and lighting, we trained our proposed models on the AFRL dataset and tested on the MMSE-HR dataset. Our proposed TS-CAN, Hybrid-CAN and 3D-CAN reduce errors by 25-50% compared to 2D-CAN (see <ref type="table" target="#tab_0">Table 1</ref>). Furthermore, MTTS-CAN and MT-Hybrid-CAN both perform strongly, showing that it is possible to share the representations between pulse and respiration.</p><p>Computation Cost and Latency: <ref type="figure" target="#fig_1">Fig. 3A</ref> and the last column of <ref type="table" target="#tab_0">Table 1</ref> show that MTTS-CAN and TS-CAN are the fastest architectures of those evaluated, taking 6 ms and 12 ms per frame for inference respectively. It is worth noting that TS-CAN is 40% faster than the 2D-CAN because the unique design of the appearance branch that only executes once and provides the generated attention mask to all the frames in the motion branch. MT-Hybrid-CAN and Hybrid-CAN also achieve 13ms and 26ms inference times respectively, this is approximately double that of our TS-based methods due to the cost of 3D convolutions relative to 2D convolutions. The 2D-CAN not only has a higher latency compared to TS-CAN, but the accuracy is significantly lower. It is not surprising that the 3D-CAN achieved the worst inference speed because it has costly 3D convolutions in both branches. Latency is important because we want our models to run at as high a frame rate as possible, 30 fps is the bare minimum required to accurately measure heart rate variability and subtle waveform dynamics and 100 fps would be preferable. Therefore, faster inference increases the precision at which we can measure inter-beat and systolic-diastolic intervals <ref type="bibr" target="#b15">[16]</ref> and could help with non-invasive blood pressure measurement <ref type="bibr" target="#b14">[15]</ref> and detecting arterial fibrillation (AFib) <ref type="bibr" target="#b13">[14]</ref>.</p><p>Temporal Modeling: Capturing such waveform dynamics requires good temporal modeling, therefore we compared several designs to help improve this. Our proposed MTTS-CAN, TS-CAN, MT-Hybrid-CAN, Hybrid-CAN and 3D-CAN all outperform the 2D-CAN and other baseline methods. This is consistent with prior work that found a 3D-CNN without attention outperformed a 2D-CNN (without attention) <ref type="bibr" target="#b19">[20]</ref>. We would anticipate that the focus on modeling the temporal aspects of the physiological waveform would lead to greater resilience to noise. We perform a systematic evaluation on videos with varying velocities of angular (rotational) head motion. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. As expected, all the proposed temporal models perform particularly strongly on tasks with greater velocity head motion; reducing the error on the most challenging task (6) by over 75%. Moreover, as <ref type="figure" target="#fig_1">Fig. 3B</ref> illustrates, although tensor shifting provides important temporal information, it also introduces extra noise. The results in <ref type="table" target="#tab_0">Table 1</ref> indicate that our attention module is effective at separating the signal from the added noise.</p><p>Multi-task Learning: Comparing our MT models with the non-MT models, we observe that the MT models do not reduce the error in pulse and respiration rate estimates. But they do significantly improve the efficiency of inference as shown in <ref type="figure" target="#fig_1">Fig. 3A</ref> which is critical in resource constrained mobile platforms. Moreover, in order to estimate heart beat and respiration rate from a video, there is a number of mandatory pre-processing and post-processing steps to be included in the pipeline such as down-sampling images, computing averaged frames, calculating the number of peaks etc. Since MTTS-CAN only takes 6ms for inference on each fraem, even with the pre-processing overhead real-time inference is still eminently feasible. Also, memory is a valuable resource on edge devices, and MTTS-CAN only requires half of the memory to store the parameters compared to TS-CAN. We believe MTTS-CAN can be deployed and especially useful in resource constrained settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications of MTTS-CAN:</head><p>The low latency and high accuracy of our system opens the door for many other applications. For example, it could be used to improve the measurement of heart rate variability which is a measure of the variation in the time between each heartbeat. Tracking the subtle changes between consecutive heart beats requires low latency like that provided by MTTS-CAN. Contactless and on-device HRV tracking could enable numerous novel applications in mental health and personalized health. Besides health applications, MTTS-CAN is also potentially be applied to various computer vision tasks that require on-device computation such as activity recognition and video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Broader Impact</head><p>Non-contact camera-based vital sign monitoring has great potential as a tool for telehealth. Our proposed system can promote global health equity and make healthcare more accessible for those in rural areas or those who find it difficult to travel to clinics and hospitals in-person (perhaps because of age, mobility issues or care responsibilities). These needs are likely to be particularly acute in low-resource settings. Non-contact sensing has other potential benefits for measuring the vitals of infants who ideally would not have contact sensors attached to their delicate skin. Furthermore, due to the exceptionally fast inference speed, the computational budget required for our proposed system is minimal. Therefore, people who cannot afford high-end computing devices still will be able to access the technology. While low-cost, ubiquitous sensing democratizes physiological measurement, it presents other challenges. If measurement can be performed from only a video, what happens if we detect a health condition in an individual when analyzing a video for other purposes. When and how should that information be disclosed? If the system fails in a context where a person is in a remote location, it may lead them to panic.</p><p>It is also important to consider how such technology could be used by "bad actors" or applied with negligence and without sufficient forethought for the implications. Non-contact sensing could be used to measure personal physiological information without the knowledge of the subject. Law enforcement might be tempted to apply this in an attempt to detect individuals who appear "nervous" via signals such as an elevated heart rate or irregular breathing, or an employer may surreptitiously screen prospective employees for health conditions without their knowledge during an interview. These applications would set a very dangerous precedent and would be illegal in many cases. Just as is the case with traditional contact sensors, it must be made transparent when these methods are being used and subjects should be required to consent before physiological data is measured or recorded. There should be no penalty for individuals who decline to be measured. Ubiquitous sensing offers the ability to measure signals in more contexts, but that does not mean that this should necessarily be acceptable. Just because cameras may be able to measure these signals in a new context, or with less effort, it does not mean they should be subject to any less regulation than existing sensors, in fact quite the contrary.</p><p>In the United States, the Health Insurance Portability and Accountability Act (HIPAA) and the HIPAA Privacy Rule sets a standard for protecting sensitive patient data and there should be no exception with regard to camera-based sensing. In the case of videos there should be particular care in how videos are transferred, given that significant health data can be contained with the channel. That was one of the motivations for designing our methods to run on-device, as it can minimize the risks involved in data transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Telehealth and the SARS-CoV-2 pandemic have acutely highlighted the specific need for accurate and computationally efficient cardiovascular and pulmonary sensing. We have presented a novel multi-task temporal shift convolutional attention network (MTTS-CAN) that improves on the state-of-the-art in both of these dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>We would like to thank the financial support from Bill &amp; Melinda Gates Foundation and University of Washington Endowment Funds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Example Waveforms</head><p>Please see our video figure in the supplementary material for video examples of results for pulse and respiration results. Below we should examples of pulse ( <ref type="figure" target="#fig_2">Fig. 4)</ref> and respiration <ref type="figure" target="#fig_3">(Fig. 5</ref>) waveforms for the MT-2DCAN, MT-Hybrid-CAN and MTTS-CAN models compared to the gold-standard contact sensor measurements. Videos are shown in the video figure included with the supplementary material.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Architecture Details</head><p>? Please see <ref type="figure">Fig. 6</ref> for details of TS-CAN and MTTS-CAN.</p><p>? Please see <ref type="figure" target="#fig_4">Fig. 7</ref> for details of Hybrid-CAN and MT-Hybrid-CAN.</p><p>? Please see <ref type="figure" target="#fig_5">Fig. 8</ref> for details of 3D-CAN and MT-3D-CAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Pre-processing</head><p>All the input images are first downsampled to the resolution of 36 ? 36. We then calcucated the normalized frames between two consecutive frames using (c(t + 1) ? c(t))/(c(t + 1) + c(t)) to reduce the dependency on the absolute frame brightness and skin appearance. By doing so, we have a normalized image and a raw image for every single frame in the video. Both of raw and normalized frame were mean subtracted and then scaled to unit standard deviation over each video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Post-processing</head><p>To calculate the performance metrics, we post-processed the outputs of all methods in the same way using a 2nd-order Butterworth filter (cut-off frequencies of 0.75 and 2.5 Hz for HR and 0.08 and 0.5 Hz for BR). For the AFRL data, we divided the dataset into 30-second windows with no overlap. For the MMSE-HR dataset we used a window size equal to the number of frames in each video. We then computed four standard metrics for each window: mean absolute error (MAE), root mean squared error (RMSE) and correlation (?) in heart/breathing rate estimations and the corresponding BVP/respiration signal-to-noise ratio (SNR).</p><p>The performance metrics are calculated as follows:</p><p>Mean Absolute Error (MAE) For calculate the MAE between our model estimates and the gold-standard heart rate calculated from the contact sensor measurements in window within each dataset. As follows:</p><p>Heart Rate:</p><formula xml:id="formula_8">M AE = 1 T T i=1 |HRi ? HR i |<label>(9)</label></formula><p>Respiration Rate:</p><formula xml:id="formula_9">M AE = 1 T T i=1 |RRi ? RR i |<label>(10)</label></formula><p>Root Mean Squared Error (RMSE) For calculate the RMSE between our model estimates and the goldstandard heart rate calculated from the contact sensor measurements in window within each dataset. As follows:</p><p>Heart Rate:</p><formula xml:id="formula_10">RM SE = i = 1 T T 1 (HRi ? HR i ) 2<label>(11)</label></formula><p>Respiration Rate:</p><formula xml:id="formula_11">RM SE = i = 1 T T 1 (RRi ? RR i ) 2<label>(12)</label></formula><p>Where HR and RR are the gold-standard heart rate and respiration rates and HR' RR' are the estimated heart rate and respiration rates, respectively, from the video. The gold-standard HR frequency was determined from the manually corrected ECG peaks in the AFRL dataset and the HR estimates provided with the dataset for the MMSE-HR dataset.</p><p>We also compute the Pearson correlation between the estimated heart rates and respiration rates and the goldstandard heart rates from the contact sensor measurements.</p><p>Pulse and Respiration Signal-to-Noise Ratios (SNR):</p><p>We calculate blood volume pulse and respiration signal-to-noise ratios (SNR) according to the method proposed by De Haan et al. <ref type="bibr" target="#b16">[17]</ref>. This captures the signal quality of the recovered pulse and respiration estimates. Again, the gold-standard HR/RR frequency was determined from the manually corrected gold-standard measurements in the AFRL and MMSE-HR datasets, respectively.</p><p>BVP SNR:</p><formula xml:id="formula_12">SN R = 10log 10 240 f =30 ((Ut(f )?(f )) 2 240 f =30 (1 ? Ut(f ))?(f )) 2 )<label>(13)</label></formula><p>where? is the power spectrum of the BVP signal (S), f is the frequency (in BPM), HR is the heart rate computed from the gold-standard device and Ut(f) is a binary template that is one for the heart rate region from HR-6 BPM to HR+6BPM and its first harmonic region from 2*HR-12BPM to 2*HR+12BPM, and 0 elsewhere.</p><p>Respiration SNR:</p><formula xml:id="formula_13">SN R = 10log 10 30 f =5 ((Ut(f )?(f )) 2 30 f =5 (1 ? Ut(f ))?(f )) 2 )<label>(14)</label></formula><p>where? is the power spectrum of the respiration signal (S), f is the frequency (in breaths/min), RR is the respiration rate computed from the gold-standard device and Ut(f) is a binary template that is one for the heart rate region from RR-6 breaths/min to RR+6breaths/min and its first harmonic region from 2*RR-12breaths/min to 2*RR+12breaths/min, and 0 elsewhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Baseline Methods</head><p>We compared the performance of our proposed approach to state-of-the-art supervised method using a convolutional attention network (CAN) and three unsupervised methods described below. For the CHROM, ICA and POS methods face detection was first performed using MATLAB's face detection (vision.CascadeObjectDetector()). This was fixed for all methods, to avoid the influence of the face detector on performance. For the CAN methods following the implementation in <ref type="bibr" target="#b8">[9]</ref> we did not use face detection but rather we passed the full frame to the network after cropping the center portion to make the frame a square with W=H.</p><p>CHROM <ref type="bibr" target="#b16">[17]</ref>. This method uses a linear combination of the chrominance signals obtained from the RGB video. </p><formula xml:id="formula_14">Swin = 3(1 ? ? 2 )yr ? 2(1 + ? 2 )yg + 3? 2 y b<label>(15)</label></formula><p>Where ? is the ratio of the standard deviations of the filtered versions of A and B:</p><formula xml:id="formula_15">A = 3yr ? 2yg<label>(16)</label></formula><formula xml:id="formula_16">B = 1.5yr + yg ? 1.5y b<label>(17)</label></formula><p>The resulting outputs are scaled using a Hanning Window and summed with the subsequent window (with 50% overlap) to construct the final blood volume pulse (BVP) signal.</p><p>ICA <ref type="bibr" target="#b6">[7]</ref>. This approach involves spatial averaging the pixels by color channel in the region of interest (ROI) for each frame to form time varying signals [xR, xG, xB]. Following this, the observation signals are detrended. A Z-transform is applied to each of the detrended signals. The Independent Component Analysis (ICA) (JADE implementation) is applied to the normalized color signals.</p><p>POS <ref type="bibr" target="#b17">[18]</ref>. The intensity signals [xR, xG, xB] are computed. A moving window of length 1.6 seconds (with overlapping windows and with a step size of one frame), is applied. For each time window, the signal is divided by its mean to give [xr,xg,x b ]. Following this, Xs and Ys are calculated where:</p><formula xml:id="formula_17">Xs =xg ?x b<label>(18)</label></formula><formula xml:id="formula_18">Ys = ?2xr +xg +x b<label>(19)</label></formula><p>Xs and Ys are then used to calculate Swin, where:</p><formula xml:id="formula_19">Swin = Xs + ?(Xs) ?(Ys) Ys<label>(20)</label></formula><p>The resulting outputs of the window-based analysis are used to construct the final BVP signal in an overlap add fashion. <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table">Table 4</ref> shows the results of all the multi-task and non-multi-task architectures. Including the MT-3DCAN and MT-2DCAN for which we did not have space in the main paper.     <ref type="table">Table 4</ref>: Pulse and respiration measurement MAE on the AFRL by motion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14">Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TS-CAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heart Rate</head><p>Respiration <ref type="table" target="#tab_0">Rate  Method  T1 T2 T3 T4 T5 T6 T1 T2 T3 T4 T5 T6  MTTS-</ref>  <ref type="figure">Figure 9</ref> and <ref type="figure" target="#fig_0">Figure 10</ref> demonstrate the relationship and distribution between estimated HR and ground-truth HR across all the subjects in the AFRL dataset. <ref type="figure" target="#fig_0">Figure 11</ref> and <ref type="figure" target="#fig_0">Figure 12</ref> demonstrate the relationship and distribution between estimated HR and ground-truth HR across all the subjects in the MMSE dataset. <ref type="figure" target="#fig_0">Figure 13</ref> illustrates the errors for 2D-CAN and MTTS-CAN on the AFRL Dataset by participant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>We perform a systematic comparison of several convolutional attention network (CAN) architectural designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(A) On-Device latency evaluation across six models; (B) An visualization of TSM on a normalized frame from motion branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Examples of pulse wave predictions from MT-2DCAN, MT-Hybrid-CAN and MTTS-CAN. Notice how the agreement with the gold-standard measurements (blue) is highest for MT-Hybrid-CAN and MTTS-CAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Examples of respiration wave predictions from MT-2DCAN, MT-Hybrid-CAN and MTTS-CAN. Notice how the agreement with the gold-standard measurements (blue) is highest for MTTS-CAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Architecture details of Hybrid-CAN and MT-Hybrid-CAN 3D-CAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Architecture details of 3D-CAN and MT-3DCAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :Figure 11 :Figure 12 :Figure 13 :</head><label>910111213</label><figDesc>AFRL Dataset: Estimated HR and Gold-standard HR reference measurements in (beatsper-minute) and the corresponding Bland-Altman plots from 2D-CAN, 3D-CAN and Hybrid-CAN. All results computed for 30-second time windows.Gold-Standard HR (BPM)Camera HR (BPM) AFRL Dataset: Estimated HR and Gold-standard HR reference measurements in (beatsper-minute) and the corresponding Bland-Altman plots from MT-Hybrid-CAN and MTTS-CAN. All results computed for 30-second time windows. MMSE-HR Dataset: Estimated HR and Gold-standard HR reference measurements in (beats-per-minute) and the corresponding Bland-Altman plots from 2D-CAN, 3D-CAN and Hybrid-CAN. Note the following files were removed due to unreliable reference measurements: F006-T11, F013-T08, F013-T11, F014-T01, F014-T08, F014-T10, F014-T11, F015-T11, F022-T10, M013-T11Gold-Standard HR (BPM) Camera HR (BPM) MMSE-HR Dataset: Estimated HR and Gold-standard HR reference measurements in (beats-per-minute) and the corresponding Bland-Altman plots from MT-Hybrid-CAN and MTTS-CAN. Box Plot of Errors for 2D-CAN and MTTS-CAN on the AFRL Dataset by Participant Box Plot of Errors for 2D-CAN and MTTS-CAN on the AFRL Dataset by Participant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Pulse and respiration measurement on the AFRL and MMSE-HR datasets. MAE RMSE ? SNR MAE RMSE ? SNR MAE RMSE ? SNR (ms) MTTS-CAN 1.45 3.72 0.94 8.64 3.00 5.66 0.92 2.37 2.30 4.52 0.40 18.7 6 MT-Hyb.-CAN 1.15 2.69 0.97 10.2 3.43 6.98 0.88 4.70 2.17 4.24 0.45 19.1 13 TS-CAN 1.32 3.25 0.95 8.86 3.41 7.82 0.84 2.92 2.25 4.47 0.41 18.9 12 Hyb.-CAN 1.12 2.60 0.97 10.6 2.55 4.16 0.96 5.47 2.06 4.17 0.46 19.8 26 3D-CAN 1.18 2.83 0.97 10.5 2.78 5.08 0.94 4.73 2.31 4.42 0.44 19.3 48 2D-CAN [9] 2.32 5.82 0.85 6.23 4.72 8.68 0.82 2.06 2.86 5.16 0.34 16.</figDesc><table><row><cell></cell><cell>Heart Rate</cell><cell></cell><cell>Respiration Rate</cell></row><row><cell></cell><cell>AFRL (All Tasks)</cell><cell>MMSE-HR</cell><cell>AFRL (All Tasks)</cell><cell>Time</cell></row><row><cell cols="5">Method 3 20</cell></row><row><cell cols="3">POS [18] 2.48 5.07 0.89 2.32 3.90 9.61 0.78 2.33</cell><cell>|</cell><cell>-</cell></row><row><cell cols="3">CHROM [17] 6.42 12.4 0.60 -4.83 3.74 8.11 0.82 1.90</cell><cell>Not Applicable</cell><cell>-</cell></row><row><cell cols="3">ICA [12] 4.36 7.84 0.77 3.64 5.44 12.00 0.66 3.03</cell><cell>|</cell><cell>-</cell></row><row><cell>ARM [13]</cell><cell>Not Applicable</cell><cell></cell><cell>3.68 5.52 0.29 -6.22</cell><cell>-</cell></row></table><note>MAE = Mean Absolute Error, RMSE = Root Mean Squared Error, ? = Pearson Correlation, SNR = BVP Signal-to-Noise Ratio.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Pulse and respiration measurement MAE on the AFRL by motion task.CAN  1.08 1.23 0.94 1.27 1.07 3.12 0.68 0.98 2.12 3.81 3.31 2.89 MT-Hybrid-CAN 1.04 1.24 0.95 1.23 0.88 1.53 0.77 0.89 2.23 3.28 3.03 2.80 TS-CAN 1.07 1.25 0.96 1.24 1.01 2.36 0.69 1.14 2.27 3.70 3.18 2.53 Hybrid-CAN 1.04 1.21 0.94 1.22 0.89 1.39 0.77 1.03 1.83 3.19 2.96 2.60 3D-CAN 1.06 1.19 0.92 1.23 0.89 1.77 0.96 0.98 2.58 3.80 2.87 2.65 2D-CAN [9] 1.08 1.21 1.02 1.43 2.15 7.05 1.25 1.11 3.35 4.63 3.77 3.08 POS [18] 1.50 1.53 1.50 1.84 2.05 6.11 | CHROM [17] 4.53 4.59 4.35 4.84 6.89 10.3 Not Applicable ICA [12] 1.17 1.70 1.70 4.00 5.22 11.8</figDesc><table><row><cell></cell><cell>Heart Rate</cell><cell>Respiration Rate</cell></row><row><cell>Method</cell><cell cols="2">T1 T2 T3 T4 T5 T6 T1 T2 T3 T4 T5 T6</cell></row><row><cell>MTTS-</cell><cell></cell></row></table><note>| ARM [13] Not Applicable 2.51 2.53 3.19 4.85 4.22 4.78</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Pulse and respiration measurement on the AFRL and MMSE-HR datasets. SNR MAE RMSE ? SNR MAE RMSE ? SNR (ms) MTTS-CAN 1.45 3.72 0.94 8.64 3.00 5.66 0.92 2.37 2.30 4.52 0.40 18.</figDesc><table><row><cell>Heart Rate</cell><cell></cell><cell>Respiration Rate</cell><cell></cell></row><row><cell>AFRL (All Tasks)</cell><cell>MMSE-HR</cell><cell>AFRL (All Tasks)</cell><cell>Time</cell></row><row><cell cols="3">Method MAE RMSE ? 7</cell><cell>6</cell></row><row><cell cols="4">MT-Hyb.-CAN 1.15 2.69 0.97 10.2 3.43 6.98 0.88 4.70 2.17 4.24 0.45 19.1 13</cell></row><row><cell cols="4">MT-3D-CAN 1.20 2.82 0.97 10.4 3.70 7.85 5.35 0.84 2.21 4.37 0.43 18.8 24</cell></row><row><cell cols="4">MT-2D-CAN 2.51 6.20 0.83 6.03 5.14 10.3 0.73 1.83 2.98 5.23 0.33 16.2 10</cell></row><row><cell cols="4">TS-CAN 1.32 3.25 0.95 8.86 3.41 7.82 0.84 2.92 2.25 4.47 0.41 18.9 12</cell></row><row><cell cols="4">Hyb.-CAN 1.12 2.60 0.97 10.6 2.55 4.16 0.96 5.47 2.06 4.17 0.46 19.8 26</cell></row><row><cell cols="4">3D-CAN 1.18 2.83 0.97 10.5 2.78 5.08 0.94 4.73 2.31 4.42 0.44 19.3 48</cell></row><row><cell cols="4">2D-CAN 2.32 5.82 0.85 6.23 4.72 8.68 0.82 2.06 2.86 5.16 0.34 16.3 20</cell></row><row><cell cols="3">MAE = Mean Absolute Error, RMSE = Root Mean Squared Error, ? = Pearson Correlation, SNR = BVP Signal-to-Noise Ratio.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The [xR, xG, xB] signals are filtered using a zero-phase, 3rd-order Butterworth bandpass filter with pass-band frequencies of [0.7 2.5] Hz. Following this, a moving window method of length 1.6 seconds (with overlapping windows and a step size of 0.8 seconds) is applied. Within each window the color signals are normalized by dividing by their mean value to give [xr,xg,x b ]. These signals are bandpass filtered using zero-phase forward and reverse 3rd-order Butterworth filters with pass-band frequencies of [0.7 2.5] Hz. The filtered signals [yr, yg, y</figDesc><table /><note>b ] are then used to calculate Swin:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>CAN 1.08 1.23 0.94 1.27 1.07 3.12 0.68 0.98 2.12 3.81 3.31 2.89 MT-Hybrid-CAN 1.04 1.24 0.95 1.23 0.88 1.53 0.77 0.89 2.23 3.28 3.03 2.80 MT-3D-CAN 1.07 1.21 0.92 1.23 0.86 1.91 0.81 1.00 2.10 3.33 3.05 3.00 MT-2D-CAN 1.08 1.28 1.13 1.52 2.60 7.46 1.47 1.24 3.77 4.53 3.58 3.32 TS-CAN 1.07 1.25 0.96 1.24 1.01 2.36 0.69 1.14 2.27 3.70 3.18 2.53 Hybrid-CAN 1.04 1.21 0.94 1.22 0.89 1.39 0.77 1.03 1.83 3.19 2.96 2.60 3D-CAN 1.06 1.19 0.92 1.23 0.89 1.77 0.96 0.98 2.58 3.80 2.87 2.65 2D-CAN 1.08 1.21 1.02 1.43 2.15 7.05 1.25 1.11 3.35 4.63 3.77 3.08 15 Additional Figures of Results</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.fda.gov/regulatory-information/search-fda-guidance-documents/cardiac-monitor-guidanceincluding-cardiotachometer-and-rate-alarm-guidance-industry#6_1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.biopac.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://en.t-firefly.com/product/rk3399.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The role of telemedicine during the COVID-19 epidemic in china-experience from shandong province</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Telehealth for global emergencies: Implications for coronavirus disease 2019 (COVID-19)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Centaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Snoswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ateev</forename><surname>Haydon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><forename type="middle">J</forename><surname>Clemensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of telemedicine and telecare</title>
		<imprint>
			<biblScope unit="page" from="1357633" to="20916567" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Embracing telemedicine into your otolaryngology practice amid the COVID-19 crisis: An invited commentary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Setzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Svider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Otolaryngology</title>
		<imprint>
			<biblScope unit="page">102490</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pathological findings of COVID-19 associated with acute respiratory distress syndrome. The Lancet respiratory medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="420" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">COVID-19 and the cardiovascular system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Ying</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Tong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Cardiology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="259" to="260" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-contact physiological monitoring of preterm infants in the neonatal intensive care unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Villarroel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitthichok</forename><surname>Chaichulee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabrielle</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Watkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-contact, automated cardiac pulse measurements using video imaging and blind source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Zher</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="10762" to="10774" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-contact measurement of oxygen saturation with an rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Alessandro R Guazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Villarroel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical optics express</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3320" to="3338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepphys: Video-based physiological measurement using convolutional attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Heart rate measurement based on a time-lapse image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chihiro</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Ohta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical engineering &amp; physics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="853" to="857" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Remote plethysmographic imaging using ambient light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><surname>Verkruysse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Stuart</forename><surname>Svaasand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="21434" to="21445" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Advancements in noncontact, multiparameter physiological measurements using a webcam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Zher</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="11" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-contact video-based vital sign monitoring using ambient light and auto-regressive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Villarroel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Da Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physiological measurement</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">807</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diagnostic performance of a smartphone-based photoplethysmographic application for atrial fibrillation screening in a primary care setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pak-Hei</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ka</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yukkee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louise</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangie Wan-Chiu</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Fai</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle Man-Ying</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Zher</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Wai-Sing</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Wah</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Heart Association</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3428</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cuffless single-site photoplethysmography for blood pressure monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Hosanee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylie</forename><surname>Welykholowa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panayiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingchang</forename><surname>Kyriacou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><forename type="middle">H</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Medicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">723</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Remote detection of photoplethysmographic systolic and diastolic peaks using a digital camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Gontarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2948" to="2954" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust pulse rate from chrominance-based rppg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Jeanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2878" to="2886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithmic principles of remote ppg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Brinker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Stuijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1479" to="1491" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual heart rate estimation with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radim</forename><surname>?petl?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vojtech</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jir?</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference<address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Heart rate estimation from facial videos using a spatiotemporal representation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rencheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vitamon: measuring heart rate variability using smartphone front camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinh</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Rajesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonggil</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngki</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference on Embedded Networked Sensor Systems</title>
		<meeting>the 17th Conference on Embedded Networked Sensor Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reasoning about human-object interactions through dual attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3919" to="3928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Leveraging data science to combat COVID-19: A comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddique</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanaullah</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adeel</forename><surname>Razi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maged N Kamel</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A deep learning algorithm using CT images to screen for corona virus disease (COVID-19)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjiao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>MedRxiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning system to screen coronavirus disease 2019 pneumonia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charmaine</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benson</forename><forename type="middle">A</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Flusense: A contactless syndromic surveillance platform for influenza-like illness in hospital waiting areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Forsad Al Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tauhidur</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ai4COVID-19: Ai enabled preliminary diagnosis for COVID-19 from cough samples via an app</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Posokhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Haneya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usama</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajid</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamran</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nabeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01275</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coronavirus effects on pregnant women in the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naushad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahnoor</forename><surname>Naushad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angila</forename><surname>Robin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Available at SSRN</title>
		<imprint>
			<biblScope unit="volume">3569040</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deepmag: Source specific motion magnification using gradient ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03338</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Respiratory sinus arrhythmia: autonomic origins, physiological mechanisms, and psychophysiological implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Berntson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">S</forename><surname>Cacioppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quigley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="196" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recovering pulse rate during motion artifact with a multi-imager array for non-contact imaging photoplethysmography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">B</forename><surname>Justin R Estepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher M</forename><surname>Blackford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umur</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaun</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploiting spatial redundancy of image sensor for motion robust rppg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Stuijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="415" to="425" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relay: A new ir for machine learning frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marisa</forename><surname>Kirisame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
