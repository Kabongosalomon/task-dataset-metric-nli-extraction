<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Attention: Attention with Linear Complexities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Zhuoran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Mingyuan</surname></persName>
							<email>zhangmingyuan@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Haiyu</surname></persName>
							<email>zhaohaiyu@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shuai</surname></persName>
							<email>yishuai@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Independent Researcher</orgName>
								<address>
									<addrLine>4244 University Way NE #85406</addrLine>
									<postCode>98105</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">SenseTime International</orgName>
								<address>
									<addrLine>182 Cecil Street, #36-02 Frasers Tower</addrLine>
									<postCode>069547</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong Sha Tin</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Attention: Attention with Linear Complexities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on highresolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-ofthe-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github. com/cmsflash/efficient-attention.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dot-product attention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> is a prevalent mechanism in neural networks for long-range dependency model-* Work during internship at SenseTime. ? Equal contribution.</p><p>ing, a key challenge to deep learning that convolution and recurrence struggle to solve. The mechanism computes the response at every position as a weighted sum of features at all positions in the previous layer. In contrast to the limited receptive fields of convolution or the recurrent layer, dotproduct attention expands the receptive field to the entire input in one pass. Using dot-product attention to efficiently model long-range dependencies allows convolution and recurrence to focus on local dependency modeling, in which they specialize. The non-local module <ref type="bibr" target="#b22">[23]</ref>, an adaptation of dot-product attention for computer vision, achieved state-of-the-art performance on video classification <ref type="bibr" target="#b22">[23]</ref> and generative adversarial image modeling <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref> and demonstrated significant improvements on object detection <ref type="bibr" target="#b22">[23]</ref>, instance segmentation <ref type="bibr" target="#b22">[23]</ref>, person re-identification <ref type="bibr" target="#b13">[14]</ref>, image de-raining <ref type="bibr" target="#b12">[13]</ref>, etc.</p><p>However, global dependency modeling on large inputs (e.g. long sequences, high-resolution images, large videos) remains an open problem. The quadratic 1 memory and computational complexities with respect to the input size of dot-product attention inhibits its application on large inputs. For instance, a non-local module uses over 1 GB of GPU memory and over 25 GMACC 2 of computation for a 64-channel 128 ? 128 feature map or over 68 GB and over 1.6 TMACC for a 64-channel 64 ? 64 ? 32 3D feature vol- <ref type="bibr" target="#b0">1</ref> The complexities are quadratic with respect to the spatiotemporal size of the input, which is quartically w.r.t. the side length of a 2D feature map, or sextically w.r.t. the dimension of a 3D feature volume. <ref type="bibr" target="#b1">2</ref> MACC stands for multiply-accumulation. 1 MACC means 1 multiplication and 1 addition operation. ume (e.g. for depth estimation or video tasks). The high memory and computational costs constrain the application of dot-product attention to the low-resolution parts of models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2]</ref> and prohibits its use for resolution-sensitive or resource-hungry tasks.</p><p>The need for global dependency modeling on large inputs motivates the exploration for a resource-efficient attention mechanism. An investigation into the non-local module revealed an intriguing discovery. As <ref type="figure" target="#fig_0">Figure 1</ref> shows, putting aside the normalization, dot-product attention involves two consecutive matrix multiplications. The first one (S = QK T ) computes pairwise similarities between pixels and forms per-pixel attention maps. The second (D = SV ) aggregates the values V by the per-pixel attention maps to produce the output. Since matrix multiplication is associative, switching the order from (QK T )V to Q(K T V ) has no impact on the effect but changes the complexities from O(n 2 ) to O(d k d v ), for n the input size and d k , d v the dimensionalities of the keys and the values, respectively. This change removes the O(n 2 ) terms in the complexities of the module, making it linear in complexities. Further, d k d v is significantly less than n 2 in practical cases, hence this new term will not become a new bottleneck. Therefore, switching the order of multiplication to Q(K T V ) results in a substantially more efficient mechanism, which this paper names efficient attention.</p><p>The new mechanism is mathematically equivalent to dot-product attention with scaling normalization and approximately equivalent with softmax normalization. Experiments empirically verified that when the equivalence is approximate, it does not impact accuracies. In addition, experiments showed that its efficiency allows the integration of more attention modules into a network and integration into high-resolution parts of a network, which lead to significantly higher accuracies. Further, experiments demonstrated that efficient attention democratizes attention to tasks where dot-product attention is inapplicable due to resource constraints.</p><p>Another discovery is that efficient attention brings a new interpretation to the attention mechanism. Assuming the keys are of dimensionality d k and the input size is n, one can interpret the d k ? n key matrix as d k template attention maps, each corresponding to a semantic aspect of the input. Then, the query at each pixel is d k coefficients for each of the d k template attention maps, respectively. Under this interpretation, efficient and dot-product attention differs in that dot-product attention first synthesizes the pixelwise attention maps from the coefficients and lets each pixel aggregate the values with its own attention map, while efficient attention first aggregates the values by the template attention maps to form template outputs (i.e. global context vectors) and lets each pixel aggregate the template outputs.</p><p>The principal contribution of this paper is the efficient attention mechanism, which:</p><p>1. has linear memory and computational complexities with respect to the size of the input;</p><p>2. possesses the same representational power as the prevalent dot-product attention mechanism;</p><p>3. allows the integration of more attention modules into a neural network and into higher-resolution parts of the network, which brings substantial performance boosts to tasks such as object detection and instance segmentation (on MS-COCO 2017); and 4. facilitates the application of attention on resourcehungry tasks, such as stereo depth estimation (on the Scene Flow dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>2.1. Dot-product attention <ref type="bibr" target="#b0">[1]</ref> proposed the initial formulation of the dot-product attention mechanism to improve word alignment in machine translation. Successively, <ref type="bibr" target="#b21">[22]</ref> proposed to completely replace recurrence with attention and named the resultant architecture the Transformer. The Transformer architecture is highly successful on sequence tasks. They hold the state-ofthe-art records on virtually all tasks in natural language processing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref> and is highly competitive on end-to-end speech recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. <ref type="bibr" target="#b22">[23]</ref> first adapted dot-product attention for computer vision and proposed the non-local module. They achieved state-of-the-art performance on video classification and demonstrated significant improvements on object detection, instance segmentation, and pose estimation. Subsequent works applied it to various fields in computer vision, including image restoration <ref type="bibr" target="#b15">[16]</ref>, video person re-identification <ref type="bibr" target="#b13">[14]</ref>, generative adversarial image modeling <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref>, image de-raining <ref type="bibr" target="#b12">[13]</ref>, and few-shot learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>, etc.</p><p>Efficient attention mainly builds upon the version of dotproduct attention in the non-local module. Following <ref type="bibr" target="#b22">[23]</ref>, the team conducted most experiments on object detection and instance segmentation. The paper compares the resource efficiency of the efficient attention module against the non-local module under the same performance and their performance under the same resource constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Scaling attention</head><p>Besides dot-product attention, there are a separate set of techniques the literature refers to as attention. This section refers to them as scaling attention. While dot-product attention is effective for global dependency modeling, scaling attention focuses on emphasizing important features and denotes matrix multiplication. When ?, ?q, ? k implement scaling normalization, the efficient attention mechanism is mathematically equivalent to dot-product attention. When they implement softmax normalization, the two mechanisms are approximately equivalent.</p><p>suppressing uninformative ones. For example, the squeezeand-excitation (SE) module <ref type="bibr" target="#b9">[10]</ref> uses global average pooling and a linear layer to compute a scaling factor for each channel and then scales the channels accordingly. SEenhanced models achieved state-of-the-art performance on image classification and substantial improvements on scene segmentation and object detection. On top of SE, CBAM <ref type="bibr" target="#b23">[24]</ref> added global max pooling beside global average pooling and an extra spatial attention submodule. GCNet <ref type="bibr" target="#b2">[3]</ref> proposes to replace the global average pooling by an adaptive pooling layer, which uses a linear layer to compute the weight for each position. These follow-up methods further improves upon the performance of SE <ref type="bibr" target="#b9">[10]</ref>.</p><p>Despite both names containing attention, dot-product attention and scaling attention are two separate sets of techniques with highly divergent goals. When appropriate, one might take both techniques and let them work in conjunction. Therefore, it is unnecessary to make comparison of efficient attention with scaling attention techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Efficient non-local operations</head><p>Recent literature proposed several methods for efficient non-local operations. LatentGNN <ref type="bibr" target="#b28">[29]</ref> proposes to approximate the single n ? n affinity matrix in the non-local <ref type="bibr" target="#b22">[23]</ref> module by the product of three lower-rank matrices. In comparison, efficient attention is not an approximation of the non-local module, but is mathematically equivalent (using scaling normalization). In addition, there is a one-toone mapping between the structural components of the non-local module and the efficient attention module. Therefore, in any field where the non-local module succeeded, one can guarantee the applicability of efficient attention as a drop-in replacement with substantially improved performance-cost trade-off.</p><p>CGNL <ref type="bibr" target="#b26">[27]</ref> proposes to flatten the height, width, and channel dimensions to a hwc-dimensional vector, applies a kernel function to expand the dimensionality to hwc ? (p + 1), for p the degree of Taylor expansion, and models global dependencies in that space. However, after flattening the input into a vector, the feature at each position becomes a scalar, which encodes limited information for interaction modeling. In contrast, efficient attention preserves a vector representation at each pixel and is capable to model richer interactions. Section 4.1.2 presents empirical comparison between efficient attention and these competing methods in detail, which shows that efficient attention outperforms each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A revisit of dot-product attention</head><p>Dot-product attention is a mechanism for long-range interaction modeling in neural networks. For each input feature vector x i ? R d that corresponds to the i-th position, dot-product attention first uses three linear layers to convert x i into three feature vectors, i.e., the query q i ? R d k , the key k i ? R d k , and the value v i ? R dv . The queries and keys must have the same feature dimension d k . One can measure the similarity between the i-th query and the j-th key as ?(q T i k j ), where ? is a normalization function. In general, the similarities are asymmetric, since the queries and keys are the outputs of two separate layers. The dotproduct attention module calculates the similarities between all pairs of positions. Using the similarities as weights, position i aggregates the values from all positions via weighted summation to obtain its output feature.</p><p>If one represents all n positions' queries, keys, and values in matrix forms as Q ? R n?d k , K ? R n?d k , V ? R n?dv , respectively, the output of dot-product attention is</p><formula xml:id="formula_0">D(Q, K, V ) = ? QK T V .<label>(1)</label></formula><p>The normalization function has two common choices:</p><formula xml:id="formula_1">Scaling: ?(Y ) = Y n , Softmax: ?(Y ) = ? row (Y ),<label>(2)</label></formula><p>where ? row denotes applying the softmax function along each row of matrix Y . An illustration of the dot-product attention module is in <ref type="figure" target="#fig_0">Figure 1</ref> (left).</p><p>The critical drawback of this mechanism is its resource demands. Since it computes a similarity between each pair of positions, there are n 2 such similarities, which results in O(n 2 ) memory complexity and O(d k n 2 ) computational complexity. Therefore, dot-product attention's resource demands get prohibitively high on large inputs. In practice, application of the mechanism is only possible on low-resolution features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Efficient attention</head><p>Observing the critical drawback of dot-product attention, this paper proposes the efficient attention mechanism, which is mathematically equivalent to dot-product attention but substantially faster and more memory efficient. In efficient attention, the individual feature vectors X ? R n?d still pass through three linear layers to form the queries Q ? R n?d k , keys K ? R n?d k , and values V ? R n?dv . However, instead of interpreting the keys as n feature vectors in R d k , the module regards them as d k single-channel feature maps. Efficient attention uses each of these feature maps as a weighting over all positions and aggregates the value features through weighted summation to form a global context vector. The name reflects the fact that the vector does not correspond to a specific position, but is a global description of the input features.</p><p>The following equation characterizes the efficient attention mechanism:</p><formula xml:id="formula_2">E(Q, K, V ) = ? q (Q) ? k (K) T V ,<label>(3)</label></formula><p>where ? q and ? k are normalization functions for the query and key features, respectively. The implementation of the same two normalization methods as for dot-production attention are</p><formula xml:id="formula_3">Scaling: ? q (Y ) = ? k (Y ) = Y ? n , Softmax: ? q (Y ) = ? row (Y ), ? k (Y ) = ? col (Y ),<label>(4)</label></formula><p>where ? row , ? col denote applying the softmax function along each row or column of matrix Y , respectively. The efficient attention module is a concrete implementation of the mechanism for computer vision data. For an input feature map X ? R h?w?d , the module flattens it to a matrix X ? R hw?d , applies the efficient attention mechanism on it, and reshapes the result to h ? w ? d v . If d v = d, it further applies a 1x1 convolution to restore the dimensionality to d. Finally, it adds the resultant features to the input features to form a residual structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Equivalence between dot-product and efficient attention</head><p>Following is a formal proof of the equivalence between dot-product and efficient attention when using scaling normalization. Substituting the scaling normalization formula in Equation <ref type="formula" target="#formula_1">(2)</ref> into Equation <ref type="formula" target="#formula_0">(1)</ref> gives</p><formula xml:id="formula_4">D(Q, K, V ) = QK T n V .<label>(5)</label></formula><p>Similarly, plugging the scaling normalization formulae in Equation <ref type="formula" target="#formula_3">(4)</ref> into Equation <ref type="formula" target="#formula_2">(3)</ref> results in</p><formula xml:id="formula_5">E(Q, K, V ) = Q ? n K T ? n V .<label>(6)</label></formula><p>Since scalar multiplication is commutative with matrix multiplication and matrix multiplication is associative, we have</p><formula xml:id="formula_6">E(Q, K, V ) = Q ? n K T ? n V = 1 n Q K T V = 1 n QK T V = QK T n V .<label>(7)</label></formula><p>Comparing Equations (5) and <ref type="formula" target="#formula_6">(7)</ref>, we get</p><formula xml:id="formula_7">E(Q, K, V ) = D(Q, K, V ).<label>(8)</label></formula><p>Thus, the proof is complete. The above proof works for the softmax normalization variant with one caveat. The two softmax operations on Q, K are not exactly equivalent to the single softmax on QK T . However, they closely approximate the effect of the original softmax function. The critical property of ? row QK T is that each row of it sums up to 1 and represents a normalized attention distribution over all positions. The matrix ? row (Q)? col (K) T shares this property. Therefore, the softmax variant of efficient attention is a close approximation of that variant of dot-product attention. Section 4.1 demonstrates this claim empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Interpretation of efficient attention</head><p>Efficient attention brings a new interpretation of the attention mechanism. In dot-product attention, selecting position i as the reference position, one can collect the similarities of all positions to position i and form an attention map s i for that position. The attention map s i represents the degree to which position i attends to each position j in the input. A higher value for position j on s i means position i attends more to position j. In dot-product attention, every position i has such an attention map s i , which the mechanism uses to aggregate the values V to produce the output at position i.</p><p>In contrast, efficient attention does not generate an attention map for each position. Instead, it interprets the keys K ? R n?d k as d k attention maps k T j . Each k T j is a global attention map that does not correspond to any specific position. Instead, each of them corresponds to a semantic aspect of the entire input. For example, one such attention map might cover the persons in the input. Another might correspond to the background. Section 6 gives several concrete examples. Efficient attention uses each k T j to aggregate the values V and produce a global context vector g j . Since k T j describes a global, semantic aspect of the input, g j also summarizes a global, semantic aspect of the input. Then, position i uses q i as a set of coefficients over g 0 , g 1 , . . . , g d k ?1 . Using the previous example, a person pixel might place a large weight on the global context vector for persons to refine its representation. A pixel at the boundary of an object might have large weights on the global context vectors for both the object and the background to enhance the contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Efficiency advantage</head><p>This section analyzes the efficiency advantage of efficient attention over dot-product attention in memory and computation. The reason behind the efficiency advantage is that efficient attention does not compute a similarity between each pair of positions, which would occupy O(n 2 ) memory and require O(d k n 2 ) computation to generate. Instead, it only generates d k global context vectors in R dv . This change eliminates the O(n 2 ) terms from both the memory and computational complexities of the module. Consequently, efficient attention has O(dn + d 2 ) mem-  <ref type="table" target="#tab_0">Table 1</ref> shows complexity formulae of the efficient attention module and the non-local module (using dot-product attention) in detail. In computer vision, this complexity difference is substantial. Firstly, the input size n is quadratic in image side length and often very large in practice. Secondly, d k is a parameter of the module, which the designer of a network can tune to meet different resource requirements. Section 4.2.2 shows that, within a reasonable range, this parameter has minimal impact on performance. This result means that an efficient attention module can typically have a small d k , which further increases its efficiency advantage over dot-product attention.</p><p>The rest of this section will give several concrete examples comparing the resource demands of the efficient attention and non-local modules. <ref type="figure" target="#fig_1">Figure 2</ref> compares their resource consumption for image features with different sizes. Directly substituting the non-local module on the 64 ? 64 feature map in SAGAN <ref type="bibr" target="#b27">[28]</ref> yields a 17-time saving of memory and 33-time saving of computation. The gap widens rapidly with the increase of the input size. For a 256 ? 256 feature map, a non-local module would require impractical amounts of memory (17.2 GB) and computation (413 GMACC). With the same input size, an efficient attention module uses 1/257 the memory and 1/513 the computation. The difference is more prominent for 3D features. For a tiny 28 ? 28 ? 4 feature volume, an efficient attention module uses less than 1/10 the memory and computation in comparison to a non-local module. On a larger 64 ? 64 ? 32 feature volume, a non-local module requires 513 times the memory and 1025 times the computation of an efficient attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on the MS-COCO task suite</head><p>This section presents comparison experiments on the MS-COCO 2017 dataset for object detection and instance  <ref type="table" target="#tab_1">Table 2</ref> reports the comparison against the non-local module. Efficient attention achieves substantially better performance-cost trade-off. As rows res3 to fpn5 show, inserting an efficient attention module or a non-local module at the same location in a network has nearly identical effects on the performance, while efficient attention uses orders of magnitude less resources. Rows res3-4+fpn3-5 and res3-4+fpn1-5 show that under the same resource cap (TI-TAN Xp GPU, 12 GB VRAM), efficient attention achieves significantly better performance. Note that res3-4+fpn3-5 is the best configuration that fits in memory for non-local modules. Further inserting non-local modules to fpn1 or fpn2 would require gigabytes of memory per example. <ref type="table" target="#tab_2">Table 3</ref> shows the comparison of absolute performance and performance improvement with competing approaches on MS-COCO 2017 object detection and instance segmentation. EA models has the highest performance and performance improvement in all settings while using the least resources. Note that EA's baseline models are significantly stronger, which make the improvements more valuable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison with competing methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Attention normalization</head><p>These experiments empirically compared the two methods Section 3.2 specified, namely scaling and softmax normalization. <ref type="table" target="#tab_3">Table 4</ref> reports the experimental outcomes. The results demonstrate that the effectiveness does not depend on the specific normalization method. Following <ref type="bibr" target="#b22">[23]</ref>, all other experiments used softmax normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Dimensionality of the keys</head><p>These experiments tested the impact of the dimensionality of the keys on the effect of efficient attention. As in <ref type="table" target="#tab_4">Table  5</ref>, decreasing the dimensionality of the keys from 128 to 32 caused minimal accuracy change. This result reinforces the hypothesis in Section 1 that most attention maps are expressible as linear combinations of a limited set of template attention maps. Therefore, researchers can reduce the dimensionality of the keys and queries in efficient attention modules to further save resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on other tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Stereo depth estimation</head><p>The experiments on efficient attention for stereo depth estimation used the Scene Flow dataset, a large-scale synthesized dataset with 39824 stereo frame pairs. The baseline is PSMNet <ref type="bibr" target="#b3">[4]</ref>, a clean model with near state-of-the-art performance. The experiments empirically determined the optimal hyperparamters, which significantly outperform the setting in <ref type="bibr" target="#b3">[4]</ref> (batch size is 24, learning rate is 2 ? 10 ?3 , training length is 100 epochs, and the rest is the same as in <ref type="bibr" target="#b3">[4]</ref>), as in <ref type="table">Table 7</ref>. On top of the strong baseline, inserting an efficient attention module after the last 3D hourglass leads to further improvement and achieves a new stateof-the-art. In comparison, inserting a non-local module at the same place would require an astronomical 9.68 TB of memory, prohibiting any attempt to verify its effectiveness. <ref type="table">Table 8</ref> compares EA-PSMNet with other state-of-the-art approaches and shows that it substantially outperforms all competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Temporal action localization</head><p>This section presents experiments for temporal action localization on the THUMOS14 <ref type="bibr" target="#b11">[12]</ref> dataset. The baseline is R-C3D <ref type="bibr" target="#b24">[25]</ref>. The experiment added two efficient attention modules after res3 and res4 in the ResNet-50 backbone. Ta-      1.09 CSPN <ref type="bibr" target="#b4">[5]</ref> 0.78 LEAStereo <ref type="bibr" target="#b5">[6]</ref> 0.78 EA-PSMNet 0.48 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper has presented the efficient attention mechanism, an attention mechanism that is quadratically more memory-and computationally-efficient than the widely adopted dot-product attention mechanism. By dramatically reducing the resource usage, efficient attention enables a large number of new use cases of attention, particularly in domains with tight resource constraints or large inputs.</p><p>The experiments verified its effectiveness on four distinct tasks, object detection, instance segmentation, and stereo depth estimation. It brought significant improvement for each task. On object detection and stereo depth estimation, efficient attention-augmented models have set new statesof-the-art. Besides the tasks this paper evaluated efficient attention on, it has promising potential in other fields where attention has demonstrated effectiveness. These fields include generative adversarial image modeling <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2]</ref> and most tasks in natural language processing <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>. Future plans include generalizing efficient attention to these fields, as well as other fields where the prohibitive costs have been preventing the application of attention.</p><p>A. Architecture details for experiments on MS-COCO 2017 <ref type="table" target="#tab_6">Table 9</ref> details the architecture the experiments used on MS-COCO 2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fine-grain metrics for experiments on MS-</head><p>COCO 2017 <ref type="table" target="#tab_0">Table 10</ref> presents fine-grain object detection metrics on MS-COCO 2017. <ref type="table" target="#tab_0">Table 11</ref> presents find-grain instance segmentation metrics on MS-COCO 2017.  <ref type="table" target="#tab_0">Table 10</ref>. Fine-grain metrics for experiments on MS-COCO 2017 object detection. +n NL means adding n non-local <ref type="bibr" target="#b22">[23]</ref> blocks to the backbone and FPN. +n EA means adding n EA modules to the backbone and FPN. OOM indicates out-of-memory errors Backbone AP AP-50 AP-75 AP-small AP-medium AP-large</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the architecture of dot-product and efficient attention. Each box represents an input, output, or intermediate matrix. Above it is the name of the corresponding matrix, and inside are the variable name and the size of the matrix. ?, ?q, ? k are the normalizers on S, Q, K, respectively. n, d, d k , dv are the input size and the dimensionalities of the input, the keys, and the values, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Resource requirements under different input sizes. The blue and orange bars depict the resource requirements of the efficient attention and non-local modules, respectively. The calculation assumes d = dv = 2d k = 64. This is a typical setting of self-attention for computer vision. The figure is in log scale. ory and O(d 2 n) computational complexities, assuming the common setting of d v = d, d k = d 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>shows visualization of the global attention maps for various examples from the efficient attention module at fpn1 in the model corresponding to the last row in Table 2. The figure illustrates 3 sets of global attention maps each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of global attention maps. The left-most column displays 4 images from MS-COCO 2017. The other three columns show three of the corresponding global attention maps from the efficient attention module at FPN level 1 for each respective example.area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of resource usage of the efficient attention and non-local modules. This table assumes that dv = d, d k = d 2 , which is a common setting in the literature for dot-product attention The batch size is 64. The learning rate is 1.25 ? 10 ?4 at the beginning of training and drops by a factor of 10 at the start of the 18th and 21st epochs. The experiments by default use softmax normalization, d k = d v = 64, and reprojection to the original number of channels.</figDesc><table><row><cell>Metric</cell><cell>Efficient attention module</cell><cell>Non-local module</cell></row><row><cell>Memory (floats) Computation (MACC)</cell><cell cols="2">4dn + d 2 2 (6d 2 + d)n (4d 2 + d)n + 3dn 2 4dn + n 2</cell></row><row><cell>Memory complexity</cell><cell>O(dn + d 2 )</cell><cell>O(dn + n 2 )</cell></row><row><cell>Comp. complexity</cell><cell>O(d 2 n)</cell><cell>O(d 2 n + dn 2 )</cell></row><row><cell cols="2">segmentation. The baseline is a ResNet-50 Mask R-CNN</cell><cell></cell></row><row><cell cols="2">with a 5-level feature pyramid [15]. More architectural de-</cell><cell></cell></row><row><cell cols="2">tails are in Appendix A. The backbones initialize from Im-</cell><cell></cell></row><row><cell cols="2">ageNet pretrainings. All other modules use random initial-</cell><cell></cell></row><row><cell cols="2">ization. All models trained for 24 epochs on 32 NVIDIA</cell><cell></cell></row><row><cell>TITAN Xp GPUs. 4.1. Comparison experiments</cell><cell></cell><cell></cell></row></table><note>4.1.1 Comparison with the non-local module</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison between the efficient attention and non-local modules on MS-COCO 2017 object detection and instance segmentation. Box, mask, mem., and comp. stand for box AP, mask AP, memory (in bytes), and computation (in MACC), respectively. Mem. and comp. only count the attention module(s). res{x} and fpn{x} indicate inserting attention modules after the x-th ResBlock group or FPN level x, respectively. res{x-y} and fpn{x-y} similarly mean inserting after every ResBlock group or FPN level within the range[x, y]    </figDesc><table><row><cell></cell><cell></cell><cell cols="2">EA module</cell><cell></cell><cell></cell><cell cols="2">Non-local module</cell><cell></cell><cell></cell></row><row><cell>Layer(s)</cell><cell cols="2">Box Mask</cell><cell cols="2">Mem. Comp.</cell><cell cols="2">Box Mask</cell><cell cols="2">Mem. Comp.</cell><cell>Input size</cell></row><row><cell>None</cell><cell>39.4</cell><cell>35.1</cell><cell>0</cell><cell>0</cell><cell>39.4</cell><cell>35.1</cell><cell>0</cell><cell>0</cell><cell>N/A</cell></row><row><cell>res3</cell><cell>40.2</cell><cell cols="3">36.0 41.3 M 1.21 G</cell><cell>40.3</cell><cell cols="3">35.9 122 M 3.74 G</cell><cell>56 ? 80</cell></row><row><cell>res4</cell><cell>40.2</cell><cell cols="3">35.9 19.5 M 596 M</cell><cell>40.1</cell><cell cols="3">36.0 24.5 M 748 M</cell><cell>28 ? 40</cell></row><row><cell>fpn1</cell><cell>39.9</cell><cell cols="6">35.8 220 M 5.28 G OOM OOM 20.8 G</cell><cell cols="2">662 G 224 ? 320</cell></row><row><cell>fpn2</cell><cell>39.7</cell><cell cols="8">35.7 55.1 M 1.32 G OOM OOM 1.34 G 42.3 G 112 ? 160</cell></row><row><cell>fpn3</cell><cell>39.7</cell><cell cols="3">35.5 13.8 M 330 M</cell><cell>39.8</cell><cell cols="3">35.5 94.0 M 2.86 G</cell><cell>56 ? 80</cell></row><row><cell>fpn4</cell><cell>39.7</cell><cell cols="3">35.4 3.46 M 82.6 M</cell><cell>39.5</cell><cell cols="3">35.3 8.46 M 234 M</cell><cell>28 ? 40</cell></row><row><cell>fpn5</cell><cell>39.6</cell><cell>35.3</cell><cell cols="2">877 K 20.6 M</cell><cell>39.4</cell><cell cols="3">35.2 1.17 M 28.4 M</cell><cell>14 ? 20</cell></row><row><cell cols="2">res3-4+fpn3-5 40.6</cell><cell cols="3">36.2 78.9 M 2.24 G</cell><cell>40.7</cell><cell cols="3">36.3 250 M 7.62 G</cell><cell>N/A</cell></row><row><cell cols="2">res3-4+fpn1-5 41.2</cell><cell cols="6">36.7 354 M 8.85 G OOM OOM 22.4 G</cell><cell>712 G</cell><cell>N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison vs. competing methods on MS-COCO 2017 object detection and instance segmentation. For each model, the number outside the parentheses is the AP, and the number inside is the AP improvement over baseline. The table reports number of parameters and amount of computation as a percentage increase over the baseline Mask R-CNN. The team obtained these metrics by measuring the official open-source implementations of<ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b26">27]</ref>. The table does not report results for CGNL with ResNet-101 and ResNeXt-101 since<ref type="bibr" target="#b26">[27]</ref> did not report such results. The Table omits parameters and computation for instance segmentation since all methods modified the backbone, which the bounding box and the instance mask branches share. Therefore, the table reports the total parameter and computation change only in the rows for object detection to avoid repetition</figDesc><table><row><cell cols="2">AP type Method</cell><cell cols="5">ResNet-50 ResNet-101 ResNeXt-101 Parameters Computation</cell></row><row><cell>Box</cell><cell>EA</cell><cell cols="2">(+1.8) 41.2 (+1.8) 43.1</cell><cell>(+1.4) 44.9</cell><cell>+2.9%</cell><cell>+5.3%</cell></row><row><cell></cell><cell cols="3">LatentGNN [29] (+1.7) 39.5 (+1.5) 41.0</cell><cell>(+1.1) 43.2</cell><cell>+11.1%</cell><cell>+7.6%</cell></row><row><cell></cell><cell>CGNL [27]</cell><cell>(+1.2) 35.7</cell><cell>-</cell><cell>-</cell><cell>+21.7%</cell><cell>+5.7%</cell></row><row><cell>Mask</cell><cell>EA</cell><cell cols="2">(+1.6) 36.7 (+1.3) 37.9</cell><cell>(+1.0) 39.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">LatentGNN [29] (+1.2) 35.4 (+1.3) 37.2</cell><cell>(+1.0) 38.8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CGNL [27]</cell><cell>(+0.8) 31.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Experiments on attention normalization methods on MS-COCO 2017 object detection and instance segmentation.ble 6 presents the results. At the table shows, efficient attention substantially improved the performance for this task.</figDesc><table><row><cell cols="3">Experiments inserted efficient attention modules at fpn1-5</cell></row><row><cell cols="3">Method Box AP Mask AP</cell></row><row><cell>Scaling</cell><cell>40.2</cell><cell>35.9</cell></row><row><cell>Softmax</cell><cell>40.2</cell><cell>36.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Experiments on the dimensionality of the keys on MS-COCO 2017 object detection and instance segmentation.</figDesc><table><row><cell>Ex-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .Table 7 .Table 8 .</head><label>678</label><figDesc>Experiments on THUMOS14 temporal action localization. mAP@x stands for mean average precision at IoU threshold x. EA R-C3D is this paper's model. Both models used ResNet-50 as the backbone Model mAP@0.1 mAP@0.2 mAP@0.3 mAP@0.4 mAP@0.5 mAP@0.6 mAP@0.7 Experiments on Scene Flow stereo depth estimation. EPE stands for end-point error and is lower the better. EA-PSMNet is this paper's model. OOM indicates out of memory. Memory only counts the attention module Comparison with the state-of-the-art on Scene Flow stereo depth estimation. EPE stands for end-point error and is lower the better. EA-PSMNet is this paper's model</figDesc><table><row><cell>R-C3D</cell><cell>54.2</cell><cell>54.1</cell><cell>50.0</cell><cell>45.6</cell><cell>37.3</cell><cell>29.2</cell><cell>18.5</cell></row><row><cell>EA R-C3D</cell><cell>60.3</cell><cell>59.8</cell><cell>56.8</cell><cell>51.3</cell><cell>43.4</cell><cell>33.2</cell><cell>21.8</cell></row><row><cell>Model</cell><cell cols="3">EPE Memory</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSMNet (original)</cell><cell cols="2">1.09</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSMNet (baseline)</cell><cell cols="2">0.51</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EA-PSMNet</cell><cell cols="3">0.48 796 MB</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Nonlocal-PSMNet OOM 9.68 TB</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>EPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">iResNet-i2 [17]</cell><cell>1.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">EdgeStereo [21] 1.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PSMNet [4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Architecture details for experiments on MS-COCO 2017 object detection and instance segmentation. This table assumes the backbone architecture is ResNet-50. For ResNet-101 and ResNeXt-101, the only difference will be the number of ResBlocks in each ResBlock group (res1-4) and/or the type of the blocks (ResNeXtBlock (32x4d) instead of ResBlock)</figDesc><table><row><cell>Block</cell><cell>Type</cell><cell>Input</cell><cell>Output size</cell></row><row><cell>input</cell><cell>Input</cell><cell>N/A</cell><cell>896 ? 1280</cell></row><row><cell>conv1</cell><cell>Conv 3 ? 3</cell><cell>input</cell><cell>448 ? 640</cell></row><row><cell cols="3">maxpool Maxpool 2 ? 2 conv1</cell><cell>224 ? 320</cell></row><row><cell>res1</cell><cell>ResBlock ? 3</cell><cell>maxpool</cell><cell>224 ? 320</cell></row><row><cell>res2</cell><cell>ResBlock ? 4</cell><cell>res1</cell><cell>112 ? 160</cell></row><row><cell>res3</cell><cell>ResBlock ? 6</cell><cell>res2</cell><cell>56 ? 80</cell></row><row><cell>res4</cell><cell>ResBlock ? 3</cell><cell>res3</cell><cell>28 ? 40</cell></row><row><cell>fpn5</cell><cell>conv 3 ? 3</cell><cell>res4</cell><cell>14 ? 20</cell></row><row><cell>fpn4</cell><cell>conv 3 ? 3</cell><cell>res4 + fpn5 (upsampled)</cell><cell>28 ? 40</cell></row><row><cell>fpn3</cell><cell>conv 3 ? 3</cell><cell>res3 + fpn4 (upsampled)</cell><cell>56 ? 80</cell></row><row><cell>fpn2</cell><cell>conv 3 ? 3</cell><cell>res2 + fpn3 (upsampled)</cell><cell>112 ? 160</cell></row><row><cell>fpn1</cell><cell>conv 3 ? 3</cell><cell>res1 + fpn2 (upsampled)</cell><cell>224 ? 320</cell></row><row><cell>rpn</cell><cell>RPN</cell><cell>fpn1-4</cell><cell>N/A</cell></row><row><cell>roi</cell><cell>RoI Align</cell><cell>fpn1-4</cell><cell>N/A</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning depth with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02695</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hierarchical neural architecture search for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13501</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4003" to="4014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Silco: Show a few images, localize the common object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Hong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5067" to="5076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Non-locally enhanced encoder-decoder network for single image de-raining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyou</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Videobased person re-identification via 3d convolutional networks and non-local attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouwang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05073</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02919</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Sj Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep self-attention networks for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thai-Son</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.13377</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Edgestereo: A context integrated residual pyramid network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Latent-GNN: Learning efficient non-local relations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fine-grain metrics for experiments on MS-COCO 2017 instance segmentation. +n NL means adding n non-local [23] blocks to the backbone and FPN. +n EA means adding n EA modules to the backbone and FPN. OOM indicates out-of-memory errors Backbone AP</title>
		<idno>AP-50 AP-75 AP-small AP-medium AP-large</idno>
	</analytic>
	<monogr>
		<title level="m">Table 11</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
