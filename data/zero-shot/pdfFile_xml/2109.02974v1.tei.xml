<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
							<email>ruiliu@link</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanming</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyi</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shi</surname></persName>
							<email>xiaoyushi@link</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<email>daijifeng@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hkdenghanming</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?#</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ? SenseTime Research ? Zhejiang University Tetras.AI # School of CST</orgName>
								<orgName type="institution" key="instit2">Xidian University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer, as a strong and flexible architecture for modelling long-range relations, has been widely explored in vision tasks. However, when used in video inpainting that requires fine-grained representation, existed method still suffers from yielding blurry edges in detail due to the hard patch splitting. Here we aim to tackle this problem by proposing FuseFormer, a Transformer model designed for video inpainting via fine-grained feature fusion based on novel Soft Split and Soft Composition operations. The soft split divides feature map into many patches with given overlapping interval. On the contrary, the soft composition operates by stitching different patches into a whole feature map where pixels in overlapping regions are summed up. These two modules are first used in tokenization before Transformer layers and de-tokenization after Transformer layers, for effective mapping between tokens and features. Therefore, sub-patch level information interaction is enabled for more effective feature propagation between neighboring patches, resulting in synthesizing vivid content for hole regions in videos. Moreover, in FuseFormer, we elaborately insert the soft composition and soft split into the feed-forward network, enabling the 1D linear layers to have the capability of modelling 2D structure. And, the sub-patch level feature fusion ability is further enhanced. In both quantitative and qualitative evaluations, our proposed FuseFormer surpasses state-of-the-art methods. We also conduct detailed analysis to examine its superiority. Code and pretrained models are available at https:// github.com/ruiliu-ai/FuseFormer. * The first three authors contribute equally to this work. Figure 1. Illustration of different patch split/composition strategies for Transformer model. The top row shows hard split/composition, based on which the trained model generates rough inpainting results. The bottom row shows soft split/composition, based on which the trained model generates smooth results due to interaction of features between neighbor patches. Double arrow indicates the corresponding overlapped regions between adjacent patches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformer has recently gained increasing attention in various vision tasks such as classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>, object detection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47]</ref> and image generation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref>. Interestingly, Transformer is suitable to video inpainting, a vision task that depends on the information propagation between flowing pixels across frames to fill the spatiotemporal holes with plausible and coherent content in a video clip.</p><p>Spatial Temporal Transformer Net (STTN) <ref type="bibr" target="#b42">[43]</ref> is the pioneer work for investigating the use of Transformer in video inpainting. However, its multi-scale variant of self-attention intertwined with fully convolutional networks makes it hard to exploit rich experience from other Transformer models due to large structural differences. On the other hand, re-cent Vision Transformer (ViT) <ref type="bibr" target="#b7">[8]</ref> demonstrates the strong capability of vanilla Transformer <ref type="bibr" target="#b33">[34]</ref> in vision recognition task. These motivate us to build a Video inpainting Baseline with vanilla Transformer (ViB-T), which differs from ViT in 2 aspects: a) the tokens are embedded from patches of multiple frames instead of a single frame; b) a light convolutional encoder and decoder before and after Transformer block is exploited to relieve the computational burden caused by high resolution frames. Experiment verifies that this simple baseline can reach competitive performance with STTN <ref type="bibr" target="#b42">[43]</ref> under similar computation cost.</p><p>Nevertheless, similar to all existing patch-based Transformer models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>, the hard split operation used in ViB-T makes it unable to effectively encode sub-token (subpatch) level representations. Since the attention score is calculated between different tokens, there is no direct subtoken level feature interaction. For us, human beings, fragmenting an image into many non-overlapping patches poses a challenging task to composite them back into an original image with masked regions filled. This is the same for deep learning systems: the lack of accurate sub-token level feature interaction can lead to inconsistent content between neighboring patches. As shown in <ref type="figure">Fig.1</ref>, to accurately rebuild the black circle on the canvas, every token corresponding to an image patch has to understand not only the patch level information but also sub-patch level information. As a result, in order to fully unleash the power of Transformers in video inpainting tasks, an improved patch splitting manner and a better sub-token level feature fusion mechanism to maintain pixel level-feature accuracy is in demand.</p><p>To achieve this goal, we propose a Soft Split (SS) module as well as its corresponding Soft Composition (SC) module. Built upon the simple and straightforward ViB-T baseline model, we propose to softly split images into patches with overlapping regions and correspondingly, to softly composite these overlapped patches back to images. Specifically, in the soft split module, we exploit an unfold operation with kernel size greater than stride to softly split the input image into overlapping 2D patches and are flattened as 1D tokens. On the contrary, in the soft composition module, tokens are reshaped to 2D patches maintaining their original sizes, and then each pixel is registered to its original spatial location according to the kernel size and stride used in soft split module. During this process, features of the pixels located in the overlapping area are fused from multiple overlapping neighboring patches' corresponding areas, thus providing sub-token level feature fusion. We design a baseline ViB-T model equipped with the Soft Split and Soft Composition modules as ViB-S where S stands for soft operations. And we find that the ViB-S model easily surpasses the state-of-the-art video inpainting model STTN <ref type="bibr" target="#b42">[43]</ref> with minimum extra computation cost.</p><p>Finally, we propose a Fusion Feed Forward Network (F3N) to replace the two-layer MLPs in the standard Transformer model, which is dubbed as FuseFormer, to further improve its sub-token fusion ability for learning finegrained feature, yet without extra parameters. In the F3N, between the two fully-connected layers, we reshape each 1D token back to 2D patch with its original spatial shape and then softly composite them to be a whole image. The overlapping features of pixel at overlapping regions would sum up the corresponding value from all neighboring patches for further fine-grained feature fusion. Then the patches are softly split and flattened into 1D vectors, which are fed to the second MLP. In this way, sub-token segment corresponding to the same pixel location are matched and registered without extra learnable parameters, and information of the same pixel location from different patches are aggregated. Subsequently, our FuseFormer model consisting of F3N even surpasses our strong baseline ViB-S by a significant margin, both qualitatively and quantitatively. Based on these novel designs, our proposed FuseFormer network achieves effective and efficient performance in video restoration and object removal. We testify the superiority of the proposed model to other state-of-the-art video inpainting approaches by thorough qualitative and quantitative comparisons. We further conduct ablation study to show how each component of our model benefits the inpainting performance.</p><p>In summary, our contributions are three-fold:</p><p>1. We first propose a simple yet strong Transformer baseline for video inpainting, and propose a soft split and composition method to boost its performance.</p><p>2. Based on the proposed strong baseline and novel soft operations, we propose FuseFormer, a sub-token fusion enabled Transformer model with no extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Extensive experiments demonstrate the superiority of</head><p>FuseFormer over state-of-the-art approaches in video inpainting, both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image Inpainting. In traditional image inpainting, the target holes are usually filled by sampling and pasting the known textures and significant progress has been made on this type of image inpainting approach <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. PatchMatch <ref type="bibr" target="#b0">[1]</ref> proposes to fill the missing region by searching the patches outside the hole based on the approximate nearest neighbor algorithm, which is finally served as a commercial product.</p><p>With the rise of deep neural network <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref> and generative adversarial network <ref type="bibr" target="#b11">[12]</ref>, some works investigated on building an end-to-end deep neural network for image inpainting task with the auxiliary discriminator and adversarial loss <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b16">17]</ref>. After that, DeepFill propose to use a contextual attention for filling target holes by propagating the feature outside the region <ref type="bibr" target="#b40">[41]</ref>. Then Liu et al. and Yu et al. apply partial convolution <ref type="bibr" target="#b24">[25]</ref> and gated convolution <ref type="bibr" target="#b39">[40]</ref> to make vanilla convolution kernels aware of given mask guidance respectively, so as to complete free-form image inpainting. Video Inpainting. Building upon patch-based image inpainting, Newson et al. extend PatchMatch algorithm <ref type="bibr" target="#b0">[1]</ref> to video for further modelling the temporal dependencies and accelerating the process of patch matching <ref type="bibr" target="#b26">[27]</ref>. Strobel et al. <ref type="bibr" target="#b32">[33]</ref> introduce an accurate motion field estimation for capturing object movement. Huang et al. perform an alternate optimization on 3 steps including patch search, color completion and motion field estimation and obtain successful video completion performance <ref type="bibr" target="#b14">[15]</ref>.</p><p>Deep learning also boosts the performance of video inpainting. Wang et al. proposes a groundbreaking deep neural network that combines 2D and 3D convolution seamlessly for completing missing contents in video <ref type="bibr" target="#b34">[35]</ref>. Kim et al. propose a recurrent neural network to cumulatively aggregate temporal features through traversing all video sequences <ref type="bibr" target="#b18">[19]</ref>. Xu et al. use existing flow extraction tools to obtain robust optical flow and then warp the regions from reference frames to fill the hole in target frame <ref type="bibr" target="#b38">[39]</ref>. Lee et al. propose a copy-and-paste network that learns to copy corresponding contents in reference frames and paste them to fill the holes in the target frame <ref type="bibr" target="#b22">[23]</ref>. Chang et al. develop a learnable Gated Temporal Shift Module and adapt gated convolution <ref type="bibr" target="#b39">[40]</ref> to a 3D version for performing freeform video inpainting <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>. Zhang et al. adopts internal learning to train one-size-fits-all model for different given videos <ref type="bibr" target="#b43">[44]</ref>. Hu et al. propose a region proposal-based strategy for picking up best inpainted result from many participants <ref type="bibr" target="#b13">[14]</ref>. Recently attention mechanisms are adopted to further promote both realism and temporal consistency via capturing long-range correspondences in video sequences. Temporally-consistent appearance is implicitly learned and propagated to the target frame with a frame-level attention <ref type="bibr" target="#b28">[29]</ref> and dynamic long-term context aggregation module <ref type="bibr" target="#b23">[24]</ref>. Transformers in Vision. Transformers are firstly proposed in 2017 <ref type="bibr" target="#b33">[34]</ref> and gradually dominated natural language processing models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26]</ref>. A Transformer block basically consists of a multi-head attention module for modelling long-range correspondence of the input vector and a multilayer perceptron for fusing and refining the feature representation. In computer vision, it has been adapted to various tasks such image classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42]</ref>, object detection and segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b10">11]</ref>, image generation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref>, video segmentation <ref type="bibr" target="#b36">[37]</ref>, video captioning <ref type="bibr" target="#b45">[46]</ref> and so on in past two years.</p><p>As far as our knowledge concerns, STTN <ref type="bibr" target="#b42">[43]</ref> is the only work for investigating the use of Transformer in video inpainting and propose to learn a deep generative Transformer model along spatial-temporal dimension. It roughly splits frames into non-overlapped patches with certain given patch size and then feeds the obtained spatiotemporal patches into a stack of Transformer encoder blocks for thorough spatiotemporal propagation. However, it suffers from capturing local texture like edges and lines and modelling the arbitrary pixel flowing. In this work, we propose a novel Transformer-based video inpainting framework endorsed by 2 carefully-designed soft operations, which improve the performance on both video restoration and object removal and make the inference much faster as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we introduce our FuseFormer model for video inpainting. We start by proposing a simple Transformer baseline, named ViB-T (Video inpainting Baseline with vanilla Transformer), then we introduce our novel designs step by step by first introducing our Soft Split (SS) and Soft Composition (SC) technique, which boost the performance of ViB-T. We term ViB-T with SS and SC as ViB-S. Finally, build upon ViB-S, we introduce FuseFormer, a fine-grained vision Transformer block whose regular feed forward network is replaced with fusion feed forward network, and term the final model as ViF (Video inpainting with FuseFormer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video inpainting Baseline with Transformer</head><p>We start by proposing a straightforward baseline model ViB-T for directly deploying patch-based Transformer in video inpainting without complex modifications. It consists of three parts: a) a convolutional encoder and a corresponding decoder; b) a stack of Transformer blocks between the encoder and decoder; and c) a pair of patch-to-token and token-to-patch module. The patch-to-token module locates between the convolutional encoder and the first Transformer block, and token-to-patch locates between the last Transformer block and the convolutional decoder. Different from STTN <ref type="bibr" target="#b42">[43]</ref>, this baseline model's Transformer block is the same as standard Transformer <ref type="bibr" target="#b33">[34]</ref> where there is neither the scheme of multi-scale frames for different multi-head self-attention nor using 3 ? 3 convolution to replace linear layers in feed forward network. Patches are hard split from feature map and linearly embedded to feature vectors with much lower channel dimension, which is more computationally friendly for following processing.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, given corrupted video frames f i ? R h?w?3 , i ? [0, t), it would work as follows:</p><p>First, it encodes video frames with a CNN encoder, obtaining c channel convolutional feature maps of frames X i ? R h/4?w/4?c , i ? [0, t), and each X is split into k ?k smaller patches with stride s. Then all patches are linearly embedded into tokens Z ? R (t?n)?d , where n is the number of tokens in one image and d is the token channel.</p><p>Second, Z is fed into standard Transformer blocks for spatial-temporal information propagation, resulting in refined tokensZ ? R (t?n)?d .</p><p>Third, each refined tokenz i ? R d , i ? [0, n ? t) from Z is linearly transformed to k ? k ? c channel vector and reshaped to patch shape k ? k ? c. All the resulting patches are registered back to its original frame's location pixel by pixel, obtaining feature mapsX i ? R h/4?w/4?c , i ? [0, t). This re-composited feature map is of the same size as the feature map input to the first Transformer block.</p><p>Finally, the re-composited feature mapsX are decoded with a couple of deconvolution layers to output the in-</p><formula xml:id="formula_0">painted video framesf i ? R h?w?3 , i ? [0, t) with original size.</formula><p>For the baseline model ViB-T, we set kernel size equal to the stride in patch splitting. As a starting point, this simple model already has competitive performance with STTN <ref type="bibr" target="#b42">[43]</ref> but with faster inference speed and fewer parameters (refer to appendix C).</p><p>The key of our proposed method is the sub-token level fine-grained feature fusion, which is realized by the newlyproposed Soft Split (SS) and Soft Composite (SC) processing, it enables precise sub-token level fusion between neighboring patches. In the following section, we will first introduce the SS and SC modules, based on which we introduce our proposed FuseFormer in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Soft Split (SS) and Soft Composite (SC)</head><p>Different from STTN <ref type="bibr" target="#b42">[43]</ref> that roughly split frames into patches without overlapping region, here we propose to softly split each frame into overlapped patches and then softly composite them back, by using an unfold and fold operator with patch size k being greater than patch stride s. When compositing patches back to its original spatial shape, we add up feature values at each overlapping spatial location of neighboring patches.</p><p>Soft Split (SS). As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, it softly split each feature map into overlapped patches of size k?k with stride s &lt; k, and flattened to a one-dimensional token, which is similar to the image spliting strategy in T2T-ViT <ref type="bibr" target="#b41">[42]</ref>. The number of tokens is then</p><formula xml:id="formula_1">n = h + 2 ? p ? k s + 1 ? w + 2 ? p ? k s + 1 ,<label>(1)</label></formula><p>where p is the padding size.</p><p>Soft Composite (SC). The SC operator composites the softly split n patches by their original spatial location and form a new feature map with the same h and w as original feature map size. However, due to the existence of overlapping area, the SC operator sums up pixel values that overlapped on the same spatial location, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. This design of soft split and composition lays foundation for our final FuseFormer, as when softly compositing patches back to its original position after Transformer processing, the overlapped position aggregated a piece of information from different tokens, contributing to smoother patch boundaries and enlarges its receptive field by fusing information from neighboring patches. As our experiment shows, the baseline model equipped with these two operators, dubbed as ViB-S, have already surpassed the state-of-the-art video inpainting performance reached by STTN <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">FuseFormer</head><p>A FuseFormer block is the same to standard Transformer block except that feed forward network is replaced with our proposed Fusion Feed Forward Network (F3N). Given input patch tokens Z l at l-th stack where l ? [0, L), L is the stacking number of FuseFormer blocks, a FuseFormer block can be formulated as:</p><formula xml:id="formula_2">Z l = MSA(LN 1 (Z l?1 )) + Z l , (2) Z l+1 = F3N(LN 2 (Z l )) + Z l ,<label>(3)</label></formula><p>where the MSA and LN respectively denote standard multihead self-attention and layer normalization in Transformers <ref type="bibr" target="#b33">[34]</ref> and our key difference from other Transformers lies in the newly-proposed Fusion Feed Forward Network (F3N).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion Feed Forward Network (F3N)</head><p>. F3N brings no extra parameter into the standard feed forward net and the difference is that F3N inserts a SC and a SS operation between the two layer of MLPs. For clear formulation, we let F = F3N(F ) = F3N(LN 2 (Z l )) where F , F ? R tn?d and the mapping functions are the same to Equ. 3. Let f i , f i be the token vectors from F , F where i ? [0, t ? n), so the F3N can be formulated as</p><formula xml:id="formula_3">p i = MLP 1 (f i ), i ? [0, t ? n) (4) A j = SC(p j,0 , ..., p j,n?1 ), j ? [0, t) (5) p j,0 , ..., p j,n?1 = SS(A j ), j ? [0, t) (6) f i = MLP 2 (p i ), i ? [0, t ? n) (7)</formula><p>where MLP 1 and MLP 2 denote the vanilla multi-layer perceptron. SC denotes soft composition for composing those 1-D vectors p j,0 , ..., p j,n?1 to a 2-D feature map A j and SS denotes the soft split for splitting A j into 1-D vectors p j,0 , ..., p j,n?1 . Note that there is a feature fusion processing during the mapping p i = SS(SC(p i )).</p><p>Besides the introduction of soft composition and soft split module, there is another difference between F3N and FFN. In FFN, the input and output channel of MLP 1 and MLP 2 are (4 ? d, d) and (d, 4 ? d), respectively. On the contrary, in F3N, we change the input and output channel of the two MLPs to (d, k 2 ? c ) and (k 2 ? c , d), where c = 10 ? 4 ? d/(10 ? k 2 ) , which aims to ensure the intermediate feature vectors are able to be reshaped to feature 2-D maps.</p><p>For each soft composition module in F3N, different pixel locations may correspond to various number of overlapping patches, which leads to large variance on pixel value. Meanwhile, the spatial location of the reshaped patch is actually mixed up after passing through the MLP 1 . Therefore, we introduce a normalization for Equ. 5. Let 1 ? R n?(k 2 ?c ) be the vectors where all elements' value are 1, so the normalized SC can be formulated as:</p><formula xml:id="formula_4">A j = SC(p j,0 , ..., p j,n?1 ) SC(1) , j ? [0, t)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Objective</head><p>We train our network by minimizing the following loss:</p><formula xml:id="formula_5">L = ? R ? L R + ? adv ? L adv ,<label>(9)</label></formula><p>where L R is the reconstruction loss for all pixels, L adv is the adversarial loss from GAN <ref type="bibr" target="#b11">[12]</ref>, ? R and ? adv weigh the importance of different loss functions. For reconstruction loss, L1 loss is utilized for measuring the distance between synthesized video? and original one Y. It can be formulated as</p><formula xml:id="formula_6">L R = (? ? Y) 1<label>(10)</label></formula><p>In addition, following <ref type="bibr" target="#b42">[43]</ref>, we also adopt a discriminator D for assisting training the FuseFormer generator, in order to obtain a better synthesis realism and temporal consistency. This discriminator takes both real videos and synthesized ones as input and outputs a scalar ranging in [0, 1] where 0 indicates fake and 1 indicates true. It is trained toward the direction that all the synthesized videos could be distinguished from real ones. The FuseFormer generator is trained towards an opposite direction where it generates videos that can not be told by D anymore. The loss function for D is formulated as</p><formula xml:id="formula_7">L D = E Y [log D(Y] + E? log (1 ? D(?))<label>(11)</label></formula><p>And the loss function for the FuseFormer generator is   <ref type="bibr" target="#b42">[43]</ref>, a test set including 60 video clips is split from the whole dataset for fair comparison with other approaches. We do not use this dataset for training. Network and training. We use 8 stacks of Transformer (FuseFormer) layers in our ViB-T, ViB-S and ViF models, whose token dimension is 512. For ViF, the token is expanded to 1960 instead of 2048 for patch reshape compatibility. Other network structures including the CNN encoder, decoder and discriminator are the same as STTN <ref type="bibr" target="#b42">[43]</ref>, except that we insert several convolutional layers between encoder and the first Transformer block to compensate for aggressive channel reduction in patch tokenization. Note that different from STTN <ref type="bibr" target="#b42">[43]</ref>, we do not finetune our model on DAVIS training set and the same checkpoint is used for evaluation on both YouTube-VOS test set and DAVIS test set. In all our ablations, we train our model with Adam optimizer <ref type="bibr" target="#b19">[20]</ref> for 250k iterations. At each iteration, 5 random frames from one video is sampled on each GPU and 8 GPU is utilized. The initial learning rate is 0.01 and is reduced by factor of 10 at 200k iteration. For our fair comparison with state-of-the-art models, we train our best model for 500k iterations, and the learning rate is reduced at 400k and 450k iterations respectively. Evaluation metrics. First, we take Video-based Fr?chet In- ception Distance (VFID) as our metric for scoring the perceptually visual quality by comparing with natural video sequences <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref>. Lower value suggests better realism and visually closer to natural videos. We also use a optical flowbased warping error E warp for measuring the temporal consistency <ref type="bibr" target="#b21">[22]</ref>. Lower value indicates better temporal consistency. Finally, we use two popular metrics for measuring the quality of reconstructed image compared with original one: Structural SIMilarity (SSIM) and Peak Signal to Noise Ratio (PSNR). The score is calculated frame by frame and their mean value is reported. Higher value of these two metrics indicates better reconstruction quality.</p><formula xml:id="formula_8">L adv = E? log D(?)<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablations</head><p>The effectiveness of soft split and soft composition. In Tab. 4.1 we show the performance under different patch size used in soft split and soft composition operation on our baseline model ViB-T and ViB-S. For ViB-T, we keep the stride the same as the patch size. For ViB-S and TiF, they share the same stride 3 to ensure the same number of tokens for each frames. First, by changing the patch size for ViB-T, we find that ViB-T with patch size 3, a straight-forward variant of Transformer has already achieved competitive performance compared to the state-of-the-art STTN <ref type="bibr" target="#b42">[43]</ref>, even without soft split and soft composition operations. For ViB-S and ViF, when patch size is larger than 3, SS and SC operations are incorporated to handle overlap area between patches. All larger patches improves the performance for a significant margin, showing the effectiveness of overlapping patches.</p><p>Here we further vary the patch size between SS and SC, limiting the overlapping area to appear in either SS or SC operations. Apart from SS, the overlapped composition in SC can also improve the performance even without SS.  inpainting, FuseFormer has slightly fewer parameters and negligible time cost but enabled the sub-token level finegrain feature fusion. <ref type="figure" target="#fig_2">Fig.4</ref> further illustrates the qualitative results of VIB-S and ViF, demonstrating that their better performance comes from more detailed inpainting results, showing the effectiveness of sub-token level feature fusion.  </p><formula xml:id="formula_9">Accuracy YouTube-VOS DAVIS Models PSNR ? SSIM ? VFID ? E warp (?10 ?2 ) ? PSNR ? SSIM ? VFID ? E warp (?10 ?2 ) ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with other methods</head><p>Qualitative comparison. In <ref type="figure" target="#fig_3">Fig.5</ref> we show the qualitative results of our model compared with state-of-the-art methods including CAP <ref type="bibr" target="#b22">[23]</ref>, LGTSM <ref type="bibr" target="#b4">[5]</ref>, and STTN <ref type="bibr" target="#b42">[43]</ref> and our proposed FuseFormer synthesize the most realistic and temporally-coherent videos. Quantitative comparison. In Tab.2 we show the performance comparison with state-of-the-art models on video completion, evaluated on both YouTubeVOS. Our ViF model outperforms all the state-of-the-art video inpainting approaches in video restoration by improving PSNR and SSIM by 3.3% and 0.7%, and it yields videos with best real- ism and temporal coherence by reducing VFID and warping error by 7.4% and 7.8%. User study. We choose CAP <ref type="bibr" target="#b22">[23]</ref> and STTN <ref type="bibr" target="#b42">[43]</ref>, two of the state-of-the-art video inpainting models as our baselines for user study. 30 videos are randomly sampled from DAVIS <ref type="bibr" target="#b30">[31]</ref> for object removal and video completion evaluation. 38 volunteers has participated this user study. Videos processed by 3 models are presented at each time for volunteers to rank the inpainting quality. On our dedicated software for this user study, volunteers can stop/replay any video until they make final judgement. The percentage of first ranking model from each user on each video are shown in <ref type="figure" target="#fig_6">Fig.8</ref>, where for both object removal and video completion we have the best performance. Visualizing inpainting process. <ref type="figure" target="#fig_4">Fig.6</ref> demonstrates images decoded at different layer of ViF, showing the process of how the our model inpaints a video frame. We can see it starts with coarse context information and gradually refine features in deeper layers. In <ref type="figure" target="#fig_5">Fig.7</ref>, we further show the detailed attention process between different multi-frame patches in an object removal task. We can see how our proposed model accurately find reference patch and explore the spatiotemporal information to inpaint the background as well as the pillar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we propose FuseFormer, a Transformer model designed for video inpainting via fine-grained feature fusion. It aims at tackling the drawbacks of lacking fine-grained information in patch-based Transformer models. The soft split divides feature map into many patches with given overlapping interval while the soft composition stitches them back into a whole feature map where pixels in overlapping regions are summed up. FuseFormer elaborately builds soft composition and soft split into its feedforward network for further enhancing sub-patch level feature fusion. Together with our strong Transformer baseline, our FuseFormer model achieve state-of-the-art performance in video restoration and object removal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustrations of our proposed FuseFormer. On the left is our proposed video inpainting pipeline with Transformers. On the right is our proposed FuseFormer block and Fusion Feed Forward Network (F3N). The tuple indicates the counting number of patch along spatial dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The illustration of Soft Split (SS) and Soft Composite (SC) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of our proposed ViB-S and ViF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison with other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Image decoded from different layers of our trained ViF. It shows that images are refined in a coarse to fine manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of attention between patches cross multiple frames in object removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>User study results. Percentage of ranking first among 38 viewers of 30 videos on video completion and object removal task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of video completion on YouTube-VOS and DAVIS dataset. : our evaluation results following descriptions in STTN<ref type="bibr" target="#b42">[43]</ref>, the numerical differences may result from different optical flow models in the evaluation process.The effectiveness of F3N in FuseFormer. As shown in Tab.4.1, by replacing standard Transformer block with our proposed FuseFormer block in ViB-S, the performance is boosted significantly, showing the effectiveness of subtoken level feature fusion. Moreover, with the proposed normalizing technique in Equ.8, the performance has been further improved. Compared to standard Transformer in video</figDesc><table><row><cell>VINet [19]</cell><cell>29.20</cell><cell>0.9434</cell><cell>0.072</cell><cell>0.1490 / -</cell><cell>28.96</cell><cell>0.9411</cell><cell>0.199</cell><cell>0.1785 / -</cell></row><row><cell>DFVI [39]</cell><cell>29.16</cell><cell>0.9429</cell><cell>0.066</cell><cell>0.1509 / -</cell><cell>28.81</cell><cell>0.9404</cell><cell>0.187</cell><cell>0.1880 / 0.1608  *</cell></row><row><cell>LGTSM [5]</cell><cell>29.74</cell><cell>0.9504</cell><cell>0.070</cell><cell>0.1859 / -</cell><cell>28.57</cell><cell>0.9409</cell><cell>0.170</cell><cell>0.2566 / 0.1640  *</cell></row><row><cell>CAP [23]</cell><cell>31.58</cell><cell>0.9607</cell><cell>0.071</cell><cell>0.1470 / -</cell><cell>30.28</cell><cell>0.9521</cell><cell>0.182</cell><cell>0.1824 / 0.1533  *</cell></row><row><cell>STTN [43]</cell><cell>32.34</cell><cell>0.9655</cell><cell>0.053</cell><cell>0.1451 / 0.0884  *</cell><cell>30.67</cell><cell>0.9560</cell><cell>0.149</cell><cell>0.1779 / 0.1449  *</cell></row><row><cell>ViB-S</cell><cell>32.47</cell><cell>0.9635</cell><cell>0.056</cell><cell>-/ 0.0889  *</cell><cell>31.50</cell><cell>0.9636</cell><cell>0.144</cell><cell>-/ 0.1346  *</cell></row><row><cell>ViF</cell><cell>33.16</cell><cell>0.9673</cell><cell>0.051</cell><cell>-/ 0.0875  *</cell><cell>32.54</cell><cell>0.9700</cell><cell>0.138</cell><cell>-/ 0.1336</cell></row></table><note>**</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 27th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simultaneous structure and texture image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="882" to="889" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Free-form video inpainting with 3d gated convolution and temporal patchgan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learnable gated temporal shift module for deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image Melding: Combining inconsistent images using patch-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 2012)</title>
		<meeting>SIGGRAPH 2012)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Texture synthesis by nonparametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast convergence of DETR with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2101.07448</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Proposal-based video completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporally coherent completion of dynamic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Generative adversarial transformers</title>
		<imprint>
			<date type="published" when="1209" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Globally and Locally Consistent Image Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Transgan: Two transformers can make one strong gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07074</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Copy-and-paste networks for deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyeun</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Short-term and long-term context aggregation network for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramamohanarao</forename><surname>Kotagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video Inpainting of Complex Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alasdair</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Fradet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="page" from="1993" to="2019" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel Synnaeve Nicolas Usunier Alexander Kirillov Sergey Zagoruyko Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Onion-peel networks for deep video completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flow and color inpainting for video completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Diebold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video inpainting by jointly learning temporal structure and spatial details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Videoto-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Endto-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep flow-guided video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<title level="m">Free-form image inpainting with gated convolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An internal learning approach to video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2720" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">End-to-end object detection with adaptive clustering transformer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
