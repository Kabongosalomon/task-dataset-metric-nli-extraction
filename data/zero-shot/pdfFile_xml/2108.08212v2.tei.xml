<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Confidence Adaptive Regularization for Deep Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdi</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Software</orgName>
								<orgName type="institution">McMaster University Hamilton</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing and Software</orgName>
								<orgName type="institution">McMaster University</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing and Software</orgName>
								<orgName type="institution">McMaster University</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Confidence Adaptive Regularization for Deep Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies on the memorization effects of deep neural networks on noisy labels show that the networks first fit the correctly-labeled training samples before memorizing the mislabeled samples. Motivated by this early-learning phenomenon, we propose a novel method to prevent memorization of the mislabeled samples. Unlike the existing approaches which use the model output to identify or ignore the mislabeled samples, we introduce an indicator branch to the original model and enable the model to produce a confidence value for each sample. The confidence values are incorporated in our loss function which is learned to assign large confidence values to correctly-labeled samples and small confidence values to mislabeled samples. We also propose an auxiliary regularization term to further improve the robustness of the model. To improve the performance, we gradually correct the noisy labels with a well-designed target estimation strategy. We provide the theoretical analysis and conduct the experiments on synthetic and real-world datasets, demonstrating that our approach achieves comparable results to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We conduct the experiments on the CIFAR-10 dataset with 40% symmetric label noise using ResNet34 <ref type="bibr">[5]</ref>. The top row shows the fraction of samples with clean labels that are predicted correctly (purple) and incorrectly <ref type="bibr">(black)</ref>. In contrast, the bottom row shows the fraction of samples with false labels that are predicted correctly (purple), memorized (i.e. the prediction equals the false label, shown in blue), and incorrectly predicted as neither the true nor the labeled class (black). For samples with clean labels, all three models predict them correctly with the increasing of epochs. However, for false labels in (a), the model trained with cross-entropy loss first predicts the true labels correctly, but eventually memorizes the false labels. With the cosine annealing learning rate scheduler <ref type="bibr">[12]</ref> in (b), the model only slows down the speed of memorizing the false labels. However, our approach shown in (c) effectively prevents memorization, allowing the model to continue learning the correctly-labeled samples to attain high accuracy on samples with both clean and false labels.</p><p>? We introduce an indicator branch to estimate the confidence of model prediction and propose a novel loss function called confidence adaptive loss (CAL) to exploit the early-learning phase. A high confidence value is likely to be associated with a clean sample and a low confidence value with a mislabeled sample. Then, we add an auxiliary regularization term forming a confidence adaptive regularization (CAR) to further segregate the mislabeled samples from the clean samples. We also develop a strategy to estimate the target probability instead of using the noisy labels directly, allowing the proposed model to suppress the influence of the mislabeled samples successfully.</p><p>? We theoretically analyze the gradients of the proposed loss functions and compare them with crossentropy loss. We demonstrate that CAL and CAR have similar effects to existing regularizationbased approaches. Both neutralize the influence of the mislabeled samples on the gradient, and ensure the contribution from correctly-labeled samples to the gradient remains dominant. We also prove the robustness of the auxiliary regularization term to label noise.</p><p>? We show that the proposed approach achieves comparable and even better performance to the state-of-the-art methods on four benchmarks with different types and levels of label noise. We also perform an ablation study to evaluate the influence of different components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We briefly discuss the related noise-robust methods that do not require a set of clean training data (as opposed to <ref type="bibr">[13]</ref><ref type="bibr">[14]</ref><ref type="bibr">[15]</ref><ref type="bibr">[16]</ref><ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref>) and assume the label noise is instance-independent (as opposed to <ref type="bibr" target="#b2">[20,</ref><ref type="bibr" target="#b3">21]</ref>).</p><p>Loss correction These approaches focus on correcting the loss function explicitly by estimating the noise transition matrix <ref type="bibr" target="#b4">[22]</ref><ref type="bibr" target="#b5">[23]</ref><ref type="bibr" target="#b6">[24]</ref><ref type="bibr" target="#b7">[25]</ref>. Robust loss functions These studies develop loss functions that are z <ref type="bibr">[i]</ref> p <ref type="bibr">[i]</ref> (a) (b) (c) <ref type="figure">Figure 2</ref>: In (a), we introduce an indicator branch in addition to the prediction branch. Given an input image x <ref type="bibr">[i]</ref> , the indicator branch produces a single scalar value ? <ref type="bibr">[i]</ref> to indicate confidence and the prediction branch produces the softmax prediction probability p <ref type="bibr">[i]</ref> . (b) and (c) show the density of confidence ? on the CIFAR-10 and CIFAR-100 with 40% symmetric label noise respectively.</p><p>robust to label noise, including L DMI <ref type="bibr" target="#b8">[26]</ref>, MAE <ref type="bibr" target="#b9">[27]</ref>, GCE <ref type="bibr" target="#b10">[28]</ref>, IMAE <ref type="bibr" target="#b11">[29]</ref>, SL <ref type="bibr" target="#b12">[30]</ref> NCE <ref type="bibr" target="#b13">[31]</ref> and TCE <ref type="bibr" target="#b14">[32]</ref>. Above two categories of methods do not utilize the early learning phenomenon.</p><p>Sample selection During the early learning stage, the samples with smaller loss values are more likely to be the correctly-labeled samples. Based on this observation, MentorNet <ref type="bibr" target="#b15">[33]</ref> pre-trains a mentor network for selecting small-loss samples to guide the training of the student network. Co-teaching related methods <ref type="bibr" target="#b16">[34]</ref><ref type="bibr" target="#b17">[35]</ref><ref type="bibr" target="#b18">[36]</ref><ref type="bibr" target="#b19">[37]</ref> maintain two networks, and each network is trained on the small-loss samples selected by its peer network. However, their limitation is that they may eliminate numerous useful samples for robust learning. Label correction <ref type="bibr" target="#b20">[38,</ref><ref type="bibr" target="#b21">39]</ref> replace the noisy labels with soft (i.e. model probability) or hard (i.e to one-hot vector) pseudo-labels. Bootstrap <ref type="bibr" target="#b22">[40]</ref> corrects the labels by using a convex combination of noisy labels and the model predictions. SAT <ref type="bibr" target="#b23">[41]</ref> weighs the sample with its maximal class probability in cross-entropy loss and corrects the labels with model predictions. <ref type="bibr" target="#b24">[42]</ref> weighs the clean and mislabeled samples by fitting a two-component Beta mixture model to loss values, and corrects the labels via convex combination as in <ref type="bibr" target="#b22">[40]</ref>. Similarly, DivideMix <ref type="bibr" target="#b25">[43]</ref> trains two networks to separate the clean and mislabeled samples via a two-component Gaussian mixture model, and further uses MixMatch <ref type="bibr" target="#b26">[44]</ref> to enhance the performance. Regularization [10] observes that when the model parameters remain close to the initialization, gradient descent implicitly ignores the noisy labels. Based on this observation, they prove the gradient descent early stopping is an effective regularization to achieve robustness to label noise. <ref type="bibr" target="#b27">[45]</ref> explicitly adds the regularizer based on neural tangent kernel <ref type="bibr" target="#b28">[46]</ref> to limit the distance between the model parameters to initialization. ELR [11] estimates the target probability by temporal ensembling <ref type="bibr" target="#b29">[47]</ref> and adds a regularization term to cross entropy loss to avoid memorization. Other regularization techniques, such as mixup augmentation <ref type="bibr" target="#b30">[48]</ref>, label smoothing <ref type="bibr" target="#b31">[49]</ref> and weight averaging <ref type="bibr" target="#b32">[50]</ref>, can enhance the performance.</p><p>Our approach is related to regularization and label correction. Compared with existing approaches <ref type="bibr" target="#b27">[45,</ref><ref type="bibr">11]</ref>, where a regularization term in loss function is necessary to resist mislabeled samples, we propose a loss function CAL which implicitly boosts the gradients of correctly labeled samples and diminishes the gradients of mislabeled samples. The auxiliary regularization term in our approach is an add-on component to further improve the performance in more challenging cases. We then propose a novel strategy to estimate the target and correct the noisy labels. In addition, our approach is simpler and yields comparable performance without applying other regularization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section presents a framework called confidence adaptive regularization (CAR) for robust learning from noisy labels. Our approach consists of three key elements: <ref type="bibr" target="#b0">(1)</ref> We add an indicator branch to the original deep neural networks and estimate the confidence of the model predictions by exploiting the early-learning phenomenon through a confidence adaptive loss (CAL). <ref type="bibr" target="#b1">(2)</ref> We propose an auxiliary regularization term explicitly designed to further separate the confidence of clean samples and mislabeled samples.</p><p>(3) We estimate the target probabilities by incorporating the model predictions with noisy labels through a confidence-driven strategy. In addition, we analyze the gradients of CAL and CAR, and provide a theoretical guarantee for the noise-robust term in CAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>Consider the K-class classification problem in noisy-label scenario, we have a training set D =</p><formula xml:id="formula_0">{(x [i] ,? [i] )} N i=1 , where x [i] is an input and? [i] ? Y = {1, .</formula><p>. . , K} is the corresponding noisy label. We denote? [i] ? {0, 1} K as one-hot vector of noisy label? <ref type="bibr">[i]</ref> . The ground truth label y is unavailable. A deep neural network model N ? (i.e. prediction branch in <ref type="figure">Figure 2</ref> (a)) maps an input x [i] to a K-dimensional logits and then feeds the logits to a softmax function S(?) to obtain p <ref type="bibr">[i]</ref> of the conditional probability of each class given</p><formula xml:id="formula_1">x [i] , thus p [i] = S(z [i] ), z [i] = N ? (x [i]</formula><p>). ? denotes the parameters of the neural network and z [i] ? R K?1 denotes the K-dimensional logits (i.e. pre-softmax output). z <ref type="bibr">[i]</ref> is calculated by the fully connected layer from penultimate layer</p><formula xml:id="formula_2">H [i] ? R M ?1 . z [i] = W H [i] + b,</formula><p>where W ? R K?M denotes the weights and b ? R K?1 denotes the bias in penultimate layer. Usually, the cross-entropy (CE) loss reflects how well the model fits the training set D:</p><formula xml:id="formula_3">L ce = ? 1 N N i=1 (? [i] ) T log(p [i] ).<label>(1)</label></formula><p>However, as noisy label? <ref type="bibr">[i]</ref> may be wrong with relatively high probability, the model gradually memorizes the samples with false labels when minimizing L ce (in <ref type="figure">Figure 1</ref> (a) and (b)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Confidence adaptive loss</head><p>In addition to the prediction branch, we introduce an indicator branch just after the penultimate layer of the original model (in <ref type="figure">Figure 2 (a)</ref>). The M -dimensional penultimate layer H <ref type="bibr">[i]</ref> is shared in both branches. For each input x <ref type="bibr">[i]</ref> , the prediction branch produces the softmax prediction p [i] as usual.</p><p>The indicator branch contains one or more fully connected layers to produce a single scalar value h <ref type="bibr">[i]</ref> , and sigmoid function is applied to scale it between 0 to 1. Assume we use one fully connected layer,</p><formula xml:id="formula_4">h [i] = W H [i] + b , where W ? R 1?M</formula><p>denotes the weights and b ? R denotes the bias in the penultimate layer of the indicator branch. Thus, we have</p><formula xml:id="formula_5">? [i] = sigmoid(h [i] ), ? [i] ? (0, 1),<label>(2)</label></formula><p>where ? [i] denotes the confidence value of model prediction given input x <ref type="bibr">[i]</ref> . The early-learning phenomenon reveals that the deep neural networks memorize the correctly-labeled samples before the mislabeled samples. Thus, we hypothesize that, a sample with a clean label in expectation has a larger confidence value than a mislabeled sample in the early learning phase. To let confidence value ? capture it, we propose the confidence adaptive cross entropy (CACE) loss</p><formula xml:id="formula_6">L cace = ? 1 N N i=1 (t [i] ) T log ? [i] (p [i] ? t [i] ) + t [i] ,<label>(3)</label></formula><p>where t [i] is the target vector for each sample x <ref type="bibr">[i]</ref> . Generally, one can directly set t [i] =? <ref type="bibr">[i]</ref> . However, it is less effective as? <ref type="bibr">[i]</ref> can be wrong, so we propose a strategy to estimate t <ref type="bibr">[i]</ref> in Section 3.4. Intuitively, L cace can be explained in two-fold: 1) In the early-learning phase, the model does not overfit the mislabeled samples. Therefore, their p ? t remain large. By minimizing L cace , it forces ? of mislabeled samples toward 0 as desired. 2) As for correctly-labeled samples, the model memorizes them first, resulting in the small p ? t. Thus, it makes ? have no influence on minimizing L cace in the case of correctly-labeled samples. As a result, by only minimizing L cace , we may obtain a trivial optimization that the model always produces ? ? 0 for any inputs. To avoid this lazy learning circumstance, we introduce a penalty loss L p as a cost.</p><formula xml:id="formula_7">L p = ? 1 N N i=1 log(? [i] ),<label>(4)</label></formula><p>wherein the target value of ? is always 1 for all inputs. By adding a term L p to L cace , ? of correctly labeled samples are pushed to 1, and ? of mislabeled samples tend to 0 as expected. Hence, we define the confidence adaptive loss as</p><formula xml:id="formula_8">L CAL = L cace + ?L p ,<label>(5)</label></formula><p>where ? controls the strength of penalty loss. As we can see in <ref type="figure">Figure 2</ref> (b) and (c), the confidence value ? successfully segregates the mislabeled samples from correctly-labeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Auxiliary regularization term</head><p>We observe that the early learning phenomenon is not obvious when a dataset contains too many classes (e.g. CIFAR100), i.e, the mean of ? distributions for clean samples and mislabeled samples are close to each other as shown in <ref type="figure">Figure 2</ref> (c). Then L CAL is likely to be reduced to L ce . To enhance the performance in this situation, we need to make ? of mislabeled samples closer to 0. Hence we propose a reverse confidence adaptive cross entropy as an auxiliary regularization term.</p><formula xml:id="formula_9">L r-cace = ? 1 N N i=1 ? [i] (p [i] ? t [i] ) + t [i] T log(t [i] ).<label>(6)</label></formula><p>As the target t [i] is inside of the logarithm in L r-cace , this could cause computational problem when t [i] contains zeros. Similar to clipping operation, we solve it by defining log(0) = A (where A is a negative constant), which will be proved important for the theoretical analysis in Section 3.5. Putting all together, the confidence adaptive regularization (CAR) is</p><formula xml:id="formula_10">L CAR = L CAL + ?L r-cace = L cace + ?L p + ?L r-cace ,<label>(7)</label></formula><p>where ? controls the strength of regularization carried by L r-cace . In summary, L cace is designed for learning confidence by exploiting the early-learning phenomenon. L p is adopted for avoiding trivial solution. L r-cace makes CAR robust to label noise even in challenging cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Target estimation</head><p>CAR requires a target probability t for each sample in the training set. To yield better performance, ELR [11] and SELF <ref type="bibr" target="#b33">[51]</ref> use temporal ensembling <ref type="bibr" target="#b29">[47]</ref> solely based on model predictions to approximate the target t. However, it may lose the information of the original training set, and the model predictions can be ambiguous in the early stage of training. Instead, we desire to correct the noisy labels and develop a strategy to estimate the target by utilizing the noisy label?, model prediction p and confidence value ? . In each epoch, the target t [i] of given x [i] is updated by</p><formula xml:id="formula_11">t [i] = ? ? ?? [i] if E &lt; E c ?t [i] + (1 ? ?)p [i] if E ? E c and ? [i] ? ? t [i] otherwise,<label>(8)</label></formula><p>where E is the current epoch number, E c is the epoch that starts performing target estimation and 0 ? ? &lt; 1 is the momentum. Performance is robust to the value of E c . We fix the E c = 60 by default. Threshold ? is used to exclude ambiguous predictions with low confidence. Thus, our strategy enhances the stability of model predictions and gradually corrects the noisy labels. We evaluate the performance of CAR and CE with different target estimation strategies in Appendix D.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Theoretical analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Gradient analysis</head><p>For sample-wise analysis, we denote the true label of sample x as y ? {1, ..., K}. The ground-truth distribution over labels for sample x is q(y|x), and K k=1 q(k|x) = 1. Consider the case of a single ground-truth label y, then q(y|x) = 1 and q(k|x) = 0 for all k = y. We denote the prediction probability as p(k|x) and K k=1 p(k|x) = 1. For notation simplicity, we denote p k , q k , p y , q y , p j , q j as abbreviations for p(k|x), q(k|x), p(y|x), q(y|x), p(j|x) and q(j|x). Besides, we assume no target estimation is performed in the following analysis.</p><p>We first explain how the cross-entropy loss L ce (Eq. (1)) fails in noisy-label scenario. The gradient of sample-wise cross entropy loss L ce with respect to z j is</p><formula xml:id="formula_12">?L ce ?z j = p j ? 1 ? 0, q j = q y = 1 p j ? 0, q j = 0 (9)</formula><p>In the noisy label scenario, if j is the true class and equals y, but q j = 0 due to the label noise, the contribution of x to the gradient is reversed. The entry corresponding to the impostor class j , is also reversed because q j = 1, causing the gradient of mislabeled samples dominates (in <ref type="figure">Figure 3</ref> (a) and (b)). Thus, performing stochastic gradient descent eventually results in memorization of the mislabeled samples.  <ref type="figure">Figure 3</ref>: On CIFAR-10 with 40% symmetric label noise using ResNet34, we observe that in (a), the gradient of clean labels dominates in early learning stage, but afterwards it vanishes and the gradient of false labels dominates. In (b), it only slows down this effect with cosine annealing learning rate scheduler. In (c), CAL effectively keeps the gradient of clean labels dominant and diminishes the gradient of false labels when epoch increases, preventing memorization of mislabeled samples. Lemma 1. For the loss function L CAL given in Eq. (5) and L CAR in Eq. (7), the gradient of sample-wise L CAL and L CAR (? = 1) with respect to the logits z j can be derived as</p><formula xml:id="formula_13">?L CAL ?z j = ? ? ? ? ? (p j ? 1) p j p j ? 1 + 1/? ? 0, q j = q y = 1 (j is the true class for x) (10a) p j p y p y ? 1 + 1/? ? 0, q j = 0 (j is not the true class for x) (10b)</formula><p>and</p><formula xml:id="formula_14">?L CAR ?z j = ? ? ? ? ? (p j ? 1) p j p j ? 1 + 1/? ? A? p j (p j ? 1) ? 0, q j = q y = 1 (11a) p j p y p y ? 1 + 1/? ? A? p j p y ? 0, q j = 0 (11b)</formula><p>respectively, where A is a negative constant defined in Section 3.3.</p><p>The proof of Lemma 1 is based on gradient derivation in two cases. We defer it in Appendix A.2.</p><p>Gradient of L CAL in Eq. (10). Compared to the gradient of L ce in Eq. (9), the gradient of L CAL has an adaptive multiplier. We denote Q = pj pj ?1+1/? . It is monotonically increasing on ? and p j . We have lim ? ?1 Q = 1, and lim ? ?0 Q = 0. For the samples with the true class j in Eq. (10a), the cross entropy gradient term p j ? 1 of correctly-labeled samples tends to vanish after early learning stage because their p j is close to q j = 1, leading mislabeled samples to dominate the gradient. However, by multiplying Q (note that Q ? 0 for mislabeled samples and Q ? 1 for correctly-labeled samples due to property of ? as we discussed in Section 3.2), it counteracts the effect of gradient dominating by mislabeled samples. For the samples that j is not the true class in Eq. (10b), the gradient term p j is positive. Multiplying Q &lt; 1 effectively dampens the magnitudes of coefficients on these mislabeled samples, thereby diminishing their effect on the gradient (in <ref type="figure">Figure 3</ref> (c)).</p><p>Gradient of L CAR in Eq. (11). Compared to the gradient of L CAL , an extra term derived from auxiliary regularization term L r-cace is added. In the case of q j = q y = 1 in Eq. (11a), the extra term ?A? p j (p j ? 1) &lt; 0 for 0 ? p j ? 1 and it is a convex quadratic function whose vertex is at p j = 0.5. It means the extra term ?A? p j (p j ? 1) provides the largest acceleration in learning around p j = 0.5 where the most ambiguous scenario occurs. Intuitively, the term ?A? p j (p j ? 1) pushes apart the peaks of ? distribution for correctly-labeled samples and mislabeled samples. In the case of q j = 0 in Eq. (11b), the extra term ?A? p j p y &gt; 0 is added. For correctly-labeled samples, p y is larger, adding ?A? p j p y leads the residual probabilities of other unlabeled classes reduce faster. For mislabeled samples, p y is close to 0, no acceleration needed. Overall, adding L r-cace amplifies the effect of confidence learning in CAL, resulting in the confidence values of mislabeled samples become smaller. The empirical results of the influence of confidence distribution on CIFAR-100 with different strengths of L r-cace are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Label noise robustness</head><p>Here we prove that the L r-cace is robust to label noise following <ref type="bibr" target="#b9">[27]</ref>. Recall that noisy label of x is? ? {1, ..., K} and its true label is y ? {1, ..., K}. We assume that the noisy sample (x,?) is drawn from distribution D ? (x,?), and the ordinary sample (x, y) is drawn from D(x, y). We hav? y = i(y = i) with probability ? ii = (1 ? ?) and? = j(y = i) with probability ? ij for all j = i and j =i ? ij = ?. If ? ij = ? K?1 for all j = i, then the noise is uniform or symmetric, otherwise, the noise is class-conditional or asymmetric. Given any classifier f and loss function L, we define the risk of f under clean labels as</p><formula xml:id="formula_15">R L (f ) = E D(x,y) [L(f (x, y))], and the risk under label noise rate ? as R ? L (f ) = E D(x,?) [L(f (x,?))].</formula><p>Let f * and f * ? be the global minimizers of R L (f ) and R ? L (f ) respectively. Then, the empirical risk minimization under loss function L is defined to be noise-tolerant if f * is a global minimum of the noisy risk R ? L (f ). Theorem 1. Under symmetric or uniform label noise with noise rate ? &lt; K?1 K , we have</p><formula xml:id="formula_16">0 ? R Lr-cace (f * ? ) ? R Lr-cace (f * ) &lt; ?A?(K ? 1) K(1 ? ?) ? 1<label>(12)</label></formula><p>and</p><formula xml:id="formula_17">A? &lt; R ? Lr-cace (f * ? ) ? R ? Lr-cace (f * ) ? 0 (13)</formula><p>where f * and f * ? be the global minimizers of R Lr-cace (f ) and R ? Lr-cace (f ) respectively.</p><formula xml:id="formula_18">Theorem 2. Under class-dependent label noise with ? ij &lt; 1 ? ? i , ?j = i, ?i, j ? [K], where ? ij = p(? = j|y = i), ?j = i and (1 ? ? i ) = p(? = i|y = i), if R Lr-cace (f * ) = 0, then 0 ? R ? Lr-cace (f * ) ? R ? Lr-cace (f * ? ) &lt; G,<label>(14)</label></formula><p>where</p><formula xml:id="formula_19">G = A(1 ? K)E D(x,y) (1 ? ? y ) &gt; 0, f * and f * ? be the global minimizers of R Lr-cace (f ) and R ?</formula><p>Lr-cace (f ) respectively.</p><p>Due to the space constraints, we defer the proof of Theorem 1 and Theorem 2 to the Appendix A.2. Theorem 1 and Theorem 2 ensure that by minimizing L r-cace under symmetric and asymmetric label noise, the difference of the risks caused by the derived hypotheses f * ? and f * are always bounded. The bounds are related to the negative constant A. Since A is the approximate of log(0) which is actually ??. A larger A (closer to 0) leads to a tighter bound but introduces a larger approximation error in implementation. A reasonable A we set is -4 in our experiments. We also compare L r-cace with existing noise-robust loss functions in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Comparison with the state-of-the-art methods We evaluate our approach on two benchmark datasets with simulated label noise, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b1">[2]</ref>, and two real-world datasets, Clothing1M [13] and WebVision [6]. More information of datasets, data preprocessing, label noise injection and training details can be found in Appendix D. <ref type="table" target="#tab_1">Table 1</ref> shows the performance of CAR on CIFAR-10 and CIFAR-100 with different levels of symmetric and asymmetric label noise. All methods use the same backbone (ResNet34). We compare CAR to the state-of-the-art approaches that only modify the training loss without extra regularization     <ref type="table" target="#tab_4">Table 3</ref> compares CAR to state-of-the-art methods trained on the mini WebVision dataset and evaluated on both WebVision and ImageNet ILSVRC12 validation sets. On WebVision, CAR outperforms others on top5 accuracy, even better than DivideMix and ELR+. On top1 accuracy, CAR is slightly superior to DivideMix and achieves comparable performance to ELR+. On ILSVRC12, DivideMix achieves superior performance in terms of top1 accuracy, while CAR achieves the best top5 accuracy. We describe the hyperparameters sensitivity of CAR in Appendix D.4.</p><p>Ablation study <ref type="table" target="#tab_5">Table 4</ref> reports the influence of three individual components in CAR: auxiliary regularization term L r-cace , target estimation and indicator branch. Removing L r-cace does not hurt the performance on CIFAR-10. However, the term L r-cace improves the performance on CIFAR-100. The larger the noise is, the more improvement we obtain. Removing the target estimation leads to a significant performance drop. This suggests that estimating the target by properly using model predictions is crucial for avoiding memorization. To validate the effect of adding the indicator branch, we conduct another way to calculate confidence value without using indicator branch: using the highest probability as the confidence value, which means ? [i] = max j p   Identification of mislabeled samples When exploiting the progress of the early learning phase by CAL, we have observed that the correctly-labeled samples have larger confidence values than the mislabeled samples. We report the average confidence values of samples in <ref type="figure">Figure 5</ref>. The (i, j)-th block represents the average confidence value of samples with clean label i and false label j. We observe that the confidence values on the diagonal blocks are higher than those on non-diagonal blocks, which means that the confidence value has an effect similar to the probability of extra class in DAC <ref type="bibr" target="#b35">[53]</ref> and AUM [56]. The key difference is that DAC and AUM perform two stages of training: identify the mislabeled samples and then drop them to perform classification, while we incorporate the confidence values in loss function and implicitly achieve the regularization effect to avoid memorization of mislabeled samples.</p><p>Label correction Recall that we perform target estimation in Section 3.4. Since the target is calculated by a moving average between noisy labels and model predictions, our approach is able to gradually correct the false labels. The correction accuracy can be calculated by 1</p><formula xml:id="formula_20">N N i 1{argmax y [i] = argmax t [i] }, where y [i]</formula><p>is the clean label of training sample x <ref type="bibr">[i]</ref> . We evaluate the correction accuracy on CIFAR-10 and CIFAR-100 with 40% symmetric label noise. CAR obtains correction accuracy of 95.1% and 86.4%, respectively. The confusion matrix of corrected labels w.r.t the clean labels on CIFAR-10 is shown in <ref type="figure">Figure 6</ref>. As we can see, CAR corrects the false labels impressively well for all classes. More results on real-world datasets can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Based on the early learning and memorization phenomenon of deep neural networks in the presence of noisy labels, we propose an adaptive regularization method that implicitly adjusts the gradient to prevent memorization on noisy labels. Through extensive experiments across multiple datasets, our approach yields comparable or even superior results to the state-of-the-art methods.</p><p>[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012.</p><p>[4] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In Proceedings of the IEEE international conference on computer vision, pages 1520-1528, 2015.</p><p>[ [10] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In International Conference on Artificial Intelligence and Statistics, pages 4313-4324. PMLR, 2020.</p><p>[11] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Earlylearning regularization prevents memorization of noisy labels. Advances in Neural Information Processing Systems, 33, 2020.</p><p>[12] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.</p><p>[13] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2691-2699, 2015.</p><p>[14] Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks. In Advances in Neural Information Processing Systems, pages 5596-5605, 2017.</p><p>[15] Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie.</p><p>Learning from noisy large-scale datasets with minimal supervision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 839-847, 2017.</p><p>[ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Theoretical analysis</head><p>A.1 Gradient derivation of L CAL and L CAR Assume the target t equals to ground truth distribution. The sample-wise L CAL can be rewrite as:</p><formula xml:id="formula_21">L CAL = L cace + ?L p = ? K k=1 q k log(? (p k ? q k ) + q k ) ? ? log ?.<label>(1)</label></formula><p>The derivation of the L CAL with respect to the logits is as follows:</p><formula xml:id="formula_22">?L CAL ?z j = ?L cace ?z j = ? K k=1 ? q k ? (p k ? q k ) + q k ?p k ?z j .<label>(2)</label></formula><p>Since p k = S(z)= e z k K j=1 e z j , we have</p><formula xml:id="formula_23">?p k ?z j = ? e z k K j=1 e z j ?z j = ?e z k ?zj ( K j=1 e zj ) ? e z k ? K j=1 e z j ?zj ( K j=1 e zj ) 2</formula><p>.</p><p>(3)</p><p>In the case of k = j :</p><formula xml:id="formula_24">?p k ?z j = ?e z k ?z k ( K k=1 e z k ) ? e z k ? K k=1 e z k ?z k ( K k=1 e z k ) 2 = e z k ( K k=1 e z k ) ? e z k ? e z k ( K k=1 e z k ) 2 = e z k K k=1 e z k ? e z k K k=1 e z k 2 = p k ? p 2 k .<label>(4)</label></formula><p>In the case of k = j :</p><formula xml:id="formula_25">?p k ?z j = 0 ? ( K j=1 e zj ) ? e z k ? e zj ( K j=1 e zj ) 2 = ? e z k K j=1 e zj e zj K j=1 e zj = ?p k p j .<label>(5)</label></formula><p>Combining Eq. (4) and (5) into Eq. (2), we obtain:</p><formula xml:id="formula_26">?L CAL ?z j = ? K k=1 ? q k ? (p k ? q k ) + q k ?p k ?z j = ? ? q j ? (p j ? q j ) + q j ?p j ?z j ? K k =j ? q k ? (p k ? q k ) + q k ?p k ?z j = ? ? q j ? (p j ? q j ) + q j (p j ? p 2 j ) ? K k =j ? q k ? (p k ? q k ) + q k (?p k p j ) = ? ? q j p j ? (p j ? q j ) + q j + p j K k=1 ? q k p k ? (p k ? q k ) + q k .<label>(6)</label></formula><p>Therefore, if q j = q y = 1, then</p><formula xml:id="formula_27">?L CAL ?z j = ? ? p j ? p j ? ? + 1 + p j ? q j p j ? (p j ? 1) + 1 = (p j ? 1) ? p j ? p j ? ? + 1 = (p j ? 1) p j p j ? 1 + 1/? .<label>(7)</label></formula><p>If q j = 0, then</p><formula xml:id="formula_28">?L CAL ?z j =p j ? q y p y ? (p y ? q y ) + q y = p j p y p y ? 1 + 1/? .<label>(8)</label></formula><p>The sample-wise L CAR can be rewrite as (assume ? = 1):</p><formula xml:id="formula_29">L CAR = L CAL + ?L r-cace = L CAL ? K k=1 (? (p k ? q k ) + q k ) log q k . (9)</formula><p>Since we have obtain the gradient of L CAL , we now only analyze the gradient of L r-cace with respect to the logits as follows:</p><formula xml:id="formula_30">?L r-cace ?z j = ? K k=1 ? ?p k ?z j log q k .<label>(10)</label></formula><p>Combining Eq. (4) and (5), into Eq. (10), we have</p><formula xml:id="formula_31">?L r-cace ?z j = ? ? (p j ? p 2 j ) log q j ? ? K k =j (?p k p j ) log q k = ? ? p j log q j + ? K k=1 p k p j log q k .<label>(11)</label></formula><p>We denote log 0 = A, thus if q j = q y = 1, then</p><formula xml:id="formula_32">?L r-cace ?z j = ?? p j log 1 + ? p j (p j log 1 + K k =j p k log 0) = ? p j (1 ? p j )A = ?A? p j (p j ? 1). (12)</formula><p>If q j = 0, then</p><formula xml:id="formula_33">?L r-cace ?z j = ?? p j log 0 + ? p j (p y log 1 + (1 ? p y ) log 0) = ?A? p j + ? p j (1 ? p y )A = ?A? p j p y .<label>(13)</label></formula><p>Therefore, the gradients of L CAR is Proof. From the Appendix A.1, we obtain the gradient of the sample-wise L CAL with respect to the logits z j is</p><formula xml:id="formula_34">?L CAR ?z j = ? ? ? (p j ? 1)</formula><formula xml:id="formula_35">?L CAL ?z j = ?L cace ?z j = ? K k=1 ? q k ? (p k ? q k ) + q k ?p k ?z j<label>(15)</label></formula><p>where ?p k ?zj can be further derived base on whether k = j by follows:</p><formula xml:id="formula_36">?p k ?z j = p k ? p 2 k k = j ?p j p k k = j (16)</formula><p>According to Eq. (15) and (16), the gradient of L CAL can be derived as:</p><formula xml:id="formula_37">?L CAL ?z j = ? ? ? (p j ? 1) pj pj ?1+1/? , q j = q y = 1 p j py py?1+1/? , q j = 0 (17)</formula><p>Since p j ? 1, we have p j ?1 ? 0. As ? &lt; 1, the term py py?1+1/? &gt; 0, we have (p j ?1) py py?1+1/? ? 0 and p j py py?1+1/? ? 0. Similarly, the gradient of simplified L CAR (? = 1) can be derived as:</p><formula xml:id="formula_38">?L CAR ?z j = ?L CAL ?z j + ?L r-cace ?z j = ? ? ? (p j ? 1) pj pj ?1+1/? ? A? p j (p j ? 1), q j = q y = 1 p j py py?1+1/? ? A? p j p y , q j = 0 (18)</formula><p>Since A is a negative constant, we obtain ?A? p j (p j ? 1) ? 0. Thus, in the case of q j = q y = 1, ?LCAR ?zj ? 0 and in the case of q j = 0, ?LCAR ?zj ? 0 as claimed. Complete derivations can be found in the Appendix A.1.</p><p>The result in Lemma 1 ensures that, during the gradient decent, learning continues on true classes when trained with L CAL and L CAR . We then prove the noise robustness of L r-cace .</p><p>Recall that noisy label of x is? ? {1, ..., K} and its true label is y ? {1, ..., K}. We assume that the noisy sample (x,?) is drawn from distribution D ? (x,?), and the ordinary sample (x, y) is drawn from D(x, y). Note that this paper follows the most common setting where label noise is instance-independent. Then we have? = i(y = i) with probability ? ii = (1 ? ?) and? = j(y = i) with probability ? ij for all j = i and j =i ? ij = ?. If ? ij = ? K?1 for all j = i, then the noise is said to be uniform or symmetric, otherwise, the noise is said to be class-conditional or asymmetric. For any x, the sum of L r-cace with respect to all the classes satisfies:</p><formula xml:id="formula_39">0 &lt; K j=1 L r-cace (f (x), j) &lt; A(1 ? K),<label>(19)</label></formula><p>where A = log(0) is a negative constant that depends on the clipping operation.</p><p>Proof. By the definition of L r-cace , we can rewrite the sample-wise L r-cace as</p><formula xml:id="formula_40">L r-cace = ? K k=1 ? (p(k|x) ? q(k|x)) + q(k|x) log q(k|x) = ? ? (p(y|x) ? q(y|x)) + q(y|x) log q(y|x) ? k =y ? (p(k|x) ? q(k|x)) + q(k|x) log q(k|x) = ? ? p(y|x) ? ? + 1 log 1 ? A? k =y p(k|x) = ?A? (1 ? p(y|x)).<label>(20)</label></formula><p>Therefore, we have As ? ? (0, 1), A is a negative constant, K is a constant, hence</p><formula xml:id="formula_41">0 &lt; K j=1 L r-cace (f (x), j) &lt; A(1 ? K),</formula><p>which concludes the proof.</p><p>Theorem 1. Under symmetric or uniform label noise with noise rate ? &lt; K?1 K , we have</p><formula xml:id="formula_42">0 ? R Lr-cace (f * ? ) ? R Lr-cace (f * ) &lt; ?A?(K ? 1) K(1 ? ?) ? 1 and A? &lt; R ? Lr-cace (f * ? ) ? R ?</formula><p>Lr-cace (f * ) ? 0 where f * and f * ? be the global minimizers of R Lr-cace (f ) and R ? Lr-cace (f ) respectively.</p><p>Proof. For symmetric noise, we have, for any f 1</p><formula xml:id="formula_43">R ? Lr-cace (f ) = E D?(x,?) [L r-cace (f (x),?)] = E x E D(y|x) E D(?|x,y) [L r-cace (f (x),?)] = E D(x,y) (1 ? ?)L r-cace (f (x), y) + ? K ? 1 j =y L r-cace (f (x), j) = (1 ? ?)R Lr-sace (f ) + ? K ? 1 K j=1 L r-cace (f (x), j) ? R Lr-cace (f ) = (1 ? ?K K ? 1 )R Lr-cace (f ) + ? K ? 1 K j=1 L r-cace (f (x), j)</formula><p>From Lemma 2, for all f , we have:</p><formula xml:id="formula_44">?R Lr-cace (f ) &lt; R ? Lr-cace (f ) &lt; ?A? + ?R Lr-cace (f )</formula><p>where ? = (1 ? ?K K?1 ). Since ? &lt; K?1 K , we have ? &gt; 0. Thus, we can rewrite the inequality in terms of R Lr-cace (f ):</p><formula xml:id="formula_45">1 ? (R ? Lr-cace (f ) + A?) &lt; R Lr-cace (f ) &lt; 1 ? R ? Lr-cace (f )</formula><p>Thus, for f * ? ,</p><formula xml:id="formula_46">R Lr-cace (f * ? ) ? R Lr-cace (f * ) &lt; 1 ? (R ? Lr-cace (f * ? ) ? R ? Lr-cace (f * ) ? A?)</formula><p>or equivalently,</p><formula xml:id="formula_47">R ? Lr-cace (f * ? ) ? R ? Lr-cace (f * ) &gt; ?(R Lr-cace (f * ? ) ? R Lr-cace (f * )) + A?</formula><p>Since f * is the global minimizer of R Lr-cace (f ) and f * ? is the global minimizer of R ? Lr-cace (f ), we have</p><formula xml:id="formula_48">0 ? R Lr-cace (f * ? ) ? R Lr-cace (f * ) &lt; ?A? ? = ?A?(K ? 1) K(1 ? ?) ? 1 and A? &lt; R ? Lr-cace (f * ? ) ? R ?</formula><p>Lr-cace (f * ) ? 0 which concludes the proof.</p><formula xml:id="formula_49">Theorem 2. Under class-dependent label noise with ? ij &lt; 1 ? ? i , ?j = i, ?i, j ? [K], where ? ij = p(? = j|y = i), ?j = i and (1 ? ? i ) = p(? = i|y = i), if R Lr-cace (f * ) = 0, then 0 ? R ? Lr-cace (f * ) ? R ? Lr-cace (f * ? ) &lt; G, where G = A(1 ? K)E D(x,y) (1 ? ? y )</formula><p>&gt; 0, f * and f * ? be the global minimizers of R Lr-cace (f ) and R ?</p><p>Lr-cace (f ) respectively.</p><p>Proof. For asymmetric or class-dependent noise, we have</p><formula xml:id="formula_50">R ? Lr-cace (f ) = E D?(x,?) [L r-cace (f (x),?)] = E x E D(y|x) (1 ? ? y )L r-cace (f (x), y) + j =y ? yj L r-cace (f (x), j) = E D(x,y) (1 ? ? y ) K j=1 L r-cace (f (x), j) ? j =y L r-cace (f (x), j) + E D(x,y) j =y ? yj L r-cace (f (x), j) &lt; E D(x,y) (1 ? ? y ) A(1 ? K) ? j =y L r-cace (f (x), j) + E D(x,y) j =y ? yj L r-cace (f (x), j) = A(1 ? K)E D(x,y) (1 ? ? y ) ? E D(x,y) j =y (1 ? ? y ? ? yj )L r-cace (f (x), j) .</formula><p>On the other hand, we also have</p><formula xml:id="formula_51">R ? Lr-cace (f ) &gt; ?E D(x,y) j =y (1 ? ? y ? ? yj )L r-cace (f (x), j)</formula><p>Hence, we obtain</p><formula xml:id="formula_52">R ? Lr-cace (f * ) ? R ? Lr-cace (f * ? ) &lt; A(1 ? K)E D(x,y) (1 ? ? y ) + E D(x,y) j =y (1 ? ? y ? ? yj ) L r-cace (f * ? (x), j) ? L r-cace (f * (x), j)</formula><p>Next, we prove the bound. First, (1 ? ? y ? ? yj ) &gt; 0 as per the assumption that ? yj &lt; 1 ? ? y . Second, our assumption has R r-cace (f * ) = 0, we have L r-cace (f * (x), y) = 0. This is only satisfied iff f * j (x) = 1 when j = y, and f * j (x) = 0 when j = y. According to the definition of L r-cace , we have L r-cace (f * (x), j) = ?A? , ?j = y, and L r-cace (f * ? (x), j) ? ?A? , ?j ? [K]. We then obtain</p><formula xml:id="formula_53">E D(x,y) j =y (1 ? ? y ? ? yj ) L r-cace (f * ? (x), j) ? L r-cace (f * (x), j) ? 0 Therefore, we have R ? Lr-cace (f * ) ? R ? Lr-cace (f * ? ) &lt; A(1 ? K)E D(x,y) (1 ? ? y ) Since f * ? is the global minimizers of R ? Lr-cace (f ), we have R ? Lr-cace (f * ) ? R ?</formula><p>Lr-cace (f * ? ) ? 0, which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Comparison with existing noise-robust loss functions</head><p>According to the definition in Section 3.5, we obtain the sample-wise  <ref type="figure">p(y|x)</ref>). where ? ? (0, 1) and A is a negative constant .</p><p>Similarly, we have sample-wise L mae <ref type="bibr" target="#b9">[27]</ref>, L rce <ref type="bibr" target="#b12">[30]</ref>, L gce <ref type="bibr" target="#b10">[28]</ref> and L tce <ref type="bibr" target="#b14">[32]</ref> as follows</p><formula xml:id="formula_55">L mae = K k=1</formula><p>|p(k|x) ? q(k|x)| = (1 ? p(y|x)) + k =y p(k|x) = 2(1 ? p(y|x));</p><formula xml:id="formula_56">L rce = ? K k=1 p(k|x) log q(k|x) = ?p(y|x) log 1 ? k =y p(k|x) log 0 = ?A(1 ? p(y|x)); L gce = K k=1 q(k|x) 1 ? p(k|x) ? ? = q(y|x) 1 ? p(y|x) ? ? = 1 ? (1 ? p(y|x) ? ), ? ? (0, 1]; L tce = t i=1</formula><p>(1 ? p(y|x)) i i , t ? N + denotes the order of Taylor Series.</p><p>We observe that when ? = 1 (even though it is impossible), L r-cace is reduced to L rce . If A = ?2 and ? = 1, L r-cace is further reduced to L mae . Since confidence ? is various for different samples, L r-cace is more like a dynamic version of L mae . As for L gce , lim ??0 L gce = L ce and L gce = 1 2 L mae when ? = 1. Similarly, lim t?? L tce = L ce and L tce = 1 2 L mae when t = 1. Therefore, both L gce and L tce can be interpreted as the generalization of MAE and CE, which benefits the noise robust from MAE and training efficiency from CE. However, parameters ? and t are fixed before training, so it is hard to tell what is the best parameter for the certain dataset. Instead, combined with L CAL , L r-cace contains a dynamic confidence value ? for each sample that automatically learned from dataset, facilitating the learning from correctly-labeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithm</head><p>Algorithm 1 provides detail pseudocode for CAR. Note that for Cosine Annealing learning rate scheduler, the condition line 8 becomes e ? E c and ? [i] ? ? and e%E p == 0, where E p is the number of epochs in each period, we fix E p = 10 in all experiments. </p><formula xml:id="formula_57">= Lcace + ?Lp + ?Lr-cace = ? 1 B B i=1 (t [i] ) T log ? [i] (p [i] ? t [i] ) + t [i] ? ? B B i=1 log(? [i] ) ? ? B B i=1 ? [i] (p [i] ? t [i] ) + t [i] T log(t [i] ) ; 11</formula><p>Update ? using stochatic gradient descent ; 12 Output ?. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More results of label correction and confidence value</head><p>We report the label correction accuracy for various level of label noise on CIFAR-10 and CIFAR-100 in <ref type="table" target="#tab_10">Table 5</ref>. <ref type="figure">Figure 7</ref> displays the confusion matrix of corrected label w.r.t. the clean labels on CIFAR-10 with 60% symmetric, 80% symmetric and 40% asymmetric label noise respectively. We also show the corrected labels for real-world datasets in <ref type="figure">Figure 12</ref> and <ref type="figure">Figure 13</ref>.</p><p>We report the confidence value for high level of label noise on CIFAR-10 in <ref type="figure">Figure 8</ref> and <ref type="figure">Figure  9</ref>. As we can see, the confidence values on the diagonal blocks remain relatively higher than those non-diagonal blocks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Detail description of experiments</head><p>Source code for the experiments is available in the zip file. All experiments are implemented in PyTorch and run in a single Nvidia GTX 1080 GPU. For CIFAR-10 and CIFAR-100, we do not perform early stopping since we don't assume the presence of clean validation data. All test accuracy are recorded from the last epoch of training. For Clothing1M, it provides 50k, 14k, 10k refined clean data for training, validation and testing respectively. Note that we do not use the 50k clean data. We report the test accuracy when the performance on validation set is optimal. All tables of CIFAR-10/CIFAR-100 report the mean and standard deviation from 3 trails with different random seeds. As for larger datasets, we only perform a single trail. The epoch starts to estimate target. ?</p><p>The momentum in target estimation. ?</p><p>The threshold of confidence in target estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Dataset description and preprocessing</head><p>The information of datasets are described in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Simulated label noise injection</head><p>Since the CIFAR-10 and CIFAR-100 are initially clean, we follow <ref type="bibr" target="#b20">[38,</ref><ref type="bibr" target="#b5">23]</ref> for symmetric and asymmetric label noise injection. Specifically, symmetric label noise is generated by randomly flipping a certain fraction of the labels in the training set following a uniform distribution. Asymmetric label noise is simulated by flipping their class to another certain class according to the mislabel confusions in the real world. For CIFAR-10, the asymmetric noisy labels are generated by mapping truck ? automobile, bird ? airplane, deer ? horse and cat ? dog. For CIFAR-100, the noise flips each class into the next, circularly within super-classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Training procedure</head><p>CIFAR-10/CIFAR-100: We use a ResNet-34 and train it using SGD with a momentum of 0.9, a weight decay of 0.001, and a batch size of 64. The network is trained for 500 epochs for both CIFAR-10 and CIFAR-100. We use the cosine annealing learning rate <ref type="bibr">[12]</ref> where the maximum number of epoch for each period is 10, the maximum and minimum learning rate is set to 0.02 and 0.001 respectively. As for cross entropy with MultiStep learning rate scheduler in <ref type="figure">Figure 1</ref> and <ref type="figure">Figure  3</ref> in the paper, we set the initial learning rate as 0.02, and reduce it by a factor of 10 after 100 and 200 epochs. The reason that we train the model 500 epochs in total is to fully evaluate whether the model will overfit mislabeled samples, which avoids the interference caused by early stopping [10] (i.e. the model may not start overfitting mislabeled samples when the number of training epochs is small, especially when learning rate scheduler is cosine annealing [12]).</p><p>Clothing1M: Following [13, 30], we use a ResNet-50 pretrained on ImageNet. We train the model with batch size 64. The optimization is done using SGD with a momentum 0.9, and weight decay 0.001. We use the same cosine annealing learning rate as CIFAR-10 except the minimum learning rate is set to 0.0001 and total epoch is 400. For each epoch, we sample 2000 mini-batches from the training data ensuring that the classes of the noisy labels are balanced.</p><p>Webvision: Following <ref type="bibr" target="#b25">[43,</ref><ref type="bibr">11]</ref>, we use an InceptionResNetV2 as the backbone architecture. All other optimization details are the same as for CIFAR-10, except for the weight decay (0.0005) and the batch size (32).  <ref type="figure" target="#fig_10">Figure 10</ref> and <ref type="figure">Figure 11</ref> shows the hyperparameters sensitivity of CAR on CIFAR-10 and CIFAR-100 with 60% symmetric label noise respectively. The coefficient of penalty loss ? needs to be large than 0 to avoid trivial solution but also cannot be too large for CIFAR-10, avoiding neglecting L cace term in the loss. As the CIFAR-10 is an easy dataset, no additional regularization requires by L r-cace term. Therefore, the regularization coefficient ? should be 0 and large ? may cause model to underfit. The performance is robust to E c and ?, as long as the momentum ? is large enough (e.g. larger than 0.7). The choice of confidence threshold ? depends on the difficulty of dataset. A larger ? will slow down the speed of target estimation but helps exclude ambiguous predictions with low confidence values. Overall, the sensitivity to hyperparameters is quite mild and the performance is quite robust, unless the parameter is set to be very large or very small, resulting in neglecting L cace term or underfitting. We can observe the similar results of CIFAR-100 in <ref type="figure">Figure 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Performance with different target estimation strategies</head><p>We compare the performance of CAR with three strategies: 1) our strategy in Section 3.4. 2) temporal ensembling <ref type="bibr" target="#b29">[47]</ref> that adopted in ELR <ref type="bibr">[11]</ref>. 3) directly using the noisy labels? without target estimation. <ref type="table" target="#tab_13">Table 7</ref> shows the results. As we can see, compared to CAR without target estimation, CAR with temporal ensembling does not improve much performance in easy cases (e.g. 40% symmetric label noise), and it even gets worse performance in hard cases (e.g. 80% symmetric label noise). However, CAR with our strategy achieves much better performance. We also conduct the experiments that use CE with different target estimation strategies. Surprisingly, CE with our strategy can achieve better performance to CAR in CIFAR-10 with 40% asymmetric noise. However, the overall performance is worse than the performance of using CAR, due to the reason that CE will memorize noisy labels after early learning phase.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Cross Entropy with MultiStep learning rate scheduler (b) Cross Entropy with Cosine Annealing learning rate scheduler (c) Confidence Adaptive Regularization with Cosine Annealing learning rate scheduler</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Entropy with MultiStep learning rate scheduler (b) Cross Entropy with Cosine Annealing learning rate scheduler (c) Confidence Adaptive Loss with Cosine Annealing learning rate scheduler</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The empirical density of confidence value ? on CIFAR-100 with 40% symmetric label noise. The mean confidence values of mislabeled samples become smaller with the increasing of ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>[i] j , j ?[1, K]. Without using the indicator branch, the model only converges in two easy cases. Hence, directly calculating the confidence by model output does interfere with the original prediction branch, while adding an extra indicator branch solves this problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Average confidence values ? of false labels w.r.t clean labels on CIFAR-10 with 40% symmetric label noise. Confusion matrix of corrected labels w.r.t clean labels on CIFAR-10 with 40% symmetric label noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 Lemma 1 .</head><label>21</label><figDesc>pj pj ?1+1/? ? A? p j (p j ? 1), q j = q y = 1 p j py py?1+1/? ? A? p j p y , q j = 0 (14) A.2 Formal proof for Lemma 1, Lemma2, Theorem 1 and Theorem For the loss function L CAL given in Eq. (5) and L CAR in Eq. (7), the gradient of sample-wise L CAL and L CAR (? = 1) with respect to the logits z j can be derived as /? ? 0, q j = q y = 1 (j is the true class for sample x) p j py py?1+1/? ? 0, q j = 0 (j is not the true class for sample x) /? ? A? p j (p j ? 1) ? 0, q j = q y = 1 p j py py?1+1/? ? A? p j p y ? 0, q j = 0 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Given any classifier f and loss function L, we define the risk of f under clean labels as R L (f ) = E D(x,y) [L(f (x, y))], and the risk under label noise rate ? as R ? L (f ) = E D(x,?) [L(f (x,?))]. Let f * and f * ? be the global minimizers of R L (f ) and R ? L (f ) respectively. Then, the empirical risk minimization under loss function L is defined to be noise-tolerant if f * is a global minimum of the noisy risk R ? L (f ). Lemma 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>L</head><label></label><figDesc>r-cace (f (x), j) = K j=1 ?A? (1 ? p(j|x)) = ?A? K + A? K j=1 p(j|x) = A? (1 ? K)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>L</head><label></label><figDesc>r-cace = ? K k=1 ? (p(k|x) ? q(k|x)) + q(k|x) log q(k|x) = ? ? (p(y|x) ? q(y|x)) + q(y|x) log q(y|x) ?k =y ? (p(k|x) ? q(k|x)) + q(k|x) log q(k|x) = ? ? p(y|x) ? ? + 1 log 1 ? A? k =y p(k|x) = ?A? (1 ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Confusion matrix of corrected labels w.r.t clean labels on CIFAR-10 with 60% symmetric, 80% symmetric and 40% asymmetric label noise respectively. 0.48 0.61 0.73 0.56 0.78 0.52 0.54 0.47 0.50 0.51 0.46 0.56 0.60 0.53 0.49 0.80 0.45 0.47 0.47 0.53 0.50 0.52 0.55 0.58 0.54 0.47 0.86 0.47 0.54 0.63 0.51 0.50 0.50 0.48 0.48 0.49 0.47 0.82 0.51 0.58 0.56 0.48 0.50 0.48 0.47 0.47 0.48 0.49 0Average confidence values ? of false labels w.r.t clean labels on CIFAR-10 with 60% symmetric label noise. ai rp la ne au to m ob ile bi rd ca t de er do g fr og ho rs e sh ip tr uc k 0.49 0.57 0.54 0.56 0.80 0.53 0.54 0.49 0.53 0.45 0.46 0.51 0.48 0.55 0.51 0.58 0.44 0.46 0.46 0.48 0.47 0.50 0.47 0.56 0.55 0.46 0.61 0.45 0.51 0.57 0.49 0.48 0.46 0.48 0.47 0.47 0.46 0.63 0.48 0.52 0.49 0.47 0.46 0.49 0.50 0.46 0.49 0.50 0Average confidence values ? of false labels w.r.t clean labels on CIFAR-10 with 80% symmetric label noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Test accuracy on CIFAR-10 with 60% symmetric label noise. The mean accuracy over three runs is reported, along with bars representing one standard deviation from the mean. In each experiment, the rest of hyperparameters are fixed to the values reported in Section D.4.D.4 Hyperparameters selection and sensitivityTable6b provides a detailed description of hyperparameters in our approach. We perform hyperparameter tuning via grid search: ? = [0.5, 10, 50], ? = [0.0, 0.1, 0.3, 0.5], E c = [20, 60, 100], ? = [0.7, 0.9, 0.99] and ? = [0, 0, 0.35, 0.65, 0.95]. For CIFAR-10, the selected value are ? = 0.5, ? = 0.0, E c = 60, ? = 0.9 and ? = 0.0. For CIFAR-100 with 40% asymmetric label noise, the selected value are ? = 10, ? = 0.1, E c = 20, ? = 0.9, ? = 0.0. For CIFAR-100 with 20%/40%/60% symmetric label noise, we set ? = 10, ? = 0.1, E c = 60, ? = 0.9, ? = 0.95 and ? = 50, ? = 0.1, E c = 60, ? = 0.9, ? = 0.0 for 80% symmetric label noise. For Webvision, we set ? = 50, ? = 0.1, E c = 200, ? = 0.9, ? = 0.0. For Clothing1M, we set ? = 50, ? = 0.1, E c = 60, ? = 0.8, ? = 0.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :Figure 12 :Figure 13 :</head><label>111213</label><figDesc>Test accuracy on CIFAR-100 with 60% symmetric label noise. The mean accuracy over three runs is reported, along with bars representing one standard deviation from the mean. In each experiment, the rest of hyperparameters are fixed to the values reported in Section D.4. Label correction of Webvision images. Given noisy labels are shown above in red and the corrected labels are shown below in green. Label correction of Clothing1M images. Given noisy labels are shown above in red and the corrected labels are shown below in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test Accuracy (%) on CIFAR-10 and CIFAR-100 with various levels of label noise injected to the training set. We compare with previous works under the same backbone ResNet34. The results are averaged over 3 trials. Results are taken from their original papers. The best results are in bold. Note that SAT<ref type="bibr" target="#b23">[41]</ref>, ELR [11] and CAR use cosine annealing learning rate scheduler [12]. ? 0.12 81.88 ? 0.29 74.14 ? 0.56 53.82 ? 1.04 80.11 ? 1.44 58.72 ? 0.26 48.20 ? 0.65 37.41 ? 0.94 18.10 ? 0.82 42.74 ? 0.61 ForwardT [23] 87.99 ? 0.36 83.25 ? 0.38 74.96 ? 0.65 54.64 ? 0.44 83.55 ? 0.58 39.19 ? 2.61 31.05 ? 1.44 19.12 ? 1.95 8.99 ? 0.58 34.44 ? 1.93 Bootstrap [40] 86.23 ? 0.23 82.23 ? 0.37 75.12 ? 0.56 54.12 ? 1.32 81.21 ? 1.47 58.27 ? 0.21 47.66 ? 0.55 34.68 ? 1.10 21.64 ? 0.97 45.12 ? 0.57 GCE [28] 89.83 ? 0.20 87.13 ? 0.22 82.54 ? 0.23 64.07 ? 1.38 76.74 ? 0.61 66.81 ? 0.42 61.77 ? 0.24 53.16 ? 0.78 29.16 ? 0.74 47.22 ? 1.15 ? 0.20 87.13 ? 0.26 82.81 ? 0.61 68.12 ? 0.81 82.51 ? 0.45 70.38 ? 0.13 62.27 ? 0.22 54.82 ? 0.57 25.91 ? 0.44 69.32 ? 0.87 ? 0.35 91.43 ? 0.21 88.87 ? 0.24 80.69 ? 0.57 90.35 ? 0.38 74.68 ? 0.31 68.43 ? 0.42 60.05 ? 0.78 30.27 ? 0.86 73.73 ? 0.34 CAR (Ours) 94.37 ? 0.04 93.49 ? 0.07 90.56 ? 0.07 80.98 ? 0.27 92.09 ? 0.12 77.90 ? 0.14 75.38 ? 0.08 69.78 ? 0.69 38.24 ? 0.55 74.89 ? 0.20</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell><cell></cell></row><row><cell>Noise type</cell><cell></cell><cell>symm</cell><cell></cell><cell></cell><cell>asymm</cell><cell></cell><cell>symm</cell><cell></cell><cell></cell><cell>asymm</cell></row><row><cell>Method/Noise ratio</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell><cell>40%</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell><cell>40%</cell></row><row><cell cols="2">Cross Entropy 86.98 Joint Opt [38] 92.25</cell><cell>90.79</cell><cell>86.87</cell><cell>69.16</cell><cell>-</cell><cell>58.15</cell><cell>54.81</cell><cell>47.94</cell><cell>17.18</cell><cell>-</cell></row><row><cell>NLNL [52]</cell><cell>94.23</cell><cell>92.43</cell><cell>88.32</cell><cell>-</cell><cell>89.86</cell><cell>71.52</cell><cell>66.39</cell><cell>56.51</cell><cell>-</cell><cell>45.70</cell></row><row><cell cols="2">SL [30] 89.83 DAC [53] 92.91</cell><cell>90.71</cell><cell>86.30</cell><cell>74.84</cell><cell>-</cell><cell>73.55</cell><cell>66.92</cell><cell>57.17</cell><cell>32.16</cell><cell>-</cell></row><row><cell>SELF [51]</cell><cell>-</cell><cell>91.13</cell><cell>-</cell><cell>63.59</cell><cell>-</cell><cell>-</cell><cell>66.71</cell><cell>-</cell><cell>35.56</cell><cell>-</cell></row><row><cell>SAT [41]</cell><cell>94.14</cell><cell>92.64</cell><cell>89.23</cell><cell>78.58</cell><cell>-</cell><cell>75.77</cell><cell>71.38</cell><cell>62.69</cell><cell>38.72</cell><cell>-</cell></row><row><cell>ELR [11]</cell><cell>92.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art methods trained on Clothing1M. Results of other methods are taken from original papers. All methods use an ResNet-50 architecture pretrained on ImageNet.</figDesc><table><row><cell>CE</cell><cell cols="9">Forward [23] GCE [28] SL [30] Joint-Optim [38] DMI [26] ELR [11] ELR+ [11] DivideMix [43] CAR</cell></row><row><cell>69.21</cell><cell>69.84</cell><cell>69.75</cell><cell>71.02</cell><cell>72.16</cell><cell>72.46</cell><cell>72.87</cell><cell>74.81</cell><cell>74.76</cell><cell>73.19</cell></row></table><note>techniques, such as mixup data augmentation, two networks, and weight averaging. CAR obtains the highest performance in most cases and achieves comparable results in the most challenging cases (e.g. under 80% symmetric noise).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 compares</head><label>2</label><figDesc>CAR to state-of-the-art methods trained on the Clothing1M dataset. Note that DivideMix and ELR+ require mixup data augmentation, two networks, and weight averaging, while CAR is a pure regularization method. Except for DivideMix and ELR+, CAR slightly outperforms other methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art methods trained on mini WebVision. Results of other methods are taken from<ref type="bibr" target="#b25">[43,</ref> 11]. All methods use an InceptionResNetV2 architecture.</figDesc><table><row><cell></cell><cell></cell><cell cols="8">D2L [54] MentorNet [33] Co-teaching [34] Iterative-CV [55] ELR [11] DivideMix [43] ELR+ [11] CAR</cell></row><row><cell>WebVision</cell><cell>top1 top5</cell><cell>62.68 84.00</cell><cell>63.00 81.40</cell><cell>63.58 85.20</cell><cell>65.24 85.34</cell><cell>76.26 91.26</cell><cell>77.32 91.64</cell><cell>77.78 91.68</cell><cell>77.41 92.25</cell></row><row><cell>ILSVRC12</cell><cell>top1 top5</cell><cell>57.80 81.36</cell><cell>57.80 79.92</cell><cell>61.48 84.70</cell><cell>61.60 84.98</cell><cell>68.71 87.84</cell><cell>75.20 90.84</cell><cell>70.29 89.76</cell><cell>74.09 92.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Influence of three components in our approach. means the model fails to converge. ? 0.07 80.98 ? 0.27 92.09 ? 0.12 75.38 ? 0.08 38.24 ? 0.55 74.89 ? 0.20 -L r-cace 93.49 ? 0.07 80.98 ? 0.27 92.09 ? 0.12 74.65 ? 0.09 34.79 ? 0.71 74.73 ? 0.12 -target estimation 89.47 ? 0.50 76.91 ? 0.22 88.23 ? 0.22 69.91 ? 0.21 31.33 ? 0.38 55.68 ? 0.17 -indicator branch 90.94 ? 0.28 91.55 ? 0.07</figDesc><table><row><cell>Dataset</cell><cell cols="2">CIFAR-10</cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Noise type</cell><cell>symm</cell><cell></cell><cell>asymm</cell><cell>symm</cell><cell></cell><cell>asymm</cell></row><row><cell>Noise ratio</cell><cell>40%</cell><cell>80%</cell><cell>40%</cell><cell>40%</cell><cell>80%</cell><cell>40%</cell></row><row><cell cols="2">CAR 93.49 Clean labels</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>False labels</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,</figDesc><table><row><cell>pages 770-778, 2016.</cell></row><row><cell>[6] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database:</cell></row><row><cell>Visual learning and understanding from web data. CoRR, 2017.</cell></row><row><cell>[7] Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complemen-</cell></row><row><cell>tary labels. In Proceedings of the European Conference on Computer Vision (ECCV), pages</cell></row><row><cell>68-83, 2018.</cell></row></table><note>[8] Devansh Arpit, Stanis?aw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 233-242. JMLR. org, 2017.[9] C Zhang, S Bengio, M Hardt, B Recht, and O Vinyals. Understanding deep learning requires rethinking generalization, 2018.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Tao Xia, Sudanthi Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. arXiv preprint arXiv:1806.02612, 2018. [55] Pengfei Chen, Ben Ben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing deep neural networks trained with noisy labels. In International Conference on Machine Learning, pages 1062-1070, 2019.</figDesc><table><row><cell>[56] Geoff Pleiss, Tianyi Zhang, Ethan R Elenberg, and Kilian Q Weinberger. Identifying mislabeled</cell></row><row><cell>data using the area under the margin ranking. arXiv preprint arXiv:2001.10528, 2020.</cell></row><row><cell>[57] Hwanjun Song, Minseok Kim, Dongmin Park, and Jae-Gil Lee. Prestopping: How does early</cell></row><row><cell>stopping help generalization against label noise? 2019.</cell></row></table><note>16] Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from noisy labels with distillation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1910-1918, 2017.[17] Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise. In NeurIPS, 2018.[18] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International Conference on Machine Learning, pages 4334-4343. PMLR, 2018.[19] Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for scalable image classifier training with label noise. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5447-5456, 2018.[54] Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Algorithm 1 :</head><label>1</label><figDesc>Confidence adaptive regularization (CAR)Input: Deep neural network N ? with trainable parameters ?; ? is the parameter for penalty term Lp; ? is the parameter for regularization term Lr-cace; Ec is the epoch that starts to estimate target; ? is the momentum in target estimation; training set D, batch size B, total epoch Emax;</figDesc><table><row><cell cols="2">1 t =?</cell><cell>Initialize the target by noisy labels;</cell></row><row><cell cols="2">2 for e = 1, 2, . . . , Emax do</cell><cell></cell></row><row><cell>3</cell><cell>Shuffle D into |D| B mini-batches ;</cell><cell></cell></row><row><cell>4 5</cell><cell>for n = 1, 2, . . . , |D| B do for i in each mini-batch Dn do</cell><cell></cell></row><row><cell>6</cell><cell>p [i] = S(N ? (x [i] ))</cell><cell>Obtain model predictions;</cell></row><row><cell>7</cell><cell>? [i] = sigmoid(h [i] )</cell><cell>Obtain corresponding confidence;</cell></row><row><cell>8</cell><cell>if e ? Ec and ? [i] ? ? then</cell><cell></cell></row><row><cell>9</cell><cell>t [i] = ?t [i] + (1 ? ?)p [i]</cell><cell>Target estimation;</cell></row></table><note>10 Calculate the loss LCAR</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Correction accuracy (%) on CIFAR-10 and CIFAR-100 with various levels of label noise injected to training set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Detail information of experiment. (a) Description of the datasets used in the experiments. Dataset # of train # of val # of test # of classes input size Noise rate (%) Control the strength of penalty loss in LCAL. ? Control the strength of regularization term Lr-cace.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Description of the hyperparameters used in our</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>approach.</cell></row><row><cell>CIFAR-10 CIFAR-100</cell><cell>50K 50K</cell><cell cols="3">Datasets with clean annotation -10K 10 -10K 100</cell><cell>32 ? 32 32 ? 32</cell><cell>? 0.0 ? 0.0</cell><cell>Hyperparameter ?</cell><cell>Description</cell></row><row><cell></cell><cell></cell><cell cols="4">Datasets with real world noisy annotation</cell><cell></cell><cell></cell></row><row><cell>Clothing1M Webvision 1.0</cell><cell>1M 66K</cell><cell>14K -</cell><cell>10K 2.5K</cell><cell>14 50</cell><cell>224 ? 224 256 ? 256</cell><cell>? 20.0 ? 38.5</cell><cell>Ec</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6a .</head><label>6a</label><figDesc>CIFAR-10 and CIFAR-100 are clean datasets, we describe the label noise injection in Appendix D.2. Clothing1M consists of 1 million training images from 14 categories collected from online shopping websites with noisy labels generated from surrounding texts. Its noise level is estimated as 38.5% [57]. Following<ref type="bibr" target="#b15">[33,</ref> 55], we use the mini WebVision dataset which contains the top 50 classes from the Google image subset of WebVision, which results in approximate 66 thousand images. The noise level of WebVision is estimated at 20% [6].As for data preprocessing, we apply normalization and regular data augmentation (i.e. random crop and horizontal flip) on the training sets of all datasets. The cropping size is consistent with existing works[11,<ref type="bibr" target="#b25">43]</ref>. Specifically, 32 for CIFAR-10 and CIFAR-100, 224 ? 224 for Clothing 1M (after resizing to 256 ? 256), and 227 ? 227 for Webvision.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>The test accuracy of CAR and CE with different target estimation strategies. All the following experiments use Cosine Annealing learning rate scheduler [12]. ? 0.07 80.98 ? 0.27 92.09 ? 0.12 75.38 ? 0.08 38.24 ? 0.55 74.89 ? 0.20 CAR with temporal ensembling [47] 89.52 ? 0.30 64.07 ? 2.04 80.52 ? 2.21 70.80 ? 0.38 10.28 ? 1.67 63.91 ? 1.65 CAR w/o target estimation 89.47 ? 0.50 76.91 ? 0.22 88.23 ? 0.22 69.91 ? 0.21 31.33 ? 0.38 55.68 ? 0.17 CE with our strategy 92.64 ? 0.21 75.51 ? 0.38 92.21 ? 0.11 68.53 ? 0.47 32.36 ? 0.44 73.01 ? 0.90 CE with temporal ensembling [47] 92.12 ? 0.16 72.87 ? 1.98 89.71 ? 1.43 70.45 ? 0.22 9.34 ? 0.78 66.38 ? 0.57 CE w/o target estimation 78.26 ? 0.74 56.42 ? 2.49 86.55 ? 1.06 46.34 ? 0.56 11.55 ? 0.35 48.86 ? 0.04</figDesc><table><row><cell>Dataset</cell><cell cols="2">CIFAR-10</cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Noise type</cell><cell>symm</cell><cell></cell><cell>asymm</cell><cell>symm</cell><cell></cell><cell>asymm</cell></row><row><cell>Noise ratio</cell><cell>40%</cell><cell>80%</cell><cell>40%</cell><cell>40%</cell><cell>80%</cell><cell>40%</cell></row><row><cell>CAR with our strategy</cell><cell>93.49</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In the following, note that ExE y|x = Ex,y = E D(x,y) , which denote expectation with respect to the corresponding conditional distributions.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with bounded instance and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1789" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07836</idno>
		<title level="m">Dacheng Tao, and Masashi Sugiyama. Parts-dependent label noise: Towards instance-dependent label noise</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning from noisy labels by regularized estimation of annotator confusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11244" to="11253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00189</idno>
		<title level="m">Are anchor points really indispensable in label-noise learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Imae for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06112</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6543" to="6553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can cross entropy loss be robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senlin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Joint Conferences on Artificial Intelligence</title>
		<meeting>the 29th International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2206" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05055</idno>
		<title level="m">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13726" to="13735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Co-matching: Combating noisy labels by augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12814</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-adaptive training: beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10319</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07394</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5050" to="5060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple and effective regularization methods for training on noisily labeled data with generalization guarantee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingli</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8580" to="8589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing neural networks by penalizing confident output distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self: learning to filter noisy labels with self-ensembling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juseung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combating label noise in deep learning using abstention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohd-Yusof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6234" to="6243" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
