<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Personalized Federated Learning with Gaussian Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Achituve</surname></persName>
							<email>idan.achituve@biu.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Shamsian</surname></persName>
							<email>aviv.shamsian@biu.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Navon</surname></persName>
							<email>aviv.navon@biu.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
							<email>gal.chechik@biu.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
							<email>ethan.fetaya@biu.ac.il</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<settlement>Isreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Personalized Federated Learning with Gaussian Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Federated learning aims to learn a global model that performs well on client devices with limited cross-client communication. Personalized federated learning (PFL) further extends this setup to handle data heterogeneity between clients by learning personalized models. A key challenge in this setting is to learn effectively across clients even though each client has unique data that is often limited in size. Here we present pFedGP, a solution to PFL that is based on Gaussian processes (GPs) with deep kernel learning. GPs are highly expressive models that work well in the low data regime due to their Bayesian nature. However, applying GPs to PFL raises multiple challenges. Mainly, GPs performance depends heavily on access to a good kernel function, and learning a kernel requires a large training set. Therefore, we propose learning a shared kernel function across all clients, parameterized by a neural network, with a personal GP classifier for each client. We further extend pFedGP to include inducing points using two novel methods, the first helps to improve generalization in the low data regime and the second reduces the computational cost. We derive a PAC-Bayes generalization bound on novel clients and empirically show that it gives non-vacuous guarantees. Extensive experiments on standard PFL benchmarks with CIFAR-10, CIFAR-100, and CINIC-10, and on a new setup of learning under input noise show that pFedGP achieves well-calibrated predictions while significantly outperforming baseline methods, reaching up to 21% in accuracy gain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there is a growing interest in applying learning in decentralized systems under the setup of federated learning (FL) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b65">66]</ref>. In FL, a server node stores a global model and connects to multiple end-devices ("clients"), which have private data that cannot be shared. The goal is to learn the global model in a communication-efficient manner. However, learning a single shared model across all clients may perform poorly when the data distribution varies significantly across clients. Personalized Federated Learning (PFL) <ref type="bibr" target="#b66">[67]</ref> addresses this challenge by jointly learning a personalized model for each client. While significant progress had been made in recent years, leading approaches still struggle in realistic scenarios. First, when the amount of data per client is limited, even though this is one of the original motivations behind federated learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b71">72]</ref>. Second, when the input distribution shifts between clients, which is often the case, as clients use different devices and sensors. Last, when we require well-calibrated predictions, which is an important demand from medical and other safety-critical applications. <ref type="bibr">35th</ref> Conference on Neural Information Processing Systems (NeurIPS 2021). <ref type="figure" target="#fig_0">Figure 1</ref>: pFedGP -learning a shared deep kernel function with client-specific GP models. Each client stores private data, possibly from a different distribution. The data is first mapped to an embedding space with a shared neural network across all clients. Then, using common kernels a GP is applied to the data of the client for model learning and inference. We illustrate the per-client kernel matrix k ? (x i , x j ). Bold cells indicate a stronger covariance.</p><p>Here, we show how Gaussian Processes (GPs) with deep kernel learning (DKL) <ref type="bibr" target="#b79">[80]</ref> is an effective alternative for handling these challenges. GPs have good predictive performance in a wide range of dataset sizes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b80">81]</ref>, they are robust to input noise <ref type="bibr" target="#b74">[75]</ref>, can adapt to shifts in the data distribution <ref type="bibr" target="#b47">[48]</ref>, and provide well-calibrated predictions <ref type="bibr" target="#b68">[69]</ref>. While regression tasks are more natural for GPs, here we focus on classification tasks for consistency with common benchmarks and learning procedures in the field; however, our approach is also applicable to regression tasks.</p><p>Consider a naive approach that fits a separate GP classifier to each client based on its personal data. Its performance heavily depends on the quality of the kernel, and standard kernels tend to work poorly in domains such as images. A popular solution to this problem is to use deep kernel learning (DKL) <ref type="bibr" target="#b79">[80]</ref>, where a kernel is applied to features outputted by a neural network (NN). Unfortunately, GPs with DKL can strongly overfit, often even worse than standard NNs <ref type="bibr" target="#b55">[56]</ref>, and thus negate the main benefit of using a GP. We solve this issue by jointly learning a shared kernel function across clients. As the kernel captures similarities between inputs, a single kernel should work well across clients, while using a separate GP per client will give the required flexibility for personalization.</p><p>We adapt a GP classifier recently proposed in <ref type="bibr" target="#b1">[2]</ref> which uses the P?lya-Gamma augmentation <ref type="bibr" target="#b56">[57]</ref> in a tree-structure model to the federated setting. We term our method pFedGP. We extend pFedGP by tailoring two inducing points (IPs) methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b69">70]</ref>. The first helps generalization in the low data regime and, unlike common inducing point methods, does not reduce the computational costs. The second does focus on reducing the computational cost to make our approach scalable and work in low-resource clients. We also adjust previous PAC-Bayes generalization bounds for GPs <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b63">64]</ref> to include the P?lya-Gamma augmentation scheme. These bounds are suitable for cases where the kernel is not learned, such as when new clients arrive after the shared NN was already learned.</p><p>Therefore, this paper makes the following contributions: (i) introduce pFedGP as a natural solution to PFL; (ii) develop two IP methods to enhance GP classifiers that use the P?lya-Gamma augmentation scheme and integrate them with pFedGP; (iii) derive a PAC-Bayes generalization bound on novel clients and show empirically that it gives meaningful guarantees; (iv) achieve state-of-the-art results in a wide array of experiments, improving accuracy by up to 21% 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Federated learning. In FL, clients collaboratively solve a learning task while preserving data privacy and maintaining communication efficiency <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b84">85]</ref>. FedAvg <ref type="bibr" target="#b50">[51]</ref> is an early but effective FL approach that updates models locally and averages them into a global model. Several optimization methods have been proposed for improving convergence in FL <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b76">77]</ref>. Other approaches focus on preserving client privacy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b89">90]</ref>, improving robustness to statistical diversity <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b87">88]</ref>, and reducing communication cost <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b60">61]</ref>. These methods aim to learn a global model across clients, limiting their ability to deal with heterogeneous datasets.</p><p>Personalized federated learning. To overcome client heterogeneity, PFL aims to introduce some personalization for each client in the federation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b72">73]</ref>. Recent methods include adapting multitask learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b66">67]</ref>, meta-learning approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b88">89]</ref>, and model mixing, where clients learn a mixture of the global and local models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref>. Other approaches utilize different regularization schemes to enforce soft parameter sharing <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b71">72]</ref>. Personalization in FL has also been explored through clustering approaches in which similar clients within the federation have a greater effect on one another <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b85">86]</ref>. Recently, <ref type="bibr" target="#b64">[65]</ref> proposed learning a central hypernetworks that acts on client representation vectors for generating personalized models.</p><p>Bayesian FL. Some studies put forward a Bayesian treatment to the FL setup. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> used variational inference with Bayesian NNs. <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b83">84]</ref> proposed a matching algorithm between local models based on the Beta-Bernoulli process to construct a global model. <ref type="bibr" target="#b13">[14]</ref> extended Bayesian optimization to FL setting via Thompson sampling. To scale the GP model they used random Fourier features. We use inducing points instead. <ref type="bibr" target="#b82">[83]</ref> proposed a federated learning framework that uses a global GP model for regression tasks and without DKL. Unlike this study, we focus on classification tasks with a personal GP classifier per client and advocate sharing information between clients through the kernel. <ref type="bibr" target="#b73">[74]</ref> used GPs in a client selection strategy. In <ref type="bibr" target="#b35">[36]</ref> an approach based on stein variational gradient descent was suggested. This method does not scale beyond small-sized networks. <ref type="bibr" target="#b46">[47]</ref> proposed a multivariate Gaussian product mechanism to aggregate local models. As we will show, this method is less suited when the data heterogeneity between clients is large.</p><p>Gaussian process classification. Unlike regression, in classification approximations must be used since the likelihood is not a Gaussian <ref type="bibr" target="#b58">[59]</ref>. Classic approaches include the Laplace approximation <ref type="bibr" target="#b78">[79]</ref>, expectation-propagation <ref type="bibr" target="#b52">[53]</ref>, and least squares <ref type="bibr" target="#b61">[62]</ref>. Recently, several methods were proposed based on the P?lya-Gamma augmentation <ref type="bibr" target="#b56">[57]</ref> for modeling multinomial distributions <ref type="bibr" target="#b45">[46]</ref>, GP classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b77">78]</ref>, few-shot learning <ref type="bibr" target="#b68">[69]</ref>, and incremental learning <ref type="bibr" target="#b1">[2]</ref>. Here we build on the last approach. Classification with GPs is commonly done with variational inference techniques <ref type="bibr" target="#b29">[30]</ref>, here we wish to exploit the conjugacy of the model to take Gibbs samples from the posterior. This approach yields well calibrated <ref type="bibr" target="#b68">[69]</ref> and more accurate models <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gaussian processes background</head><p>We first provide a brief introduction to the main components of our model. Detailed explanations are deferred to the Appendix. Scalars are denoted with lower-case letters (e.g., x), vectors with bold lowercase letters (e.g., x), and matrices with bold capital letters (e.g., X). In general, y = [y 1 , ..., y N ] T is the vector of labels, and X ? R N ?d is the design matrix with N data points whose i th row is x i . Gaussian processes. GPs map input points to target output values via a random latent function f . f is assumed to follow a Gaussian process prior f ? GP(m(x), k(x, x )), where the evaluation vector of f on X, f = [f (x 1 ), ..., f (x N )] T , has a Gaussian distribution f ? N (?, K) with means ? i = m(x i ) and covariance K ij = k(x i , x j ). The mean m(x) is often set to be the constant zero function, and the kernel k(x, x ) is a positive semi-definite function. The target values are assumed to be independent when conditioned on f . For Gaussian process regression the likelihood is Gaussian, p(y|f ) = N (f, ? 2 ). Therefore, the posterior p(f |y, X) is also Gaussian, and both the marginal and the predictive distributions have known analytic expressions. This is one of the main motivations behind using GPs, as most other Bayesian models have intractable inference.</p><p>Unfortunately, for Gaussian process classification (GPC) the likelihood, p(y|f ), is not a Gaussian and the posterior does not admit a closed-form expression. One approach for applying GPs to binary classification tasks is the P?lya-Gamma augmentation <ref type="bibr" target="#b56">[57]</ref>. Using this approach, we can augment the GP model with random variables ? from a P?lya-Gamma distribution, one for each example. As a result, p(f |y, X, ?) is a Gaussian density and p(?|y, X, f ) is a P?lya-Gamma density. This allows to use Gibbs sampling to efficiently sample from the posterior p(f , ?|y, X) for inference and prediction.</p><p>A key advantage of the P?lya-Gamma augmentation is that it benefits from fast mixing and has the ability of even a single value of ? to capture much of the volume of the marginal distribution over function values <ref type="bibr" target="#b45">[46]</ref>. Full equations and further details on the P?lya-Gamma augmentation scheme are given in Appendix A.1.</p><p>Deep kernel learning (DKL). The quality of the GP model heavily depends on the kernel function k(x i , x j ). For many data modalities, such as images, common kernels are not a good measure of semantic similarity. Therefore, in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b79">80]</ref> standard kernels are used over features outputted by a neural network g ? . For example, the RBF kernel k ? (</p><formula xml:id="formula_0">x i , x j ) = exp ? ||g ? (xi)?g ? (xj )|| 2 2 2</formula><p>. In regression, it is possible to directly backpropagate through the GP inference as it is given in closed-form. In our case, we use Fisher's identity <ref type="bibr" target="#b18">[19]</ref> to obtain stochastic gradients <ref type="bibr" target="#b68">[69]</ref>.</p><p>Inducing points. GPs require storing and inverting a kernel matrix on the entire training set which often limits its usage. A common solution to this problem is to use inducing point methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b69">70]</ref>.</p><p>The key idea is to replace the exact kernel with an approximation for fast computation. Usually, M N pseudo-inputs are learned such that the main computational bottleneck is in inverting M ? M matrices.</p><p>GP-Tree. We build on GP-Tree <ref type="bibr" target="#b1">[2]</ref>, a recent GP classifier that was shown to scale well with dataset size and the number of classes. GP-Tree turns the multi-class classification problem into a sequence of binary decisions along the tree nodes. Each node in the tree fits a binary GP classifier based on the P?lya-Gamma augmentation scheme and the data associated with that node. The leaf nodes correspond to the classes in the dataset. The tree is constructed by first computing a prototype for each class and then recursively performing divisive hierarchical clustering on these prototypes to two clusters at each node. Further details are given in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">pFedGP: federated learning with Gaussian processes</head><p>Now we describe our approach for applying personalized federated learning (PFL) with Gaussian processes. First, we extend GP-Tree to the FL setup and show how to use Gibbs sampling to learn the NN parameters. Then, we present two alternatives for this method that use inducing points. The first is for extremely limited-size datasets, while the second allows controlling the computational resources. We name our method pFedGP. An illustration of our method is given in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A full GP model</head><p>The training procedure follows the standard protocol in this field <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51]</ref>. We assume the existence of a server that holds the shared parameters ? (a NN). Let C denote the set of clients. For each client c ? C we denote by D c its local dataset of size N c . At each training iteration (round) the model is sent to S clients to perform kernel learning (|S| ? |C|). Each client c ? S updates its copy of the global model and then sends the updated model to the server. The server then averages over the updates to obtain a new global model.</p><p>At each client c, we perform kernel learning in the following manner. We first compute the feature representation of the data samples associated with the client using the shared network. Then, we build the hierarchical classification tree as discussed in Section 3 &amp; Appendix A.2. In <ref type="bibr" target="#b1">[2]</ref> the tree was built only once after a pre-training stage and the model parameters were learned using a variational inference approach. Here, we re-build the tree at each round using the most recent features and we use a Gibbs sampling procedure, as it allows this flexibility in building the tree and performs better when not prohibitive by computational limitations. Learning the network parameters ? with the Gibbs sampling approach can be done with two common objectives, the marginal likelihood, and the predictive distribution.</p><p>We denote by X v the data associated with the tree node v, i.e., the data points which have v on the path from the root node to their class leaf node. We denote by y v the binary label of these points, i.e., does their path go left or right at this node. And we denote by ? v the P?lya-Gamma random variables associated with node v. The marginal likelihood term for the full hierarchical classification tree is the sum of the separate marginal likelihood terms of all the nodes v in the tree:</p><formula xml:id="formula_1">L M L c (?; D c ) = v log p ? (y v |X v ) = v log p ? (y v |? v , X v )p(? v )d? v .<label>(1)</label></formula><p>Similar to <ref type="bibr" target="#b68">[69]</ref> we use a gradient estimator based on Fisher's identity <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_2">? ? L M L c (?; Dc) = v p ? (?v|yv, Xv)? ? log p ? (yv|?v, Xv)d?v ? v 1 L L l=1 ? ? log p ? (yv|? (l) v , Xv).<label>(2)</label></formula><p>Here, ? To use the predictive distribution as an objective, in each training iteration, after building the tree model, at each node we randomly draw a portion from the (node) training data and use it to predict the class label for the remaining part. We denote with X v and y v the training portion, x * v and y * v the input and the label of the point we are predicting, and P y * the path from the root node to the y * leaf node (i.e., the original class). Here we also take advantage of the independence between nodes to maximize the predictive distribution per node individually. The predictive distribution for a single data point:</p><formula xml:id="formula_3">L P D c (?; x * , y * ) = v?P y * log p ? (y * v |x * v , yv, Xv) = v?P y * log p ? (y * v |?v, x * v , yv, Xv)p(?v|yv, Xv)d?v.<label>(3)</label></formula><p>We use an approximate-gradient estimator based on posterior samples of ?:</p><formula xml:id="formula_4">? ? L P D c (?; x * , y * ) ? v?P y * 1 L L l=1 ? ? log p ? (y * v |? (l) v , x * v , y v , X v ).<label>(4)</label></formula><p>Where</p><formula xml:id="formula_5">p ? (y * v |? (l) v , x * v , y v , X v ) = p(y * v |f * )p ? (f * |? (l) v , x * v , y v , X v )df * does not have an analytical expression, but p ? (f * |? (l) v , x * v , y v , X v ) = p ? (f * |f , x * v , X v )p ? (f |? (l) v , y v , X v )df * is</formula><p>Gaussian with known parameters. We then compute the predictive distribution by performing Gauss-Hermite integration over f * . See exact expression in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Augmenting the model with inducing points: sample efficiency</head><p>The GP model described in Section 4.1 works well in most situations. However, when the number of data points per client is small, performance naturally degrades. To increase information sharing between clients and improve the per-client performance, we suggest augmenting the model with global inducing points shared across clients. When sending the model from the server to a client, we also send the inducing inputs and their labels. To streamline optimization and reduce the communication burden, we define the inducing inputs in the feature space of the last embedding layer of the shared NN. Therefore, usually, their size will be negligible compared to the network size.</p><p>We denote byX the learned inducing inputs and by? their fixed class labels. They are set evenly across classes. During training, we regard only the set of inducing inputs-labels (X,?) as the available (training) data and use them for posterior inference. More formally, we first compute p ? (f |?,?,X, X) = p ? (f |f ,X, X)p ? (f |?,?,X)df using its analytical expression for the actual training data and then compute the probability of y using Gauss-Hermite integration. Then we use Eq. 3 &amp; 4 for learning the network parameters and the inducing locations. At test time, to make full use of the training data, we combine the inducing inputs with the training data and use both to obtain the GP formulas and to make predictions. We note that with just using the inducing inputs at test time the model performs remarkably well, despite having almost no personalization component. See Appendix E.8 for a further discussion.</p><p>One potential issue with using IPs in this manner is that it distorts the true class distribution. As a result, the classifier may be more likely to predict a low-probability class during test time. We address this issue by adjusting the output distribution. In general, let p(y, x) and q(y, x) be two distributions that differ only in the class probabilities, i.e. p(x|y) = q(x|y), the predictive distribution follows:</p><formula xml:id="formula_6">q(y * |x * ) p(y * |x * ) ? q(x * |y * )q(y * ) p(x * |y * )p(y * ) =? q(y * |x * ) ? q(y * ) p(y * ) p(y * |x * ).<label>(5)</label></formula><p>We use this to correct the GP predictions to the original class ratios at each tree node. We found in our experiments that this correction generally improves the classifier performance for class imbalanced data. As an example for this phenomena, consider a binary classification problem having 90 examples from the first class and 10 examples from the second class (therefore, q(y = 0) = 0.9, and q(y = 1) = 0.1). Assume we defined 50 inducing inputs per class, so now during test time the model sees 140 samples from the first class and 60 samples from the second class which corresponds to probabilities p(y = 0) = 0.7 and p(y = 1) = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Augmenting the model with inducing points: computational efficiency</head><p>Learning the full GP model described in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Server executes:</head><p>Initialize shared network ? ? ? 0 Initialize M inducing inputs per class for all classes in the system # in pFedGP-IP variants only for each round t ? 1, 2, ... do: Sample S clients uniformly at random for each client c ? S in parallel:</p><formula xml:id="formula_7">? c t+1 , M c t+1 ? ClientUpdate(? t , M t ) # obtain updates from client c Update ? t+1 , M t+1 using FedAvg [51] update rule. ClientUpdate(?, M ): for each local epoch e ? 1, ..., E do: if e = 1:</formula><p>Build GP tree classifier using the personal dataset D c Update ?, M using gradient-based optimization methods on D c with L M L or L P D return ?, M Therefore, we propose an additional procedure based on inducing points to allow reduced complexity in low resource environments and scalability to larger dataset sizes.</p><p>This variant is based on the fully independent training conditional (FITC) method <ref type="bibr" target="#b69">[70]</ref>. The key idea is to cast all the dependence on the inducing points and assume independence between the latent function values given the inducing points. Here for brevity, we omit the subscripts denoting the client and the tree node. However, all quantities and data points are those that belong to a specific client and tree node. LetX ? R M ?d denote the pseudo-inputs (defined in the embedding space of the last layer of the NN), andf ? R M the corresponding latent function values. Here as well, the inducing inputs are defined globally at the server level and they are set evenly across classes.</p><p>We assume the following GP prior p(f ,f ) = N 0,</p><formula xml:id="formula_8">K N N K N M K M N K M M , where K M M is the kernel</formula><p>between the inducing inputs, K N N is a diagonal matrix between the actual training data, K N M is the kernel between the data and the inducing inputs, and we placed a zero mean prior. The likelihood of the dataset when factoring the inducing variables and the P?lya-Gamma variables (one per training sample), and the posterior over f , both have known analytical expressions. We can then obtain the posterior and marginal distributions by marginalizing overf . Here we will present the posterior and marginal distributions:</p><formula xml:id="formula_9">p(f |X, y, ?,X) = N (f |K N M B ?1 K M N ? ?1 ? ?1 ?, K N N ? K N M (K ?1 M M ? B ?1 )K M N ),<label>(6)</label></formula><formula xml:id="formula_10">p(y|X, ?,X) ? N (? ?1 ?|0, ? + K N M K ?1 M M K M N ).<label>(7)</label></formula><p>Where</p><formula xml:id="formula_11">? = diag(?), ? j = y j ? 1/2, ? = ? ?1 + diag(K N N ? K N M K ?1 M M K M N ), and B = K M M + K M N ? ?1 K N M . Importantly, we only need to invert M ? M or diagonal matrices. See full derivation in Appendix B.</formula><p>During test time, we usef to get the posterior of f * to compute the predictive distribution. Now we can use either the marginal or the predictive distribution to learn the shared NN parameters and the inducing locations. The complexity of applying this procedure is reduced to O(M 2 N c + M 3 ) in run-time, and O(M N c + M 2 ) in memory. While the (conditional) independence assumption between the latent function values may be restrictive, we found this method to be comparable with the full GP alternative in our experiments. Potentially, this can be attributed to the effect of sharing the inducing inputs among clients and the information that ? stores on f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Generalization bound</head><p>It is reasonable to expect that after we learned the system new clients will arrive. In such cases, we would like to use pFedGP without re-training the kernel function. Under this scenario, we can derive generalization bounds concerning only the GP classifier without taking into account the fixed neural network using PAC-Bayes bound <ref type="bibr" target="#b49">[50]</ref>. Having meaningful guarantees can be very important in safety-critical applications. The PAC-Bayes bound for GPC <ref type="bibr" target="#b63">[64]</ref> (with the Gibbs risk):</p><formula xml:id="formula_12">Theorem 1. Given i.i.d. samples D c = {(x i , y i )} Nc i=1</formula><p>of size N c drawn from any data distribution over X ? {?1, 1}, a GP posterior Q, and a GP prior P , the following bound holds, where the probability is over random data samples:</p><formula xml:id="formula_13">P r Dc {R(Q) &gt; R Dc (Q) + KL ?1 ber (R Dc (Q), (?, n, P, Q))} ? ?.<label>(8)</label></formula><p>Here, we have,</p><formula xml:id="formula_14">R(Q) = E (x * ,y * ) [P r f * ?Q(f * |x * ,Dc) {sign f * = y * }], RD c (Q) = 1 Nc Nc i=1 P r f i ?Q(f i |Dc) {sign fi = yi} (?, Nc, P, Q) = 1 Nc KL[Q || P ] + log Nc + 1 ? , KL ?1 ber (q, ) = max p?[0,1] KL ber [q || p] ?<label>(9)</label></formula><p>An important observation in <ref type="bibr" target="#b63">[64]</ref> is that the KL-divergence between the posterior and prior Gaussian processes is equivalent to the KL-divergence between the posterior and prior distribution of their values on the N c training samples. While <ref type="bibr" target="#b63">[64]</ref> assumed Q to be Gaussian, this observation still holds even without this assumption. However, when Q is no longer Gaussian, as is the case here, KL[Q(f ) || P (f )] no longer has a closed-form expression. We can show that for the P?lya-Gamma augmentation: where MI denotes the mutual information.</p><formula xml:id="formula_15">KL[Q(f ) || P (f )] = E Q(?) {KL[Q(f |?)||P (f )]} ? M I[f ; ?] = E Q(?) {KL[Q(f |?)||P (f )]} + E Q(f ,?) log Q(?) Q(?|f )<label>(10)</label></formula><p>Since Q(f |?) and P (f ) are Gaussian, the KL[Q(f |?)||P (f )] term has a close form expression so we only need to perform Monte-Carlo approximation on the expectation on ? on the first element. In the second expectation, Q(?) does not have a known expression. To estimate it, given To assess the quality of the bound, we partitioned the CIFAR-10 dataset to 100 clients. We trained a shared network using our full-GP variant on 90 clients and then recorded the generalization and test error on the remaining 10 clients four times, each with a different training set size. <ref type="figure" target="#fig_2">Figure 2</ref> shows the estimation of the generalization error bound (? = 0.01) vs the actual error on the novel clients with the Gibbs classifier. First, we observe that indeed the bound is greater than the actual test error for all points and that it is not vacuous. There is a strong correlation between the actual error and the bound. Secondly, unlike worst-case bounds (e.g., VC-dimension), this bound depends on the actual data and not only the number of data points.</p><formula xml:id="formula_16">{(? i , f i )} N i=1 sam- ples, we use Q(? i ) ? 1 N ?1 j =i Q(? i |f j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluated pFedGP against baseline methods in various learning setups. We present the result for the following model variants: (i) pFedGP, the full GP model (Section 4.1); (ii) pFedGP-IP-data, the model with IPs described in Section 4.2; and (iii) pFedGP-IP-compute, the model with IPs described in Section 4.3. For pFedGP and pFedGP-IP-compute, the results obtained by maximizing the predictive and marginal likelihood were similar, with a slight advantage to the former. Therefore, we present here the results only for the predictive alternative and defer the results of the marginal alternative to the Appendix. Additional experiments, ablation study, and further analyses are provided in Appendix E. Unless stated otherwise, we report the average and the standard error of the mean (SEM) over three random seeds of the federated accuracy, defined as the average accuracy across all clients and samples.</p><p>Datasets. All methods were evaluated on CIFAR-10, CIFAR-100 <ref type="bibr" target="#b37">[38]</ref>, and CINIC-10 <ref type="bibr" target="#b14">[15]</ref> datasets. CINIC-10 is more diverse since it combines images from CIFAR-10 and ImageNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>Compared methods. We compared our method against the following baselines: (1) Local, pFedGP full model on each client with a private network and no collaboration with other clients;</p><p>(2) FedAvg <ref type="bibr" target="#b50">[51]</ref>, a standard FL model with no personalization component; <ref type="formula" target="#formula_3">(3)</ref> FOLA <ref type="bibr" target="#b46">[47]</ref>, a Bayesian method that used a multivariate Gaussian product mechanism to aggregate local models; (4) FedPer <ref type="bibr" target="#b3">[4]</ref>, a PFL approach that learns a personal classifier for each client on top of a shared feature extractor; <ref type="bibr" target="#b4">(5)</ref> LG-FedAvg <ref type="bibr" target="#b43">[44]</ref>, a PFL method that uses local feature extractor per client and global output layers; (6) pFedMe <ref type="bibr" target="#b71">[72]</ref>, a PFL method which adds a Moreau-envelopes loss term; (7) FedU <ref type="bibr" target="#b17">[18]</ref>, a recent multi-task learning approach for PFL that learns a model per client; (8) pFedHN <ref type="bibr" target="#b64">[65]</ref>, a recent PFL approach that uses a hypernetwork to generate client-specific networks.</p><p>Training protocol. We follow the training strategy proposed in <ref type="bibr" target="#b64">[65]</ref>. We limit the training process to 1000 communication rounds, in each we sample five clients uniformly at random for model updates. The training procedure is different in the FOLA and pFedHN baselines, so we used an equivalent communication cost. In LG-FedAvg, we made an extra 200 communication rounds after a pre-training stage with the FedAvg model for 1000 communication rounds. In the local model, we performed 100 epochs of training for each client. In all experiments, we used a LeNet-based network <ref type="bibr" target="#b39">[40]</ref> having two convolution layers followed by two fully connected layers and an additional linear layer. We tuned the hyperparameters of all methods using a pre-allocated held-out validation set. Full experimental details are given in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Standard PFL setting</head><p>We first evaluated all methods in a standard PFL setting <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b71">72]</ref>. We varied the total number of clients in the system from 50 to 500 and we set the number of classes per client to two/ten/four for CIFAR-10/CIFAR-100/CINIC-10 respectively. Since the total number of samples in the system is fixed, the number of samples per client changed accordingly. For each client, the same classes appeared in the training and test set.</p><p>The results are presented in <ref type="table" target="#tab_0">Table 1</ref>. They show that: (1) The performance of the local baseline is significantly impaired when the number of samples per client decreases, emphasizing the importance of federated learning in the presence of limited local data. (2) FedAvg and FOLA, which do not use personalized FL, perform poorly in this heterogeneous setup. (3) pFedGP outperforms or is on par with previous state-of-the-art approaches when local data is sufficient (e.g., 50 clients on all datasets). When the data per client becomes limited, pFedGP achieves significant improvements over competing methods; note the 9% and 21% difference in CIFAR-100 over 100 and 500 clients, respectively. (4) pFedGP-IP-compute often achieves comparable results to pFedGP and is often superior to pFedGP-IP-data. We believe that it can be attributed to the fact that in pFedGP-IP-compute   <ref type="table" target="#tab_0">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy Decrease (%)</head><p>FedPer <ref type="bibr" target="#b3">[4]</ref> 28.1 ? 0.9 -35.6 LG-FedAvg <ref type="bibr" target="#b43">[44]</ref> 26.9 ? 0.9 -28.3 pFedme <ref type="bibr" target="#b71">[72]</ref> 33. A desired property from PFL classifiers is the ability to provide uncertainty estimation. For example, in decision support systems, such as in healthcare applications, the decision-maker should have an accurate estimation of the classifier confidence in the prediction. Here, we quantify the uncertainty through calibration. <ref type="figure" target="#fig_4">Figure 3</ref> compares all methods both visually and using common metrics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b54">55]</ref> on the CIFAR-100 dataset with 50 clients. Expected calibration error (ECE) measures the weighted average between the classifier confidence and accuracy. Maximum calibration error (MCE) takes the maximum instead of the average. And, Brier score (BRI) <ref type="bibr" target="#b6">[7]</ref> measures the average squared error between the labels and the prediction probabilities. The figure shows that pFedGP classifiers are best calibrated across all metrics in almost all cases. We note that with temperature scaling, the calibration of the baseline methods can be improved <ref type="bibr" target="#b24">[25]</ref>; however, choosing the right temperature requires optimization over a separate validation set, which our model does not need. Additional calibration results, including temperature scaling, are presented in Appendix E.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">PFL with input noise</head><p>In real-world federated systems, the clients may employ different measurement devices for data collection (cameras, sensors, etc.), resulting in different input noise characteristics per client. Here, we investigate pFedGP performance in this type of personalization. To simulate that, we partitioned CIFAR-10/100 to 100 clients similar to the protocol described in Section 6.1, we defined 57 unique distributions of image corruption noise <ref type="bibr" target="#b28">[29]</ref>, and we assigned a noise model to each client. Then for each example in each client, we sampled a corruption noise according to the noise model allocated to that client. Here we show the results for the noisy CIFAR-100 dataset in <ref type="table" target="#tab_1">Table 2</ref>. Further details on the perturbations performed and result for the noisy CIFAR-10 are given in the Appendix 2 . We observe a significant gap in favor of the pFedGP variants compared to baseline methods. Note that using global inducing points is slightly less beneficial in this case since they are defined globally and therefore are not tied to a specific noise type as the real client data is. FL are dynamic systems. For example, novel clients may enter the system after the model was trained, possibly with a data distribution shift. Adapting to a new OOD client is both challenging and important for real-world FL systems. To evaluate pFedGP in this scenario, we followed the learning protocol proposed in <ref type="bibr" target="#b64">[65]</ref>. We partitioned the CIFAR-10 dataset into two groups. The data in the first group was distributed between 90 clients for model training. The remaining data from the second group was distributed between an additional 10 clients that were excluded during training. Within each group, we set the class probabilities in each client by sampling from a Dirichlet distribution with the same ? parameter. For the training group, we set ? = 0.1, trained the shared model using these clients, and froze it. Then, we evaluated the models on the second group by varying ? ? {.1, .25, .5, .75, 1}, on the remaining 10 clients. As ? moves away from 0.1 the distribution shift between the two groups increases, resulting in more challenging OOD clients. <ref type="figure" target="#fig_5">Figure 4</ref> reports the generalization gap as a function of the Dirichlet parameter ?. The generalization gap is computed by taking the difference between the average test accuracy of the (ten) novel clients and the average test accuracy of the (ninety) clients used for training. From the figure, here as well, pFedGP achieves the best generalization performance for all values of ?. Moreover, unlike baseline methods, pFedGP does not require any parameter tuning. Several baselines were excluded from the figure since they had a large generalization gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Generalization to out-of-distribution (OOD) novel clients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this study, we proposed pFedGP, a novel method for PFL. pFedGP learns a kernel function, parameterized by a NN, that is shared between all clients using a personal GP classifier on each client. We proposed three variants for pFedGP, a full model approach that generally shows the best performance and two extensions to it. The first is most beneficial when the number of examples per class are small while the second allows controlling the computational requirements of the model. We also derived PAC-Bayes generalization bound on novel clients and empirically showed that it gives non-vacuous guarantees. pFedGP provides well-calibrated predictions, generalizes well to OOD novel clients, and consistently outperforms competing methods.</p><p>Broader impact: Our method shares the standard communication procedure of FL approaches, where no private data is directly communicated across the different nodes in the system. This protocol does not explicitly guarantee that no private information can be inferred at this time. As we show, pFedGP is particularly useful for clients with little data, and for clients that have strongly different distribution. This has great potential to improve client personalization in real-world systems, and do better at handling less common data. The latter is of great interest for decision support systems in sensitive domains such as health care or legal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Extended background A.1 The P?lya-Gamma augmentation</head><p>A random variable ? has a P?lya-Gamma distribution if it can be written as an infinite sum of independent gamma random variables:</p><formula xml:id="formula_17">? D = 1 2? 2 ? k=1 g k (k ? 1/2) 2 + c 2 /(4? 2 ) ,<label>(11)</label></formula><p>where b &gt; 0, c ? R, and g k <ref type="figure" target="#fig_0">? Gamma(b, 1)</ref>.</p><p>This random variable was proposed in <ref type="bibr" target="#b56">[57]</ref> as it has the following desired property -For b &gt; 0 the following identity holds:</p><formula xml:id="formula_18">(e f ) a (1 + e f ) b = 2 ?b e ?f E ? [e ??f 2 /2 ],<label>(12)</label></formula><p>where ? = a ? b/2, and ? has the P?lya-Gamma distribution, ? ? P G(b, 0). In <ref type="bibr" target="#b56">[57]</ref> the authors also devised an efficient sampling algorithms for P?lya-Gamma random variables.</p><p>Suppose we are given with latent function values f ? R N having a binary classification assignment y ? {0, 1} N . Let the prior over f ? N (?, K). The likelihood can be written as,</p><formula xml:id="formula_19">p(y|f ) = n j=1 ?(f j ) yj (1 ? ?(f j )) 1?yj = n j=1 (e fj ) yj 1 + e fj = E ? ? ? 2 ?n exp ? ? n j=1 ? j f j ? ? j f 2 j 2 ? ? ? ? ,<label>(13)</label></formula><p>where ?(?) is the sigmoid function. Namely, we used Eq. 12 to augment the model with P?lya-Gamma variables (one per sample) such that the original likelihood is recovered when marginalizing them out. Now, the augmented likelihood, p(y|f , ?), is proportional to a diagonal Gaussian and the posteriors in the augmented space have known expressions:</p><formula xml:id="formula_20">p(f |y, ?) = N (f |?(K ?1 ? + ?), ?), p(?|y, f ) = P G(1, f ).<label>(14)</label></formula><p>Where ? j = y j ? 1/2, ? = (K ?1 + ?) ?1 , and ? = diag(?). We can now sample from p(f , ?|y, X) using block Gibbs sampling and get Monte-Carlo estimations of the marginal and predictive distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 GP-Tree</head><p>Our method builds upon the method presented in <ref type="bibr" target="#b1">[2]</ref>. It was shown to scale well both with dataset size and the number of classes, outperforming other GPC methods on standard benchmarks. We provide here a summary of this method, termed GP-Tree. GP-Tree uses the P?lya-Gamma augmentation, which is designed for binary classification tasks, in a (binary) tree-structure hierarchical model for multi-class classification tasks. Given a training dataset D = (X, y) of features and corresponding labels from {1, ..., T} classes, D is partitioned recursively to two subsets, according to classes, at each tree level until reaching leaf nodes with data from only one class. More concretely, initially, feature vectors for all samples are obtained (using a NN), then a class prototype is generated by averaging the feature vectors belonging to the same class for all classes. Finally, a tree is formed using the divisive hierarchical clustering algorithm k-means++ <ref type="bibr" target="#b4">[5]</ref> with k = 2 on the class prototypes. Partitioning the data in this manner is sensible since NNs tend to generate points that cluster around a single prototype for each class <ref type="bibr" target="#b67">[68]</ref>. After building the tree, a GP model is assigned to each node to make a binary decision based on the data associated with that node. Let f v ? GP(m v , k v ) be the GP associated with node v. We denote all the GPs in the tree with F . The induced likelihood of a data point having the class t is given by the unique path P t from the root to the leaf node corresponding to that class:</p><formula xml:id="formula_21">p(y = t|F ) = v?P t ?(f v ) yv (1 ? ?(f v )) 1?yv ,<label>(15)</label></formula><p>where y v = 1 if the path goes left at v and zero otherwise. y v can be viewed as the (local) node label assignment of the example. Since this likelihood factorizes over the nodes, we can look at the nodes separately. Therefore, in the following, we will omit the subscript v for brevity; however, all datum and quantities are those that belong to a specific node v.</p><p>In <ref type="bibr" target="#b1">[2]</ref> two methods for applying GP inference were suggested: a variational inference (VI) approach and a Gibbs sampling procedure. The former is used when datasets are large by constructing a variational lower bound to learn the model parameters (e.g., the NN parameters), while the latter is used for Bayesian inference only when using a fixed model. Here we will focus on learning and inference with the Gibbs sampling procedure only (see main text for further details). To obtain the augmented marginal distribution and augmented predictive distribution for a novel point x * at each node, we can sample ? (a vector for each node) and use the following rules:</p><formula xml:id="formula_22">p(y|?, X) = p(y|?, X, f )p(f )df ? N (? ?1 ?|0, K + ? ?1 ),<label>(16)</label></formula><formula xml:id="formula_23">p(f * |x * , X, y, ?) = N (f * |? * , ? * ), ? * = (k * ) T (? ?1 + K) ?1 ? ?1 ?, ? * = k * * ? (k * ) T (? ?1 + K) ?1 k * ,<label>(17)</label></formula><p>p(y * |x * , X, y, ?) = p(y * |f * )p(f * |x * , X, y, ?)df * .</p><p>Where we assumed a zero mean prior,</p><formula xml:id="formula_25">k * * = k(x * , x * ), k * [i] = k(x i , x * ), and K[i, j] = k(x i , x j ).</formula><p>The integral in Eq. 18 is intractable, but can be computed numerically with 1D Gaussian-Hermite quadrature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B pFedGP-IP-compute detailed derivation</head><p>Here we describe in more detail our pFedGP-IP-compute variant. The key idea behind this method is to cast all the dependence on the inducing points and assume independence between the latent function values given the inducing points. Since the inference problem factorizes over the clients and tree nodes, we may compute all quantities separately for each client and tree node and only afterward aggregate the results. Therefore, in the below, we omit the subscripts denoting the client and node; however, all quantities belong to a specific client and node. You may recall thatX ? R M ?d denote the pseudo-inputs, defined in the embedding space of the last layer of the NN and are shared by all clients, andf ? R M are the corresponding latent function values. We assume the following GP prior</p><formula xml:id="formula_26">p(f ,f ) = N (0, [ K N N K N M K M N K M M ])</formula><p>. The data likelihood of the dataset when factoring the inducing variables and the P?lya-Gamma variables (one per training sample) is proportional to a Gaussian:</p><formula xml:id="formula_27">p(y|X, ?,f ,X) = N n=1 p(y n |x n , ?,f ,X) ? N (? ?1 ?|K N M K ?1 M Mf , ?),<label>(19)</label></formula><p>where, ? = diag(?), ? j = y j ? 1/2, and</p><formula xml:id="formula_28">? = ? ?1 + diag(K N N ? K N M K ?1 M M K M N )</formula><p>. The posterior overf is obtained using Bayes rule:</p><formula xml:id="formula_29">p(f |?, y, X,X) = N (f |K M M B ?1 K M N ? ?1 ? ?1 ?, K M M B ?1 K M M ),<label>(20)</label></formula><p>where</p><formula xml:id="formula_30">B = K M M + K M N ? ?1 K N M .</formula><p>The posterior distribution over f can be obtained by marginalization overf :</p><p>p(f |?, y, X,X) = p(f |f , X,X)p(f |?, y, X,X)df</p><formula xml:id="formula_31">= N (f |K N M B ?1 K M N ? ?1 ? ?1 ?, K N N ? K N M (K ?1 M M ? B ?1 )K M N ).<label>(21)</label></formula><p>We note that while we usef for predictions, we still need f in order to sample ?.</p><p>Given ? and the expression for the posterior overf , we can compute the predictive distribution for a novel input x * : p(f * |x * , ?, y, X,X) = p(f * |x * ,f )p(f |?, y, X,X)df</p><formula xml:id="formula_32">= N (f * |(k * ) T B ?1 K M N ? ?1 ? ?1 ?, k * * ? (k * ) T (K ?1 M M ? B ?1 )k * ),<label>(22)</label></formula><p>where k * * = k(x * , x * ), and k * [i] = k(x i , x * ).</p><p>The marginal distribution is given by:</p><formula xml:id="formula_33">p(y|?, X,X) = p(y|?,f , X,X)p(f |X)df ? N (? ?1 ?|0, ? + K N M K ?1 M M K M N ).<label>(23)</label></formula><p>The full model marginal likelihood is given by:</p><formula xml:id="formula_34">L M L c (?; D c ) = v log p ? (y v |X v ,X v ) = v log p ? (y v |? v , X v ,X v )p(? v )d? v ,<label>(24)</label></formula><p>and the predictive distribution for a single data point x * having the class y * :</p><formula xml:id="formula_35">L P D c (?; x * , y * ) = v?P y * log p ? (y * v |x * v , y v , X v ,X v ) = v?P y * log p ? (y * v |? v , x * v , y v , X v ,X v )p(? v |y v , X v )d? v . = v?P y * log p(? v |y v , X v ) p ? (y * v |f * v )p(f * v |? v , x * v , y v , X v ,X v )df * v d? v .<label>(25)</label></formula><p>To learn the model parameters, we first use block gibbs sampling with the posterior distributions p(f |y, ?, X,X), and p(?|y, f ) = P G(1, f ). Then, we use Fisher's identity <ref type="bibr" target="#b18">[19]</ref> to obtain gradients w.r.t the model parameters with the marginal or predictive distributions.</p><p>Note that to speed up inference at test time, some computations that do not depend on x * can be done offline. Importantly, we can sample and cache ? and use it to compute ? and the Cholesky decomposition of B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Generalization bound -derivation</head><p>In Section 5 we presented an expression for the KL-divergence between the posterior and the prior (Eq. 10). Now we present the derivation:</p><formula xml:id="formula_36">KL[Q(f ) || P (f )] = Q(f ) log Q(f ) P (f ) df = Q(f , ?) log Q(f ) P (f ) df d? = Q(f , ?) log Q(f )Q(?|f ) P (f )Q(?|f ) df d? = Q(f , ?) log Q(f |?) P (f ) df d? + Q(f , ?) log Q(?) Q(?|f ) df d? = Q(f , ?) log Q(f |?) P (f ) df d? + Q(f , ?) log Q(f )Q(?) Q(f , ?) df d? = E Q(?) {KL[Q(f |?)||P (f )]} ? M I[f ; ?],<label>(26)</label></formula><p>Where, the KL-divergence in the expectation term is between the prior, P (f ) = N (?, K), and the posterior, Q(f |?) = N (?(K ?1 ? + ?), ?). Therefore, it has the following closed-form:  </p><formula xml:id="formula_37">KL[Q(f |?)||P (f )] = 1 2 {log |K| |?| ? Nc + tr(K ?1 ?) + (?(K ?1 ? + ?) ? ?) T K ?1 (?(K ?1 ? + ?) ? ?)}<label>(27)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental details</head><p>Datasets. We evaluated pFedGP on the CIFAR-10, CIFAR-100 <ref type="bibr" target="#b37">[38]</ref>, and CINIC-10 <ref type="bibr" target="#b14">[15]</ref> datasets. CIFAR-10/100 contain 60K images from 10/100 distinct classes, respectively, split to 50K for training and 10K for testing. CINIC-10 is a more diverse dataset. It is an extension of CIFAR-10 via the addition of down-sampled ImageNet <ref type="bibr" target="#b15">[16]</ref> images. It contains 270K images split equally to train, validation, and test sets from 10 distinct classes.</p><p>Data assignment. For partitioning the data samples across clients we followed the procedure suggested in <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b71">72]</ref>. This procedure produces clients with a varying number of samples and a unique set of k classes for each client. First, we sampled k classes for each client. In general, we used k = 2 in CIFAR-10 experiments (e.g., Sections 5 &amp; 6.1), k = 10 in CIFAR-100 experiments (e.g., Sections 6.1 &amp; 6.2), and k = 4 in CINIC-10 experiments (e.g., Section 6.1). Next, to assign unique images for each client, we iterated over the classes and clients; for each client i having the class c, we sampled an unnormalized class fraction ? i,c ? U (.4, .6). We then assigned to the i th client ?i,c j ?j,c images from the overall samples of class c. Hyperparameter tuning. We used a validation set for hyperparameter selection and early stopping in all methods. For the CIFAR datasets, we pre-allocated a validation set of size 10K from the training set. For the CINIC-10 dataset, we used the original split having a validation set of size 90K. The hyperparameters for all methods and all datasets were selected based on the learning setups with 50 clients. We searched over the learning-rates {1e ? 1, 5e ? 2, 1e ? 2, 5e ? 3} for all methods, and personal learning-rates {5e ? 2, 1e ? 2, 5e ? 3, 1e ? 3} for baseline methods only (pFedGP is a non-parametric approach and therefore does not optimize any private parameters). For pFedGP we searched over the number of epochs on sampled clients during training in {1, 3}. For baseline methods, since the training procedure varies significantly, we ran the baselines FOLA, LG-FedAvg, pFedMe, Per-FedAvg, FedU, and pFedHN according to the recommended configuration in their papers or code (which is usually a few epochs). Regarding FedAvg, FedPer, and pFedMe, we followed the protocol suggested by pFedME of using 20 iterations per client. For pFedGP We also searched over a scaling factor for the loss function in {1, 2}. We used the RBF kernel function with a fixed length scale of 1 and an output scale of 8. We used 20 parallel Gibbs chains for training and 30 parallel Gibbs chains for testing with 5 MCMC steps between samples in both. To compute the predictive distribution, at each tree node, we averaged over the log probabilities since it didn't impact the results but yielded a more calibrated model. In the reliability diagrams, we used 50 steps since as the number of steps increases usually the model becomes better calibrated (without a discernible accuracy change). In pFedGP-IP-data and pFedGP-IP-compute experiments, we used 100 inducing points per class. In all baselines that use FedAvg update rule, we used a variant of FedAvg in which a uniform weight was given to all clients during the global network update. All experiments were done on NVIDIA GeForce RTX 2080 Ti having 11GB of memory. Noisy datasets (Sections 6.2 &amp; E.3). To generate a noisy version of the CIFAR-10 and CIFAR-100 datasets, we used the image-corruptions package <ref type="bibr" target="#b28">[29]</ref>. We simulated the following 19 corruptions (Gaussian noise, shot noise, impulse noise, defocus blur, glass blur, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transform, pixelate, jpeg compression, speckle noise, Gaussian blur, spatter, saturate) with a corruption severity of <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5)</ref>, resulting in 57 unique noise models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional experiments E.1 Generalization bound -additional experiments</head><p>Generalization bound with the Bayes risk. In Section 5 we assessed the quality of the lower bound with the Gibbs risk. However, often we are interested in a deterministic predictor. Also, we would like to get an estimate of the error with a classifier that is closer to our estimate of y * with the Gauss-Hermite quadrature. This can be achieved with the Bayes risk <ref type="bibr" target="#b59">[60]</ref>   <ref type="figure" target="#fig_6">Figure 5</ref> shows an estimation of the generalization error bound vs. the actual error on the novel clients with the Bayes classifier. From this figure we observe similar patterns to those seen in <ref type="figure" target="#fig_2">Figure 2</ref>. In general, the Bayes classifier performs better than the Gibbs classifier.</p><p>Generalization with bigger networks. To test how the bound behaves with larger networks, we also evaluated the Gibbs risk on ResNet18 <ref type="bibr" target="#b27">[28]</ref> and MobileNetV2 <ref type="bibr" target="#b62">[63]</ref>, having ? 11.4M and ? 2.8M parameters correspondingly. In both networks, we replaced the final fully-connected layer with a linear layer of size 512 and we removed batch normalization layers. Results are presented in <ref type="figure" target="#fig_8">Figure 7</ref>. We observe a similar behavior with these networks to the one seen with the LeNet backbone. Namely, the bound is data-dependent and gives non-vacuous guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Varying the training set size</head><p>To decouple the effect of the local dataset size from the number of clients in the system, we altered the setting in Section 6.1. Here, we fixed the number of clients to 50 and used stratified sampling to sample {1000, 2000, 5000, 10000, 20000, 40000} examples from the training dataset of CIFAR-10, where 40000 is the total number of training examples. Then, similar to Section 6.1 we randomly assigned two classes per client and partitioned the (sampled) data across clients. <ref type="figure" target="#fig_7">Figure 6</ref> shows that all methods suffer from accuracy degradation when the training data size decrease; however, in pFedGP methods, the reduction is less severe compared to baseline methods. We especially note pFedGP-IP-data which shows remarkable accuracy in the extremely low data regime (77.7% accuracy with only 1000 training examples). These results can be attributed to the sharing of the inducing  <ref type="bibr" target="#b43">[44]</ref> 41.5 ? 0.2 pFedme <ref type="bibr" target="#b71">[72]</ref> 36.3 ? 0.0 FedU <ref type="bibr" target="#b17">[18]</ref> 24.8 ? 0.0 pFedHN <ref type="bibr" target="#b64">[65]</ref> 35.7 ? 0.3</p><p>Ours pFedGP-IP-data 46.1 ? .05 pFedGP-IP-compute 46.5 ? 0.5 pFedGP 46.7 ? 0.2  inputs which effectively increase the size of the training data per client. The LG-FedAvg baseline was excluded from this figure since it showed low accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 PFL with input noise under a homogeneous class distribution</head><p>In Section 6.2 we evaluated pFedGP under two types of variations between the clients: (i) a unique input noise, and (ii) in the class distribution. It was done on the CIFAR-100 dataset with 100 clients. Here, we consider only a shift in the input noise between clients while having a balanced class distribution in all clients. We do so on the CIFAR-10 dataset (i.e., each client has 10 classes distributed approximately equal). Similarly to Section 6.2 we configured 100 clients, distributed the data among them, and assigned a noise model to each client from a closed-set of 57 noise distributions. <ref type="table" target="#tab_3">Table 3</ref> shows that in this setting as well, pFedGP and its variants achieve high accuracy and surpass all baseline methods by a large margin. From the table, we also notice that FedAvg, which performs well in balanced class distribution setups, outperforms all competing methods except ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Homogeneous federated learning with CIFAR-10</head><p>pFedGP is a non-parametric classifier offered for personalized federated learning. Therefore, it only needs to model classes that are present in the training set. Here, for completeness, we evaluate pFedGP against top-performing baselines on the CIFAR-10 dataset, where all classes are represented in all clients. We do so on a homogeneous federated learning setup, i.e., all classes are distributed equally across all clients. Data assignment was done similarly to the procedure described in Section 6.1 (see Section D for more details). <ref type="table" target="#tab_4">Table 4</ref> shows that pFedGP outperforms all PFL baseline methods by a large margin in this setting as well. An interesting, yet expected, observation from the table is that FedAvg performs well under this (IID) setup. This result connects to a recent study that suggested that under a smooth, strongly convex loss when the data heterogeneity is below some threshold, FedAvg is minimax optimal <ref type="bibr" target="#b10">[11]</ref>. We note here that modeling classes that are not present in the training set with pFedGP can be accomplished easily with one of the inducing points variants of pFedGP. We note here that including the network processing time will add a constant factor of 0.03 seconds.</p><p>In addition to the above test, we also tracked pFedGP full model and baseline methods memory usage and runtime on CIFAR-10 and CIFAR-100 with 50 clients during training. For comparability, we fixed the number of epochs that each sampled client makes to one for all methods. We found that pFedGP computational requirements are reasonable for running it in current FL systems <ref type="bibr" target="#b8">[9]</ref>. pFedGP needed ? 1.4/1.6 GB memory, and a run-time of ? 1/2 hours for CIFAR-10/100 correspondingly. Baseline methods needed 1.1-1.3 GB memory and took ? 25 minutes to run on both datasets. According to this naive testing, pFedGP is computationally more intensive compared to standard methods such as FedAvg. However, in return for that additional complexity, it obtains substantial performance gains compared to the baseline methods. Furthermore, when implementing pFedGP there are some trade-offs, for example, iterating over the tree can be either sequential to obtain lower memory cost or parallelized to obtain shorter running times. Or, when applying the Gibbs sampling, the complexity can be affected by the number of parallel Gibbs chains and the number of MCMC steps which constitute a trade-off between accuracy and runtime. Finally, note that using only one epoch of training damaged severely the performance of several baseline methods. Therefore, to obtain the same accuracy as reported in this paper the gaps in run-time are actually smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.6 Meta-Learning approaches for FL</head><p>In this study we advocate the use of GPs in general, and pFedGP specifically, in FL systems. A key motivation for using GPs is that often the data on clients is limited. Alternatively, we could have used other methods that were found to work well with limited data. Specifically, methods that are based on the model-agnostic meta-learning (MAML) <ref type="bibr" target="#b42">[43]</ref> learning procedure. Here, we compare pFedGP against the MAML-based federated learning approach Per-FedAvg <ref type="bibr" target="#b20">[21]</ref> under the setting presented in Section 6.1 in the main text. We observed the following results: 83.5 ? .05/81.7 ? 0.4/76.4 ? 1.0 on CIFAR-10, and 45.6 ? 0.2/41.0 ? 1.4/30.2 ? .02 on CIFAR-100 with 50/100/500 clients respectively. Comparing this method to pFedGP reveals that our approach outperforms this baseline as well.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7 Predictive vs marginal likelihood</head><p>In the main text, we presented two alternatives for learning the model parameters with pFedGP and pFedGP-IP-compute, the predictive distribution, and the marginal likelihood (see <ref type="bibr">Section 4)</ref>. We now compare between these two alternatives in <ref type="table" target="#tab_6">Table 6</ref> under the standard setup presented in Section 6.1. The table shows that for both pFedGP and pFedGP-IP-compute the two variants are comparable with a slight advantage to the predictive distribution objective. Nevertheless, using the marginal likelihood usually results in a better-calibrated model (Section E.9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.8 pFedGP-IP-data Ablation</head><p>Recall that for the pFedGP-IP-data variant during training we build the kernel with the (shared) inducing inputs only. Yet, during test time, to account for the personal data, we use both the inducing inputs and the training data of the client for building the kernel. This method is especially effective in cases where the data per client is limited. Here, we evaluate this method without using the actual training data during test time. This means that the only personalization derives from the personal tree structure that is formed based on the actual training data of the client. Remarkably, <ref type="table" target="#tab_6">Table 6</ref> shows that using this strategy yields high accuracy as well and it is often comparable to pFedGP-IP-data.</p><p>In pFedGP-IP-data we also introduced a correction term based on the class probabilities. Here we investigated the impact of this functionality as well. When the data is distributed uniformly among the client's classes, this correction term does not have any effect. Therefore, we tested its effect under a similar setting to the one presented in Section 6.3 on CIFAR-10. Namely, we sampled examples from classes according to a Dirichlet distribution with parameter ? = 0.1 for each client. With the class balancing, we noticed an accuracy of 84.4 ? 0.5 without it the accuracy dropped to 83.7 ? 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.9 Reliability diagrams</head><p>Now we present additional reliability diagrams for CIFAR-100 with 50 and 100 clients, with and without temperature scaling (See <ref type="figure" target="#fig_11">Figure 8</ref> for unified diagrams and <ref type="figure" target="#fig_0">Figures 9 -12</ref> for separate diagrams). For each baseline we applied a grid search over a temperature t ? {0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0, 200.0, 500.0, 1000.0}, chose the best temperature based on the pre-allocated validation set according to the ECE, and generated the diagram using the test set data. In addition, we present here reliability diagrams obtained by optimizing the marginal likelihood for pFedGP and pFedGP-IP-compute. From the figures, pFedGP does not gain from temperature scaling as baseline methods do since it is a calibrated classifier by design. Although this procedure improves the calibration of baseline methods, we note that finding the right temperature requires having a separate validation set which often can be challenging to obtain for problems in the low data regime.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 1 )</head><label>1</label><figDesc>v , ..., ? (L) v are samples from the posterior at node v. Due to the P?lya-Gamma augmentation p ? (y v |? (l) v , X v ) is proportional to a Gaussian density. The exact expression is give in Appendix A.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Section 4.1 requires inverting a matrix of size N c in the worst case (at the root node), which has O(N 3 c ) run-time complexity and O(N 2 c ) memory complexity. Algorithm 1 pFedGP. C clients indexed by c; E -number of local epochs; |S| -number of sampled clients; M -number of inducing inputs per class</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Test error vs an estimated upper bound over 10 clients with varying degrees of a training set data size. Each dot represents a combination of client and data size. In parenthesis -the average difference between the empirical and the test error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Note that if the summation for j includes f i , it might result in a biased estimator. Further details on estimating KL[Q(f ) || P (f )] are in Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Reliability diagrams on CIFAR-100 with 50 clients. Diagonal indicates perfect calibration. Each plot also shows the expected &amp; maximum calibration error (ECE &amp; MCE) and the Brier Score (BRI). Lower is better.the training data take an active part in the GP inference formulas (Eq. 6), while in pFedGP-IP-data the data impact in a weak manner only through the loss function. (5) pFedGP-IP-data is especially helpful when few samples per class are available, e.g., CIFAR-100 with 500 clients. That last point is further illustrated by decoupling the effect of the number of clients from that of the training set size.To illustrate that, in Appendix E.2 we fixed the number of clients and varied the number of training samples per class. From this experiment, we deduced that both factors (individually) contribute to pFedGP success.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Generalization to novel clients on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Test error vs. an estimated upper bound over 10 clients on CIFAR-10 with varying degrees of a training set data size using the Bayes classifier. Each dot represents a combination of client and data size. In parenthesis -the average difference between the empirical and the test error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Model performance with varying degrees of an average number of training samples per client (x-axis in log scale). Results are over 50 clients on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Test error vs. an estimated upper bound over 10 clients on CIFAR-10 with varying degrees of a training set data size on ResNet-18 (left) and MobileNetV2 (right). Each dot represents a combination of client and data size. In parenthesis -the average difference between the empirical and the test error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>defined by R Bayes (Q) := E (x * ,y * ) [sign E f * ?Q(f * |x * ,Dc) [f * ] = y * ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>50 clients. t = 1</head><label>1</label><figDesc>50 clients. Best temp. 100 clients. t = 1 100 clients. Best temp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Reliability diagrams over 50, 100 clients on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>-data pFedGP-IP-compute-pred.pFedGP-IP-compute-marg. pFedGP-pred. pFedGP-marg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Reliability diagrams on CIFAR-100 with 50 clients. Default temperature (t = 1). The last 5 figures are ours. -data pFedGP-IP-compute-pred. pFedGP-IP-compute-marg. pFedGP-pred. pFedGP-marg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Reliability diagrams on CIFAR-100 with 50 clients. Best temperature. The last 5 -data pFedGP-IP-compute-pred.pFedGP-IP-compute-marg. pFedGP-pred. pFedGP-marg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Reliability diagrams on CIFAR-100 with 100 clients. Default temperature (t = 1). The last 5 figures are ours. pFedGP-IP-compute-pred. pFedGP-IP-compute-marg. pFedGP-pred. pFedGP-marg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Reliability diagrams on CIFAR-100 with 100 clients. Best temperature. The last 5 figures are ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (? SEM) over 50, 100, 500 clients on CIFAR-10, CIFAR-100, and CINIC-10. The # samples/client indicates the average number of training samples per client. ? 0.2 82.9 ? 0.4 74.8 ? 0.5 52.1 ? 0.2 45.6 ? 0.3 30.9 ? 0.2 61.1 ? 0.3 56.9 ? 0.7 46.4 ? 0.1 FedAvg [51] 56.4 ? 0.5 59.7 ? 0.5 54.0 ? 0.5 23.6 ? 0.2 24.0 ? 0.2 20.4 ? 0.0 45.6 ? 0.4 44.7 ? 0.5 45.7 ? 0.5 FOLA [47] 55.9 ? 3.3 52.1 ? 3.1 45.9 ? 0.3 25.5 ? 1.5 22.4 ? 1.3 18.7 ? 0.1 45.2 ? 0.3 43.4 ? 0.3 38.3 ? 0.2 FedPer [4] 83.8 ? 0.8 81.5 ? 0.5 76.8 ? 1.2 48.3 ? 0.6 43.6 ? 0.2 25.6 ? 0.3 70.6 ? 0.2 68.4 ? 0.5 62.2 ? .05 LG-FedAvg [44] 87.9 ? 0.3 83.6 ? 0.7 64.7 ? 0.7 43.6 ? 0.2 37.5 ? 0.9 20.3 ? 0.5 59.5 ? 1.1 59.9 ? 2.1 52.5 ? 0.8 pFedMe [72] 86.4 ? 0.8 85.0 ? 0.3 80.3 ? 0.5 49.8 ? 0.5 47.7 ? 0.4 32.5 ? 0.8 69.9 ? 0.5 68.9 ? 0.7 58.8 ? 0.1 FedU [18] 80.6 ? 0.3 78.1 ? 0.5 65.6 ? 0.4 41.1 ? 0.2 36.0 ? 0.2 15.9 ? 0.4 59.3 ? 0.2 55.4 ? 0.6 41.6 ? 0.5 pFedHN [65] 90.2 ? 0.6 87.4 ? 0.2 83.2 ? 0.8 60.0 ? 1.0 52.3 ? 0.5 34.1 ? 0.1 70.4 ? 0.4 69.4 ? 0.5 64.2 ? .05 ? 0.2 87.4 ? 0.2 86.9 ? 0.7 60.2 ? 0.3 58.5 ? 0.3 55.7 ? 0.4 69.8 ? 0.2 68.3 ? 0.6 67.6 ? 0.3 pFedGP-IP-compute 89.9 ? 0.6 88.8 ? 0.1 86.8 ? 0.4 61.2 ? 0.4 59.8 ? 0.3 49.2 ? 0.3 72.0 ? 0.3 71.5 ? 0.5 68.2 ? 0.2 pFedGP 89.2 ? 0.3 88.8 ? 0.2 87.6 ? 0.4 63.3 ? 0.1 61.3 ? 0.2 50.6 ? 0.2 71.8 ? 0.3 71.3 ? 0.4 68.1 ? 0.3</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell></cell><cell>CINIC-10</cell><cell></cell></row><row><cell># clients</cell><cell>50</cell><cell>100</cell><cell>500</cell><cell>50</cell><cell>100</cell><cell>500</cell><cell>50</cell><cell>100</cell><cell>500</cell></row><row><cell># samples/client</cell><cell>800</cell><cell>400</cell><cell>80</cell><cell>800</cell><cell>400</cell><cell>80</cell><cell>1800</cell><cell>900</cell><cell>180</cell></row><row><cell cols="2">Local 86.2 Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pFedGP-IP-data</cell><cell>88.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Test accuracy (? SEM) over 100 clients on noisy CIFAR-100. We also provide the relative accuracy decrease (%) w.r.t. the performance on the original CIFAR-100 data (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy (? SEM) over 100 clients on noisy CIFAR-10 under a homogeneous class distribution.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>FedAvg [51]</cell><cell>42.0 ? 0.2</cell></row><row><cell>FedPer [4]</cell><cell>38.2 ? 2.0</cell></row><row><cell>LG-FedAvg</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy (? SEM) over 50, 100 clients on CIFAR-10 under a homogeneous class distribution.</figDesc><table><row><cell>Method</cell><cell># clients</cell></row><row><cell></cell><cell>50</cell><cell>100</cell></row><row><cell>FedAvg [51]</cell><cell cols="2">66.2 ? 0.3 64.6 ? 0.2</cell></row><row><cell>FedPer [4]</cell><cell cols="2">56.8 ? 0.1 50.9 ? 0.4</cell></row><row><cell>pFedMe [72]</cell><cell cols="2">46.9 ? 0.5 44.4 ? 0.3</cell></row><row><cell>FedU [18]</cell><cell cols="2">29.6 ? 0.6 26.3 ? 0.5</cell></row><row><cell>pFedHN [65]</cell><cell cols="2">62.8 ? 0.5 56.5 ? 0.1</cell></row><row><cell>Ours</cell><cell></cell></row><row><cell>pFedGP-IP-data</cell><cell cols="2">62.7 ? 0.7 62.4 ? 0.5</cell></row><row><cell cols="3">pFedGP-IP-compute 65.8 ? 0.3 65.2 ? 0.4</cell></row><row><cell>pFedGP</cell><cell cols="2">66.2 ? 0.3 65.7? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Run time (sec.) 0.15 ? .02 0.21 ? .03 0.27 ? .04 0.34 ? .05 0.42 ? .06 | 1.08 ? .16</figDesc><table><row><cell cols="8">: pFedGP-IP-compute vs. pFedGP full model test accuracy and average predictive posterior</cell></row><row><cell cols="8">inference run-time (? STD) as a function of the number of inducing points (IPs) over 50 clients on</cell></row><row><cell>CINIC-10.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Num. IPs</cell><cell>80</cell><cell>160</cell><cell>240</cell><cell>320</cell><cell>400</cell><cell cols="2">| Full GP</cell></row><row><cell>Accuracy (%)</cell><cell>70.2</cell><cell>71.0</cell><cell>71.4</cell><cell>71.5</cell><cell>71.5</cell><cell>|</cell><cell>71.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>pFedGP model variants test accuracy (? SEM) over 50, 100, 500 clients on CIFAR-10, CIFAR-100, and CINIC-10. The # samples/client indicates the average number of training samples per client. ? 1.0 87.0 ? 0.2 86.4 ? 0.7 58.1 ? 0.3 57.4 ? 0.6 55.4 ? 0.2 69.2 ? 0.3 68.2 ? 0.9 67.9 ? 0.1 pFedGP-IP-data 88.6 ? 0.2 87.4 ? 0.2 86.9 ? 0.7 60.2 ? 0.3 58.5 ? 0.3 55.7 ? 0.4 69.8 ? 0.2 68.3 ? 0.6 67.6 ? 0.3 pFedGP-IP-compute-marginal 89.8 ? 0.6 88.8 ? 0.3 87.8 ? 0.3 60.9 ? 0.4 58.8 ? 0.3 46.7 ? 0.3 72.1 ? 0.2 71.1 ? 0.6 67.5 ? 0.2 pFedGP-IP-compute-predictive 89.9 ? 0.6 88.8 ? 0.1 86.8 ? 0.4 61.2 ? 0.4 59.8 ? 0.3 49.2 ? 0.3 72.0 ? 0.3 71.5 ? 0.5 68.2 ? 0.2 pFedGP-marginal 89.0 ? 0.1 88.0 ? 0.2 86.8 ? 0.2 63.7 ? 0.1 61.4 ? 0.3 50.3 ? .05 71.6 ? 0.3 71.0 ? 0.6 68.5 ? 0.2 pFedGP-predictive 89.2 ? 0.3 88.8 ? 0.2 87.6 ? 0.4 63.3 ? 0.1 61.3 ? 0.2 50.6 ? 0.2 71.8 ? 0.3 71.3 ? 0.4 68.1 ? 0.3 E.5 Computing demands In this section we evaluate pFedGP computational requirements. First we compared between pFedGP full model (Section 4.1) and pFedGP-IP-compute (Section 4.3) in terms of accuracy and run-time during test time. The key component controlling the computational demand of pFedGP during test time is the predictive distribution (Equations 17 &amp; 22). After the training phase, when a new test sample arrives, computing the predictive distributions can be done efficiently by using cached components that depend only on the training data (e.g., the Cholesky decomposition of B) and can be calculated offline. Therefore, to quantify the impact of using pFedGP-IP-compute compared to the full GP model, we recorded in Table 5 the federated accuracy and average time per client for calculating the predictive distribution for all test examples as a function of the number of inducing points. The comparison was done on the pre-allocated test set from the CINIC-10 dataset over 50 clients (i.e., ? 1800 test examples per client divided to 4 classes) using a model that was trained with 100 inducing points. The table shows a significant improvement in the run time compared to the full model without any (or only minor) accuracy degradation.</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell></cell><cell>CINIC-10</cell><cell></cell></row><row><cell># clients</cell><cell>50</cell><cell>100</cell><cell>500</cell><cell>50</cell><cell>100</cell><cell>500</cell><cell>50</cell><cell>100</cell><cell>500</cell></row><row><cell># samples/client</cell><cell>800</cell><cell>400</cell><cell>80</cell><cell>800</cell><cell>400</cell><cell>80</cell><cell>1800</cell><cell>900</cell><cell>180</cell></row><row><cell cols="2">pFedGP-IP-data w/o personalization 88.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is publicly available at https://github.com/IdanAchituve/pFedGP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The noisy CIFAR-10/100 datasets are available at: https://idanachituve.github.io/projects/pFedGP</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This study was funded by a grant to GC from the Israel Science Foundation (ISF 737/2018), and by an equipment grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). IA was funded by a grant from the Israeli innovation authority, through the AVATAR consortium.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on federated learning: The journey from centralized to distributed on-site learning and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanine</forename><surname>Sawsan Abdulrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakima</forename><surname>Tout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azzam</forename><surname>Ould-Slimane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamseddine</forename><surname>Mourad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Talhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guizani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5476" to="5497" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GP-Tree: A Gaussian process classifier for few-shot incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Idan Achituve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Navon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Yemini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fetaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="54" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">cpSGD: Communication-efficient and differentially-private distributed SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda</forename><forename type="middle">Theertha</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Xinnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7564" to="7575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Federated learning with personalization layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Manoj Ghuhan Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><forename type="middle">Kumar</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunav</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choudhary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00818</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">k-means++ the advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the eighteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harkirat</forename><surname>Singh Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">At?l?m</forename><surname>G?ne? Baydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07435</idno>
		<title level="m">Alpha MAML: Adaptive model-agnostic meta-learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Glenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly weather review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Partitioned variational inference: A unified framework encompassing federated and continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Swaroop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11206</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards ubiquitous learning: A first measurement of on-device training performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning</title>
		<meeting>the 5th International Workshop on Embedded and Mobile Deep Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Manifold Gaussian processes for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3338" to="3345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A theorem of the alternative for personalized federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie J</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01901</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Corinzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ami</forename><surname>Beuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06268</idno>
		<title level="m">Variational federated multi-task learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hyper-sphere quantization: Communication-efficient SGD for federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04655</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Federated Bayesian optimization via Thompson sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan Kian Hsiang</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jaillet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03505</idno>
		<title level="m">CINIC-10 is not Imagenet or CIFAR-10</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adaptive personalized federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Mahdi</forename><surname>Kamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13461</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">FedU: A unified framework for federated multi-task learning with Laplacian regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Canh T Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07148</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Nonlinear time series: Theory, methods and applications with R examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randal</forename><surname>Douc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Moulines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stoffer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Privacy aware learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Personalized federated learning: A metalearning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryan</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the convergence theory of gradientbased model-agnostic meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryan</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asuman</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1082" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-class Gaussian process classification made conjugate: Efficient inference via data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Galy-Fajou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="755" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated augmented conjugate inference for non-conjugate gaussian process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Galy-Fajou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3025" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the convergence of local descent methods in federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzin</forename><surname>Haddadpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14425</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Federated learning of a mixture of global and local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Hanzely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Richt?rik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05516</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable variational Gaussian process classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Measuring the effects of non-identical data distribution for federated visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<idno>abs/1909.06335</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Personalized cross-silo federated learning on non-IID data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving federated learning personalization via model agnostic meta learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Kone?n?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreeram</forename><surname>Kannan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12488</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Avent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04977</idno>
		<title level="m">Aur?lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji</title>
		<meeting><address><addrLine>Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances and open problems in federated learning</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SCAFFOLD: Stochastic controlled averaging for federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Sai Praneeth Karimireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda Theertha</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5132" to="5143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Federated generalized Bayesian learning via distributed Stein variational gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahif</forename><surname>Kassab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvaldo</forename><surname>Simeone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06419</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Federated optimization: Distributed machine learning for on-device intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Kone?n?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Richt?rik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02527</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Survey of personalization techniques for federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="794" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On the convergence of federated optimization in heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06127</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Federated learning: Challenges, methods, and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><forename type="middle">Li</forename><surname>Meta-Sgd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Learning to learn quickly for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Paul Pu Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ziyin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">P</forename><surname>Nicholas B Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auerbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01523</idno>
		<title level="m">Think locally, act globally: Federated learning with local and global representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Don&apos;t use large mini-batches, use local SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar Kshitij</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dependent multinomial models made easy: stick breaking with the P?lya-Gamma augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott W Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3456" to="3464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A Bayesian federated learning framework with multivariate gaussian product</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01936</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast adaptation with linearized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2737" to="2745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Three approaches for personalization with applications to federated learning. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">PAC-Bayesian stochastic model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="5" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eider</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Aguera Y Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning differentially private recurrent language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>H Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A family of algorithms for approximate Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Peter Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A survey on security and privacy of federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viraaji</forename><surname>Mothukuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyedamin</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Dehghantanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="619" to="640" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using Bayesian binning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Mahdi Pakdaman Naeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The promises and pitfalls of deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">E</forename><surname>Ober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Wilk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bayesian inference for logistic models using P?lya-Gamma latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">G</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Windle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page" from="1339" to="1349" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A unifying view of sparse approximate Gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Quinonero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1939" to="1959" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning Gaussian processes by minimizing PAC-Bayesian generalization bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Doerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gerwinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Rakitsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3341" to="3351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">FedPAQ: A communication-efficient federated learning method with periodic averaging and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Reisizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryan</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jadbabaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramtin</forename><surname>Pedarsani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">In defense of one-vs-all classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldebaro</forename><surname>Klautau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="141" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">PAC-Bayesian generalisation error bounds for Gaussian process classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="233" to="269" />
			<date type="published" when="2002-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Personalized federated learning using hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Shamsian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Navon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSAC conference on computer and communications security</title>
		<meeting>the 22nd ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1310" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Federated multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Kai</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4427" to="4437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bayesian few-shot classification with one-vs-each P?lya-Gamma augmented Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Sparse Gaussian processes using pseudo-inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Local SGD converges fast and communicates little</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Personalized federated learning with Moreau envelopes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Canh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><forename type="middle">Dung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alysa Ziying</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00710</idno>
		<title level="m">Towards personalized federated learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Fedgp: Correlation-based active client selection for heterogeneous federated learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxue</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefei</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13822</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Multi-class Gaussian process classification with noisy inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Villacampa-Calvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Zald?var</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">C</forename><surname>Garrido-Merch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="1" to="52" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Federated learning with matched averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuekai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Khazaeni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gauri</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Coding Theory for Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Efficient Gaussian process classification using P?lya-Gamma data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Galy-Fajou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5417" to="5424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Bayesian classification with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1342" to="1351" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Andrew Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Stochastic variational deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Andrew Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2594" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Federated machine learning: Concept and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">FedLoc: Federated learning framework for data-driven cooperative localization and location data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinglei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergios</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang Robert</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Open Journal of Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="187" to="215" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric federated learning of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristjan</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nghia</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Khazaeni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7252" to="7261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">A survey on federated learning. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page">106775</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Personalized federated learning with first order model optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose M</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08565</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Civin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00582</idno>
		<title level="m">Federated learning with non-IID data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">On the convergence properties of a K-step averaging stochastic gradient descent algorithm for nonconvex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojing</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3219" to="3227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Efficient meta learning via minibatch proximal update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1534" to="1544" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Federated heavy hitters discovery with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wennan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haicheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3837" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
