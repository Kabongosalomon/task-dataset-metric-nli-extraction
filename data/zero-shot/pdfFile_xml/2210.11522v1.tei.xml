<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COMPOSING ENSEMBLES OF PRE-TRAINED MODELS VIA ITERATIVE CONSENSUS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
							<email>lishuang@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
							<email>yilundu@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
							<email>torralba@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
							<email>imordatch@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">MIT CSAIL</orgName>
								<orgName type="institution">BCS</orgName>
								<address>
									<region>CBMM</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COMPOSING ENSEMBLES OF PRE-TRAINED MODELS VIA ITERATIVE CONSENSUS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as "generators" or "scorers" and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. Project page: https://energy-basedmodel.github.io/composing-pretrained-models. * Correspondence to: Shuang Li &lt;lishuang@mit.edu&gt;. ? indicates equal contribution. Shuang Li did all the experiments on image generation, video question answering, and mathematical reasoning. Yilun Du did all the experiments on robot manipulation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Large pre-trained models have shown remarkable zero-shot generalization abilities, ranging from zero-shot image generation and natural language processing to machine reasoning and action planning. Such models are trained on large datasets scoured from the internet, often consisting of billions of datapoints. Individual pre-trained models capture different aspects of knowledge on the internet, with language models (LMs) capturing textual information in news, articles, and Wikipedia pages, and visual-language models (VLMs) modeling the alignments between visual and textual information. While it is desirable to have a single sizable pre-trained model capturing all possible modalities of data on the internet, such a comprehensive model is challenging to obtain and maintain, requiring intensive memory, an enormous amount of energy, months of training time, and millions of dollars. A more scalable alternative approach is to compose different pre-trained models together, leveraging the knowledge from different expert models to solve complex multimodal tasks.</p><p>Building a unified framework for composing multiple models is challenging. Prior works <ref type="bibr" target="#b1">(Alayrac et al., 2022;</ref> have explored composing pre-trained models in two main ways:</p><p>(jointly) finetuning models on large datasets, or using common interfaces such as language to combine different models. However, these works have several key limitations: First, simply combining models does not fully utilize each pre-trained model as there is no closed-loop feedback between models. Cascading models, such as Socratic models , allows one-way communication but prevents information processed by later models from propagating back to earlier models to correct errors. Secondly, common interfaces are limited to particular types of models. Language is used as the intermediate connection in Socratic models , but a language interface is insufficient to solve many real-world tasks, such as continuous robot control, which requires continuous representations. In addition, Socratic models require pre-designed language templates for the communication between models, which limits scalability. Thirdly, jointly finetuning multiple models <ref type="bibr" target="#b1">(Alayrac et al., 2022)</ref> requires careful optimization to ensure that the model behaviors remain stable. Such models also require intensive memory and large datasets and can only be used for solving specific tasks.</p><p>To resolve these difficulties, we propose a unified framework to compose models in a zero-shot manner 1 without any training/finetuning. Our framework employs a single model as a generator and an ensemble of scorers. The generator iteratively generates proposals, and each scorer provides a feedback score indicating their agreement. The generator refines its outputs until all the scorers achieve a final consensus. This iterative closed-loop communication between the generator and scorers enables models to correct the errors caused by other models, substantially boosting performance.</p><p>The ensemble of scorers is inspired by the idea of "wisdom of the crowds". Each scorer provides complementary feedback to the generator, compensating for the potential weaknesses of other scorers. A Vision-Language scorer, for example, may correct the biases of a language model. We notice that different pre-trained model instances from the same family have diversity of outputs, which leads to more robust scorers. We demonstrate that guiding the generator with such an ensemble of scorers significantly outperforms a generator guided by a single scorer.</p><p>To summarize, our work has three main contributions.</p><p>? First, we propose a unified framework for composing pre-trained models across a variety of tasks, such as image generation, video question answering, mathematical reasoning, and robot manipulation.</p><p>? Second, we illustrate how the proposed framework can effectively solve zero-shot multimodal tasks without any training/finetuning. The closed-loop communication between the generator and scorers allows the models to interact with each other to improve performance iteratively.</p><p>? Finally, we illustrate how our framework enables the use of ensembles of different pre-trained models as scorers, significantly improving the zero-shot results by leveraging the strengths of multiple expert models.</p><p>These observations point to the effectiveness of the proposed method as a general purpose framework for composing pre-trained models for solving various zero-shot multimodal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Large pre-trained models have shown great success across a variety of domains, such as language generation/translation, image generation, and decision-making.</p><p>Language models. Large language models, such as ELMo <ref type="bibr" target="#b24">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>, and GPT-2 <ref type="bibr" target="#b25">(Radford et al., 2019)</ref>, are able to achieve state-of-the-art performance on many standard NLP benchmarks. More recent works, such as GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref>, PALM <ref type="bibr" target="#b4">(Chowdhery et al., 2022)</ref>, and Chinchilla <ref type="bibr" target="#b15">(Hoffmann et al., 2022)</ref> further enable few-shot learning from textual prompts.</p><p>Vision-language models. Large pre-trained vision-language generative models, such as DALL-E Generator(G)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scorers(E)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Consensus</head><p>Video Question Answering Q: How to make the food step by step? A: Put water in the pot, ?, add sausage, add seasoning on top of the pizza ? Q: What food is being made? A: Make pizza Grade School Math Q: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? A: 3 Q: Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks? A: 7 <ref type="figure">Figure 1</ref>: The proposed framework that composes a "generator" and an ensemble of "scorers" through iterative consensus enables zero-shot generalization across a variety of multimodal tasks.</p><p>Decision-making models. Large pre-trained models have been widely applied to solve decisionmaking tasks, such as learning general purpose policies <ref type="bibr" target="#b28">(Reed et al., 2022;</ref><ref type="bibr" target="#b32">Shridhar et al., 2022)</ref>, making planners <ref type="bibr" target="#b16">(Huang et al., 2022;</ref><ref type="bibr" target="#b0">Ahn et al., 2022)</ref>, and learning world models <ref type="bibr" target="#b10">(Ebert et al., 2018)</ref>. However, due to the large variability in decision-making tasks, no existing pre-trained models can be readily applied across different tasks.</p><p>Composing pre-trained models. Composing large pre-trained models has been widely studied recently. The predominant way to compose pre-trained models is to (joint) finetune them on new tasks <ref type="bibr" target="#b18">(Li et al., 2019;</ref><ref type="bibr" target="#b34">Wang et al., 2021;</ref><ref type="bibr" target="#b1">Alayrac et al., 2022;</ref><ref type="bibr" target="#b22">Mokady et al., 2021)</ref>, but such approaches are computationally expensive. Alternative approaches compose models through a common interface such as language <ref type="bibr" target="#b33">(Tewel et al., 2021;</ref>. Other works compose pre-trained models by composing learned probability distributions of the data, such as energy-based models <ref type="bibr" target="#b21">(Liu et al., 2022;</ref><ref type="bibr" target="#b9">Du et al., 2020)</ref>, which can be applied to image generation. In this paper, we propose a general framework to compose pre-trained models across a variety of domains without any training or finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Given a set of large pre-trained models, we aim to utilize the expert knowledge from different models to solve zero-shot multimodal tasks. We separate pre-trained models into two categories -generators (G) such as GPT <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b25">Radford et al., 2019)</ref> and Diffusion models <ref type="bibr" target="#b14">(Ho et al., 2020)</ref> that can generate candidate solutions, and scorers (E) such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021)</ref> and classifiers that output a scalar score to evaluate each generated solution. We propose PIC (composing ensembles of Pre-trained models via Iterative Consensus), a framework which composes ensembles of pre-trained models for multimodal tasks. The core idea of PIC is to generate solutions through iterative optimization, where we leverage the knowledge from different models to jointly construct a consensus solution. In PIC, a generator G iteratively and sequentially generate candidate solutions, each of which is refined based on the feedback from a set of scorers. In particular, we seek to obtain a solution x * such that x * = arg min</p><formula xml:id="formula_0">x?G n E n (x),<label>(1)</label></formula><p>where {E n } is the set of scorers. At each iteration, we refine the solutions to have a lower score than the previous iterations. This procedure, described in Equation (1), converges to a solution that minimizes the energy across multiple pre-trained models, which maximizes the agreement between the generator and scorers. In contrast to Socratic Models where different pre-trained models are called sequentially, the closed-loop iterative refinement through which we obtain x * enables the generator and scorers to communicate with each other to reach a consensus on the final solution.</p><p>Below, we illustrate how PIC can be broadly applied across tasks in image generation, video question answering, grade school math, and robot manipulation. To optimize Equation <ref type="formula" target="#formula_0">(1)</ref>, we consider two different optimization procedures -either a continuous approach that leverages the gradients of each scorer E n (x) or a discrete approach that directly samples possible solutions. to generate image proposals. Our method can compose the generator with one or multiple scorers, such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021</ref>), text-image classifiers , and the classifier-free guidance <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref>.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 2 (right)</ref>, the image x t generated at iteration t is first sent to the GLIDE diffusion model to generate an image proposalx t+1 . Each scorer outputs a score to evaluate whether the generated image matches the given text input. For example, CLIP computes the cosine distance of the image feature and text feature. The text-image classifier predicts a probability of the image matching the text label. The classifier-free guidance can be treated as an implicit classifier that provides pixel-wise gradient feedback to the generator directly. The energy scores generated by different scorers are summed up. We compute the gradient of summed energy score with respect to the original image proposal to update the generated image:</p><formula xml:id="formula_1">x t+1 = x t 2 r x N X n=1 E n ? (x t , c) ,<label>(4)</label></formula><p>where N is the number of scorers.</p><p>Robot planning.</p><p>Video Question Answering. We first use the proposed framework to generate video frame captions. We then use GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, our framework combines GPT-2 (Medium size) and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. Given a video frame, the CLIP models compute its feature distance (score) to the feature of the generated caption. Similar to image generation, the gradient of summed scores are propagated to the generator to update the next token x t+1 . We cascade the video frame captions and questions about this video to prompt GPT-3. Results show that utilizing the proposed framework and GPT-3 enables effective video question answering.</p><p>Grade school math. We treat the grade school math problem as the text generation problem. Similar to video question answering, the generator is a GPT-2 model (Medium size) and the scorers provide feedback to the generator to guide the generation of next token x t+1 . The scorers can be text classifiers to evaluate the correctness of the output answer for the given math problem (See ??.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing large models on four representative zeroshot tasks, including image generation, video question answering, grade school math, and robot manipulation.</p><p>Image Generation. We first show that composing the image generation model, i.e. GLIDE, and multiple scorer models, i.e. CLIP, text-image classifier, and classifier-free guidance, enables effective zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref>  ? Text "a bowl with" "rice"</p><p>"egg"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision Making Video Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input video frame</head><p>Used as the input of the next iteration to generate image proposals. Our method can compose the generator with one or multiple scorers, such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021</ref>), text-image classifiers , and the classifier-free guidance <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref>.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 2 (right)</ref>, the image x t generated at iteration t is first sent to the GLIDE diffusion model to generate an image proposalx t+1 . Each scorer outputs a score to evaluate whether the generated image matches the given text input. For example, CLIP computes the cosine distance of the image feature and text feature. The text-image classifier predicts a probability of the image matching the text label. The classifier-free guidance can be treated as an implicit classifier that provides pixel-wise gradient feedback to the generator directly. The energy scores generated by different scorers are summed up. We compute the gradient of summed energy score with respect to the original image proposal to update the generated image:</p><formula xml:id="formula_2">x t+1 = x t 2 r x N X n=1 E n ? (x t , c) ,<label>(4)</label></formula><p>where N is the number of scorers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robot planning.</head><p>Video Question Answering. We first use the proposed framework to generate video frame captions. We then use GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, our framework combines GPT-2 (Medium size) and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. The history tokens {x 1 , ? ? ? , x t } is first sent to the generator to predict the next tokenx t+1 . Then the scorers compute the feature distances (scores) between the new sentence (concatenation of history tokens and the new token) and the given video frame. Similar to image generation, the gradient of summed scores are propagated to the generator to update the next token x t+1 . We cascade the video frame captions and questions about this video to prompt GPT-3. Results show that utilizing the proposed framework and GPT-3 enables effective video question answering.</p><p>Grade school math. We treat the grade school math problem as the text generation problem. Similar to video question answering, the generator is a GPT-2 model (Medium size) and the scorers provide feedback to the generator to guide the generation of next token x t+1 . The scorers can be text classifiers to evaluate the correctness of the output answer for the given math problem (See ??.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing large models on four representative zeroshot tasks, including image generation, video question answering, grade school math, and robot manipulation.</p><p>Image Generation. We first show that composing the image generation model, i.e. GLIDE, and multiple scorer models, i.e. CLIP, text-image classifier, and classifier-free guidance, enables effective zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr">(Deng et al.,</ref><ref type="bibr">4</ref> "egg"</p><p>Generator (G): e.g. GPT2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Energy Scorers (E)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Updated result</head><p>Original result CLIP 1:</p><p>CLIP 2: ? Text bowl with" "rice" "egg"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input video frame</head><p>Used as the input of the next iteration tails 2.</p><p>ose the generator with one or multiple scorers, assifiers , and the t iteration t is first sent to the GLIDE diffusion corer outputs a score to evaluate whether the example, CLIP computes the cosine distance e classifier predicts a probability of the image e can be treated as an implicit classifier that ator directly. The energy scores generated by adient of summed energy score with respect to mage: </p><formula xml:id="formula_3">N =1 E n ? (x t , c) ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision Making Video Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input video frame</head><p>Used as the input of the next iteration to generate image proposals. Our method can compose the generator with one or multiple scorers, such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021</ref>), text-image classifiers , and the classifier-free guidance <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref>.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 2 (right)</ref>, the image x t generated at iteration t is first sent to the GLIDE diffusion model to generate an image proposalx t+1 . Each scorer outputs a score to evaluate whether the generated image matches the given text input. For example, CLIP computes the cosine distance of the image feature and text feature. The text-image classifier predicts a probability of the image matching the text label. The classifier-free guidance can be treated as an implicit classifier that provides pixel-wise gradient feedback to the generator directly. The energy scores generated by different scorers are summed up. We compute the gradient of summed energy score with respect to the original image proposal to update the generated image:</p><formula xml:id="formula_4">x t+1 = x t 2 r x N X n=1 E n ? (x t , c) ,<label>(4)</label></formula><p>where N is the number of scorers.</p><p>Robot planning.</p><p>Video Question Answering. We first use the proposed framework to generate video frame captions. We then use GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, our framework combines GPT-2 (Medium size) and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. Given a video frame, the CLIP models compute its feature distance (score) to the feature of the generated caption. Similar to image generation, the gradient of summed scores are propagated to the generator to update the next token x t+1 . We cascade the video frame captions and questions about this video to prompt GPT-3. Results show that utilizing the proposed framework and GPT-3 enables effective video question answering.</p><p>Grade school math. We treat the grade school math problem as the text generation problem. Similar to video question answering, the generator is a GPT-2 model (Medium size) and the scorers provide feedback to the generator to guide the generation of next token x t+1 . The scorers can be text classifiers to evaluate the correctness of the output answer for the given math problem (See ??.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing large models on four representative zeroshot tasks, including image generation, video question answering, grade school math, and robot manipulation.</p><p>Image Generation. We first show that composing the image generation model, i.e. GLIDE, and multiple scorer models, i.e. CLIP, text-image classifier, and classifier-free guidance, enables effective zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision Making Video Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input video frame</head><p>Used as the input of the next iteration to generate image proposals. Our method can compose the generator with one or multiple sc such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021</ref>), text-image classifiers , an classifier-free guidance <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref>.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 2 (right)</ref>, the image x t generated at iteration t is first sent to the GLIDE diff model to generate an image proposalx t+1 . Each scorer outputs a score to evaluate whethe generated image matches the given text input. For example, CLIP computes the cosine dis of the image feature and text feature. The text-image classifier predicts a probability of the i matching the text label. The classifier-free guidance can be treated as an implicit classifie provides pixel-wise gradient feedback to the generator directly. The energy scores generat different scorers are summed up. We compute the gradient of summed energy score with resp the original image proposal to update the generated image:</p><formula xml:id="formula_5">x t+1 = x t 2 r x N X n=1 E n ? (x t , c) ,</formula><p>where N is the number of scorers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robot planning.</head><p>Video Question Answering. We first use the proposed framework to generate video frame cap We then use GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> to summarize the captions and answer questions shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, our framework combines GPT-2 (Medium size) and multiple CLIP models, tr with different configurations, for zero-shot video frame captioning. Given a video frame, the models compute its feature distance (score) to the feature of the generated caption. Similar to i generation, the gradient of summed scores are propagated to the generator to update the next x t+1 . We cascade the video frame captions and questions about this video to prompt GPT-3. R show that utilizing the proposed framework and GPT-3 enables effective video question answe</p><p>Grade school math. We treat the grade school math problem as the text generation problem. Si to video question answering, the generator is a GPT-2 model (Medium size) and the scorers pr feedback to the generator to guide the generation of next token x t+1 . The scorers can be classifiers to evaluate the correctness of the output answer for the given math problem (See ??</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing large models on four representative shot tasks, including image generation, video question answering, grade school math, and manipulation.</p><p>Image Generation. We first show that composing the image generation model, i.e. GLIDE multiple scorer models, i.e. CLIP, text-image classifier, and classifier-free guidance, enables eff zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr" target="#b6">(Deng 2009)</ref>  ? Text "a bowl with" "rice"</p><p>"egg"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision Making Video Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input video frame</head><p>Used as the input of the next iteration to generate image proposals. Our method can compose the generator with one or multiple scorers such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021</ref>), text-image classifiers , and the classifier-free guidance <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref>.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 2</ref> (right), the image x t generated at iteration t is first sent to the GLIDE diffusion model to generate an image proposalx t+1 . Each scorer outputs a score to evaluate whether the generated image matches the given text input. For example, CLIP computes the cosine distance of the image feature and text feature. The text-image classifier predicts a probability of the image matching the text label. The classifier-free guidance can be treated as an implicit classifier that provides pixel-wise gradient feedback to the generator directly. The energy scores generated by different scorers are summed up. We compute the gradient of summed energy score with respect to the original image proposal to update the generated image:</p><formula xml:id="formula_6">x t+1 = x t 2 r x N X n=1 E n ? (x t , c) ,<label>(4)</label></formula><p>where N is the number of scorers.</p><p>Robot planning.</p><p>Video Question Answering. We first use the proposed framework to generate video frame captions We then use GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, our framework combines GPT-2 (Medium size) and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. The history tokens {x 1 , ? ? ? , x t } is first sent to the generator to predict the next tokenx t+1 . Then the scorers compute the feature distances (scores) between the new sentence (concatenation of history tokens and the new token) and the given video frame. Similar to image generation, the gradient of summed scores are propagated to the generator to update the next token x t+1 . We cascade the video frame captions and questions about this video to prompt GPT-3. Results show that utilizing the proposed framework and GPT-3 enables effective video question answering.</p><p>Grade school math. We treat the grade school math problem as the text generation problem. Similar to video question answering, the generator is a GPT-2 model (Medium size) and the scorers provide feedback to the generator to guide the generation of next token x t+1 . The scorers can be text classifiers to evaluate the correctness of the output answer for the given math problem (See ??.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing large models on four representative zeroshot tasks, including image generation, video question answering, grade school math, and robot manipulation.</p><p>Image Generation. We first show that composing the image generation model, i.e. GLIDE, and multiple scorer models, i.e. CLIP, text-image classifier, and classifier-free guidance, enables effective zero-shot image generation. We evaluate the image generation results on ImageNet (Deng et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>State cascade the captions of multiple video frames and questions about this video to prompt GPT-3 for video question answering.</p><p>Grade school math. We further apply PIC to solve grade school math problems. We use GPT-2 as the generator and treat the grade school math problem as a text generation problem. The scorer, a pre-trained question-solution classifier, provides the generator feedback to guide the next token's generation xt+1. We follow the approach used in VQA to iteratively optimize the generations based on the feedback from scorers. Our generator G first generates a set of candidate words {x i t+1 }, and then the classifier predicts the probability of each solution (the concatenation of previous words and each new word {x1, x2, ? ? ? ,x i t+1 }) matching the given question. The classifier score is the cross-entropy loss between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the classifier score is used to update Ct through iterative refinement. The updated Ct is used to predict the next word xt+1 = G(xt, Ct). We repeat this process until we generate the complete solution.</p><p>Robot manipulation. Finally, we illustrate how PIC can be applied to manipulate objects in the robot environment to conform to a set of object relations such as "red bowl on top of blue mug" shown in <ref type="figure" target="#fig_5">Fig. 2 (d)</ref>. We use the combination of the Model Predictive Control (MPC) <ref type="bibr" target="#b35">(Williams et al., 2015)</ref> and the World Model as the generator. At each time step, we first use MPC to sample a set of possible actions and then render the state images (after executing an action) from multiple camera views using the world model. For each action, the scorer computes a summed score across all camera views as its final score, which is used to select the best action to execute.</p><p>For the generator, we assume that there is a pre-trained model, i.e. world model, that can accurately render and simulate the dynamic changes in the robot world. Since such a large pre-trained model does not directly exist, we approximate it using an environment simulator combined with MPC as the generator. For the scorer, we use the pre-trained ViLD <ref type="bibr" target="#b11">(Gu et al., 2021)</ref> to generate segmentation maps for images captured by different camera views, and the corresponding text label for each segment, which are used to obtain object relations. We compare the generated object relations and the relations specified by the text description to obtain the scorer, i.e. score equals 0 if they match; otherwise, 1 (here the score means the distance) (see Appendix A.4 for details). To obtain a final world state xT that satisfies the specified relations, and the action sequence {a1, ? ? ? , aT } that manipulates the objects into the final state xT , the generator iteratively samples possible actions? k t+1 and gets feedback from scorers. The best action is selected by:</p><formula xml:id="formula_7">at+1 = arg min a k t+1 N X n=1 E n ? (xt,? k t+1 ).<label>(4)</label></formula><p>Each scorer, E n ? , outputs a score for the resultant state obtained when a candidate action? k t+1 is applied to the current world state xt. We execute at+1 in the environment and get a new state xt+1. We repeat this process until the task is accomplished or we are at the final step T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing pre-trained models on four representative tasks, including image generation, video question answering, grade school math, and robot manipulation.</p><p>Image generation. We first show that composing the pre-trained image generation model and scorer models such as CLIP enables effective zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref> with the image resolution of 64 ? 64. The class labels are used as text input to guide image generation. Each method generates 50 images for each class. We evaluate the image generation quality using Inception Score (IS) <ref type="bibr" target="#b31">(Salimans et al., 2016)</ref>, Fr?chet Inception Distance (FID) <ref type="bibr" target="#b12">(Heusel et al., 2017)</ref>, and Kernel Inception Distance (KID) <ref type="bibr" target="#b2">(Bi?kowski et al., 2018)</ref>. IS measures the distribution of generated images. Higher values mean the models can generate more distinct images. FID considers both the distribution of generated images and the distribution of real images. Lower scores represent the generated images are closer to the real images. KID is similar to FID, measuring the similarity between two data distributions but in the kernel space.</p><p>Video question answering. We evaluate methods for solving VQA tasks on ActivityNet-QA <ref type="bibr" target="#b39">(Yu et al., 2019)</ref>. Our method generates free-form language answers instead of selecting an answer from a pre-defined answer set <ref type="bibr" target="#b37">(Yang et al., 2021;</ref><ref type="bibr" target="#b17">Lei et al., 2022)</ref>. To evaluate such free-form VQA, we ask workers from Amazon Mechanical Turk to measure whether the generated answer matches the 5 <ref type="figure" target="#fig_5">Figure 2</ref>: The proposed unified framework and examples on three representative tasks. (a) Overv the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A prediffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are u provide feedback to the generator. (c) Video question answering. GPT-2 is used as the generator, and a CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator pre-trained image segmentation model is used to compute the scores from multiple camera views to sel best action. Orange lines represent the components used to refine the generated result.</p><p>image and text features as the score. The scores generated by different scorers are summed, and gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_8">x k+1 x k+1 + r x k N X n=1 E n ? x k , c ,</formula><p>where N is the number of scorers and c is the text label. We denote the reverse process predict x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across task Video question answering (VQA). We first use PIC to generate video frame captions. We the GPT-3 to summarize the captions and answer questions about this video. Caption generation single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different models, trained with different configurations, as the scorers. Given a video frame I, we gen a sequence of words to describe it. To integrate feedback from scorers to the generator, sim <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in GPT-2) that the context information generated so far, which is updated iteratively based on the feedback scorers. The prediction of the next word from the generator G is given by x t+1 = G(x t , C update C t , we first use G to generate a set of candidate wordsX t+1 = {x t+1 }, and then u feature distance (after softmax) between each sentence (the concatenation of previous words and new word {x 1 , x 2 , ? ? ? ,x t+1 }, wherex t+1 2X t+1 ) and the video frame as the probability of matching. The CLIP score is the cross-entropy loss L CLIP between this new probability distrib and the original distribution of the next word obtained from the generator G. The gradient summed score (multiple CLIP models) is then propagated to G to update C t :</p><formula xml:id="formula_9">C k+1 t C k t + r x N X n=1 L CLIP (E n ? (x 1 , x 2 , ? ? ? ,x t+1 , I)),</formula><p>where k is the step of iterative refinement. After several iterations, the updated C t is used to ge the next token x t+1 = G(x t , C t ). We repeat this process until we generate the entire captio image and text features as the score. The scores generated by different scorers are summed, and thei gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_10">x k+1 x k+1 + r x k N X n=1 E n ? x k , c , (2</formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction a x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across tasks. Video question answering (VQA). We first use PIC to generate video frame captions. We then us GPT-3 to summarize the captions and answer questions about this video. Caption generation for single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generat a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in GPT-2) that store the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by</p><formula xml:id="formula_11">x t+1 = G(x t , C t ).</formula><p>To update C t , we first use G to generate a set of candidate wordsX t+1 = {x t+1 }, and then use th feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x t+1 }, wherex t+1 2X t+1 ) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss L CLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of th summed score (multiple CLIP models) is then propagated to G to update C t :</p><formula xml:id="formula_12">C k+1 t C k t + r x N X n=1 L CLIP (E n ? (x 1 , x 2 , ? ? ? ,x t+1 , I)), (3</formula><p>where k is the step of iterative refinement. After several iterations, the updated C t is used to generat the next token x t+1 = G(x t , C t ). We repeat this process until we generate the entire caption. W GPT-2 is used as the generator, and a set of CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action. Orange lines represent the components used to refine the generated result.</p><p>image and text features as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_13">x k+1 x k+1 + r x k N X n=1 E n ? x k , c ,<label>(2)</label></formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction as x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across tasks. Video question answering (VQA). We first use PIC to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions about this video. Caption generation for a single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache Ct (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by xt+1 = G(xt, Ct). To update Ct, we first use G to generate a set of candidate wordsXt+1 = {xt+1}, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x1, x2, ? ? ? ,xt+1}, wherext+1 2Xt+1) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss LCLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the summed score (multiple CLIP models) is then propagated to G to update Ct:</p><formula xml:id="formula_14">C k+1 t C k t + rx N X n=1 LCLIP(E n ? (x1, x2, ? ? ? ,xt+1, I)),<label>(3)</label></formula><p>where k is the step of iterative refinement. After several iterations, the updated Ct is used to generate the next token xt+1 = G(xt, Ct). We repeat this process until we generate the entire caption. We GPT-2 is used as the generator, and a set of CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action. Orange lines represent the components used to refine the generated result.</p><p>image and text features as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_15">x k+1 x k+1 + r x k N X n=1 E n ? x k , c ,<label>(2)</label></formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction as x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across tasks. Video question answering (VQA). We first use PIC to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions about this video. Caption generation for a single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache Ct (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by xt+1 = G(xt, Ct). To update Ct, we first use G to generate a set of candidate wordsXt+1 = {xt+1}, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x1, x2, ? ? ? ,xt+1}, wherext+1 2Xt+1) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss LCLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the summed score (multiple CLIP models) is then propagated to G to update Ct:</p><formula xml:id="formula_16">C k+1 t C k t + rx N X n=1 LCLIP(E n ? (x1, x2, ? ? ? ,xt+1, I)),<label>(3)</label></formula><p>where k is the step of iterative refinement. After several iterations, the updated Ct is used to generate the next token xt+1 = G(xt, Ct). We repeat this process until we generate the entire caption. We GPT-2 is used as the generator, and a set of CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action. Orange lines represent the components used to refine the generated result.</p><p>image and text features as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_17">x k+1 x k+1 + r x k N X n=1 E n ? x k , c ,<label>(2)</label></formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction as x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across tasks. Video question answering (VQA). We first use PIC to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions about this video. Caption generation for a single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache Ct (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by xt+1 = G(xt, Ct). To update Ct, we first use G to generate a set of candidate wordsXt+1 = {xt+1}, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x1, x2, ? ? ? ,xt+1}, wherext+1 2Xt+1) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss LCLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the summed score (multiple CLIP models) is then propagated to G to update Ct:</p><formula xml:id="formula_18">C k+1 t C k t + rx N X n=1 LCLIP(E n ? (x1, x2, ? ? ? ,xt+1, I)),<label>(3)</label></formula><p>where k is the step of iterative refinement. After several iterations, the updated Ct is used to generate the next token xt+1 = G(xt, Ct). We repeat this process until we generate the entire caption. We image and text features as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_19">x k+1 x k+1 + r x k N X n=1 E n ? x k , c ,<label>(2)</label></formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction as x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across tasks. Video question answering (VQA). We first use PIC to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions about this video. Caption generation for a single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by</p><formula xml:id="formula_20">x t+1 = G(x t , C t ).</formula><p>To update C t , we first use G to generate a set of candidate wordsX t+1 = {x t+1 }, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x t+1 }, wherex t+1 2X t+1 ) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss L CLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the summed score (multiple CLIP models) is then propagated to G to update C t :</p><formula xml:id="formula_21">C k+1 t C k t + r x N X n=1 L CLIP (E n ? (x 1 , x 2 , ? ? ? ,x t+1 , I)),<label>(3)</label></formula><p>where k is the step of iterative refinement. After several iterations, the updated C t is used to generate the next token x t+1 = G(x t , C t ). We repeat this process until we generate the entire caption. We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">APPLICATIONS TO ZERO-SHOT TASKS</head><p>Image generation. We first apply the proposed framework to image generation to generate images conditioned on a text description or a class label. We use the reverse diffusion process of GLIDE , a text-guided diffusion model, as the generator to generate image proposals. At each step of the diffusion process (corresponding to a step of the iterative refinement), we use the gradient from an ensemble of scorers, such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021)</ref>, to guide and update the generated proposals. We iteratively repeat this procedure until the final step.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 2 (b)</ref>, the image x k generated at iteration k is first sent to the diffusion model to generate an image proposalx k+1 . Each scorer outputs a score to evaluate whether the generated image matches the given text input. For example, CLIP computes the cosine similarity between the image and text features as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_22">x k+1 ?x k+1 + ?? x k N n=1 E n ? x k , c ,<label>(2)</label></formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction as x k+1 instead of x k?1 (used by most diffusion models) to keep the consistent notation across tasks.</p><p>Video question answering (VQA). Caption generation for a single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2</ref> (c). We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by x t+1 = G(x t , C t ). To update C t , we first use G to generate a set of candidate wordsX t+1 = {x t+1 }, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x t+1 }, wher? x t+1 ?X t+1 ) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss L CLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the summed score (multiple CLIP models) is then propagated to G to update C t :</p><formula xml:id="formula_23">C k+1 t ? C k t + ?? C k t N n=1 L CLIP (E n ? (x 1 , x 2 , ? ? ? ,x t+1 , I)),<label>(3)</label></formula><p>where k is the step of iterative refinement. After several iterations, the updated C t is used to generate the next token x t+1 = G(x t , C t ). We repeat this process until we generate the entire caption. We cascade the captions of multiple video frames and questions about this video to prompt GPT-3 for video question answering (See Appendix A.2).</p><p>Grade school math. We further apply PIC to solve grade school math problems. We use GPT-2 as the generator and treat the grade school math problem as a text generation problem. The scorer, a pre-trained question-solution classifier, provides the generator feedback to guide the next token's generation x t+1 . We follow the approach used in VQA to iteratively optimize the generations based on the feedback from scorers. Our generator G first generates a set of candidate wordsX t+1 = {x t+1 }, and then the classifier predicts the probability of each solution (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x t+1 }, wherex t+1 ?X t+1 ) matching the given question. The classifier score is the cross-entropy loss between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the classifier score is used to update C t through iterative refinement, same as Eq. <ref type="formula" target="#formula_14">(3)</ref>. The updated C t is used to predict the next word x t+1 = G(x t , C t ). We repeat this process until we generate the complete solution.</p><p>Robot manipulation. Finally, we illustrate how PIC can be applied to manipulate objects in the robot environment to conform to a set of object relations such as "red bowl on top of blue mug" shown in <ref type="figure" target="#fig_5">Fig. 2 (d)</ref>. We use the combination of Model Predictive Control (MPC) <ref type="bibr" target="#b35">(Williams et al., 2015)</ref> and the World Model as the generator. At each time step, we first use MPC to sample a set of possible actions and then render the state images (after executing an action) from multiple camera views using the world model. For each action, the scorer computes a summed score across all camera views as its final score, which is used to select the best action to execute. Thus, in this domain, the ensemble consists of scorers based on different views of the scene.</p><p>For the generator, we assume that there is a pre-trained model, i.e. world model, that can accurately render and simulate the dynamic changes in the robot world. Since such a large pre-trained model does not directly exist, we approximate it using an environment simulator combined with MPC as the generator. For the scorer, we use the pre-trained ViLD <ref type="bibr" target="#b11">(Gu et al., 2021)</ref> to generate segmentation maps for images captured by different camera views n, and the corresponding text label for each segment, which are used to obtain object relations. We compare the generated object relations and the relations specified by the text description to obtain the score, i.e. score equals 0 if they match; otherwise, 1 (here the score means the distance) (see Appendix A.4 for details). To obtain a final world state x T that satisfies the specified relations, and the action sequence {a 1 , ? ? ? , a T } that manipulates the objects into the final state x T , the generator iteratively samples possible actions? k t+1 and gets feedback from scorers. The best action is selected as:</p><formula xml:id="formula_24">a t+1 = arg min a k t+1 N n=1 E n ? (x t ,? k t+1 ).<label>(4)</label></formula><p>Each scorer, E n ? , outputs a score for the resultant state obtained when a candidate action? k t+1 is applied to the current world state x t . We execute a t+1 in the environment and get a new state x t+1 . We repeat this process until the task is accomplished or we are at the final step T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing pre-trained models on four representative tasks, including image generation, video question answering, grade school math, and robot manipulation.  Image generation. We first show that composing the pre-trained image generator and scorer models such as CLIP enables effective zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref> with the image resolution of 64 ? 64. The class labels are used as the text input to guide image generation. Each method generates 50 images for each class. We evaluate the image generation quality using Inception Score (IS) <ref type="bibr" target="#b31">(Salimans et al., 2016)</ref>, Fr?chet Inception Distance (FID) <ref type="bibr" target="#b12">(Heusel et al., 2017)</ref>, and Kernel Inception Distance (KID) <ref type="bibr" target="#b2">(Bi?kowski et al., 2018)</ref>. IS measures the distribution of generated images. Higher values mean the models can generate more distinct images. FID considers the distributions of both generated images and real images. Lower scores represent that the generated images are closer to the real images. KID is similar to FID, measuring the similarity between two data distributions, but is in the kernel space.</p><p>Video question answering. We evaluate methods for solving VQA tasks on ActivityNet-QA <ref type="bibr" target="#b39">(Yu et al., 2019)</ref>. Our method generates free-form language answers instead of selecting an answer from a pre-defined answer set <ref type="bibr" target="#b37">(Yang et al., 2021;</ref><ref type="bibr" target="#b17">Lei et al., 2022)</ref>. To evaluate such free-form VQA, we ask workers from Amazon Mechanical Turk to measure whether the generated answer matches the given question and video (See Appendix B for IRB approval and experimental details). For fair comparisons, all the approaches answer the same 300 video questions, and each answer is evaluated by three different workers. The accuracy rate and vocabulary size are reported. An answer is correct if at least two workers believe it is correct. The accuracy rate is the percentage of correctly answered questions over all the questions. To evaluate the diversity of generated answers, we also report the vocabulary size (i.e. the number of words) of answers generated by each method.</p><p>Grade school math. GSM8K <ref type="bibr" target="#b5">(Cobbe et al., 2021</ref>) is a dataset for grade school math problems. Each problem consists of a question, intermediate analyses, and a final solution. We evaluate approaches to solving problems on the 1K test set. We use beam search to generate candidate solutions. The accuracy of beam size 1 and beam size 5 are reported. For beam size of 1, we mark the result as correct if it matches the final solution. For beam size of 5, we mark the result as correct if any of the five generated results matches the solution.</p><p>Robot manipulation. We next evaluate how pre-trained models may be used to manipulate objects in Ravens <ref type="bibr" target="#b40">(Zeng et al., 2020)</ref>. In Ravens, the action space of robot is to drop an object at a 2D location on the table. The goal is to obtain a scene configuration that satisfies the object relations specified by a textual description or a real-world image, such as "blue mug to the left of purple bowl". The task is successful if the object relations in the final state satisfy all the relations specified by the input text or image. We report the success rate of tasks with two and three specified object relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We compare the proposed method with baselines on the above four zero-shot tasks.  <ref type="figure" target="#fig_0">Figure 3</ref>: Video question answering example results. Our approach successfully identifies gender and clothing, but its failure to count objects is a reflection of GPT-2 and CLIP's inability to count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">IMAGE GENERATION</head><p>We evaluate the zero-shot conditional image generation on ImageNet in <ref type="table" target="#tab_4">Table 1</ref>. We first show results of composing a single generator (G) and a single scorer (E). We compose GLIDE  with three different types of scorers, respectively. E1 is CLIP <ref type="bibr" target="#b26">(Radford et al., 2021</ref>) that computes the cosine similarity between the image and text features as the score, E2 is the image classifier (CLS) ) that predicts the probability of the image matching the text label as the score, and E3 is the classifier-free guidance (CLS-FREE) <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref> which can be treated as an implicit classifier that directly provides pixel-wise gradient feedback to the generated image (Appendix A.1). We then compose the generator with all scorers, i.e. G+E1+E2+E3.</p><p>Composing the generator and a single scorer allows zero-shot image generation. Composing multiple scorers significantly outperforms a single scorer. We note that the generator is not trained on ImageNet; thus the results in <ref type="table" target="#tab_4">Table 1</ref> cannot be directly compared with methods trained on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">VIDEO QUESTION ANSWERING</head><p>Quantitative results. We compare PIC with one of the state-of-the-art VQA approaches, i.e. Jus-tAsk <ref type="bibr" target="#b37">(Yang et al., 2021)</ref>, on ActivityNet-QA <ref type="bibr" target="#b39">(Yu et al., 2019)</ref>. In <ref type="table" target="#tab_5">Table 2</ref>, JustAsk (FT) is finetuned on ActivityNet-QA, thus achieving the best results. We then compare PIC with JustAsk (Pretrain) for zero-shot VQA. The generator of our method, GPT-2 (medium size), is trained on Webtext <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> using the Huggingface library <ref type="bibr" target="#b36">(Wolf et al., 2019)</ref>. Our scorers are CLIP models <ref type="bibr" target="#b26">(Radford et al., 2021;</ref><ref type="bibr" target="#b29">Reimers &amp; Gurevych, 2019</ref>) trained on different datasets or using different configurations. PIC (G+E1) outperforms JustAsk (Pretrain) by %7.72. Composing more scorers further improves the accuracy by %2.78. In addition, the vocabulary size of answers generated by our method is larger than other approaches, indicating that our method can answer questions using richer language and more diverse phrasing. Note that our method solves a "more challenging" problem than JustAsk (Pretrain) and JustAsk (FT). Our method generates open-language answers while JustAsk (Pretrain) and JustAsk (FT) select an answer from a pre-defined answer set. Generating free-form responses requires both semantic and grammatical correctness. PIC performs well on both these dimensions while also using a richer vocabulary.</p><p>Qualitative results. In <ref type="figure" target="#fig_0">Fig. 3</ref>, we show answers generated by different approaches given a video (only showing a single video frame) and questions. Our approach successfully identifies gender and clothing, but none of the approaches know how to count numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">GRADE SCHOOL MATH</head><p>Quantitative results. In <ref type="table" target="#tab_7">Table 3</ref>, we compare PIC with two baselines, i.e. GPT-Pretrain and GPT-FT, for solving math problems on GSM8K <ref type="bibr" target="#b5">(Cobbe et al., 2021)</ref>. GPT-Pretrain uses the pre-trained GPT-2 (medium size GPT-2 trained on Webtext using Huggingface) to generate numeric strings. GPT-FT is Q: In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?</p><p>A: 25% 60 GPT Pretrain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT FT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PIC (G+E)</head><p>A: 20</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>A: 60 Q: Melanie is a door-to-door saleswoman. She sold a third of her vacuum cleaners at the green house, 2 more to the red house, and half of what was left at the orange house. If Melanie has 5 vacuum cleaners left, how many did she start with?</p><p>A: 5 18</p><p>A: 15 A: 18 Q: A fog bank rolls in from the ocean to cover a city. It takes 10 minutes to cover every 3 miles of the city. If the city is 42 miles across from the oceanfront to the opposite inland edge, how many minutes will it take for the fog bank to cover the whole city?</p><p>A: 10 140 A: 10 A: 140</p><p>Grade school math questions <ref type="figure">Figure 4</ref>: Grade school math example results. Our method can solve math problems involving addition, subtraction, multiplication, and division.</p><p>based on GPT-Pretrain and then finetuned on GSM8K. Our method uses the same GPT-2 (Pretrain) as the generator and a question-solution classifier (CLS) as the scorer. The classifier is trained on GSM8K to distinguish whether a solution is correct for a given question. We surprisingly find that PIC achieves significantly better performance than GPT-FT (%13.344 higher on beam size 1), even though the generator has never seen the math problems before. The classifier only provides feedback to the generator, but through iterative refinement, combining a generator and a scorer without joint training is more effective than directly finetuning GPT-2 on GSM8K (we find the overfitting problem when finetuning GPT-2 on GSM8K).</p><p>Qualitative results. Example results of different methods are shown in <ref type="figure">Fig. 4</ref>. Our method can solve math problems involving addition, subtraction, multiplication, and division, even for solutions with three-digit numbers. In contrast, GPT-FT often fails to understand math problems. Quantitative results. We evaluate the proposed method of manipulating objects to achieve object relations specified by the textual descriptions (Text) or real-world images (Image). In <ref type="table" target="#tab_8">Table 4</ref>, we find that using scorers of multiple camera views substantially improves the accuracy on both settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">ROBOT MANIPULATION</head><p>Qualitative results. <ref type="figure" target="#fig_12">Figure 5</ref> shows the example results of the proposed method manipulating objects to accomplish the given task. Our method enables zero-shot robot manipulation on objects with different sizes, colors, and shapes given either the language goal or image goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ANALYSIS</head><p>PIC exhibits effective zero-shot generalization ability on a variety of tasks. To further understand the source of such generalization, we investigate two key components in PIC, i.e. the composition of multiple scorers (consensus optimization) (Section 6.1) and the iterative refinement (Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">EFFECT OF CONSENSUS OPTIMIZATION</head><p>We have shown that composing multiple scorers contributes to zero-shot generalization. We further explore the influence of gradually adding each new scorer on the zeros-shot performance.   Image generation. In <ref type="table" target="#tab_9">Table 5</ref>, we first show results of composing GLIDE and the CLIP scorer. We then gradually add a new scorer, the image classifier or classifier-free guidance, each time. Finally, we report the results of composing the generator and all scorers. The performance improves every time we add a new scorer, indicating that composing multiple scorers improves zero-shot performance.</p><p>Robot manipulation. In <ref type="table" target="#tab_10">Table 7</ref>, we analyze the effect of composing multiple scores on robot manipulation. The goal is specified by textual descriptions. Composing scores from multiple views, PIC (G+ 3 n=1 E n ) and PIC (G+ 5 n=1 E n ), leads to higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EFFECT OF ITERATIVE REFINEMENT</head><p>Next, we explore the influence of iterative refinement on zero-shot generalization, i.e. the feedback loop between the generator and scorers. We compare PIC with baselines that compose the generator and scorers, but with the scorers only providing feedback to the generator at the end. Grade school math. In <ref type="table">Table 6</ref>, the baselines, GPT-Pretrain+E and GPT-FT+E, generate five proposal solutions of a given math problem. Then the scorer, i.e. the same question-solution classifier used in PIC, selects the best solution based on its score. PIC iteratively refines the generated answer while the baselines refine the entirely generated solutions in the end. PIC and GPT-Pretrain+E use the same generator and scorer, but PIC outperforms GPT-Pretrain+E by %7.507. PIC still achieves better performance than GPT-FT+E, which uses a stronger generator (finetuned on the GSM8K dataset).</p><p>Robot manipulation. In <ref type="table" target="#tab_10">Table 7</ref>, the baseline, No-IR (G+ 5 n=1 E n ), first samples 100 trajectories without using the feedback from scorers. Then the scorers select the best trajectories based on the summed score. The generator and scorers of this baseline are the same as our method, i.e. PIC (G+ 5 n=1 E n ), but our method outperforms the baseline by %37.5 on the "2 Relations" setting, indicating the effectiveness of iterative refinement in the proposed framework. <ref type="table">Table 6</ref>: Effect of iterative refinement. Grade school math results on GSM8K. PIC with iterative refinement outperforms baselines where the scorer only provides feedback to the generator at the end stage (t = T ). BS is the beam search size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Name</head><p>Generator</p><formula xml:id="formula_25">Scorer Interaction BS=1 ? GPT-Pretrain+E GPT-2 (Medium) (Pretrain) CLS t = T 9.704 GPT-FT+E GPT-2 (Medium) (FT) CLS t = T 14.481 PIC (G+E) GPT-2 (Medium) (Pretrain) CLS t = {1, ? ? ? , T } 17.210</formula><p>Together, these results show that the composition of multiple scorers and iterative refinement are both important for zero-shot generalization. These results point to the potential broader applicability of the proposed method as a general purpose framework for zero-shot multimodal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a unified framework for composing ensembles of pre-trained models through iterative consensus without any training or finetuning. Our framework consists of a generator and an ensemble of scorers. The scorers provide feedback to the generator to iteratively improve its generated results. We show the proposed method allows effective zero-shot generalization on four representative tasks, i.e. image generation, video question answering, grade school math, and robot manipulation, and even outperforms methods that directly finetune models on certain tasks. We further analyze the source of such zero-shot generalization by exploring the effect of the composition of multiple scorers and the iterative refinement, and find that both are important for zero-shot generalization.</p><p>As our method does not need any training or finetuning, one drawback is that its performance depends on the pre-trained models. Training large models are complementary to the framework and methods we proposed and may be directly applied. We hope to explore these directions for zero-shot generalization in future work. In addition, our framework enables the composition of separately trained models and boosts performance by leveraging the knowledge from multiple expert models. The scorers can be learned at different times on different data in an incremental-learning manner, enabling the combination of incrementally learned knowledge. Our framework thus paves the way for many potential applications in lifelong learning / continual learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL DETAILS</head><p>In this section, we provide more experimental details of each task. We use TITAN RTX 24GB GPUs for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 IMAGE GENERATION</head><p>We use the reverse diffusion process of GLIDE, a text-guided diffusion model, as the generator to generate image proposals. At each step of the diffusion process (corresponding to a step of the iterative refinement), we use the gradient from an ensemble of scorers to guide and update the generated proposals. We iteratively repeat this procedure until the final step.</p><p>As shown in <ref type="figure">Fig. A1</ref>, the image x k generated at iteration k is first sent to the diffusion model to generate an image proposalx k+1 . The scorers provide feedback to refine the generated result. The CLIP model computes the cosine similarity between the image and text features as the score (we used the pre-trained CLIP model from <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref>.). The image classifier  predicts the probability of the image matching the text label as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 . The classifier-free guidance <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref> can be treated as an implicit classifier that directly provides pixel-wise gradient feedback to the generated image. Our framework enables the use of ensembles of different pre-trained models as scorers, significantly improving the zero-shot results by leveraging the strengths of multiple expert models.</p><p>Our implementation for image generation is modified based on the code of GLIDE  and the classifier guidance diffusion . We use DDIM to sample images from GLIDE in 100 steps. The guidance scale is set to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 VIDEO QUESTION ANSWERING</head><p>In video question answering, we use the proposed method to generate captions for the video frames and then use GPT-3 to summarize the captions to answer questions. We use GPT-2 as the generator and a set of CLIP models as scorers to generate captions for each video frame. The CLIP models <ref type="bibr" target="#b26">(Radford et al., 2021;</ref><ref type="bibr" target="#b29">Reimers &amp; Gurevych, 2019)</ref> are from the Huggingface library <ref type="bibr" target="#b36">(Wolf et al., 2019)</ref>:</p><p>? CLIP-32: https://huggingface.co/openai/clip-vit-base-patch32.</p><p>? CLIP-14: https://huggingface.co/openai/clip-vit-large-patch14.</p><p>? CLIP-multilingual: https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1. <ref type="figure" target="#fig_5">Fig. A2</ref> shows the framework for generating frame captions. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to ZeroCap <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by x t+1 = G(x t , C t ).</p><p>Our implementation is based on the code of ZeroCap <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>. The context cache C t is updated in the same way as Equation 5 in <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, but we compose multiple CLIP scores when providing the feedback to C t . The CLIP loss L CLIP is similar to their Equation 4. We also used the cross-entropy loss L CE in their Equation 2 to ensure the generated sentence is grammatically sound. After several iterations, the updated C t is used to generate the next token x t+1 = G(x t , C t ).</p><p>We repeat this process until we generate the entire caption.</p><p>To answer the video questions, we cascade the generated captions of the video frames and the questions about this video to prompt GPT-3 to generate answers. For each video, we delete the first  image and text features as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_26">x k+1 x k+1 + r x k N X n=1 E n ? x k , c ,<label>(2)</label></formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction as x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across tasks. Video question answering (VQA). We first use PIC to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions about this video. Caption generation for a single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by x t+1 = G(x t , C t ). To update C t , we first use G to generate a set of candidate wordsX t+1 = {x t+1 }, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x t+1 }, wherex t+1 2X t+1 ) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss L CLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the summed score (multiple CLIP models) is then propagated to G to update C t :</p><formula xml:id="formula_27">C k+1 t C k t + r x N X n=1 L CLIP (E n ? (x 1 , x 2 , ? ? ? ,x t+1 , I)),<label>(3)</label></formula><p>where k is the step of iterative refinement. After several iterations, the updated C t is used to generate the next token x t+1 = G(x t , C t ). We repeat this process until we generate the entire caption. We image and text features as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_28">x k+1 x k+1 + r x k N X n=1 E n ? x k , c ,<label>(2)</label></formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction as x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across tasks. Video question answering (VQA). We first use PIC to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions about this video. Caption generation for a single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by x t+1 = G(x t , C t ). To update C t , we first use G to generate a set of candidate wordsX t+1 = {x t+1 }, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x t+1 }, wherex t+1 2X t+1 ) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss L CLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the summed score (multiple CLIP models) is then propagated to G to update C t :</p><formula xml:id="formula_29">C k+1 t C k t + r x N X n=1 L CLIP (E n ? (x 1 , x 2 , ? ? ? ,x t+1 , I)),<label>(3)</label></formula><p>where k is the step of iterative refinement. After several iterations, the updated C t is used to generate the next token x t+1 = G(x t , C t ). We repeat this process until we generate the entire caption. We image and text features as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_30">x k+1 x k+1 + r x k N X n=1 E n ? x k , c ,<label>(2)</label></formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction as x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across tasks. Video question answering (VQA). We first use PIC to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions about this video. Caption generation for a single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by x t+1 = G(x t , C t ). To update C t , we first use G to generate a set of candidate wordsX t+1 = {x t+1 }, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x t+1 }, wherex t+1 2X t+1 ) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss L CLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the summed score (multiple CLIP models) is then propagated to G to update C t :</p><formula xml:id="formula_31">C k+1 t C k t + r x N X n=1 L CLIP (E n ? (x 1 , x 2 , ? ? ? ,x t+1 , I)),<label>(3)</label></formula><p>where k is the step of iterative refinement. After several iterations, the updated C t is used to generate the next token x t+1 = G(x t , C t ). We repeat this process until we generate the entire caption. We image and text features as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 :</p><formula xml:id="formula_32">x k+1 x k+1 + r x k N X n=1 E n ? x k , c ,<label>(2)</label></formula><p>where N is the number of scorers and c is the text label. We denote the reverse process prediction as x k+1 instead of x k 1 (used by most diffusion models) to keep consistent notation across tasks. Video question answering (VQA). We first use PIC to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions about this video. Caption generation for a single video frame is shown in <ref type="figure" target="#fig_5">Fig. 2 (c)</ref>. We use GPT-2 as the generator and multiple different CLIP models, trained with different configurations, as the scorers. Given a video frame I, we generate a sequence of words to describe it. To integrate feedback from scorers to the generator, similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. The prediction of the next word from the generator G is given by x t+1 = G(x t , C t ). To update C t , we first use G to generate a set of candidate wordsX t+1 = {x t+1 }, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x t+1 }, wherex t+1 2X t+1 ) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss L CLIP between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the summed score (multiple CLIP models) is then propagated to G to update C t :</p><formula xml:id="formula_33">C k+1 t C k t + r x N X n=1 L CLIP (E n ? (x 1 , x 2 , ? ? ? ,x t+1 , I)),<label>(3)</label></formula><p>where k is the step of iterative refinement. After several iterations, the updated C t is used to generate the next token x t+1 = G(x t , C t ). We repeat this process until we generate the entire caption. We 4 + <ref type="figure">Figure A1</ref>: Overview of image generation. We use the reverse diffusion process of GLIDE , a text-guided diffusion model, as the generator to generate image proposals. At each step of the diffusion process (corresponding to a step of the iterative refinement), we use the gradient from an ensemble of scorers, such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021)</ref>, to guide and update the generated proposals. The image x k generated at iteration k is first sent to the diffusion model to generate an image proposalx k+1 . The scorers provide feedback to refine the generated result. The CLIP model computes the cosine similarity between the image and text features as the score. The image classifier  predicts the probability of the image matching the text label as the score. The scores generated by different scorers are summed, and their gradient with respect to x k is used to compute the next reverse prediction x k+1 . Classifier-free guidance <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref> can be treated as an implicit classifier that directly provides pixel-wise gradient feedback to the generated image. We iteratively repeat this procedure until the final step. Our framework enables the use of ensembles of different pre-trained models as scorers, significantly improving the zero-shot results by leveraging the strengths of multiple expert models.</p><p>10 frames and the last 10 frames to remove the beginning or ending advertisements. We then take 30 video frames evenly from the rest frames and send them to GPT-3. To guide GPT-3 to generate proper answers, we randomly select 30 question-answer pairs from the training set of ActivityNet-QA <ref type="bibr" target="#b39">(Yu et al., 2019)</ref> and use them as part of the prompt of GPT-3. As shown in <ref type="figure" target="#fig_0">Fig. A3</ref>, the prompt of GPT-3 consists of examples of question-answer pairs, the video frame captions generated by the proposed method, and the question about this video that needs to be answered. The text generated by GPT-3 is used as the answer to the question asked. We also used the profanity check tool (https://github.com/vzhou842/profanity-check) to remove the improper answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 GRADE SCHOOL MATH</head><p>We treat the grade school math problem as a text generation problem. As shown in <ref type="figure">Fig. A4</ref>, we use GPT-2 as the generator and a pre-trained question-solution classifier as the scorer. The pre-trained classifier is a binary classifier trained on the training set of GSM8K <ref type="bibr" target="#b5">(Cobbe et al., 2021)</ref>. Given a math problem, such as "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?", and an answer, such as "72". If the answer is correct for the given problem, then the label is 1; otherwise, the label is 0.</p><p>After training, the classifier is used as the scorer to provide feedback to the generator to guide the next token's generation x t+1 . Similar to VQA, the generator G first generates a set of candidate word? X t+1 = {x t+1 }, and then the classifier predicts the probability of each solution (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x t+1 }, wherex t+1 ?X t+1 ) matching the given question. The classifier score is the cross-entropy loss between this new probability distribution and candidate word A human is making</p><p>Generator: GPT-2 Context information summed up, and their gradient with respect to x t is used to obtain x t+1 :</p><formula xml:id="formula_34">x t+1 = N (x t+1 + r x N X n=1 E n ? (x t , c) , 2 ),<label>(2)</label></formula><p>where N is the normal distribution, N is the number of scorers and 2 is the variance.</p><p>Video question answering (VQA). We first use the proposed framework to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_5">Fig. 2  (c)</ref>, our framework combines GPT-2 and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. Given a video frame and a text prompt, such as "Image of", we generate a sequence of words to describe the frame. Similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in Transformer <ref type="bibr">(Vaswani et al., 2017)</ref>) that store the context information generated so far. The prediction of the next word can be written as</p><formula xml:id="formula_35">x t+1 = LM (x t , C t ),</formula><p>where LM is the language model (GPT-2). The goal is to update C t iteratively based on the CLIP score to generate the next word such that the sentence is grammatically sound as well as accurately describes the given video frame. To do this, we first use GPT-2 to generate a set of candidate words {x i t+1 }, and then use the feature distance between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x i t+1 }) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss between this clip distribution and the original distribution of the next word obtained from GPT-2. Similar to image generation, the gradient of summed scores (multiple CLIP models) is propagated to GPT-2 to update C t . After several iterations, the updated C t is used to generate the next token x t+1 = LM (x t , C t ). We repeat this process until we generate the entire frame caption. We cascade the video frame captions and questions about this video to prompt GPT-3 for video question answering. 4 pasta a CLIP 1 + ? + CLIP N word generated after iterative refinement Video question answering (VQA). We first use the proposed framework to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_5">Fig. 2  (c)</ref>, our framework combines GPT-2 and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. Given a video frame and a text prompt, such as "Image of", we generate a sequence of words to describe the frame. Similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in Transformer <ref type="bibr">(Vaswani et al., 2017)</ref>) that store the context information generated so far. The prediction of the next word can be written as x t+1 = LM (x t , C t ), where LM is the language model (GPT-2). The goal is to update C t iteratively based on the CLIP score to generate the next word such that the sentence is grammatically sound as well as accurately describes the given video frame. To do this, we first use GPT-2 to generate a set of candidate words {x i t+1 }, and then use the feature distance between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x i t+1 }) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss between this clip distribution and the original distribution of the next word obtained from GPT-2. Similar to image generation, the gradient of summed scores (multiple CLIP models) is propagated to GPT-2 to update C t . After several iterations, the updated C t is used to generate the next token x t+1 = LM (x t , C t ). We repeat this process until we generate the entire frame caption. We cascade the video frame captions and questions about this video to prompt GPT-3 for video question answering. to generate image proposals. Our method can compose the generator with one or multiple scorers, such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021</ref>), text-image classifiers , and the classifier-free guidance <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref>.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 2 (right)</ref>, the image x t generated at iteration t is first sent to the GLIDE diffusion model to generate an image proposalx t+1 . Each scorer outputs a score to evaluate whether the generated image matches the given text input. For example, CLIP computes the cosine distance of the image feature and text feature. The text-image classifier predicts a probability of the image matching the text label. The classifier-free guidance can be treated as an implicit classifier that provides pixel-wise gradient feedback to the generator directly. The energy scores generated by different scorers are summed up. We compute the gradient of summed energy score with respect to the original image proposal to update the generated image:</p><formula xml:id="formula_36">x t+1 = x t 2 r x N X n=1 E n ? (x t , c) ,<label>(4)</label></formula><p>where N is the number of scorers.</p><p>Robot planning.</p><p>Video Question Answering. We first use the proposed framework to generate video frame captions. We then use GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, our framework combines GPT-2 (Medium size) and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. The history tokens {x 1 , ? ? ? , x t } is first sent to the generator to predict the next tokenx t+1 . Then the scorers compute the feature distances (scores) between the new sentence (concatenation of history tokens and the new token) and the given video frame. Similar to image generation, the gradient of summed scores are propagated to the generator to update the next token x t+1 . We cascade the video frame captions and questions about this video to prompt GPT-3. Results show that utilizing the proposed framework and GPT-3 enables effective video question answering.</p><p>Grade school math. We treat the grade school math problem as the text generation problem. Similar to video question answering, the generator is a GPT-2 model (Medium size) and the scorers provide feedback to the generator to guide the generation of next token x t+1 . The scorers can be text classifiers to evaluate the correctness of the output answer for the given math problem (See ??.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing large models on four representative zeroshot tasks, including image generation, video question answering, grade school math, and robot manipulation.</p><p>Image Generation. We first show that composing the image generation model, i.e. GLIDE, and multiple scorer models, i.e. CLIP, text-image classifier, and classifier-free guidance, enables effective zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr">(Deng et al.,</ref><ref type="bibr">4</ref> Figure A2: Overview of video frame captioning for video question answering. We use GPT-2 as the generator and a set of CLIP models as scorers to generate captions for each video frame. To integrate feedback from scorers to the generator, similar to ZeroCap <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache Ct (a set of embedding functions in GPT-2) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. To update Ct, we first use G to generate a set of candidate word? Xt+1 = {xt+1}, and then use the feature distance (after softmax) between each sentence (the concatenation of previous words and each new word {x1, x2, ? ? ? ,xt+1}, wherext+1 ?Xt+1) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss LCLIP between this new probability distribution and the original distribution of the next word obtained from the generator G (see <ref type="table" target="#tab_8">Equation 4</ref> in <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>). The gradient of summed scores (multiple CLIP models) is propagated to G to update Ct (see <ref type="table" target="#tab_9">Equation 5</ref> in <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>). After several iterations, the updated Ct is used to generate the next token xt+1 = G(xt, Ct). We repeat this process until we generate the entire caption. We cascade the captions of multiple video frames and questions about this video to prompt GPT-3 for video question answering. # Q: is the person with a golden hair long hair <ref type="figure" target="#fig_0">Figure A3</ref>: Prompt given to GPT-3 for video question answering. Text in black contains the question-answer pairs randomly sampled from the ActivityNet-QA training dataset. Text in blue has the video frame captions generated by the proposed method. Text in orange is the question about this video that needs to be answered.</p><p>the original distribution of the next word obtained from the generator G (the way to compute the classifier score is the same as computing the CLIP score in VQA). We also used the cross-entropy loss L CE in Equation 2 of ZeroCap <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref> to ensure the generated sentence is grammatically sound. The context cache C t is updated in the same way as Equation 5 in <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, but we use the classifier score when providing the feedback to C t . The updated C t is used to predict the next word x t+1 = G(x t , C t ). We repeat this process until we generate the complete solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 ROBOT MANIPULATION</head><p>In robot manipulation, we use the proposed method to manipulate objects in Ravens <ref type="bibr" target="#b40">(Zeng et al., 2020)</ref> to conform to a set of object relations specified by text descriptions or real-world images. We use MPC+World Model as the generator and ViLD <ref type="bibr" target="#b11">(Gu et al., 2021)</ref> as the scorer. As shown in <ref type="figure" target="#fig_12">Figure A5</ref>, given a real-world image, our model manipulates objects in the environment to achieve a candidate word A : 2 5</p><p>Generator: GPT-2 Context information pixel-wise gradient feedback to the generated image. The scores generated by different scorers are summed up, and their gradient with respect to x t is used to obtain x t+1 :</p><formula xml:id="formula_37">x t+1 = N (x t+1 + r x N X n=1 E n ? (x t , c) , 2 ),<label>(2)</label></formula><p>where N is the normal distribution, N is the number of scorers and 2 is the variance.</p><p>Video question answering (VQA). We first use the proposed framework to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_5">Fig. 2</ref> (c), our framework combines GPT-2 and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. Given a video frame and a text prompt, such as "Image of", we generate a sequence of words to describe the frame. Similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in Transformer <ref type="bibr">(Vaswani et al., 2017)</ref>) that store the context information generated so far. The prediction of the next word can be written as</p><formula xml:id="formula_38">x t+1 = LM (x t , C t ),</formula><p>where LM is the language model (GPT-2). The goal is to update C t iteratively based on the CLIP score to generate the next word such that the sentence is grammatically sound as well as accurately describes the given video frame. To do this, we first use GPT-2 to generate a set of candidate words {x i t+1 }, and then use the feature distance between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x i t+1 }) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss between this clip distribution and the original distribution of the next word obtained from GPT-2. Similar to image generation, the gradient of summed scores (multiple CLIP models) is propagated to GPT-2 to update C t . After several iterations, the updated C t is used to generate the next token x t+1 = LM (x t , C t ). We repeat this process until we generate the entire frame caption. We cascade the video frame captions and questions about this video to prompt GPT-3 for video question answering. 4 5 0</p><p>Question-solution classifier word generated after iterative refinement Video question answering (VQA). We first use the proposed framework to generate video frame captions. We then use GPT-3 to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_5">Fig. 2  (c)</ref>, our framework combines GPT-2 and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. Given a video frame and a text prompt, such as "Image of", we generate a sequence of words to describe the frame. Similar to <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>, we define a context cache C t (a set of embedding functions in Transformer <ref type="bibr">(Vaswani et al., 2017)</ref>) that store the context information generated so far. The prediction of the next word can be written as x t+1 = LM (x t , C t ), where LM is the language model (GPT-2). The goal is to update C t iteratively based on the CLIP score to generate the next word such that the sentence is grammatically sound as well as accurately describes the given video frame. To do this, we first use GPT-2 to generate a set of candidate words {x i t+1 }, and then use the feature distance between each sentence (the concatenation of previous words and each new word {x 1 , x 2 , ? ? ? ,x i t+1 }) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss between this clip distribution and the original distribution of the next word obtained from GPT-2. Similar to image generation, the gradient of summed scores (multiple CLIP models) is propagated to GPT-2 to update C t . After several iterations, the updated C t is used to generate the next token x t+1 = LM (x t , C t ). We repeat this process until we generate the entire frame caption. We cascade the video frame captions and questions about this video to prompt GPT-3 for video question answering. to generate image proposals. Our method can compose the generator with one or multiple scorers, such as CLIP <ref type="bibr" target="#b26">(Radford et al., 2021</ref>), text-image classifiers , and the classifier-free guidance <ref type="bibr" target="#b13">(Ho &amp; Salimans, 2022)</ref>.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 2 (right)</ref>, the image x t generated at iteration t is first sent to the GLIDE diffusion model to generate an image proposalx t+1 . Each scorer outputs a score to evaluate whether the generated image matches the given text input. For example, CLIP computes the cosine distance of the image feature and text feature. The text-image classifier predicts a probability of the image matching the text label. The classifier-free guidance can be treated as an implicit classifier that provides pixel-wise gradient feedback to the generator directly. The energy scores generated by different scorers are summed up. We compute the gradient of summed energy score with respect to the original image proposal to update the generated image:</p><formula xml:id="formula_39">x t+1 = x t 2 r x N X n=1 E n ? (x t , c) ,<label>(4)</label></formula><p>where N is the number of scorers.</p><p>Robot planning.</p><p>Video Question Answering. We first use the proposed framework to generate video frame captions. We then use GPT-3 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> to summarize the captions and answer questions. As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, our framework combines GPT-2 (Medium size) and multiple CLIP models, trained with different configurations, for zero-shot video frame captioning. The history tokens {x 1 , ? ? ? , x t } is first sent to the generator to predict the next tokenx t+1 . Then the scorers compute the feature distances (scores) between the new sentence (concatenation of history tokens and the new token) and the given video frame. Similar to image generation, the gradient of summed scores are propagated to the generator to update the next token x t+1 . We cascade the video frame captions and questions about this video to prompt GPT-3. Results show that utilizing the proposed framework and GPT-3 enables effective video question answering.</p><p>Grade school math. We treat the grade school math problem as the text generation problem. Similar to video question answering, the generator is a GPT-2 model (Medium size) and the scorers provide feedback to the generator to guide the generation of next token x t+1 . The scorers can be text classifiers to evaluate the correctness of the output answer for the given math problem (See ??.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing large models on four representative zeroshot tasks, including image generation, video question answering, grade school math, and robot manipulation.</p><p>Image Generation. We first show that composing the image generation model, i.e. GLIDE, and multiple scorer models, i.e. CLIP, text-image classifier, and classifier-free guidance, enables effective zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr">(Deng et al.,</ref><ref type="bibr">4</ref> Figure A4: Overview of solving grade school math problems. We use GPT-2 as the generator and treat the grade school math problem as a text generation problem. The scorer, a pre-trained question-solution classifier, provides the generator feedback to guide the next token's generation xt+1. We follow the approach used in VQA to iteratively optimize the generations based on the feedback from scorers. Our generator G first generates a set of candidate wordsXt+1 = {xt+1}, and then the classifier predicts the probability of each solution (the concatenation of previous words and each new word {x1, x2, ? ? ? ,xt+1}, wherext+1 ?Xt+1) matching the given question. The classifier score is the cross-entropy loss between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the classifier score is used to update Ct through iterative refinement (see <ref type="table" target="#tab_9">Equation 5</ref> in <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>). The updated Ct is used to predict the next word xt+1 = G(xt, Ct). We repeat this process until we generate the complete solution.</p><p>state with objects having the same object relations as the given image. We first use ViLD to generate a 2D segmentation of the real-world image and the corresponding text label, such as "mug", for each segment. We then use the relative pixel-wise offsets of segmentation masks and the text labels to infer a set of object relations (top panel of <ref type="figure" target="#fig_12">Figure A5</ref>).</p><p>Given the current world state x t , we aim to generate an action a t+1 so that the new world state after executing a t+1 has object relations closer to the object relations in the given image. To do this, we first use the generator (MPC+World Model) to generate a set of candidate actions {? k t+1 } and the corresponding world states {x k t+1 } after executing each candidate action. For each new world stat? x k t+1 , we render N 2D images from N camera views. Each rendered image is sent to VILD to get a segmentation map and text labels. We project the objects into 3D space based on the segmentation map and the depth map of the image. We then obtain the object relations based on their 3D positions and the predicted text labels. We compare the object relations obtained from each rendered image and the object relations obtained from the real-world image to compute the score. The score is 0 if the relations are matching; otherwise, 1. We sum the scores from each rendered image to obtain the final score. We choose the action a t+1 that leads to a world state with the minimum summed score. We execute a t+1 in the environment and get a new state x t+1 . We repeat this process until the task is accomplished or we are at the final step T , where T equals to the number of relations extracted from the real-world image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 A UNIFIED FRAMEWORK FOR COMPOSING PRE-TRAINED MODELS</head><p>Our method shares some similar architecture with existing works, such as ZeroCap <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref> and CLIP-guided diffusion models . However, the focus of our paper is to propose a general framework for composing different pre-trained models across a variety of tasks, and these particular methods are concrete instantiations of our proposed framework. In addition, in this work, we also illustrate how we may combine ensembles of different pre-trained models as scorers to leverage the "wisdom of the crowds" where each scorer provides complementary feedback to the generator, compensating for the potential weaknesses of other scorers. Through iterative optimization and the composition of multiple scorers, our method shows effective zero-shot generalization ability on various multimodal tasks. Generator: GPT-2 Context information and each new word {x1, x2, ? ? ? ,x i t+1}) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss between this clip distribution and the original distribution of the next word obtained from GPT-2. Similar to image generation, the gradient of summed scores (multiple CLIP models) is propagated to GPT-2 to update Ct. After several iterations, the updated Ct is used to generate the next token xt+1 = LM (xt, Ct). We repeat this process until we generate the entire frame caption. We cascade the video frame captions and questions about this video to prompt GPT-3 for video question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>The CLIP score is the cross-entropy loss between this clip distribution and the original distribution of the next word obtained from GPT-2. Similar to image generation, the gradient of summed scores (multiple CLIP models) is propagated to GPT-2 to update Ct. After several iterations, the updated Ct is used to generate the next token xt+1 = LM (xt, Ct). We repeat this process until we generate the entire frame caption. We cascade the video frame captions and questions about this video to prompt GPT-3 for video question answering. 4 entire frame caption. We cascade the video frame captions and questions about this video to prompt GPT-3 for video question answering. 4 <ref type="figure">Figure 6</ref>: Overview of video frame captioning for video question answering. We define a context cache Ct (a set of embedding functions in GPT-2 as in <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>) that stores the context information generated so far, which is updated iteratively based on the feedback from scorers. To update Ct, we first use G to generate a set of candidate words {x i t+1}, and then use the feature distance between each sentence (the concatenation of previous words and each new word {x1, x2, ? ? ? ,x i t+1}) and the video frame as the probability of them matching. The CLIP score is the cross-entropy loss LCLIP between this new probability distribution and the original distribution of the next word obtained from the generator G (see <ref type="table" target="#tab_8">Equation 4</ref> in <ref type="bibr" target="#b33">(Tewel et al., 2021)</ref>). The gradient of summed scores (multiple CLIP models) is propagated to G to update Ct. After several iterations, the updated Ct is used to generate the next token xt+1 = G(xt, Ct). We repeat this process until we generate the entire caption. We cascade the captions of multiple video frames and questions about this video to prompt GPT-3 for video question answering.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 ROBOT MANIPULATION</head><p>In robot manipulation, we use the proposed method to manipulate objects in Ravens <ref type="bibr" target="#b40">(Zeng et al., 2020)</ref> to conform to a set of object relations specified by text descriptions or real-world images. We use MPC+World model as the generator and the ViLD <ref type="bibr" target="#b11">(Gu et al., 2021)</ref> as the scorer. As shown in <ref type="figure">Figure 9</ref>, given a real-world image, our model manipulates objects in the environment to achieve a state with objects having the same object relations as the given image. We first use ViLD to generate a 2D segmentation of the real-world image and the corresponding text label, such as "mug", for each segment. We then use the relative pixel-wise offsets of segmentation masks and the text labels to infer a set of object relations (top panel of <ref type="figure">Figure 9</ref>).</p><p>Given the current world state xt, we aim to generate an action at+1 so that the new world state after executing at+1 has object relations the same as object relations in the given image. Under review as a conference paper at ICLR 2023 cascade the captions of multiple video frames and questions about this video to prom video question answering.</p><p>Grade school math. We further apply PIC to solve grade school math problems. We u the generator and treat the grade school math problem as a text generation problem. T pre-trained question-solution classifier, provides the generator feedback to guide the generation xt+1. We follow the approach used in VQA to iteratively optimize the gener on the feedback from scorers. Our generator G first generates a set of candidate words then the classifier predicts the probability of each solution (the concatenation of pre and each new word {x1, x2, ? ? ? ,x i t+1 }) matching the given question. The classifier cross-entropy loss between this new probability distribution and the original distributio word obtained from the generator G. The gradient of the classifier score is used to updat iterative refinement. The updated Ct is used to predict the next word xt+1 = G(xt, Ct this process until we generate the complete solution.</p><p>Robot manipulation. Finally, we illustrate how PIC can be applied to manipulate object environment to conform to a set of object relations such as "red bowl on top of blue mu <ref type="figure" target="#fig_5">Fig. 2 (d)</ref>. We use the combination of the Model Predictive Control (MPC) (Williams and the World Model as the generator. At each time step, we first use MPC to sample a se actions and then render the state images (after executing an action) from multiple camera the world model. For each action, the scorer computes a summed score across all camera final score, which is used to select the best action to execute.</p><p>For the generator, we assume that there is a pre-trained model, i.e. world model, that ca render and simulate the dynamic changes in the robot world. Since such a large pre-tr does not directly exist, we approximate it using an environment simulator combined with generator. For the scorer, we use the pre-trained ViLD <ref type="bibr" target="#b11">(Gu et al., 2021)</ref> to generate s maps for images captured by different camera views, and the corresponding text la segment, which are used to obtain object relations. We compare the generated object r the relations specified by the text description to obtain the scorer, i.e. score equals 0 if otherwise, 1 (here the score means the distance). To obtain a final world state xT that specified relations, and the action sequence {a1, ? ? ? , aT } that manipulates the objects state xT , the generator iteratively samples possible actions? i t+1 and gets feedback from best action is selected by: <ref type="figure">?t+1)</ref>. Each scorer, E n ? , outputs a score for the resultant state obtained when a candidate ac applied to the current world state xt. We execute at+1 in the environment and get a ne We repeat this process until the task is accomplished or we are at the final step T .</p><formula xml:id="formula_40">at+1 = arg min at+1 N X n=1 E n ? (xt,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing pre-trained models on four represe including image generation, video question answering, grade school math, and robot m Image generation. We first show that composing the pre-trained image generation mod models such as CLIP enables effective zero-shot image generation. We evaluate the imag results on ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref> with the image resolution of 64 ? 64. The cla used as text input to guide image generation. Each method generates 50 images for ea evaluate the image generation quality using Inception Score (IS) <ref type="bibr">(Salimans et al.,</ref><ref type="bibr">20</ref> Inception Distance (FID) <ref type="bibr" target="#b12">(Heusel et al., 2017)</ref>, and Kernel Inception Distance <ref type="bibr">(KID) et al., 2018)</ref>. IS measures the distribution of generated images. Higher values mean can generate more distinct images. FID considers both the distribution of generated im distribution of real images. Lower scores represent the generated images are closer to the KID is similar to FID, measuring the similarity between two data distributions but in the Video question answering. We evaluate methods for solving VQA tasks on Activity et al., 2019). Our method generates free-form language answers instead of selecting an a pre-defined answer set <ref type="bibr" target="#b37">(Yang et al., 2021;</ref><ref type="bibr" target="#b17">Lei et al., 2022)</ref>. To evaluate such free-for ask workers from Amazon Mechanical Turk to measure whether the generated answer given question and video (See Appendix B for IRB approval and experimental deta </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPC + World Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>World state</head><p>Under review as a conference paper at ICLR 2023 cascade the captions of multiple video frames and questions about this video to prompt GPT-3 for video question answering.</p><p>Grade school math. We further apply PIC to solve grade school math problems. We use GPT-2 as the generator and treat the grade school math problem as a text generation problem. The scorer, a pre-trained question-solution classifier, provides the generator feedback to guide the next token's generation xt+1. We follow the approach used in VQA to iteratively optimize the generations based on the feedback from scorers. Our generator G first generates a set of candidate words {x i t+1 }, and then the classifier predicts the probability of each solution (the concatenation of previous words and each new word {x1, x2, ? ? ? ,x i t+1 }) matching the given question. The classifier score is the cross-entropy loss between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the classifier score is used to update Ct through iterative refinement. The updated Ct is used to predict the next word xt+1 = G(xt, Ct). We repeat this process until we generate the complete solution.</p><p>Robot manipulation. Finally, we illustrate how PIC can be applied to manipulate objects in the robot environment to conform to a set of object relations such as "red bowl on top of blue mug" shown in <ref type="figure" target="#fig_5">Fig. 2 (d)</ref>. We use the combination of the Model Predictive Control (MPC) <ref type="bibr" target="#b35">(Williams et al., 2015)</ref> and the World Model as the generator. At each time step, we first use MPC to sample a set of possible actions and then render the state images (after executing an action) from multiple camera views using the world model. For each action, the scorer computes a summed score across all camera views as its final score, which is used to select the best action to execute.</p><p>For the generator, we assume that there is a pre-trained model, i.e. world model, that can accurately render and simulate the dynamic changes in the robot world. Since such a large pre-trained model does not directly exist, we approximate it using an environment simulator combined with MPC as the generator. For the scorer, we use the pre-trained ViLD <ref type="bibr" target="#b11">(Gu et al., 2021)</ref> to generate segmentation maps for images captured by different camera views, and the corresponding text label for each segment, which are used to obtain object relations. We compare the generated object relations and the relations specified by the text description to obtain the scorer, i.e. score equals 0 if they match; otherwise, 1 (here the score means the distance). To obtain a final world state xT that satisfies the specified relations, and the action sequence {a1, ? ? ? , aT } that manipulates the objects into the final state xT , the generator iteratively samples possible actions? i t+1 and gets feedback from scorers. The best action is selected by:</p><formula xml:id="formula_41">at+1 = arg min at+1 N X n=1 E n ? (xt,?t+1).<label>(4)</label></formula><p>Each scorer, E n ? , outputs a score for the resultant state obtained when a candidate action?t+1 is applied to the current world state xt. We execute at+1 in the environment and get a new state xt+1. We repeat this process until the task is accomplished or we are at the final step T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing pre-trained models on four representative tasks, including image generation, video question answering, grade school math, and robot manipulation.</p><p>Image generation. We first show that composing the pre-trained image generation model and scorer models such as CLIP enables effective zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref> with the image resolution of 64 ? 64. The class labels are used as text input to guide image generation. Each method generates 50 images for each class. We evaluate the image generation quality using Inception Score (IS) <ref type="bibr" target="#b31">(Salimans et al., 2016)</ref>, Fr?chet Inception Distance (FID) <ref type="bibr" target="#b12">(Heusel et al., 2017)</ref>, and Kernel Inception Distance (KID) <ref type="bibr" target="#b2">(Bi?kowski et al., 2018)</ref>. IS measures the distribution of generated images. Higher values mean the models can generate more distinct images. FID considers both the distribution of generated images and the distribution of real images. Lower scores represent the generated images are closer to the real images. KID is similar to FID, measuring the similarity between two data distributions but in the kernel space.</p><p>Video question answering. We evaluate methods for solving VQA tasks on ActivityNet-QA <ref type="bibr" target="#b39">(Yu et al., 2019)</ref>. Our method generates free-form language answers instead of selecting an answer from a pre-defined answer set <ref type="bibr" target="#b37">(Yang et al., 2021;</ref><ref type="bibr" target="#b17">Lei et al., 2022)</ref>. To evaluate such free-form VQA, we ask workers from Amazon Mechanical Turk to measure whether the generated answer matches the given question and video (See Appendix B for IRB approval and experimental details). For fair 5 Candidate action ? Action sampled in different iterations Under review as a conference paper at ICLR 2023 cascade the captions of multiple video frames and questions about this video to prompt GPT-3 for video question answering.</p><p>Grade school math. We further apply PIC to solve grade school math problems. We use GPT-2 as the generator and treat the grade school math problem as a text generation problem. The scorer, a pre-trained question-solution classifier, provides the generator feedback to guide the next token's generation xt+1. We follow the approach used in VQA to iteratively optimize the generations based on the feedback from scorers. Our generator G first generates a set of candidate words {x i t+1 }, and then the classifier predicts the probability of each solution (the concatenation of previous words and each new word {x1, x2, ? ? ? ,x i t+1 }) matching the given question. The classifier score is the cross-entropy loss between this new probability distribution and the original distribution of the next word obtained from the generator G. The gradient of the classifier score is used to update Ct through iterative refinement. The updated Ct is used to predict the next word xt+1 = G(xt, Ct). We repeat this process until we generate the complete solution.</p><p>Robot manipulation. Finally, we illustrate how PIC can be applied to manipulate objects in the robot environment to conform to a set of object relations such as "red bowl on top of blue mug" shown in <ref type="figure" target="#fig_5">Fig. 2 (d)</ref>. We use the combination of the Model Predictive Control (MPC) <ref type="bibr" target="#b35">(Williams et al., 2015)</ref> and the World Model as the generator. At each time step, we first use MPC to sample a set of possible actions and then render the state images (after executing an action) from multiple camera views using the world model. For each action, the scorer computes a summed score across all camera views as its final score, which is used to select the best action to execute.</p><p>For the generator, we assume that there is a pre-trained model, i.e. world model, that can accurately render and simulate the dynamic changes in the robot world. Since such a large pre-trained model does not directly exist, we approximate it using an environment simulator combined with MPC as the generator. For the scorer, we use the pre-trained ViLD <ref type="bibr" target="#b11">(Gu et al., 2021)</ref> to generate segmentation maps for images captured by different camera views, and the corresponding text label for each segment, which are used to obtain object relations. We compare the generated object relations and the relations specified by the text description to obtain the scorer, i.e. score equals 0 if they match; otherwise, 1 (here the score means the distance) (see Appendix A.4 for details). To obtain a final world state xT that satisfies the specified relations, and the action sequence {a1, ? ? ? , aT } that manipulates the objects into the final state xT , the generator iteratively samples possible actions? k t+1 and gets feedback from scorers. The best action is selected by:</p><formula xml:id="formula_42">at+1 = arg min a k t+1 N X n=1 E n ? (xt,? k t+1 ).<label>(4)</label></formula><p>Each scorer, E n ? , outputs a score for the resultant state obtained when a candidate action? k t+1 is applied to the current world state xt. We execute at+1 in the environment and get a new state xt+1. We repeat this process until the task is accomplished or we are at the final step T . a k+2 t+1 (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT SETUP</head><p>We evaluate the proposed framework for composing pre-trained models on four representative tasks, including image generation, video question answering, grade school math, and robot manipulation.</p><p>Image generation. We first show that composing the pre-trained image generation model and scorer models such as CLIP enables effective zero-shot image generation. We evaluate the image generation results on ImageNet <ref type="bibr" target="#b6">(Deng et al., 2009)</ref> with the image resolution of 64 ? 64. The class labels are used as text input to guide image generation. Each method generates 50 images for each class. We evaluate the image generation quality using Inception Score (IS) <ref type="bibr" target="#b31">(Salimans et al., 2016)</ref>, Fr?chet Inception Distance (FID) <ref type="bibr" target="#b12">(Heusel et al., 2017)</ref>, and Kernel Inception Distance (KID) <ref type="bibr" target="#b2">(Bi?kowski et al., 2018)</ref>. IS measures the distribution of generated images. Higher values mean the models can generate more distinct images. FID considers both the distribution of generated images and the distribution of real images. Lower scores represent the generated images are closer to the real images. KID is similar to FID, measuring the similarity between two data distributions but in the kernel space.</p><p>Video question answering. We evaluate methods for solving VQA tasks on ActivityNet-QA <ref type="bibr" target="#b39">(Yu et al., 2019)</ref>. Our method generates free-form language answers instead of selecting an answer from 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Goal object relations</head><p>Goal object relations <ref type="figure" target="#fig_12">Figure A5</ref>: Overview of robot manipulation. We use MPC+World Model as the generator and ViLD as the scorer to manipulate objects to conform to a set of object relations specified by text descriptions or real-world images. Top: given a real-world image, we first use ViLD to generate a 2D segmentation of the real-world image and the corresponding text label, such as "mug", for each segment. We then use the relative pixel-wise offsets of segmentation masks and the text labels to infer a set of object relations. Bottom: Given the current world state xt, we aim to generate an action at+1 so that the new world state after executing at+1 has object relations closer to the object relations in the given image. To do this, we first use the generator (MPC+World model) to generate a set of candidate actions {? k t+1 } and the corresponding world states {x k t+1 } after executing each candidate action. For each new world statex k t+1 , we render N 2D images from N camera views. Each rendered image is sent to VILD to get a segmentation map and text labels. We project the objects into 3D space based on the segmentation map and the depth map of the image. We then obtain the object relations based on their 3D positions and predicted text labels. We compare the object relations obtained from each rendered image and the object relations obtained from the real-world image to compute the score. The score is 0 if the relations are matching; otherwise, 1. We sum the scores from each rendered image to obtain the final score. We choose the action at+1 that leads to a world state with the minimum summed score. We execute at+1 in the environment and get a new state xt+1. We repeat this process until the task is accomplished or we are at the final step T . <ref type="figure">Figure A6</ref>: Screenshot of the approval form from the Committee on the Use of Humans as Experimental Subjects. <ref type="figure" target="#fig_20">Figure A7</ref>: Screenshot of Amazon Mechanical Turk we used for the video question answering experiment. Workers are shown a video, three questions, and the answer to each question. The answers are generated by different methods. The workers are not told which method generates each answer. The workers are asked to select "yes" or "no" based on their measurement of whether the answer is correct for the given video and question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ETHICS STATEMENT OF AMAZON MECHANICAL TURK EXPERIMENTS</head><p>To evaluate approaches on solving the zero-shot video question answering tasks, we ask workers from Amazon Mechanical Turk to evaluate the generated answer based on the video and the asked question. Before showing the questions and answers to the workers, we used the profanity check tool (https://github.com/vzhou842/profanity-check) to remove the improper questions and answers. As shown in <ref type="figure">Fig. A6</ref>, this experiment was approved by the Committee on the Use of Humans as Experimental Subjects. A screenshot of the task is shown in <ref type="figure" target="#fig_20">Fig. A7</ref>. The instructions shown to participants are listed as follows:</p><p>Instructions: By making judgments about these questions and answers, you are participating in a study being performed by <ref type="bibr">[XXX]</ref>. Your participation in this research is voluntary. You may decline further participation, at any time, without adverse consequences. Your anonymity is assured; the researchers who have requested your participation will not receive any personal information about you.</p><p>Given a video, a question, and a generated answer, the workers from Amazon Mechanical Turk measure whether the answer is correct for the given question and video. Each video shows three question-answer pairs (only one question-answer pair is shown in the screenshot). The answers are generated by different methods. The workers are not told which method generates each answer. The workers are asked to choose "yes" or "no". If the worker thinks the answer matches the given video and question, they should choose "yes"; otherwise, "no".</p><p>To control the quality, each task is evaluated by three different workers. The workers are required to have an approval rate greater than 98%. Our test shows that each task takes around 10 seconds, but the workers are given up to one hour to complete each task. The workers are paid $0.05 for finishing each task with an estimated hourly payment of $18, more than the United States federal minimum wage. There are 33 workers in total who joined our experiment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Details 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Details 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Details 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Details 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Details 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview o the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-traine diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used t provide feedback to the generator. (c) Video question answering. GPT-2 is used as the generator, and a set o CLIP models are used as scorers. (d) Robot manipulation.MPC+World model is used as the generator, and pre-trained image segmentation model is used to compute the scores from multiple camera views to select th best action. Orange lines represent the components used to refine the generated result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview of the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator. (c) Video question answering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview of the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator. (c) Video question answering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview of the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator. (c) Video question answering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview of the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator. (c) Video question answering.GPT-2 is used as the generator, and a set of CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action. Orange lines represent the components used to refine the generated result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview of the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator. (c) Video question answering. GPT-2 is used as the generator, and a set of CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action. Orange lines represent the components used to refine the generated result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Robot manipulation example results. The robot manipulates objects to achieve certain object relations that are specified by textual descriptions (first row) or real-world images (second row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview of the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator. (c) Video question answering. GPT-2 is used as the generator, and a set of CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action. Orange lines represent the components used to refine the generated result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>4Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview of the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator. (c) Video question answering. GPT-2 is used as the generator, and a set of CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action. Orange lines represent the components used to refine the generated result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>4Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview of the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator. (c) Video question answering.GPT-2 is used as the generator, and a set of CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action. Orange lines represent the components used to refine the generated result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>4Figure 2 :</head><label>2</label><figDesc>The proposed unified framework and examples on three representative tasks. (a) Overview of the proposed unified framework. Dashed lines are omitted for certain tasks. (b) Image generation. A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator. (c) Video question answering.GPT-2 is used as the generator, and a set of CLIP models are used as scorers. (d) Robot manipulation. MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action. Orange lines represent the components used to refine the generated result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 3 :</head><label>3</label><figDesc>Details 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 3 :</head><label>3</label><figDesc>Details 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 7 :</head><label>7</label><figDesc>Prompt given to GPT-3 for video question answering. Text in black contains the question-answer pairs randomly sampled from the ActivityNet-QA dataset. Text in blue has the video frame captions generated by the proposed method. Text in orange is the question about this video that needs to be answered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Red Bowl on Top of Blue Mug Energy Scorers (E)</head><label></label><figDesc></figDesc><table><row><cell cols="2">Under review as a conference paper at ICLR 2023</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Iteratively try different</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>actions to apply an object</cell><cell></cell><cell></cell><cell></cell><cell>"egg"</cell><cell></cell></row><row><cell cols="2">Updated result</cell><cell></cell><cell></cell><cell>Updated result</cell><cell></cell></row><row><cell>Generator (G): e.g. World Model</cell><cell>View 1: ?</cell><cell>"a bowl with"</cell><cell>Generator (G): e.g. GPT2</cell><cell>"rice"</cell><cell>CLIP 1: ?</cell></row><row><cell></cell><cell>View K:</cell><cell></cell><cell></cell><cell></cell><cell>CLIP 2:</cell></row><row><cell>State</cell><cell>Original result</cell><cell>Text</cell><cell></cell><cell>Original result</cell><cell>Energy Scorers (E)</cell></row><row><cell></cell><cell cols="3">Select the action that has</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">the minimal summed score</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>State</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Generator:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MPC+</cell><cell></cell><cell>?</cell><cell></cell></row><row><cell></cell><cell cols="2">World Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>State</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Input text: red bowl</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">on top of blue mug</cell><cell></cell><cell></cell></row><row><cell>ng large models on four representative zero-tion answering, grade school math, and robot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the image generation model, i.e. GLIDE, and r, and classifier-free guidance, enables effective generation results on ImageNet (Deng et al.,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>ed framework to generate video frame captions. e the captions and answer questions. As shown size) and multiple CLIP models, trained with captioning. The history tokens {x 1 , ? ? ? , x t } nx t+1 . Then the scorers compute the feature nation of history tokens and the new token) and the gradient of summed scores are propagated ascade the video frame captions and questions t utilizing the proposed framework and GPT-3 problem as the text generation problem. Similar model (Medium size) and the scorers provide of next token x t+1 . The scorers can be text swer for the given math problem (See ??.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>with the image resolution of 64 ? 64. The class labels are used as text input to guide the image generation. Each method generate 50 images on each class.</figDesc><table><row><cell cols="3">Under review as a conference paper at ICLR 2023</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Iteratively try different</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>actions to apply an object</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>"egg"</cell></row><row><cell cols="2">Updated result</cell><cell></cell><cell></cell><cell></cell><cell>Updated result</cell></row><row><cell>Generator (G): e.g. World Model</cell><cell></cell><cell>View 1: ?</cell><cell>"a bowl with"</cell><cell>Generator (G): e.g. GPT2</cell><cell>"rice"</cell><cell>CLIP 1: ?</cell></row><row><cell></cell><cell></cell><cell>View K:</cell><cell></cell><cell></cell><cell></cell><cell>CLIP 2:</cell></row><row><cell>State</cell><cell>Original result</cell><cell>Energy Scorers (E)</cell><cell>Text</cell><cell></cell><cell>Original result</cell><cell>Energy Scor</cell></row><row><cell cols="2">Red Bowl on Top of Blue Mug</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Image generation results on ImageNet. Our PIC can compose the pre-trained generator (G) and scorers (E) through iterative optimization. Composing multiple scorers further boosts performance.</figDesc><table><row><cell>Method Name PIC (G+E1) PIC (G+E2) PIC (G+E3)</cell><cell cols="2">Generator Scorer GLIDE CLIP GLIDE CLS GLIDE CLS-FREE</cell><cell>IS ? 25.017 30.462 FID ? KID ? 6.174 22.077 30.871 7.952 25.926 29.219 5.325</cell></row><row><cell cols="2">PIC (G+E1+E2+E3) GLIDE</cell><cell cols="2">CLIP + CLS + CLS-FREE 34.952 29.184</cell><cell>3.766</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Video question answering results on ActivityNet-QA. JustAsk (FT) is finetuned on ActivityNet-QA, thus achieving the best results. For zero-shot VQA, our method (PIC) significantly outperforms JustAsk (Pretrain), one of the best VQA methods. Using multiple scorers further improves the performance.</figDesc><table><row><cell>Method Name JustAsk (FT)</cell><cell cols="3">Zero-Shot Generator Scorer No --</cell><cell cols="2">Accuracy ? Vocab ? 64.667 160</cell></row><row><cell cols="2">JustAsk (Pretrain) PIC (G+E1) PIC (G+E1+E2+E3) Yes Yes Yes</cell><cell>-GPT-2 GPT-2</cell><cell>-CLIP-32 CLIP-32 + CLIP-14 + CLIP-multilingual</cell><cell>50.671 58.389 61.168</cell><cell>210 267 304</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Method Name Generator GPT-Pretrain GPT-2 (Pretrain) -Scorer BS=1 ? BS=5 ? 1.744 12.206 GPT-FT GPT-2 (FT) -3.487 18.271</cell></row><row><cell>PIC (G+E)</cell><cell>GPT-2 (Pretrain) CLS</cell><cell>16.831</cell><cell>20.773</cell></row></table><note>Grade school math results on GSM8K. Our method (PIC) that composes GPT-2 and a pre-trained question-solution classifier significantly outperforms the base- lines, including GPT-FT that is finetuned on GSM8K.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Robot manipulation results on Ravens.</figDesc><table><row><cell>Method Name PIC (G+E1) PIC (G+ 5 n=1 En) 67.5 2 Relations Text ? Image ? Text ? Image ? 3 Relations 35.0 27.5 50.0 45.0 52.6 75.0 65.3</cell></row></table><note>PIC can manipulate objects to achieve object relations specified by textual descriptions (Text) or real-world images (Image). Using scorers of multiple camera views substantially improves the success rate.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Effect of composing multiple scorers. Image generation results on ImageNet. Gradually adding new scorers keeps improving the performance, indicating that composing multiple scorers contributes to zero-shot image generation.</figDesc><table><row><cell>Method Name PIC (G+E1) PIC (G+E1+E2) PIC (G+E1+E3) PIC (G+E1+E2+E3) GLIDE Generator Scorer GLIDE CLIP GLIDE CLIP + CLS GLIDE CLIP + CLS-FREE CLIP + CLS + CLS-FREE 34.952 29.184 3.766 IS ? FID ? KID ? 25.017 30.462 6.174 30.438 29.543 5.435 30.500 29.726 4.304</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Effect of composing multiple scorers and iterative refinement on robot manipulation. Both components are important for zero-shot generalization.</figDesc><table><row><cell>Method Name</cell><cell>Interaction</cell><cell cols="2">2 Relations 3 Relations</cell></row><row><cell>PIC (G+E1) PIC (G+ 3 n=1 En) PIC (G+ 5 n=1 En) No-IR (G+ 5 n=1 En)</cell><cell>t = {1, ? ? ? , T } t = {1, ? ? ? , T } t = {1, ? ? ? , T } t = T</cell><cell>35.0 57.5 67.5 30.0</cell><cell>50.0 63.3 75.0 46.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Summarize the following descriptions and answer the question as shown above: a Video showing the new Hair tutorial; a video showing young blond hair clip attaching to top pony tail of teens hair; ?; a video on the head hair clip website showing blonde long hair twisted in two knots. # Q: is the person with a golden hair long hair</figDesc><table><row><cell># Q: how many people are there in the video</cell></row><row><cell># A: 2</cell></row><row><cell># Q: what is behind the person in white clothes</cell></row><row><cell># A: tree</cell></row><row><cell># Q: what is in front of the person with braid</cell></row><row><cell># A: chair</cell></row><row><cell>...</cell></row><row><cell># Q: what is the person in white doing</cell></row><row><cell># A: tie hair</cell></row><row><cell># Q: what happened to the person in gray after he threw a goal</cell></row><row><cell># A: clap with your teammates</cell></row><row><cell>#</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b27">(Ramesh et al., 2022)</ref>, Parti<ref type="bibr" target="#b38">(Yu et al., 2022)</ref>, and Imagen<ref type="bibr" target="#b30">(Saharia et al., 2022)</ref>, can generate high-resolution images given natural language descriptions. Large pre-trained vision-language discriminative models, such as CLIP<ref type="bibr" target="#b26">(Radford et al., 2021)</ref>, convert images and languages into the same feature space, achieving remarkable zero-shot generalization ability on downstream tasks. 1 By zero-shot, we mean the composed models are never trained together on the evaluation task.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. Shuang Li is partially supported by Meta Research Fellowship. This research is partially supported by the US Army, under the DEVCOM Army Research Laboratory project, reg. no. 1130233-442111. The content does not necessarily reflect the position or the policy of any government, and no official endorsement should be inferred. Yilun Du is supported by a NSF Graduate Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we first show experimental details of each task in Appendix A. We then show the ethics statement of the Amazon Mechanical Turk experiment for video question answering in Appendix B.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Do as i can, not as i say: Grounding language in robotic affordances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Brohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keerthana</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Herzog</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01691</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yana</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynolds</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14198</idno>
		<title level="m">visual language model for few-shot learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Demystifying mmd gans. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compositional visual generation with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6637" to="6647" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visual foresight: Model-based deep reinforcement learning for vision-based robotic control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00568</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13921</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">Classifier-free diffusion guidance. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacky</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05608</idno>
		<title level="m">Embodied reasoning through planning with language models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Revealing single frame bias for video-and-language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03428</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pre-trained language models for interactive decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Akyurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01771</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to compose visual relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="23166" to="23178" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01714</idno>
		<title level="m">Compositional visual generation with composable diffusion models</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clipcap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09734</idno>
		<title level="m">Clip prefix for image captioning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with clip latents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06175</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">A generalist agent. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.10084" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; S Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
	</analytic>
	<monogr>
		<title level="j">Burcu Karagol Ayan</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Seyed Kamyar Seyed Ghasemipour</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cliport: What and where pathways for robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Manuelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="894" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Zero-shot image-to-text generation for visual-semantic arithmetic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoad</forename><surname>Tewel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14447</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Model predictive path integral control using covariance variable importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grady</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aldrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Theodorou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01149</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Just ask: Learning to answer questions from millions of narrated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1686" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Scaling autoregressive models for contentrich text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>Baid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burcu Karagol Ayan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10789</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Activitynet-qa: A dataset for understanding complex web videos via question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9127" to="9134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Welker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Attarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14406</idno>
		<title level="m">Transporter networks: Rearranging the visual world for robotic manipulation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Socratic models: Composing zero-shot multimodal reasoning with language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Welker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aveek</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00598</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
