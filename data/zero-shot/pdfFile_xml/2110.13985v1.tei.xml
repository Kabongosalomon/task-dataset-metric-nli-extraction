<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-28">October 28, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
							<email>albertgu@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isys</forename><surname>Johnson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Saab</surname></persName>
							<email>ksaab@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<email>trid@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University at Buffalo</orgName>
								<address>
									<region>SUNY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-28">October 28, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence u ? y by simply simulating a linear continuous-time state-space representation? = Ax + Bu, y = Cx + Du. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices A that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A longstanding challenge in machine learning is efficiently modeling sequential data longer than a few thousand time steps. The usual paradigms for designing sequence models involve recurrence (e.g. RNNs), convolutions (e.g. CNNs), or differential equations (e.g. NDEs), which each come with tradeoffs. For example, RNNs are a natural stateful model for sequential data that require only constant computation/storage per time step, but are slow to train and suffer from optimization difficulties (e.g., the "vanishing gradient problem" <ref type="bibr" target="#b38">[39]</ref>), which empirically limits their ability to handle long sequences. CNNs encode local context and enjoy fast, parallelizable training, but are not sequential, resulting in more expensive inference and an inherent limitation on the context length. NDEs are a principled mathematical model that can theoretically address continuous-time problems and long-term dependencies <ref type="bibr" target="#b36">[37]</ref>, but are very inefficient. Ideally, a model family would combine the strengths of these paradigms, providing properties like parallelizable training (convolutional), stateful inference (recurrence) and time-scale adaptation (differential equations), while handling very long sequences in a computationally efficient way. Several recent works have turned to this question. These include the CKConv, which models a continuous convolution kernel <ref type="bibr" target="#b43">[44]</ref>; several ODE-inspired RNNs, such as the UnICORNN <ref type="bibr" target="#b46">[47]</ref>; the LMU, which speeds up a specific linear recurrence using convolutions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b57">58]</ref>; and HiPPO <ref type="bibr" target="#b23">[24]</ref>, a generalization of the LMU that introduces a theoretical framework for continuous-time memorization. However, these model families come at the price of reduced expressivity: intuitively, a family that is both convolutional and recurrent should be more restrictive than either. Our first goal is to construct an expressive model family that combines all 3 paradigms while preserving their strengths. The Linear State-Space Layer (LSSL) is a simple sequence model that maps a 1-dimensional function or sequence u(t) ? y(t) through an implicit state x(t) by simulating a linear continuous-time state-space representation in discrete-time? (t) = Ax(t) + Bu(t)</p><p>(1)</p><formula xml:id="formula_0">y(t) = Cx(t) + Du(t),<label>(2)</label></formula><p>where A controls the evolution of the system and B, C, D are projection parameters. The LSSL can be viewed as an instantiation of each family, inheriting their strengths ( <ref type="figure" target="#fig_0">Fig. 1</ref>):</p><p>? LSSLs are recurrent. If a discrete step-size ?t is specified, the LSSL can be discretized into a linear recurrence using standard techniques, and simulated during inference as a stateful recurrent model with constant memory and computation per time step.</p><p>? LSSLs are convolutional. The linear time-invariant systems defined by (1)+(2) are known to be explicitly representable as a continuous convolution. Moreover, the discrete-time version can be parallelized during training using convolutions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>? LSSLs are continuous-time. The LSSL itself is a differential equation. As such, it can perform unique applications of continuous-time models, such as simulating continuous processes, handling missing data <ref type="bibr" target="#b44">[45]</ref>, and adapting to different timescales.</p><p>Surprisingly, we show that LSSLs do not sacrifice expressivity, and in fact generalize convolutions and RNNs. First, classical results from control theory imply that all 1-D convolutional kernels can be approximated by an LSSL <ref type="bibr" target="#b58">[59]</ref>. Additionally, we provide two results relating RNNs and ODEs that may be of broader interest, e.g. showing that some RNN architectural heuristics (such as gating mechanisms) are related to the step-size ?t and can actually be derived from ODE approximations. As corollaries of these results, we show that popular RNN methods are special cases of LSSLs.</p><p>The generality of LSSLs does come with tradeoffs. In particular, we describe and address two challenges that naive LSSL instantiations face when handling long sequences: (i) they inherit the limitations of both RNNs and CNNs at remembering long dependencies, and (ii) choosing the state matrix A and timescale ?t appropriately are critical to their performance, yet learning them is computationally infeasible. We simultaneously address these challenges by specializing LSSLs using a carefully chosen class of structured matrices A, such that (i) these matrices generalize prior work on continuous-time memory <ref type="bibr" target="#b23">[24]</ref> and mathematically capture long dependencies with respect to a learnable family of measures, and (ii) with new algorithms, LSSLs with these matrices A can be theoretically sped up under certain computation models, even while learning the measure A and timescale ?t.</p><p>We empirically validate that LSSLs are widely effective on benchmark datasets and very long time series from healthcare sensor data, images, and speech.</p><p>? On benchmark datasets, LSSLs obtain SoTA over recent RNN, CNN, and NDE-based methods across sequential image classification tasks (e.g., by over 10% accuracy on sequential CIFAR) and healthcare regression tasks with length-4000 time series (by up to 80% reduction in RMSE).</p><p>? To showcase the potential of LSSLs to unlock applications with extremely long sequences, we introduce a new sequential CelebA classification task with length-38000 sequences. A small LSSL comes within 2.16 accuracy points of a specialized ResNet-18 vision architecture that has 10x more parameters and is trained directly on images.</p><p>? Finally, we test LSSLs on a difficult dataset of high-resolution speech clips, where usual speech pipelines pre-process the signals to reduce the length by 100x. When training on the raw length-16000 signals, the LSSL not only (i) outperforms previous methods by over 20 accuracy points in 1/5 the training time, but (ii) outperforms all baselines that use the pre-processed length-160 sequences, overcoming the limitations of hand-crafted feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Contributions</head><p>? We introduce Linear State-Space Layers (LSSLs), a simple sequence-to-sequence transformation that shares the modeling advantages of recurrent, convolutional, and continuous-time methods. Conversely, we show that RNNs and CNNs can be seen as special cases of LSSLs (Section 3).</p><p>? We prove that a structured subclass of LSSLs can learn representations that solve continuous-time memorization, allowing it to adapt its measure and timescale (Section 4.1). We also provide new algorithms for these LSSLs, showing that they can be sped up computationally under an arithmetic complexity model Section 4.2.</p><p>? Empirically, we show that LSSLs stacked into a deep neural network are widely effective on time series data, even (or especially) on extremely long sequences (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Technical Background</head><p>We summarize the preliminaries on differential equations that are necessary for this work. We first introduce two standard approximation schemes for differential equations that we will use to convert continuous-time models to discrete-time, and will be used in our results on understanding RNNs. We give further context on the step size or timescale ?t, which is a particularly important parameter involved in this approximation process. Finally, we provide a summary of the HiPPO framework for continuous-time memorization <ref type="bibr" target="#b23">[24]</ref>, which will give us a mathematical tool for constructing LSSLs that can address long-term dependencies.</p><p>Approximations of differential equations. Any differential equation?(t) = f (t, x(t)) has an equivalent integral equation x(t) = x(t 0 )+ t t0 f (s, x(s)) ds. This can be numerically solved by storing some approximation for x, and keeping it fixed inside f (t, x) while iterating the equation. For example, Picard iteration is often used to prove the existence of solutions to ODEs by iterating the equation x i+1 (t) := x i (t 0 ) + t t0 f (s, x i (s)) ds . In other words, it finds a sequence of functions x 0 (t), x 1 (t), . . . that approximate the solution x(t) of the integral equation.</p><p>Discretization. On the other hand, for a desired sequence of discrete times t i , approximations to x(t 0 ), x(t 1 ), . . . can be found by iterating the equation x(t i+1 ) = x(t i ) + ti+1 ti f (s, x(s)) ds. Different ways of approximating the RHS integral lead to different discretization schemes. We single out a discretization method called the generalized bilinear transform (GBT) which is specialized to linear ODEs of the form <ref type="bibr" target="#b0">(1)</ref>. Given a step size ?t, the GBT update is</p><formula xml:id="formula_1">x(t + ?t) = (I ? ??t ? A) ?1 (I + (1 ? ?)?t ? A)x(t) + ?t(I ? ??t ? A) ?1 B ? u(t).<label>(3)</label></formula><p>Three important cases are: ? = 0 becomes the classic Euler method which is simply the first-order approximation x(t + ?t) = x(t) + ?t ? x (t); ? = 1 is called the backward Euler method; and ? = 1 2 is called the bilinear method, which preserves the stability of the system <ref type="bibr" target="#b60">[61]</ref>.</p><p>In Section 3.2 we will show that the backward Euler method and Picard iteration are actually related to RNNs. On the other hand, the bilinear discretization will be our main method for computing accurate discrete-time approximations of our continuous-time models. In particular, define A and B to be the matrices appearing in (3) for ? = 1 2 . Then the discrete-time state-space model is</p><formula xml:id="formula_2">x t = Ax t?1 + Bu t (4) y t = Cx t + Du t .<label>(5)</label></formula><p>?t as a timescale. In most models, the length of dependencies they can capture is roughly proportional to 1 ?t . Thus we also refer to the step size ?t as a timescale. This is an intrinsic part of converting a continuous-time ODE into a discrete-time recurrence, and most ODE-based RNN models have it as an important and non-trainable hyperparameter <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b57">58]</ref>. On the other hand, in Section 3.2 we show that the gating mechanism of classical RNNs is a version of learning ?t. Moreover when viewed as a CNN, the timescale ?t can be viewed as controlling the width of the convolution kernel (Section 3.2). Ideally, all ODE-based sequence models would be able to automatically learn the proper timescales.</p><p>Continuous-time memory. Consider an input function u(t), a fixed probability measure ?(t), and a sequence of N basis functions such as polynomials. At every time t, the history of u before time t can be projected onto this basis, which yields a vector of coefficients x(t) ? R N that represents an optimal approximation of the history of u with respect to the provided measure ?. The map taking the function u(t) ? R to coefficients x(t) ? R N is called the High-Order Polynomial Projection Operator (HiPPO) with respect to the measure ?. In special cases such as the uniform measure ? = I{[0, 1]} and the exponentially-decaying measure ?(t) = exp(?t), Gu et al. <ref type="bibr" target="#b23">[24]</ref> showed that x(t) satisfies a differential equatio? <ref type="bibr" target="#b0">(1)</ref>) and derived closed forms for the matrix A. Their framework provides a principled way to design memory models handling long dependencies; however, they prove only these few special cases.</p><formula xml:id="formula_3">x(t) = A(t)x(t) + B(t)u(t) (i.e.,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linear State-Space Layers (LSSL)</head><p>We define our main abstraction, a model family that generalizes recurrence and convolutions. Section 3.1 first formally defines the LSSL, then discusses how to compute it with multiple views. Conversely, Section 3.2 shows that LSSLs are related to mechanisms of the most popular RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Different Views of the LSSL</head><p>Given a fixed state space representation A, B, C, D, an LSSL is the sequence-to-sequence mapping defined by discretizing the linear state-space model (1) and <ref type="bibr" target="#b1">(2)</ref>.</p><p>Concretely, an LSSL layer has parameters A, B, C, D, and ?t. It operates on an input u ? R L?H representing a sequence of length L where each timestep has an H-dimensional feature vector. Each feature</p><formula xml:id="formula_4">h ? [H] defines a sequence (u (h) t ) t?[L]</formula><p>, which is combined with a timescale ?t h to define an output y (h) ? R L via the discretized state-space model (4)+ <ref type="bibr" target="#b4">(5)</ref>.</p><p>Computationally, the discrete-time LSSL can be viewed in multiple ways ( <ref type="figure" target="#fig_0">Fig. 1)</ref>.</p><p>As a recurrence. The recurrent state x t?1 ? R H?N carries the context of all inputs before time t. The current state x t and output y t can be computed by simply following equations (4)+ <ref type="bibr" target="#b4">(5)</ref>. Thus the LSSL is a recurrent model with efficient and stateful inference, which can consume a (potentially unbounded) sequence of inputs while requiring fixed computation/storage per time step.</p><p>As a convolution. For simplicity let the initial state be x ?1 = 0. Then (4)+(5) explicitly yields</p><formula xml:id="formula_5">y k = C A k Bu0 + C A k?1 Bu1 + ? ? ? + CABu k?1 + Bu k + Du k .<label>(6)</label></formula><p>Then y is simply the (non-circular) convolution</p><formula xml:id="formula_6">y = K L (A, B, C) * u + Du, where K L (A, B, C) = CA i B i?[L] ? R L = (CB, CAB, . . . , CA L?1 B).<label>(7)</label></formula><p>Thus the LSSL can be viewed as a convolutional model where the entire output y ? R H?L can be computed at once by a convolution, which can be efficiently implemented with three FFTs.</p><p>The computational bottleneck. We make a note that the bottleneck of (i) the recurrence view is matrix-vector multiplication (MVM) by the discretized state matrix A when simulating (4), and (ii) the convolutional view is computing the Krylov function K L <ref type="bibr" target="#b6">(7)</ref>. Throughout this section we assumed the LSSL parameters were fixed, which means that A and K L (A, B, C) can be cached for efficiency. However, learning the parameters A and ?t would involve repeatedly re-computing these, which is infeasible in practice. We revisit and solve this problem in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Expressivity of LSSLs</head><p>For a model to be both recurrent and convolutional, one might expect it to be limited in other ways. Indeed, while <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">44]</ref> also observe that certain recurrences can be replaced with a convolution, they note that it is not obvious if convolutions can be replaced by recurrences. Moreover, while the LSSL is a linear recurrence, popular RNN models are nonlinear sequence models with activation functions between each time step. We now show that LSSLs surprisingly do not have limited expressivity.</p><p>Convolutions are LSSLs. A well-known fact about state-space systems (1)+(2) is that the output y is related to the input u by a convolution y(t) = h(? )u(t ? ? )d? with the impulse response h of the system. Conversely, a convolutional filter h that is a rational function of degree N can be represented by a state-space model of size N <ref type="bibr" target="#b58">[59]</ref>. Thus, an arbitrary convolutional filter h can be approximated by a rational function (e.g., by Pad? approximants) and represented by an LSSL.</p><p>In the particular case of LSSLs with HiPPO matrices (Sections 2 and 4.1), there is another intuitive interpretation of how LSSL relate to convolutions. Consider the special case when A corresponds to a uniform measure (in the literature known as the LMU <ref type="bibr" target="#b57">[58]</ref> or HiPPO-LegT [24] matrix). Then for a fixed dt, equation (1) is simply memorizing the input within sliding windows of 1 ?t elements, and equation (2) extracts features from this window. Thus the LSSL can be interpreted as automatically learning convolution filters with a learnable kernel width.</p><p>RNNs are LSSLs. We show two results about RNNs that may be of broader interest. Our first result says that the ubiquitous gating mechanism of RNNs, commonly perceived as a heuristic to smooth optimization <ref type="bibr" target="#b27">[28]</ref>, is actually the analog of a step size or timescale ?t.</p><formula xml:id="formula_7">Lemma 3.1. A (1-D) gated recurrence x t = (1 ? ?(z))x t?1 + ?(z)u t ,</formula><p>where ? is the sigmoid function and z is an arbitrary expression, can be viewed as the GBT(? = 1) (i.e., backwards-Euler) discretization of a 1-D linear ODE?(t) = ?x(t) + u(t).</p><p>Proof. Applying a discretization requires a positive step size ?t. The simplest way to parameterize a positive function is via the exponential function ?t = exp(z) applied to any expression z. Substituting this into (3) with A = ?1, B = 1, ? = 1 exactly produces the gated recurrence.</p><p>While Lemma 3.1 involves approximating continuous systems using discretization, the second result is about approximating them using Picard iteration (Section 2). Roughly speaking, each layer of a deep linear RNN can be viewed as successive Picard iterates x 0 (t), x 1 (t), ... approximating a function x(t) defined by a non-linear ODE. This shows that we do not lose modeling power by using linear instead of non-linear recurrences, and that the nonlinearity can instead be "moved" to the depth direction of deep neural networks to improve speed without sacrificing expressivity. </p><formula xml:id="formula_8">) = ?x + f (t, x(t)).</formula><p>We note that many of the most popular and effective RNN variants such as the LSTM <ref type="bibr" target="#b27">[28]</ref>, GRU <ref type="bibr" target="#b13">[14]</ref>, QRNN <ref type="bibr" target="#b4">[5]</ref>, and SRU <ref type="bibr" target="#b32">[33]</ref>, involve a hidden state x t ? R H that involves independently "gating" the H hidden units. Applying Lemma 3.1, they actually also approximate an ODE of the form in Lemma 3.2. Thus LSSLs and these popular RNN models can be seen to all approximate the same type of underlying continuous dynamics, by using Picard approximations in the depth direction and discretization (gates) in the time direction. Appendix C gives precise statements and proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep LSSLs</head><p>The basic LSSL is defined as a sequence-to-sequence map from R L ? R L on 1D sequences of length L, parameterized by parameters A ? R N ?N , B ? R N ?1 , C ? R 1?N , D ? R 1?1 , ?t ? R. Given an input sequence with hidden dimension H (in other words a feature dimension greater than 1), we simply broadcast the parameters B, C, D, ?t with an extra dimension H. Each of these H copies is learned independently, so that there are H different versions of a 1D LSSL processing each of the input features independently. Overall, the standalone LSSL layer is a sequence-to-sequence map with the same interface as standard sequence model layers such as RNNs, CNNs, and Transformers.</p><p>The full LSSL architecture in a deep neural network is defined similarly to standard sequence models such as deep ResNets and Transformers, involving stacking LSSL layers connected with normalization layers and residual connections. Full architecture details are described in Appendix B, including the initialization of A and ?t, computational details, and other architectural details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Combining LSSLs with Continuous-time Memorization</head><p>In Section 3 we introduced the LSSL model and showed that it shares the strengths of convolutions and recurrences while also generalizing them. We now discuss and address its main limitations, in particular handling long dependencies (Section 4.1) and efficient computation (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Incorporating Long Dependencies into LSSLs</head><p>The generality of LSSLs means they can inherit the issues of recurrences and convolutions at addressing long dependencies (Section 1). For example, viewed as a recurrence, repeated multiplication by A could suffer from the vanishing gradients problem <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44]</ref>. We confirm empirically that LSSLs with random state matrices A are actually not effective (Section 5.4) as a generic sequence model. However, one advantage of these mathematical continuous-time models is that they are theoretically analyzable, and specific A matrices can be derived to address this issue. In particular, the HiPPO framework (Section 2) describes how to memorize a function in continuous time with respect to a measure ? <ref type="bibr" target="#b23">[24]</ref>. This operator mapping a function to a continuous representation of its past is denoted hippo(?), and was shown to have the form of equation <ref type="bibr" target="#b0">(1)</ref> in three special cases. However, these matrices are non-trainable in the sense that no other A matrices were known to be hippo operators.</p><p>To address this, we theoretically resolve the open question from <ref type="bibr" target="#b23">[24]</ref>, showing that hippo(?) for any measure ? 1 results in (1) with a structured matrix A.</p><p>Theorem 1 (Informal). For an arbitrary measure ?, the optimal memorization operator hippo(?) has the form?(t) = Ax(t) + Bu(t) (1) for a low recurrence-width (LRW) <ref type="bibr" target="#b16">[17]</ref> state matrix A.</p><p>For measures covering the classical orthogonal polynomials (OPs) <ref type="bibr" target="#b51">[52]</ref> (in particular, corresponding to Jacobi and Laguerre polynomials), there is even more structure. Although beyond the scope of this section, we mention that LRW matrices are a type of structured matrix that have linear MVM <ref type="bibr" target="#b16">[17]</ref>. In Appendix D we define this class and prove Theorem 1. Quasi-separable matrices are a related class of structured matrices with additional algorithmic properties. We define these matrices in Definition 4 and prove Corollary 4.1 in Appendix D.3.</p><p>Theorem 1 tells us that a LSSL that uses a state matrix A within a particular class of structured matrices would carry the theoretical interpretation of continuous-time memorization. Ideally, we would be able to automatically learn the best A within this class; however, this runs into computational challenges which we address next (Section 4.2). For now, we define the LSSL-fixed or LSSL-f to be one where the A matrix is fixed to one of the HiPPO matrices prescribed by <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Theoretically Efficient Algorithms for the LSSL</head><p>Although A and ?t are the most critical parameters of an LSSL which govern the state-space (c.f. Section 4.1) and timescale (Sections 2 and 3.2), they are not feasible to train in a naive LSSL. In particular, Section 3.1 noted that it would require efficient matrix-vector multiplication (MVM) and Krylov function <ref type="bibr" target="#b6">(7)</ref> for A to compute the recurrent and convolutional views, respectively. However, the former seems to involve a matrix inversion (3), while the latter seems to require powering A up L times.</p><p>In this section, we show that the same restriction of A to the class of quasiseparable (Corollary 4.1), which gives an LSSL the ability to theoretically remember long dependencies, simultaneously grants it computational efficiency.</p><p>First of all, it is known that quasiseparable matrices have efficient (linear-time) MVM <ref type="bibr" target="#b39">[40]</ref>. We show that they also have fast Krylov functions, allowing efficient training with convolutions.</p><p>Theorem 2. For any k-quasiseparable matrix A (with constant k) and arbitrary B, C, the Krylov function K L (A, B, C) can be computed in quasi-linear time and space?(N + L) and logarithmic depth (i.e., is parallelizable). The operation count is in an exact arithmetic model, not accounting for bit complexity or numerical stability.</p><p>We remark that Theorem 2 is non-obvious. To illustrate, it is easy to see that unrolling <ref type="bibr" target="#b6">(7)</ref> for a general matrix A takes time LN 2 . Even if A is extremely structured with linear computation, it requires LN operations and linear depth. The depth can be reduced with the squaring technique (batch multiply by A, A 2 , A 4 , . . .), but this then requires LN intermediate storage. In fact, the algorithm for Theorem 2 is quite sophisticated (Appendix E) and involves a divide-and-conquer recursion over matrices of polynomials, using the observation that <ref type="bibr" target="#b6">(7)</ref> is related to the power series C(I ? Ax) ?1 B .</p><p>Unless specified otherwise, the full LSSL refers to an LSSL with A satisfying Corollary 4.1. In conclusion, learning within this structured matrix family simultaneously endows LSSLs with long-range memory through Theorem 1 and is theoretically computationally feasible through Theorem 2. We note the caveat that Theorem 2 is over exact arithmetic and not floating point numbers, and thus is treated more as a proof of concept that LSSLs can be computationally efficient in theory. We comment more on the limitations of the LSSL in Section 6.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Evaluation</head><p>We test LSSLs empirically on a range of time series datasets with sequences from length 160 up to 38000 (Sections 5.1 and 5.2), where they substantially improve over prior work. We additionally validate the computational and modeling benefits of LSSLs from generalizing all three main model families (Section 5.3), and analyze the benefits of incorporating principled memory representations that can be learned (Section 5.4).</p><p>Baselines. Our tasks have extensive prior work and we evaluate against previously reported best results. We highlight our primary baselines, three very recent works explicitly designed for long sequences: CKConv (a continuous-time CNN) <ref type="bibr" target="#b43">[44]</ref>, UnICORNN (an ODE-inspired RNN) <ref type="bibr" target="#b46">[47]</ref>, and Neural Controlled/Rough Differential Equations (NCDE/NRDE) (a sophisticated NDE) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref>. These are the only models we are aware of that have experimented with sequences of length &gt;10k. We test on the sequential MNIST, permuted MNIST, and sequential CIFAR tasks <ref type="table" target="#tab_0">(Table 1)</ref>, popular benchmarks which were originally designed to test the ability of recurrent models to capture long-term dependencies of length up to 1k <ref type="bibr" target="#b1">[2]</ref>. LSSL sets SoTA on sCIFAR by more than 10 points. We note that all results were achieved with at least 5x fewer parameters than the previous SoTA (Appendix F). We additionally use the BDIMC healthcare datasets <ref type="table" target="#tab_1">(Table 2</ref>), a suite of widely studied time series regression problems of length 4000 on estimating vital signs. LSSL reduces RMSE by more than two-thirds on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image and Time Series Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Speech and Image Classification for Very Long Time Series</head><p>Raw speech is challenging for ML models due to high-frequency sampling resulting in very long sequences. Traditional systems involve complex pipelines that require feeding mixed-and-matched hand-crafted features into DNNs <ref type="bibr" target="#b41">[42]</ref>. <ref type="table" target="#tab_3">Table 4</ref> reports results for the Speech Commands (SC) dataset <ref type="bibr" target="#b30">[31]</ref> for classification of 1-second audio clips. Few methods have made progress on the raw speech signal, instead requiring preprocessing with standard mel-frequency cepstrum coefficients (MFCC). By contrast, LSSL sets SoTA on this dataset while training on the raw signal. We note that MFCC extracts sliding window frequency coefficients  To stress-test the LSSL's ability to handle extremely long sequences, we create a challenging new sequential-CelebA task, where we classify 178 ? 218 images = 38000-length sequences for 4 facial attributes: Attractive (Att.), Mouth Slightly Open (MSO), Smiling (Smil.), Wearing Lipstick (WL) <ref type="bibr" target="#b35">[36]</ref>. We chose the 4 most class-balanced attributes to avoid well-known problems with class imbalance. LSSL-f comes close to matching the performance of a specialized ResNet-18 image classification architecture that has 10? the parameters <ref type="table" target="#tab_2">(Table 3)</ref>. We emphasize we are the first to demonstrate that this is possible to do with a generic sequence model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Advantages of Recurrent, Convolutional, and Continuous-time Models</head><p>We validate that the generality of LSSLs endows it with the strengths of all three families.</p><p>Convergence Speed. As a recurrent and NDE model that incorporates new theory for continuous-time memory (Section 4.1), the LSSL has strong inductive bias for sequential data, and converges rapidly to SoTA results on our benchmarks. With its convolutional view, training can be parallelized and it is also computationally efficient in practice. <ref type="table" target="#tab_4">Table 5</ref> compares the time it takes the LSSL-f to achieve SoTA, in either sample (measured by epochs) or computational (measured by wall clock) complexity. In all cases, LSSLs reached the target in a fraction of the time of the previous model.</p><p>Timescale Adaptation. <ref type="table" target="#tab_3">Table 4</ref> also reports the results of continuous-time models that are able to handle unique settings such as missing data in time series, or test-time shift in timescale (we note that this is a realistic problem, e.g., when deployed healthcare models are tested on EEG signals that are sampled at a different rate <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>). We note that many of these baselines were custom designed for such settings, which is of independent interest. On the other hand, LSSLs perform timescale adaptation by simply changing its ?t values at inference time, while still outperforming the performance of prior methods with no shift. Additional results on the CharacterTrajectories dataset from prior work <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">44]</ref> are in Appendix F, where LSSL is competitive with the best baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">LSSL Ablations: Learning the Memory Dynamics and Timescale</head><p>We demonstrate that the ?t and A parameters, which LSSLs are able to automatically learn in contrast to prior work, are indeed critical to the performance of these continuous-time models. We note that learning ?t adds only O(H) parameters and learning A adds O(N ) parameters, adding less than 1% parameter count compared to the base models with O(HN ) parameters.</p><p>Memory dynamics A. We validate that vanilla LSSLs suffer from the modeling issues described in Section 4. We tested that LSSLs with random A matrices (normalized appropriately) perform very poorly (e.g., 62% on pMNIST). Further, we note the consistent increase in performance from LSSL-f to LSSL despite the negligible parameter difference. These ablations show that (i) incorporating the theory of Theorem 1 is actually necessary for LSSLs, and (ii) further training the structured A is additionally helpful, which can be interpreted as learning the measure for memorization (Section 4.1).</p><p>Timescale ?t. Section 3.2 showed that LSSL's ability to learn ?t is its direct generalization of the critical gating mechanism of popular RNNs, which previous ODE-based RNN models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b57">58]</ref> cannot learn. We note that on sCIFAR, LSSL-f with poorly-specified ?t gets only 49.3% accuracy. Additional results in Appendix F show that learning ?t alone provides an orthogonal boost to learning A, and visualizes the noticeable change in ?t over the course of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this work we introduced a simple and principled model (LSSL) inspired by a fundamental representation of physical systems. We showed theoretically and empirically that it generalizes and inherits the strengths of the main families of modern time series models, that its main limitations of long-term memory can be resolved with new theory on continuous-time memorization, and that it is empirically effective on difficult tasks with very long sequences.</p><p>Related work. The LSSL is related to several rich lines of work on recurrent, convolutional, and continuous-time models, as well as sequence models addressing long dependencies. Appendix A provides an extended related work connecting these topics.</p><p>Tuning. Our models are very simple, consisting of identical L(inear )SSL layers with simple position-wise non-linear modules between layers (Appendix B). Our models were able to train at much higher learning rates than baselines and were not sensitive to hyperparameters, of which we did light tuning primarily on learning rate and dropout. In contrast to previous baselines <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>, we did not use hyperparameters for improving stability and regularization such as weight decay, gradient clipping, weight norm, input dropout, etc. While the most competitive recent works introduce at least one hyperparameter of critical importance (e.g. depth and step size <ref type="bibr" target="#b36">[37]</ref>, ? and ?t <ref type="bibr" target="#b46">[47]</ref>, ? 0 <ref type="bibr" target="#b43">[44]</ref>) that are difficult to tune, the LSSL-fixed has only ?t, which the full LSSL can even learn automatically (at the expense of speed).</p><p>Limitations. Sections 1 and 3 and <ref type="figure" target="#fig_0">Fig. 1</ref> mention that a potential benefit of having the recurrent representation of LSSLs may endow it with efficient inference. While this is theoretically possible, this work did not experiment on any applications that leverage this. Follow-up work showed that it is indeed possible in practice to speed up some applications at inference time.</p><p>Theorem 2's algorithm is sophisticated (Appendix D) and was not implemented in the first version of this work. A follow-up to this paper found that it is not numerically stable and thus not usable on hardware. Thus the algorithmic contributions in Theorem 2 serve the purpose of a proof-of-concept that fast algorithms for the LSSL do exist in other computation models (i.e., arithmetic operations instead of floating point operations), and leave an open question as to whether fast, numerically stable, and practical algorithms for the LSSL exist.</p><p>As described in Appendix B, by freezing the A matrix and ?t timescale, the LSSL-fixed is able to be computed much faster than the full LSSL, and is comparable to prior models in practice <ref type="table" target="#tab_4">(Table 5</ref>). However, beyond computational complexity, there is also a consideration of space efficiency. Both the LSSL and LSSL-fixed suffer from a large amount of space overhead (described in Appendix B) -using O(N L) instead of O(L) space when working on a 1D sequence of length L -that essentially stems from using the latent state representation of dimension N . Consequently, the LSSL can be space inefficient and we used multi-GPU training for our largest experiments (speech and high resolution images, <ref type="table" target="#tab_2">Tables 3 and 4</ref>).</p><p>These fundamental issues with computation and space complexity were revisited and resolved in followup work to this paper, where a new state space model (the Structured State Space) provided a new parameterization and algorithms for state spaces.</p><p>Conclusion and future work. Modern deep learning models struggle in applications with very long temporal data such as speech, videos, and medical time-series. We hope that our conceptual and technical contributions can lead to new capabilities with simple, principled, and less engineered models. We note that our pixel-level image classification experiments, which use no heuristics (batch norm, auxiliary losses) or extra information (data augmentation), perform similar to early convnet models with vastly more parameters, and is in the spirit of recent attempts at unifying data modalities with a generic sequence model <ref type="bibr" target="#b17">[18]</ref>. Our speech results demonstrate the possibility of learning better features than hand-crafted processing pipelines used widely in speech applications. We are excited about potential downstream applications, such as training other downstream models on top of pre-trained state space features.</p><p>[62] Huaguang Zhang, Zhanshan Wang, and Derong Liu. A comprehensive review of stability analysis of continuous-time recurrent neural networks. IEEE Transactions on Neural Networks and Learning Systems, 25 <ref type="formula" target="#formula_6">(7)</ref>:1229-1262, 2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>We provide an extended related work comparing the LSSL to previous recurrent, convolutional, and continuoustime models.</p><p>HiPPO The LSSL is most closely related to the HiPPO framework for continuous-time memory <ref type="bibr" target="#b23">[24]</ref> and its predecessor, the Legendre Memory Unit (LMU) <ref type="bibr" target="#b57">[58]</ref>. The HiPPO-RNN and the LMU define dynamics of the form of equation <ref type="formula">(1)</ref>, and incorporate it into an RNN architecture. A successor to the LMU, the LMU-FFT <ref type="bibr" target="#b11">[12]</ref> keeps the original linear dynamics, allowing the LMU to be computed with a cached convolution kernel. These methods all suffer from two main limitations. First, the state matrix A and discretization timescale ?t cannot be trained due to both limitations in theoretical understanding of which A matrices are effective, as well as computational limitations. Second, (1) is a 1-D to N -D map, requiring states to be projected back down to 1-D. This creates an overall 1-D bottleneck in the state, limiting the expressivity of the model.</p><p>Compared to these, the LSSL does not use a conventional RNN architecture, instead keeping the linear recurrence (4) and downprojecting it with the second part of the state space representation <ref type="bibr" target="#b4">(5)</ref>. To avoid the 1-D feature bottlneck, it simply computes H copies of this 1-D to 1-D independently, creating an overall H-dimensional sequence-to-sequence model. However, this exacerbates the computational issue, since the work is increased by a factor of H.</p><p>This work resolves the expressivity issue with new theory. Compared to HiPPO and the LMU, LSSL allows training the A matrix by showing generalized theoretical results for the HiPPO framework, showing that there is a parameterized class of structured state spaces that are HiPPO operators.</p><p>The LSSL makes progress towards the second issue with new algorithms for these structured matrices (Theorem 2). However, as noted in Sections 4.2 and 6, the algorithm presented in Theorem 2 was later found to be not practical, and an improved representation and algorithm was found in subsequent work.</p><p>Continuous-time CNNs. The CKConv is the only example of a continuous-time CNN that we are aware of, and is perhaps the strongest baseline in our experiments. Rather than storing a finite sequence of weights for a convolution kernel, the CKConv parameterizes it as an implicit function from [0, 1] ? R which allows sampling it at any resolution. A successor to the CKConv is the FlexConv <ref type="bibr" target="#b42">[43]</ref>, which learns convolutional kernels with a flexible width. This is similar to the convolution interpretation of LSSL when using certain HiPPO bases (Section 3.2).</p><p>Continuous-time RNNs. The connection from RNNs to continuous-time models have been known since their inception, and recent years have seen an explosion of CT-RNN (continuous-time RNN) models based on dynamical systems or ODEs. We briefly mention a few classic and modern works along these lines, categorizing them into a few main topics.</p><p>First are theoretical works that analyze the expressivity of RNNs from a continuous-time perspective. The connection between RNNs and dynamical systems has been studied since the 90s <ref type="bibr" target="#b21">[22]</ref>, fleshing out the correspondence between different dynamical systems and RNN architectures <ref type="bibr" target="#b37">[38]</ref>. Modern treatments have focused on analyzing the stability [62] and dynamics <ref type="bibr" target="#b28">[29]</ref> of RNNs.</p><p>Second, a large class of modern RNNs have been designed that aim to combat vanishing gradients from a dynamical systems analysis. These include include the AntisymmetricRNN <ref type="bibr" target="#b6">[7]</ref>, iRNN <ref type="bibr" target="#b29">[30]</ref>, and Lips-chitzRNN <ref type="bibr" target="#b19">[20]</ref>, which address the exploding/vanishing gradient problem by reparatermizing the architecture or recurrent matrix based on insights from an underlying dynamical system.</p><p>Third is a class of models that are based on an explicit underlying ODE introduced to satisfy various properties. This category includes the UnICORNN <ref type="bibr" target="#b46">[47]</ref> and its predecessor coRNN <ref type="bibr" target="#b45">[46]</ref> which discretize a second-order ODE inspired by oscillatory systems. Other models include the Liquid Time-Constant Networks (LTC) <ref type="bibr" target="#b26">[27]</ref> and successor CfC <ref type="bibr" target="#b25">[26]</ref>, which use underlying dynamical systems with varying time-constants with stable behavior and provable rates of expressivity measured by trajectory length. The LTC is based on earlier dynamic causal models (DCM) <ref type="bibr" target="#b20">[21]</ref>, which are a particular ODE related to state spaces with an extra bilinear term. Finally, the LMU <ref type="bibr" target="#b57">[58]</ref> and HiPPO <ref type="bibr" target="#b23">[24]</ref> also fall in this category, whose underlying ODEs are mathematically derived for continuous-time memorization.</p><p>Fourth, the recent family of neural ODEs <ref type="bibr" target="#b9">[10]</ref>, originally introduced as continuous-depth models, have been adapted to continuous-time, spawning a series of "ODE-RNN" models. Examples include the ODE-RNN <ref type="bibr" target="#b44">[45]</ref>, GRU-ODE-Bayes <ref type="bibr" target="#b15">[16]</ref>, and ODE-LSTM <ref type="bibr" target="#b31">[32]</ref>, which extend adjoint-based neural ODEs to the discrete input setting as an alternative to standard RNNs. Neural Controlled Differential Equations (NCDE) <ref type="bibr" target="#b30">[31]</ref> and Neural Rough Differential Equations (NRDE) <ref type="bibr" target="#b36">[37]</ref> are memory efficient versions that integrate observations more smoothly and can be extended to very long time series.</p><p>Gating mechanisms. As a special case of continuous-time RNNs, some works have observed the relation between gating mechanisms and damped dynamical systems <ref type="bibr" target="#b53">[54]</ref>. Some examples of continuous-time RNNs based on such damped dynamical systems include the LTC <ref type="bibr" target="#b26">[27]</ref> and iRNN <ref type="bibr" target="#b29">[30]</ref>. Compared to these, Lemma 3.1 shows a stronger result that sigmoid gates are not just motivated by being an arbitrary monotonic function with range (0, 1), but the exact formula appears out of discretizing a damped ODE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 (M)LSSL Computation</head><p>Section 3.1 noted that some of the computations for using the LSSL are expensive to compute. When the LSSL fixes the A and ?t parameters (e.g. when they are not trained, or at inference time), these computational difficulties can be circumvented by caching particular computations. In particular, this case applies to the LSSL-f. Note that in this case, the other state-space matrices C and D comprise the O(HN ) trainable parameters of the fixed-transition LSSL.</p><p>In particular, we assume that there is a black-box inference algorithm for this system, i.e. matrix-vector multiplication by A (an example of implementing this black box for a particular structured class is in Appendix E.2). We then compute and cache ? the transition matrix A, which is computed by applying the black-box A MVM algorithm to the identity matrix I.</p><formula xml:id="formula_9">? the Krylov matrix K(A, B) = (B, AB, (A) 2 B, ...) ? R N ?L ,<label>(8)</label></formula><p>which is computed in a parallelized manner by the squaring technique for exponentiation, i.e. batch multiply by A, (A) 2 , (A) 4 , . . ..</p><p>At inference time, the model can be unrolled recurrently with A. At training time, the convolutional filter K L (A, B, C) (equation <ref type="formula" target="#formula_6">(7)</ref>) is computed with a matrix multiplication C ? K(A, B) before convolving with the input u. <ref type="table" target="#tab_6">Table 7</ref> provides more detailed complexity of this version of the LSSL with fixed A, ?t.</p><p>Note that as mentioned in Section 6, this cached algorithm is fairly fast, but the main drawback is that materializing the Krylov matrix <ref type="bibr" target="#b7">(8)</ref> </p><formula xml:id="formula_10">requires O(N L) instead of O(L) space.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Initialization of A</head><p>The LSSL initializes the A parameter in (1) to the HiPPO-LegS operator, which was derived to solve a particular continuous-time memorization problem. This matrix A ? R N ?N is</p><formula xml:id="formula_11">A nk = ? ? ? ? ? (2n + 1) 1/2 (2k + 1) 1/2 if n &gt; k n + 1 if n = k 0 if n &lt; k .</formula><p>Note that the LSSL-f is the LSSL with a non-trainable A (and ?t), so that A is fixed to the above matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Initialization of ?t</head><p>One distinction between the LSSL and the most related prior work is that the inclusion of the projection (2) makes the layer a 1-dimensional to 1-dimensional map, instead of 1-D to N -D <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b57">58]</ref>. This enables us to concatenate H copies of this map (at the expense of computation, cf. Section 4.2 and Appendix D). Even when ?t is not trained as in the LSSL-f, these H copies allow multiple timescales to be considered by setting ?t differently for each copy.</p><p>In particular, we initialize ?t log-uniformly in a range ?t min , ?t max (i.e., ?t is initialized within this range, such that log ?t is uniformly distributed). The maximum and minimum values were generally chosen to be a factor of 100 apart such that the length of the sequences in the dataset are contained in this range. Specific values for each model and dataset are in Appendix F. We did not search over these as a hyperparameter, but we note that it can be tuned for additional performance improvements in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Deep Neural Network Architecture</head><p>The Deep LSSL models used in our experiments simply stack together LSSL layers in a simple deep neural network architecture. We note the following architecture details.</p><p>Channels. The state-space model (1)+(2) accepts a 1-dimensional input u, but does not strictly have to return a 1-dimensional output y. By making the matrices in <ref type="formula" target="#formula_0">(2)</ref> </p><formula xml:id="formula_12">dimension C ? R M ?N , D ? R M ?1 , the output y will be dimension M instead of 1.</formula><p>We call M the number of channels in the model.</p><p>Feedforward. There are two drawbacks with the current definition of LSSL:</p><p>? They are defined by running H independent copies of a state-space model, which means the H input features do not interact at all.</p><p>? If the channel dimension is M &gt; 1, then the LSSL is a map from dimension 1 to M , which means residuals cannot be applied.</p><p>These are both addressed by introducing a position-wise feedforward layer after the LSSL of shape H ? M ? H. This simultaneously mixes the hidden features, and projects the output back to dimension 1 if necessary. There is also an optional non-linearity in between the LSSL and this feedforward projection; we fix it to the GeLU activation function in our models. We note that this factorization of parallel convolutions on the H features followed by a position-wise linear map is very similar to depth-wise separable convolutions <ref type="bibr" target="#b12">[13]</ref>.</p><p>Residuals and normalization. To stack multiple layers of LSSLs together, we use very standard architectures for deep neural networks. In particular, we use residual connections and a layer normalization (either pre-norm or post-norm) in the style of standard Transformer architectures. Whether to use pre-norm or post-norm was chosen on a per-dataset basis, and depended on whether the model overfit; recent results have shown that pre-norm architectures are more stable <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>, so we used it on harder datasets with less overfitting. We note that we could have additionally inserted MLP modules in between LSSL layers, in the style of Transformers <ref type="bibr" target="#b56">[57]</ref>, but did not experiment with this.</p><p>Parameter count. The overall parameter count of an LSSL model is M ? H ? (H + N ).</p><p>We primarily used two model sizes in our experiments, which were chosen simply to produce round numbers of parameters: We did not search over additional sizes, but for some datasets reduced the model size for computational reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C LSSL Proofs</head><p>This section gives refinements of the statements in Section 3, additional results, and proofs of all results.</p><p>Appendix C.1 has a more detailed (and self-contained) summary of basic methods in ODE approximation which will be used in the results and proofs.</p><p>Appendix C.2 give more general statements and proofs of Lemma 3.1 and Lemma 3.2 in Lemma C.1 and Theorem 4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Approximations of ODEs</head><p>We consider the standard setting of a first-order initial value problem (IVP) ordinary differential equation</p><formula xml:id="formula_13">(ODE) for a continuous function f (t, x)? (t) = f (t, x(t)) x(t 0 ) = x 0 .<label>(9)</label></formula><p>This differential form has an equivalent integral form</p><formula xml:id="formula_14">x(t) = x 0 + t t0 f (s, x(s)) ds.<label>(10)</label></formula><p>Appendices C.1.1 and C.1.2 overview the Picard theorem and first-order numerical integration methods, which apply to any IVP (9). Appendix C.1.3 then shows how to specialize it to linear systems as in equation <ref type="formula">(1)</ref>.</p><p>At a high level, the basic approximation methods considered here use the integral form (10) and approximate the integral in the right-hand side by simple techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 Picard Iteration</head><p>The Picard-Lindel?f Theorem gives sufficient conditions for the existence and uniqueness of solutions to an IVP. As part of the proof, it provides an iteration scheme to compute this solution.</p><p>Theorem 3 (Picard-Lindel?f). In the IVP (9), if there is an interval around t 0 such that f is Lipschitz in its second argument, then there is an open interval I t 0 such that there exists a unique solution x(t) to the IVP in I. Furthermore, the sequence of Picard iterates x (0) , x <ref type="bibr" target="#b0">(1)</ref> , . . . defined by</p><formula xml:id="formula_15">x (0) (t) = x 0 x ( ) (t) = x 0 + t t0 f (s, x ( ?1) (s)) ds converges to x.</formula><p>The Picard iteration can be viewed as approximating (10) by holding the previous estimate of the solution x ( ?1) fixed inside the RHS integral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 Numerical Integration Methods</head><p>Many methods for numerical integration of ODEs exist, which calculate discrete-time approximations of the solution. We discuss a few of the simplest methods, which are first-order methods with local error O(h 2 ) <ref type="bibr" target="#b5">[6]</ref>.</p><p>These methods start by discretizing (10) into the form</p><formula xml:id="formula_16">x(t k ) ? x(t k?1 ) = t k t k?1 f (s, x(s)) ds.<label>(11)</label></formula><p>Here we assume a sequence of discrete times t 0 , t 1 , t 2 , . . . is fixed. For convenience, let x k denote x(t k ) and let ?t k := t k ? t k?1 . The goal is now to approximate the integral in the RHS of <ref type="bibr" target="#b10">(11)</ref>.</p><p>Euler method. The Euler method approximates (11) by holding the left endpoint constant throughout the integral (i.e., the "rectangle rule" with left endpoint), f (s, x(s)) ? f (t k?1 , x(t k?1 )). The discrete-time update becomes</p><formula xml:id="formula_17">x k ? x k?1 = (t k ? t k?1 )f (t k?1 , x(t k?1 )) = ?t k f (t k?1 , x k?1 ).<label>(12)</label></formula><p>Backward Euler method. The backward Euler method approximates (11) by holding the right endpoint constant throughout the integral (i.e., the "rectangle rule" with right endpoint), f (s, x(s)) ? f (t k , x(t k )). The discrete-time update becomes</p><formula xml:id="formula_18">x k ? x k?1 = (t k ? t k?1 )f (t k , x(t k )) = ?t k f (t k , x k ).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.3 Discretization of State-Space Models</head><p>In the case of a linear system, the IVP is specialized to the case</p><formula xml:id="formula_19">f (t, x(t)) = Ax(t) + Bu(t).</formula><p>Note that here u is treated as a fixed external input, which is constant from the point of view of this ODE in x. Let u k denote the average value in each discrete time interval,</p><formula xml:id="formula_20">u k = 1 ?t k t k t k?1 u(s) ds.</formula><p>The integral equation <ref type="formula" target="#formula_16">(11)</ref> can be specialized to this case, and more generally a convex combination of the left and right endpoints can be taken to approximate the integral, weighing them by 1 ? ? and ? respectively. Note that the case ? = 0, 1 are specializations of the forward and backward Euler method, and the case ? = 1 2 is the classic "trapezoid rule" for numerical integration.</p><formula xml:id="formula_21">x(t k ) ? x(t k?1 ) = t k t k?1 Ax(s) ds + t k t k?1 Bu(s) ds = t k t k?1 Ax(s) ds + ?t k Bu k ? ?t k [(1 ? ?)Ax k?1 + ?Ax k ] + ?t k Bu k .</formula><p>Rearranging yields</p><formula xml:id="formula_22">(I ? ??t k ? A)x k = (I + (1 ? ?)?t k ? A)x k?1 + ?t k ? Bu k x k = (I ? ??t k ? A) ?1 (I + (1 ? ?)?t k ? A)x k?1 + (I ? ??t k ? A) ?1 ?t k ? Bu k</formula><p>This derives the generalized bilinear transform (GBT) <ref type="bibr" target="#b60">[61]</ref>. The bilinear method is the case ? = 1 2 of special significance, and was numerically found to be better than the forward and backward Euler methods ? = 0, 1 both in synthetic function approximation settings and in end-to-end experiments [24, <ref type="figure">Figure  4</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 RNNs are LSSLs: Proof of Results in Section 3.2</head><p>We provide more detailed statements of Lemmas 3.1 and 3.2 from Section 3.2. In summary, LSSLs and popular families of RNN methods all approximate the same continuous-time dynamic?</p><formula xml:id="formula_23">x(t) = ?x + f (t, x(t))<label>(14)</label></formula><p>by viewing them with a combination of two techniques. We note that these results are about two of the most commonly used architecture modifications for RNNs. First, the gating mechanism is ubiquitous in RNNs, and usually thought of as a heuristic for smoothing <ref type="table">Table 6</ref>: A summary of the characteristics of popular RNN methods and their approximation mechanisms for capturing the dynamics?(t) = ?x(t) + f (t, x(t)) (equation <ref type="formula" target="#formula_23">(14)</ref>). The LSSL entries are for the very specific case with order N = 1 and A = ?1, B = 1, C = 1, D = 0; LSSLs are more general. optimization <ref type="bibr" target="#b27">[28]</ref>. Second, many of the effective large-scale RNNs use linear (gated) recurrences and deeper models, which is usually thought of as a heuristic for computational efficiency <ref type="bibr" target="#b4">[5]</ref>. Our results suggest that neither of these are heuristics after all, and arise from standard ways to approximate ODEs.</p><p>To be more specific, we show that:</p><p>? Non-linear RNNs discretize the dynamics <ref type="formula" target="#formula_23">(14)</ref> by applying backwards Euler discretization to the linear term, which arises in the gating mechanism of RNNs (Appendix C.2.2, Lemma C.1).</p><p>? A special case of LSSLs approximates the dynamics <ref type="formula" target="#formula_23">(14)</ref>  A comparison is summarized in <ref type="table">Table 6</ref>.</p><p>In the remainder of this section, we assume that there is an underlying function x(t) that satisfies (14) on some interval for any initial condition, and that f is continuous and Lipschitz in its second argument. Our goal is to show that several families of models approximate this in various ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Intuition / Proof Sketches</head><p>We sketch the idea of how LSSLs capture popular RNNs. More precisely, we will show how approximating the dynamics <ref type="bibr" target="#b13">(14)</ref> in various ways lead to types of RNNs and LSSLs.</p><p>The first step is to look at the simpler dynamic?</p><formula xml:id="formula_24">x(t) = ?x(t) + u(t)</formula><p>where there is some input u(t) that is independent of x. (In other words, in <ref type="formula" target="#formula_23">(14)</ref>, the function f (t, x) does not depend on the second argument.) By directing applying the GBT discretization with ? = 1, this leads to a gated recurrence (Lemma 3.1). The second step is that by applying the backwards Euler discretization more directly to <ref type="bibr" target="#b13">(14)</ref>, this leads to a gated RNN where the input can depend on the state (Lemma C.1).</p><p>Alternatively, we can apply Picard iteration on <ref type="bibr" target="#b13">(14)</ref>, which says that the iteration</p><formula xml:id="formula_25">x ( ) (t) = x 0 + t t0 ?x ( ?1) (s) ds + t t0 f (s, x ( ?1) (s)) ds</formula><p>converges to the solution x(t).</p><p>However, the first integral term is simple and can be tightened. We can instead try to apply Picard iteration on only the second term, leaving the first integral in terms of x ( ) . Intuitively this should still converge to the right solution, since this is a weaker iteration; we're only using the Picard approximation on the second term.</p><formula xml:id="formula_26">x ( ) (t) = x 0 + t t0 ?x ( ) (s) ds + t t0 f (s, x ( ?1) (s)) ds</formula><p>Differentiating, this equation is the OD?</p><formula xml:id="formula_27">x ( ) (t) = ?x ( ) (t) + f (t, x ( ?1) (t))</formula><p>This implies that alternating point-wise functions with a simple linear ODE? ( ) (t) = ?x ( ) (t) + u ( ) (t) also captures the dynamics <ref type="bibr" target="#b13">(14)</ref>. But this is essentially what an LSSL is.</p><p>To move to discrete-time, this continuous-time layer can be discretized with gates as in Lemma 3.1, leading to deep linear RNNs such as the QRNN, or with the bilinear discretization, leading to the discrete-time LSSL. We note again that in the discrete-time LSSL, A and B play the role of the gates ?, 1 ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 Capturing gates through discretization</head><p>Lemma C.1. Consider an RNN of the form</p><formula xml:id="formula_28">x k = (1 ? ?(z k ))x k?1 + ?(z k )f (k, x k?1 ),<label>(15)</label></formula><p>where f (k, x) is an arbitrary function that is Lipschitz in its second argument (e.g., it may depend on an external input u k ). Then equation <ref type="formula" target="#formula_2">(15)</ref> is a discretization of the dynamics (14) with step sizes</p><formula xml:id="formula_29">?t k = exp(z k ), i.e. x k ? x(t k ) where t k = k i=1 ?t i . Proof.</formula><p>Apply the backwards Euler discretization <ref type="bibr" target="#b12">(13)</ref> to equation <ref type="bibr" target="#b13">(14)</ref> to get</p><formula xml:id="formula_30">x k ? x k?1 = ?t k [?x k + f (t k , x k )] (1 + ?t k )x k = x k?1 + ?t k f (t k , x k ) x k = 1 1 + ?t k x k?1 + ?t k 1 + ?t k f (t k , x k ).</formula><p>Note that ?t k 1+?t k = e z k 1+e z k = 1 1+e ?z k and 1 1+?t k = 1 ? ?t k 1+?t k , thus</p><formula xml:id="formula_31">x k = (1 ? ?(z k ))x k?1 + ?(z k )f (k, x k?1 ).</formula><p>Here we are denoting f (k, x) = f (t k , x) to be a discrete-time version of f evaluatable at the given timesteps t k .</p><p>Note that a potential external input function u(t) or sequence u k is captured through the abstraction f (t, x). For example, a basic RNN could define f (k, x) = f (t k , x) = tanh(W x + U u k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.3 Capturing non-linearities through Picard iteration</head><p>The main result of this section is Theorem 4 showing that LSSLs can approximate the same dynamics as the RNNs in the previous section. This follows from a technical lemma.</p><p>Lemma C.2. Let f (t, x) be any function that satisfies the conditions of the Picard-Lindel?f Theorem (Theorem 3).</p><p>Define a sequence of functions x ( ) by alternating the (point-wise) function f with solving an ODE</p><formula xml:id="formula_32">x (0) (t) = x 0 u ( ) (t) = f (t, x ( ?1) (t))</formula><p>x ( ) (t) = Ax ( ) (t) + u ( ) (t).</p><p>Then x ( ) converges to a solution x ( ) (t) ? x(t) of the IV?</p><formula xml:id="formula_33">x(t) = Ax(t) + f (t, x(t)) x(t 0 ) = x 0 .</formula><p>Theorem 4. A (continuous-time) deep LSSL with order N = 1 and A = ?1, B = 1, C = 1, D = 0 approximates the non-linear dynamics <ref type="bibr" target="#b13">(14)</ref>.</p><p>Proof. Applying the definition of an LSSL (equations (1)+(2)) with these parameters results in a layer mapping u(t) ? y(t) where y is defined implicitly through the OD?</p><formula xml:id="formula_34">y(t) = ?y(t) + u(t).</formula><p>This can be seen since the choice of C, D implies y(t) = x(t) and the choice of A, B gives the above equation.</p><p>Consider the deep LSSL defined by alternating this LSSL with position-wise (in time) non-linear functions</p><formula xml:id="formula_35">u ( ) (t) = f (t, y ( ?1) (t)) y ( ) (t) = ?y ( ) (t) + u ( ) (t).</formula><p>But this is exactly a special case of Lemma C.2, so that we know y ( ) (t) ? y(t) such that y(t) satisfie?</p><formula xml:id="formula_36">y(t) = ?y(t) + f (t, y(t))</formula><p>as desired.</p><p>Proof of Lemma C. = e ?At f (t, e At z(t)).</p><p>Since f satisfies the conditions of the Picard Theorem (i.e., is continuous in the first argument and Lipschitz in the second), so does the function g where g(t, x) := e ?At f (t, e At x) for some interval around the initial time. By Theorem 3, the iterates z ( ) defined by</p><formula xml:id="formula_37">z ( ) (t) = z 0 + t t0</formula><p>e ?As f (s, e As z ( ?1) (s)) ds</p><formula xml:id="formula_38">converges to z. Define x ( ) (t) = e At z ( ) (t). Differentiate (16) to ge? z ( ) (t) = e ?At f (t, e At z ( ?1) (t)) = e ?At f (t, x ( ?1) (t)) = e ?At u ( ) (t). But? ( ) (t) = e ?At ? ( ) (t) ? Ax ( ) (t) , so? ( ) (t) = Ax ( ) (t) + u ( ) (t).<label>(16)</label></formula><p>Since z ( ) ? z and x ( ) (t) = e At z ( ) (t) and x(t) = e At z(t), we have x ( ) ? x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.4 Capturing Deep, Linear, Gated RNNs</head><p>We finally note that several types of RNNs exist which were originally motivated by approximating linearizing gated RNNs for speed. Although these were treated as a heuristic for efficiency reasons, they are explained by combining our two main technical results. Lemma C.1 shows that a single-layer, discrete-time, non-linear RNN approximates the dynamics (14) through discretization, which arises in the gating mechanism.</p><p>Theorem 4 shows that a deep, continuous-time, linear RNN approximates <ref type="bibr" target="#b13">(14)</ref> through Picard iteration, where the non-linearity is moved to the depth direction.</p><p>Combining these two results leads to Corollary C.3, which says that a deep, discrete-time, linear RNN can also approximate the same dynamics <ref type="bibr" target="#b13">(14)</ref>. </p><formula xml:id="formula_39">k = (1 ? ?(z k ))x ( ) k?1 + ?(z k )u ( ) k u ( ) k = f (k, x ( ?1) k ).</formula><p>This is a discretization of the dynamics (14) with step sizes</p><formula xml:id="formula_40">?t k = exp(z k ), i.e. x k ? x(t k ) where t k = k i=1 ?t i .</formula><p>Proof. By Lemma C.1, the first equation is a discretization of the continuous-time equatio?</p><formula xml:id="formula_41">x ( ) (t) = ?x ( ) (t) + u ( ) (t) where u ( ) (t) = f (t, x ( ?1) (t))</formula><p>uses the continuous-time version f of f . But by Lemma C.2, this is an approximation of the dynamics <ref type="bibr" target="#b13">(14)</ref> using Picard iteration.</p><p>Notable examples of this type of model include the Quasi-RNN or QRNN <ref type="bibr" target="#b4">[5]</ref> and the Simple Recurrent Unit (SRU) <ref type="bibr" target="#b32">[33]</ref>, which are among the most effective models in practice. We remark that these are the closest models to the LSSL and suggest that their efficacy is a consequence of the results of this section, which shows that they are not heuristics.</p><p>We note that there are many more RNN variants that use a combination of these gating and linearization techniques that were not mentioned in this section, and can be explained similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D LSSL Proofs and Algorithms</head><p>This section proves the results in Section 4.1, and is organized as follows:</p><p>? Appendix D.1 gives a self-contained synopsis of the HiPPO framework <ref type="bibr" target="#b23">[24]</ref>.</p><p>? Appendix D.2 proves Theorem 1, which shows that the hippo operators for any measure lead to a simple linear ODE of the form of equation <ref type="formula">(1)</ref>. Notation This section is technically involved and we adopt notation to simplify reasoning about the shapes of objects. In particular, we use bold capitals (e.g. A) to denote matrices and bold lowercase (e.g. b) to denote vectors. For example, equation (1) becomes? = Ax + bu. These conventions are adopted throughout Appendices D and E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Preliminaries: HiPPO Framework and Recurrence Width</head><p>This section summarizes technical preliminaries taken directly from prior work. We include this section so that this work is self-contained and uses consistent notation, which may deviate from prior work. For example, we use modified notation from Gu et al. <ref type="bibr" target="#b23">[24]</ref> in order to follow conventions in control theory (e.g., we denote input by u and state by x as in <ref type="formula">(1)</ref>). Appendix D.1.1 formally defines the HiPPO operator mathematically as in [24, Section 2.2], and Appendix D.1.2 overviews the steps to derive the HiPPO operator as in [24, Appendix C]. Appendix D.1.3 defines the class of Low Recurrence Width (LRW) matrices, which is the class of matrices that our generalization of the HiPPO results (Theorem 1) uses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.1 Definition of HiPPO Operator</head><p>Definition 1 ([24], Definition 1). Given a time-varying measure ? (t) supported on (??, t], an N-dimensional subspace G of polynomials, and a continuous function u : R ?0 ? R, HiPPO defines a projection operator proj t and a coefficient extraction operator coef t at every time t, with the following properties:</p><p>1. proj t takes a function u restricted up to time t, u ?t := u(x)| x?t , and maps it to a polynomial g (t) ? G, that minimizes the approximation error u ?t ? g (t) L2(? (t) ) .</p><p>2. coef t : G ? R N maps the polynomial g (t) to the coefficients c(t) ? R N of the basis of orthogonal polynomials defined with respect to the measure ? (t) .</p><p>The composition coef t ? proj t is called hippo, which is an operator mapping a function u : R ?0 ? R to the optimal projection coefficients c : R ?0 ? R N (i.e (hippo(u))(t) = coef t (proj t (f )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.2 HiPPO Framework for Deriving the HiPPO Operator</head><p>The main ingredients of HiPPO consists of an approximation measure and an orthogonal polynomial basis. We recall how they are defined in <ref type="bibr" target="#b23">[24]</ref> (we note that compared to Gu et al. <ref type="bibr" target="#b23">[24]</ref>, our notation has changed from input f (t) coefficients (state) c(t) to input u(t) and coefficients (state) x(t), following conventions in controls).</p><p>Approximation Measures At every t, the approximation quality is defined with respect to a measure ? (t) supported on (??, t]. We assume that the measures ? (t) have densities ?(t, Y ) := d? <ref type="bibr">(t)</ref> dY . Note that this implies that integrating with respect to d? (t) is the same as integrating with respect to ?(t, Y ) dY .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orthogonal Polynomial basis Let {P</head><p>(t) n } n?N denote a sequence of orthogonal polynomials with respect to some time-varying measure ? (t) . Let p (t) n be the normalized version of of orthogonal P (t) n , and define p n (t, Y ) = p (t) n (Y ). In particular, the above implies that</p><formula xml:id="formula_42">t ?? p (t) n (Y ) ? p (t) m (Y )?(t, Y ) dY = ? m,n .</formula><p>In the general framework, HiPPO does not require an orthogonal polynomial basis as the selected basis. The choice of basis is generalized by tilting with ?.</p><p>Tilted measure and basis For any scaling function ?(t, Y ), the functions p n (t, Y )?(t, Y ) are orthogonal with respect to the density ? ? 2 at every time t. Define ?(t) to be the normalized measure with density proportional to ? ? 2 , with normalization constant ?(t) = t 0</p><formula xml:id="formula_43">?(t,Y )</formula><p>?(t,Y ) 2 dx. We express the coefficients x n (t) calculated by the HiPPO framework as:</p><formula xml:id="formula_44">x n (t) = 1 ?(t) t 0 u(Y )p n (t, Y ) ?(t, Y ) ?(t, Y ) dY.<label>(17)</label></formula><p>To use this to derive? n (t), let h(t, Y ) = u(Y )p n (t, Y )?(t, Y ). We see tha?</p><formula xml:id="formula_45">x n (t) = d dt t 0 u(Y )p n (t, Y )?(t, Y )dY = d dt t 0 h(t, Y )dY = t 0 ? ?t h(t, Y )dY + h(t, t) = t 0 u(Y ) ? ?t p n (t, Y ) ? (t, Y ) dY + t 0 f (Y )p n (t, Y ) ? ?t ? (t, Y ) dY + u(t)p n (t, t)?(t, t).</formula><p>This allows? n (t) to be written a?</p><formula xml:id="formula_46">x n (t) = u(t)p n (t, t)?(t, t) + t 0 u(Y ) ? ?t p n (t, Y ) ? (t, Y ) dY + t 0 f (Y )p n (t, Y ) ? ?t ? (t, Y ) dY.<label>(18)</label></formula><p>Although Gu et al. <ref type="bibr" target="#b23">[24]</ref> describe the framework in the full generality above and use ? as another degree of freedom, in their concrete derivations they always fix ? = ?. Our general results also use this setting. For the remainder of this section, we assume the "full tilting" case ? = ?. In particular, this means that in Eq. <ref type="formula" target="#formula_9">(18)</ref>, we essentially substitute ? above with 1 and divide each term by the inverse square root of our normalization constant, ?, to get the coefficient dynamics that we will use in our arguments:</p><formula xml:id="formula_47">x n (t) = 1 ?(t) u(t)p n (t, t) + 1 ?(t) t 0 u(Y ) ? ?t p n (t, Y ) dY<label>(19)</label></formula><p>Now, if we can show that each of the integrated terms in <ref type="bibr" target="#b17">(18)</ref> are linear combinations of x n (t), this would be the same as saying that? n (t) = A(t)x(t) + b(t)u(t) for some A(t). Therefore, the incremental update operation would be bounded by the runtime of the matrix-vector operation A(t)x(t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.3 Recurrence Width</head><p>Our final goal is to show that? n (t) = A(t)x(t) + b(t)u(t) for some A(t) with constant recurrence width (see Definition 2). This will show Theorem 1, and also imply that the MVM A(t)x(t) can be computed in?(N ) time. To build this argument, we borrow the fact that OPs all have recurrence width 2 and results regarding matrix-vector multiplication of matrices with constant recurrence width along with their inverses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 ([17]</head><p>). An N ? N matrix A has recurrence width t if the polynomials a i (X) =</p><formula xml:id="formula_48">N ?1 j=0 A[i, j]X j satisfy deg(a i ) ? i for i &lt; t, and a i (X) = t j=1 g i,j (X)a i?j (X) for i ? t,</formula><p>where the polynomials g i,j ? R[X] have degree at most j. For the rest of the note we'll assume that any operation over R can be done in constant time. It would be useful for us to define P ? R N ?N such that the coefficients of the OP p i (X), i.e. p i (X) = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.1 Relating Orthogonal Polynomials and Recurrence Width</head><p>Next we introduce the following lemma, which will be useful in our arguments:</p><p>Lemma D.1. For any n, there exists ordered sets of coefficients ? n = {? n,i }, ? n = {? n,i },</p><formula xml:id="formula_49">(i) p n (Z) = n?1 i=0 ? n,i p i (Z) (ii) Zp n (Z) = n?1 i=0 ? n,i p i (Z) Proof.</formula><p>Follows from the fact that p i (z) for 0 ? i &lt; N forms a basis and the observation of the degrees of the polynomials on the LHS.</p><p>The following matrices will aid in showing verifying that matrix vector multiplication with a given matrix A can be computed in O(? ) time.</p><formula xml:id="formula_50">Definition 3. D 1 , D 2 ? R N ?N are the matrices such that p i (Z) = N ?1 j=0 D 1 [i, j]Z j , and Zp i (Z) = N ?1 j=0 D 2 [i, j]Z j .</formula><p>Let S be the "right shift" matrix, i.e. for any matrix M, MS has the columns of M shifted to right by one. Note that S T corresponds to the "left shift" matrix.</p><p>We now note that:</p><p>Lemma D.2. D 1 = P ? diag(0, 1, . . . , N ? 1) ? S T and D 2 = P ? diag(0, 1, . . . , N ? 1). In particular, D 1 z and D 2 z can be computed in?(N ) time for any z ? R N .</p><p>Proof. Recall that P has the coefficients of the OP polynomials p 0 (Z), . . . , p N ?1 (Z) as its rows. Then note that</p><formula xml:id="formula_51">p n (Z) = n?1 i=0 i ? P[n, i] ? Z i?1 .<label>(20)</label></formula><p>The claim on D 1 = P ? diag(0, 1, . . . , N ? 1) ? S T follows from the above. Recall that D has recurrence width of 2. The claim on the runtime of computing D 1 z then follows from Theorem 5 and the fact that both diag(0, 1, . . . , N ? 1) and S T is n-sparse. From Eq. <ref type="bibr" target="#b19">(20)</ref>, it is easy to see that</p><formula xml:id="formula_52">Zp n (Z) = n?1 i=0 i ? P[n, i] ? Z i .</formula><p>The claim on the structure of D 2 then follows from the above expression. The claim on runtime of computing D 2 z follows from essentially the same argument as for D 1 z.</p><p>Finally, we make the following observation:</p><p>Lemma D.3. Let A and B be defined such that A [n, i] = ? n,i and B [n, i] = ? n,i . Then both A and B are both products of three matrices: two of which have recurrence width at most 2 and the third is the inverse of a matrix that has recurrence width 2.</p><p>Proof. We note that since P expresses the orthogonal polynomials in standard basis, P ?1 changes from OP basis to standard basis. This along with Lemma D.2 implies that A = P ? diag(0, 1, . . . , N ? 1) ? S T ? P ?1 .</p><p>It is easy to check that diag(0, 1, . . . , N ? 1) ? S T has recurrence width 1 and the claim on A follows since P has recurrence width 2. A similar argument proves the claim on B .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2 HiPPO for General Measures</head><p>Let ? : R ?0 ? R ?0 be a function such that for all t, ?(t) ? t and ?(t) is differentiable.</p><p>In what follows, define</p><formula xml:id="formula_53">z = 2(Y ? t) ?(t) + 1.</formula><p>We note that dz dY = 2 ?(t) .</p><p>Further, note that:</p><formula xml:id="formula_55">dz dt = d dt 2(Y ? t) ?(t) + 1 = ? 2 ?(t) ? 2(Y ? t)? (t) ? 2 (t) = ? 2 ? 2 (t) (?(t) + (Y ? t)? (t))</formula><p>.</p><p>From the definition of z, we see that</p><formula xml:id="formula_56">Y ? t = (z?1)?(t) 2</formula><p>.</p><p>Then</p><formula xml:id="formula_57">dz dt = ? 2 ?(t) 1 + z ? 1 2 ? (t) = ? 2 ?(t) ? (z ? 1)? (t) ?(t) .<label>(22)</label></formula><p>Additionally, given a measure ? on [-1,1] and OP family p 0 (Y ), p 1 (Y ), . . . such that for all i = j,</p><formula xml:id="formula_58">1 ?1 p i (Y )p j (Y )?(Y )dY = ? i,j ,</formula><p>define ?(Y, t) = 2 ?(t) ?(z) and p n (Y, t) = p n (z).</p><p>Then we can adjust <ref type="bibr" target="#b16">(17)</ref> to:</p><formula xml:id="formula_59">x n (t) = t t??(t) u(Y )p n (z) 2 ?(t) dY.<label>(23)</label></formula><p>The Leibniz integral rule states that ? ?t</p><formula xml:id="formula_60">?(t) ?(t) h(t, Y )dY = ?(t) ?(t) ? ?t h(t, Y )dY ? ? (t)h(?(t), t) + ? (t)h(t, t).</formula><p>If we let ?(t) = t ? ?(t) and ?(t) = t, then applying the Leibniz rule to (23) we get:</p><formula xml:id="formula_61">x n (t) = t t??(t) u(Y ) ? ?t p n (z) 2 ?(t) dY ? (1 ? ? (t))u(t ? ?(t))p n (t ? ?(t), t) 2 ?(t) + u(t)p n (t, t) 2 ?(t) = ?(1 ? ? (t))u(t ? ?(t))p n (?1) 2 ?(t) + u(t)p n (1) 2 ?(t) + t t??(t) u(Y ) dz dt p n (z) 2 ?(t) dY ? ? (t) ?(t) t t??(t) u(Y )p n (z) 2 ?(t) dY.</formula><p>From <ref type="formula" target="#formula_0">(22)</ref>, it follows tha?</p><formula xml:id="formula_62">x n (t) = ? 2(1 ? ? (t))u(t ? ?(t))p n (?1) ?(t) + 2 ? u(t)p n (1) ?(t) ? 2 ?(t) t t??(t) u(Y )p n (z) 2 ?(t) dY ? ? (t) ?(t) t t??(t) u(Y )(z ? 1)p n (z) 2 ?(t) dY ? ? (t) ?(t) t t??(t) u(Y )p n (z) 2 ?(t) dY.<label>(24)</label></formula><p>Because deg (p n (z)) ? n ? 1 and deg ((z ? 1)p n (z)) ? n, they can be written as a linear combination of {p i } i?n . Let us define {? n,j }, {? n,j } such that p n (z) = n?1 j=0 ? n,j p j (z) and (z ? 1)p n (z) = n j=0 ? n,j p j (z).</p><p>Then by using <ref type="bibr" target="#b24">(25)</ref> in <ref type="formula" target="#formula_0">(24)</ref>, we get:</p><formula xml:id="formula_64">x n (t) = ? 2(1 ? ? (t))u(t + ?(t))p n (?1) ?(t) + 2 ? u(t)p n (1) ?(t) ? 2 ?(t) n?1 j=0 ? n,j t t??(t) u(Y )p j (z) 2 ?(t) dY ? ? (t) ?(t) n j=0 ? n,j t t??(t) u(Y )p j (z) 2 ?(t) dY ? ? (t) ?(t) t t??(t) u(Y )p n (z) 2 ?(t) dY = ? 2(1 ? ? (t))u(t + ?(t))p n (?1) ?(t) + 2 ? u(t)p n (1) ?(t) ? 2 ?(t) n?1 j=0 ? n,j x j (t) ? ? (t) ?(t) n j=0 ? n,j x j (t) ? ? (t) ?(t) x n (t).</formula><p>Thus, in vector form we get</p><formula xml:id="formula_65">Theorem 7.? n (t) = ? 1 ?(t) A 1 (t)x(t) ? 2 ?(t) (1 ? ? (t))u(t ? ?(t)) ? ? ? ? . . . p n (?1) . . . ? ? ? ? + 2 ?(t) u(t) ? ? ? ? . . . p n (1) . . . ? ? ? ? where A 1 (t)[n, k] = ? ? ? ? ? 2? n,k + ? (t)? n,k if k &lt; n ? (t)? n,n + ? (t) if k = n 0 otherwise</formula><p>for ? n,k ,? n,k as defined in <ref type="bibr" target="#b24">(25)</ref>.</p><p>Corollary D.4. The matrix A 1 in Theorem 7 can be re-written as</p><formula xml:id="formula_66">A 1 = 2 ? A + ? (t) ? B + ? (t) ? I.<label>(26)</label></formula><p>In particular, both A and B both products of three matrices: two of which have recurrence width at most 2 and the third is the inverse of a matrix that has recurrence width 2.</p><p>Proof. Eq. (26) follows from Theorem 7 and defining A and B to contain the ? n,k and ? n,k coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.3 Translated HiPPO (Sliding Windows)</head><p>The case when ?(t) = ? for all t represents a constant-size sliding window, which Gu et al. <ref type="bibr" target="#b23">[24]</ref> denote as the "Translated HiPPO" case with instantiations such as HiPPO-LegT (Translated Legendre) and HiPPO-LagT (Translated Laguerre).</p><p>We now state a corollary of Theorem 7 for the case of ?(t) = ? for all t.</p><p>Corollary D.5. Let ?(t) = ? for all t. The?</p><formula xml:id="formula_67">x n (t) = ? 1 ? A 1 x(t) ? 2 ? u(t ? ?) ? ? ? ? . . . p n (?1) . . . ? ? ? ? + 2 ? u(t) ? ? ? ? . . . p n (1) . . . ? ? ? ? .</formula><p>where A 1 [n, j] = 2? n,k if k &lt; n 0 otherwise .</p><p>Next, we use the approximation</p><formula xml:id="formula_68">u(x) ? N ?1 k=0 x k (t)p k (z).</formula><p>to handle the u(t ? ?) term in Corollary D.5.</p><p>Corollary D.6. Let ?(t) = ? for all t. The?</p><formula xml:id="formula_69">x n (t) ? ? 1 ? Ax(t) + 2 ? u(t) ? ? ? ? . . . p n (1) . . . ? ? ? ? where A = A 1 + 2A 2 for A 1 as defined in Corollary D.5 and A 2 [n, k] = p n (?1)p k (?1).</formula><p>Proof. To approximate u(t ? ?), we note that when Y = t ? ?, z = ?1. Then</p><formula xml:id="formula_70">u(t ? ?) ? N ?1 k=0 x k (t)p k (?1).</formula><p>Then by Corollary D.5,</p><formula xml:id="formula_71">x n (t) ? ? 1 ? A 1 x(t) ? 2 ? N ?1 k=0 x k (t)p k (?1) ? ? ? ? . . . p n (?1) . . . ? ? ? ? + 2 ? u(t) ? ? ? ? . . . p n (?1) . . . ? ? ? ? .<label>(27)</label></formula><p>Let us define a matrix, A 2 ? R N ?N matrix such that A 2 [n, k] = p n (?1)p k (?1). Then the claim follows.</p><p>We now show that the special case of Corollary D.6 for Legendre matches the results from <ref type="bibr" target="#b23">[24]</ref>.</p><p>Corollary D.7. Let p n (z) = 2n+1 2 1/2 P n (z) where P n (z) are the Legendre polynomials. The?</p><formula xml:id="formula_72">x n (t) ? 1 ? Ax(t) + 2 ? bu(t)</formula><p>where A[n, k] = (2n + 1) Proof. From Corollary D.6,?</p><formula xml:id="formula_73">n (t) ? ? 1 ? Ax(t) + 2 ? u(t) ? ? ? ? . . . p n (1) . . . ? ? ? ?</formula><p>where A = A 1 + 2A 2 for A 1 as defined in Corollary D.5 and A 2 [n, k] = p n (?1)p k (?1). It is known from (7.21.1) and in <ref type="bibr" target="#b52">[53]</ref> that p n (?1) = 2n + 1 2 1 2 P n (?1) and P n (?1) = (?1) n .</p><p>Further, p n (1) = 2n + 1 2 1 2 P n (1) and P n (1) = 1.</p><p>Then b[n] = 2n+1 (2n + 1)P n (z) = P n+1 (z) + P n?1 (z) implies that P n+1 (z) = (2n + 1)P n (z) + (2n + 1)P n?2 (z) + ? ? ? +, which in turn implies P n = (2n ? 1)P n?1 (z) + (2n ? 5)P n?3 (z) + . . . . if k &lt; n and n ? k is odd, 0 is otherwise. ,</p><p>Recalling that A 1 [n, k] = 2? n,k .</p><p>We note that from <ref type="formula" target="#formula_0">(28)</ref>,</p><formula xml:id="formula_76">A 2 [n, k] = 2n+1 2 1 2 2k+1 2 1 2 (?1) n (?1) k = (2n+1) 1 2 (2k+1) 1 2 2 (?1) n?k . Recalling A = A 1 + 2A 2 , we get: A[n, k] = (2n + 1) 1 2 (2k + 1) 1 2 ? ? ? ? ? 2 + (?1) n?k</formula><p>if k &lt; n and n ? k is odd 0 + (?1) n?k if k &lt; n and n ? k is even</p><formula xml:id="formula_77">(?1) n?k if k ? n .</formula><p>Note that the above is the same as:</p><p>A[n, k] = (2n + 1)</p><formula xml:id="formula_78">1 2 (2k + 1) 1 2 1 if k ? n (?1) n?k if k ? n ,</formula><p>which completes our claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.4 Scaled HiPPO: Recovering HiPPO-LegS</head><p>We now use Theorem 7 to recover the HiPPO-LegS instantiation for the "Scaled Legendre" measure, the main method from Gu et al. <ref type="bibr" target="#b23">[24]</ref>.</p><p>Corollary D.8. Let p n (z) = 2n+1 2 1/2 P n (z) where P n (z) are the Legendre polynomials and let ?(t) = t for all t. Then?</p><formula xml:id="formula_79">n (t) = 1 t Ax(t) + 2 t bu(t) where A[n, k] = ? ? ? ? ? (2n + 1) 1 2 (2k + 1) 1 2 if k &lt; n n + 1 if k = n 0 if k &gt; n , and b[n] = 2n+1 2 1 2 .</formula><p>Proof. Let ?(t) = t. By Theorem 7 and noting that ?(t) = 1, we get:</p><formula xml:id="formula_80">x n (t) = ? 1 t A 1 x(t) + 2 t u(t) ? ? ? ? . . . p n (1) . . . ? ? ? ? where A 1 (t)[n, k] = ? ? ? ? ? 2? n,k + ? n,k if k &lt; n ? n,n + 1 if k = n 0 otherwise<label>(30)</label></formula><p>for ? n,k ,? n,k as defined in <ref type="bibr" target="#b24">(25)</ref>.</p><p>Using the same arguments as in the proof of Corollary D.7, b[n] = 2n+1 2 1 2 follows from Corollary D.6 and <ref type="bibr" target="#b28">(29)</ref>. Also using similar arguments as the proof of Corollary D.7, we have ? n,k = (2n + 1) .</p><p>From <ref type="bibr" target="#b7">(8)</ref> in <ref type="bibr" target="#b23">[24]</ref>, we know that (z + 1)P n (z) = nP n (z) + (2n + 1)P n?1 (z) + (2n ? 3)P n?2 (z) + . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Including the normalization constant (2n + 1)</head><p>1 2 , we note that (z ? 1)p n (z) = (z + 1)p n (z) ? 2p n (z). Then we get (z + 1)p n (z) = np n (z) ? (2n + 1)    Recalling that the definition for A 1 from (30), we get:</p><formula xml:id="formula_81">1 2 (2n ? 1) 1 2 p n?1 (z) + (2n + 1) 1 2 (2n ? 3) 1 2 p n?2 (z) ? . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In other words,</head><formula xml:id="formula_82">? n,k = ? ? ? ? ? ? ? ? ? ?(2n + 1) 1 2 (2k + 1)</formula><formula xml:id="formula_83">A[n, k] = ? ? ? ? ? (2n + 1) 1 2 (2k + 1) 1 2 if k &lt; n n + 1 if k = n 0 if k &gt; n ,</formula><p>which completes our claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Proof of Corollary 4.1: HiPPO for Classical Orthogonal Polynomials</head><p>This section proves Corollary 4.1, showing that the HiPPO matrices for measures corresponding to classical families of orthogonal polynomials <ref type="bibr" target="#b10">[11]</ref> are quasiseparable. We define quasi-separability in Appendix D.3.1. Theorem 8 proves the claimed result for Jacobi polynomials and Lemma D.11 proves the claimed result for Laguerre polynomials. We note that there is a third family of classical OPs, the Hermite polynomials <ref type="bibr" target="#b10">[11]</ref>, which have a two-sided infinite measure. However, since HiPPO is about continuous-time memorization of a function's history, it requires a one-sided measure and therefore the Hermite polynomials are not appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.1 Quasiseparable Matrices</head><p>Definition 4 (from <ref type="bibr" target="#b18">[19]</ref></p><formula xml:id="formula_84">). A matrix R ? R N ?N is (p, q)-quasiseparable if</formula><p>? Every matrix contained strictly above the diagonal has rank at most p.</p><p>? Every matrix contained strictly below the diagonal has rank at most q.</p><p>A (q, q)-quasiseparable matrix is called q-quasiseparable.</p><p>We are interested in showing the A matrices for a broad class of OPs in Corollary D.6 are O(1)quasiseperable. We now state some properties of q-quasiseparable matrices:</p><p>Lemma D.9. Let Q be q-quasiseparable. Then:</p><formula xml:id="formula_85">(i) For any q -quasiseparable matrix Q ? R N ?N , Q ? Q is (q + q )-quasiseparable. (ii) For any E ? R N ?N , E is r-quasiseparable where r = rank(E). (iii) For any two diagonal matrices D 1 , D 2 ? R N ?N , D 1 QD 2 is q-quasiseparable.</formula><p>Proof. We argue each point separately:</p><p>(i) Any submatrix contained strictly below or above the diagonal in Q has rank ? q and its corresponding submatrix in Q also has rank ? q . This implies that the corresponding submatrix in Q ? Q has rank ? q + q . Therefore Q ? Q is (q + q )-quasiseparable.</p><p>(ii) Let the r = rank(E). Thus any submatrix in E has rank ? r. Then E is r-quasiseparable.</p><p>(iii) Multiplication by diagonal matrices only scales the rows and columns, leaving the rank of each submatrix unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.2 Jacobi Polynomials</head><p>The Jacobi polynomial of degree n with parameters ?, ? &gt; ?1 will be denoted J ?,? n (z). The Jacobi polynomials are orthogonal with respect to measure ?(z) = (1 ? z) ? (1 + z) ? . In particular, it is known from (eq. (4.3.3) from <ref type="bibr" target="#b52">[53]</ref>) that form an orthonormal OP family. We now discuss some useful properties of Jacobi polynomials. It is known that ([50], eq. (3.100)):</p><formula xml:id="formula_86">J ?,? n (z) = 1 n + ? + ? (n + ?)J ?,??1 n (z) + (n + ?)J ??1,? n (z) .<label>(32)</label></formula><p>From (4.21.7) in <ref type="bibr" target="#b52">[53]</ref>, it is known that the derivative of J ?,? n (z) is proportional to J ?+1,?+1 n?1 (z):</p><formula xml:id="formula_87">? ?z J ?,? n (z) = 1 2 (n + ? + ? + 1)J ?+1,?+1 n?1 (z) .<label>(33)</label></formula><p>From <ref type="formula" target="#formula_0">(32)</ref> and <ref type="bibr" target="#b32">(33)</ref>, it follows that</p><formula xml:id="formula_88">? ?z J ?,? n (z) = 1 2 ? (n + ?)J ?+1,? n?1 (z) + (n + ?)J ?,?+1 n?1 (z) .<label>(34)</label></formula><p>Additionally, the Jacobi polynomials J ?+1,? n?1 (z) and J ?,?+1 n?1 (z) can be written as sums of J ?,? n?1 (z) polynomials. In particular from <ref type="bibr" target="#b49">[50]</ref> (3.112) and (3.115),</p><formula xml:id="formula_89">J ?+1,? n?1 (z) = ?(n + ?) ?(n + ? + ? + 1) ? n?1 k=0 (2k + ? + ? + 1)?(k + ? + ? + 1) ?(k + ? + 1) J ?,? k (z) ,<label>(35)</label></formula><p>and</p><formula xml:id="formula_90">J ?,?+1 n?1 (z) = ?(n + ?) ?(n + ? + ? + 1) ? n?1 k=0 (?1) n?k?1 (2k + ? + ? + 1)?(k + ? + ? + 1) ?(k + ? + 1) J ?,? k (z) .<label>(36)</label></formula><p>Using <ref type="formula" target="#formula_1">(35)</ref>  </p><p>We use these properties to write ? ?z p ?,? n (z) as a sum of p ?,? k (z) k?n :</p><p>Corollary D.10. Let p ?,? n (z) and ? ?,? n be as defined in <ref type="bibr" target="#b30">(31)</ref>. Then</p><formula xml:id="formula_92">? ?z ? ?,? n p ?,? n (z) = (n + ?) 2 ? ?(n + ?) ?(n + ? + ? + 1) n?1 k=0 (2k + ? + ? + 1)?(k + ? + ? + 1) ?(k + ? + 1) ? ?,? k p ?,? k (z) ? (n + ?) 2 ? ?(n + ?) ?(n + ? + ? + 1) n?1 k=0 (?1) n?k (2k + ? + ? + 1)?(k + ? + ? + 1) ?(k + ? + 1) ? ?,? k p ?,? k (z)</formula><p>Proof. Recall that J ?,? n (z) = ? ?,? n p ?,? n . Then the claim follows from (37).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.3 HiPPO for Jacobi Polynomials</head><p>Theorem 8. Let p ?,? n (z) be defined as in <ref type="bibr" target="#b30">(31)</ref> and ?(z) = (1 ? z) ? (1 + z) ? . The?</p><formula xml:id="formula_93">x n (t) ? ? 1 ? Ax(t) + 2 ? bu(t) where A is 3-quasiseperable.</formula><p>Proof. From Corollary D.6,?</p><formula xml:id="formula_94">n (t) ? ? 1 ? Ax(t) + 2 ? u(t) ? ? ? ? . . . p ?,? n (1) . . . ? ? ? ?</formula><p>where A = A 1 + 2A 2 for A 1 as defined in Corollary D.5 and A 2 [n, k] = p ?,? n (?1)p ?,? n (?1). From Corollary D.10, we observe that</p><formula xml:id="formula_95">A 1 [n, k] = 2 ? ? ? ? ? ? ? ? (n+?) 2 ? ?,? n ? ?(n+?) ?(n+?+?+1) ? (2k+?+?+1)?(k+?+?+1) ?(k+?+1) ? ?,? k ? (n+?) 2 ? ?,? n ? ?(n+?) ?(n+?+?+1) ? (?1) n?k (2k+?+?+1)?(k+?+?+1) ?(k+?+1) ? ?,? k if k &lt; n 0 otherwise .<label>(38)</label></formula><p>Then we note that,</p><formula xml:id="formula_96">A 1 = D 11 Q 1 D 12 ? D 21 Q 1 D 22 ,<label>(39)</label></formula><p>where D 11 , D 12 , D 21 , D 22 are the diagonal matrices such that </p><formula xml:id="formula_97">D 22 [k, k] = (?1) k ? (2k + ? + ? + 1)?(k + ? + ? + 1) ?(k + ? + 1) ? ?,? k , and Q 1 [n, k] = 1 if k &lt; n 0 otherwise. .<label>(39)</label></formula><p>makes use of the fact that (?1) n+k = (?1) n?k along with the definitions above. Any submatrix of Q 1 below the diagonal contains all 1s, and submatrix of Q 1 above the diagonal contains all 0s. Then any submatrix above or below the diagonal has rank 1. Therefore Q 1 is 1-quasiseparable. Since Q 1 is 1-quasiseparable and D 11 , D 12 , D 21 , D 22 are all diagonal matrices, part (iii) of Lemma D.9 implies that the matrices D 11 Q 1 D 12 and D 21 Q 1 D 22 are both 1-quasiseparable. Therefore part (i) of Lemma D.9 implies that A 1 is 2-quasiseparable.</p><p>From if n &lt; 0 .</p><p>Then A 2 can be written D 3 Q 2 D 4 where D 3 , D 4 are the diagonal matrices such that</p><formula xml:id="formula_98">D 3 [n, n] = (?1) n ? ?,? n n + ? n , D 4 [k, k] = (?1) k ? ?,? k k + ? k ,</formula><p>where Q 2 [n, k] = 1 for all 0 ? n, k &lt; N . Q 2 has rank 1, and D 3 , D 4 are diagonal matrices. Hence by part (ii) and (iii) Lemma D.9, A 2 is 1-quasiseparable.</p><p>Since A 1 is 2-quasiseparable and A 2 is 1-quasiseparable, part (i) of Lemma D.9 implies that A = A 1 +2A 2 is 3-quasiseparable and the claim follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.4 HiPPO-LagT</head><p>The Laguerre polynomial of degree n with parameters ? &gt; ?1 will be denoted L ? n (z). The Laguerre polynomials are orthogonal with respect to measure z ? e ?z . In particular, from (5.1.1) in <ref type="bibr" target="#b52">[53]</ref> we know that</p><formula xml:id="formula_99">? ?1 L ? n (z) L ? m (z) z ? e ?z dz = ?(n + ? + 1)! ?(n + 1) ? n,m .</formula><p>Let ? n = ?(n+1) ?(n+?+1) 1 2 be our normalization constant. We note that the normalized Laguerre polynomials</p><formula xml:id="formula_100">p n (z) = ? n L ? n (t ? Y )<label>(40)</label></formula><p>form an orthonormal OP family with respect to measure ? = (t ? Y ) ? e ?(t?Y ) 1 (??,t) for a fixed ? and tilting</p><formula xml:id="formula_101">? = (t ? Y ) ? exp ? 1?? 2 (t ? Y ) 1 (??,t) for a fixed ?.</formula><p>We use the following result from <ref type="bibr" target="#b23">[24]</ref>:</p><p>Theorem 9. Let p n (z) be defined as in <ref type="bibr" target="#b39">(40)</ref>. The?</p><formula xml:id="formula_102">x n (t) = ?Ax(t) + bu(t) where A[n, k] = ? ? ? ? ? 1+? 2 if k = n 1 if k &lt; n 0 otherwise , b[n] = ? n n + ? n ,</formula><p>We now show that A as defined in Theorem 9 is 1-quasiseperable.</p><p>Lemma D.11. Let A be defined as in Theorem 9. Then A is 1-quasiseperable.</p><p>Proof. From Theorem 9, we know that</p><formula xml:id="formula_103">A[n, k] = ? ? ? ? ? 1+? 2 if k = n 1 if k &lt; n 0 otherwise , b[n] = ? n n + ? n .</formula><p>Below the diagonal, all entries A[n, k] = 1. Then any submatrix below the diagonal has rank 1. Similarly, above the diagonal, all entries A[n, k] = 0. Then any submatrix above the diagonal also has rank 1. Then by Definition 4, the claim follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E LSSL Algorithms</head><p>? Appendix E.1 proves Theorem 2, providing an algorithm to compute the Krylov function efficiently for LSSLs.</p><p>? Appendix E.2 shows a further simplification of Corollary 4.1, presenting an even simpler class of structured matrices that we use in our implementation of LSSL.</p><p>? Appendix E.3 provides technical details of the implementation of LSSL, in particular for computing the MVM black box (multiplication by A) and for computing gradients during backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Proof of Theorem 2</head><p>This section addresses the computational aspects of the LSSL. In particular, we prove Theorem 2 for the computational speed of computing the Krylov function <ref type="bibr" target="#b6">(7)</ref> for quasiseparable matrices A, by providing a concrete algorithm in Appendix E.1.1. We restate the Krylov function <ref type="formula" target="#formula_6">(7)</ref> here for convenience. Recall that L is the length of the input sequence and N is the order of the LSSL internate state, e.g. A ? R N ?N .</p><formula xml:id="formula_104">K L (A, B, C) = CA i B i?[L] ? R L = (CB, CAB, . . . , CA L?1 B)</formula><p>Remark E.1. We call <ref type="bibr" target="#b6">(7)</ref> the Krylov function following the notation of <ref type="bibr" target="#b16">[17]</ref>, since it can be written K(A, B) T C where K <ref type="figure">(A, B)</ref> is the Krylov matrix defined in <ref type="bibr" target="#b7">(8)</ref>. Alternative naming suggestions are welcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.1 The Algorithm</head><p>We follow the similar problem of [17, Lemma 6.6] but track the dependence on L and the log factors more precisely, and optimize it in the case of stronger structure than quasiseparability, which holds in our setting (particularly Theorem 11).</p><p>The first step is to observe that the Krylov function K L <ref type="figure">(A, B, C)</ref> is actually the coefficient vector of C(I ? Ax) ?1 B (mod x L ) as a polynomial in x. (Note that Ax means simply multiplying every entry in A by a scalar variable x.) This follows from expanding the power series (I ? Ax) ?1 = I + Ax + A 2 x 2 + . . .. Thus we first compute C(I ? Ax) ?1 B, which is a rational function of degree at most N in the numerator and denominator (which can be seen by the standard adjoint formula for the matrix inverse).</p><p>The second step is simply inverting the denominator of this rational function (mod x L ) and multiplying by the numerator, both of which are operations that need L log(L) time by standard results for polynomial arithmetic <ref type="bibr" target="#b50">[51]</ref>.</p><p>For the remainder of this section, we focus on computing the first part. We make two notational changes: First, we transpose C to make it have the same shape as B. We consider the more general setting where B and C have multiple columns; this can be viewed as handling a "batch" problem with several queries for B, C at the same time.</p><p>Lemma E.2. Let A be a q-quasiseparable matrix. Then</p><formula xml:id="formula_105">C T (I ? Ax) ?1 B where A ? R N ?N B, C ? R N ?k</formula><p>is a k ? k matrix of rational functions of degree at most N , which can be computed in O(q 3 log 4 N ) operations.</p><p>The main idea is that quasiseparable matrices are recursively "self-similar", in that the principal submatrices are also quasiseparable, which leads to a divide-and-conquer algorithm. In particular, divide A = A 00 A 01 A 10 A 11 into quadrants. Then by Definition 4, A 00 , A 11 are both q-quasiseparable and A 01 , A 10 are rank q. Therefore the strategy is to view I ? Ax as a low-rank perturbation of smaller quasiseparable matrices and reduce the problem to a simpler one.</p><p>Proposition 10 (Binomial Inverse Theorem or Woodbury matrix identity <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b59">60]</ref>). Over a commutative ring R, let A ? R N ?N and U, V ? R N ?p . Suppose A and A + UV T are invertible. Then</p><formula xml:id="formula_106">I p + V T A ?1 U ? R p?p is invertible and (A + UV T ) ?1 = A ?1 ? A ?1 U(I p + V T A ?1 U) ?1 V T A ?1</formula><p>For our purposes, R will be the ring of rational functions over R.</p><p>Proof of Lemma E.2. Since A is q-quasiseparable, we can write A 10 = U L V T L and A 01 = U U V T U where U ? , V ? ? F N ?q . Notice that we can write I ? Ax as</p><formula xml:id="formula_107">I ? Ax = I ? A 00 x 0 0 I ? A 11 x + 0 U U U L 0 V L 0 0 V U T x.</formula><p>Suppose we know the expansions of each of</p><formula xml:id="formula_108">M 1 ? R k?k = C T I ? A 00 x 0 0 I ? A 11 x ?1 B (41) M 2 ? R k?2q = C T I ? A 00 x 0 0 I ? A 11 x ?1 0 U U U L 0 (42) M 3 ? R 2q?2q = V L 0 0 V U T I ? A 00 x 0 0 I ? A 11 x ?1 0 U U U L 0 (43) M 4 ? R 2q?k = V L 0 0 V U T I ? A 00 x 0 0 I ? A 11 x ?1 B.<label>(44)</label></formula><p>By Proposition 10, the desired answer is</p><formula xml:id="formula_109">C T (X ? A) ?1 B = M 1 ? M 2 (I 2q + M 3 ) ?1 M 4 .</formula><p>Then the final result can be computed by inverting I 2t + M 3 (O(q 3 N log(N )) operations), multiplying by M 2 , M 4 (O((kq 2 + k 2 q)N log(N )) operations), and subtracting from M 1 (O(k 2 N log(N )) operations). This is a total of O((q 3 + kq 2 + k 2 q)N log(N )) operations. Note that when k = O(q log N ), this becomes O(q 3 N log 3 N ); we will use this in the analysis shortly.</p><p>To compute M 1 , M 2 , M 3 , M 4 , it suffices to compute the following:</p><formula xml:id="formula_110">C T 1 (I ? A 00 x) ?1 B 0 C T 1 (I ? A 11 x) ?1 B 1 C T 0 (I ? A 00 x) ?1 U U C T 1 (I ? A 11 x) ?1 U L V T L (I ? A 00 x) ?1 U U V T U (I ? A 11 x) ?1 U L V T L (I ? A 00 x) ?1 B 0 V T U (I ? A 11 x) ?1 B 1 .<label>(45)</label></formula><p>But to compute those, it suffices to compute the following (k + t) ? (k + t) matrices:</p><formula xml:id="formula_111">C 0 V L T (I ? A 00 x) ?1 B 0 U U C 1 V U T (I ? A 11 x) ?1 B 1 U L<label>(46)</label></formula><p>Since A 00 and A 11 have the same form as A, this is two recursive calls of half the size. Notice that the size of the other input (dimensions of B, C) is growing, but when the initial input is k = 1, it never exceeds 1 + q log N (since they increase by q every time we go down a level). Earlier, we noticed that when k = O(q log N ), the reduction step has complexity O(q 3 N log 3 (N )) for any recursive call. The recursion adds an additional log N multiplicative factor on top of this. This follows from the fact that in the recursion <ref type="bibr" target="#b44">(45)</ref> and <ref type="bibr" target="#b45">(46)</ref>, the U, V matrices do not have to be appended if they already exist in B, C. For intuition, this happens in the case when A is tridiagonal, so that U, V have the structure (1, 0, . . . , 0), or the case when the off-diagonal part of A is all 1 (such as the HiPPO-LegT matrix). The matrices in Appendix D.3 and Appendix E.2 (Theorems 8, 9 and 11) actually satisfy this stronger structure, so Corollary E.3 applies.</p><p>Combining everything, this proves Theorem 2 with the exact bound N log 2 (N ) + L log(L) operations. The memory claim follows similarly, and the depth of the algorithm is log 2 (N )+log(L) from the divide-and-conquer recursions. We provide a summary of complexity requirements for various sequence model mechanisms, including several versions of the LSSL. Note that these are over exact arithmetic as in Theorem 2. First, the self-attention mechanism is another common sequence model that has an L 2 dependence on the length of the sequence, so it is not suitable for the very long sequences we consider here. (We do note that there is an active line of work on reducing this complexity.) Second, we include additional variants of the LSSL. In <ref type="table" target="#tab_6">Table 7</ref>, LSSL-naive denotes learning A and ?t for unstructured A; LSSL-fixed denotes not learning A, ?t (see Appendix B for details); LSSL denotes the learning A and ?t for the structured class A.</p><p>We include brief explanations of these complexities for the LSSL variants.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSSL-naive</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Further Simplification with Tridiagonal Matrices</head><p>The algorithm for Theorem 2 for general quasiseparable matrices is still difficult to implement in practice, and we make a further simplification using a particular subclass of quasiseparable matrices. Theorem 11 shows that a simple representation involving tridiagonal and diagonal matrices captures all of the original HiPPO matrices. In particular, our LSSL implementation initializes A to be the HiPPO-LegS matrix (Appendix B) and learns within the class defined by Theorem 11.</p><p>We note that the matrices in Theorem 11 are all 1-quasiseparable and in particular also contain the HiPPO matrices for Gegenbauer and generalized Laguerre orthogonal polynomials derived in Theorem 9. In fact, the notion of semiseparability, which is closely related to (and actually is the predecessor of) quasiseparability, was originally motivated precisely to capture inverses of tridiagonal matrices. Thus the structured class in Theorem 11 can be viewed as an approximation of 3-quasiseparable matrices (Corollary 4.1) to 1-quasiseparable, which still contains many of the HiPPO families of interest.</p><p>Proof. We simply show that each of these specific matrices can be represented in the proposed form.</p><p>HiPPO-LegT. Let A denote the HiPPO-LegT transition matrix. Up to row/column scaling (i.e. left-and rightmultiplication by diagonal P and Q), we can write</p><formula xml:id="formula_112">A nk = (?1) n?k if n ? k 1 if n ? k .</formula><p>The main observation is that</p><formula xml:id="formula_113">A ?1 = 1 2 ? ? ? ? ? ? ? ? ? ? ? 1 1 0 . . . 0 0 0 ?1 0 1 . . . 0 0 0 0 ?1 0 . . . 0 0 0 . . . . . . . . . . . . . . . . . . . . . 0 0 0 . . . 0 1 0 0 0 0 . . . ?1 0 1 0 0 0 . . . 0 ?1 1 ? ? ? ? ? ? ? ? ? ? ?</formula><p>HiPPO-LegS. The HiPPO-LegS matrix is</p><formula xml:id="formula_114">A nk = ? ? ? ? ? ? (2n + 1) 1/2 (2k + 1) 1/2 if n &gt; k n + 1 if n = k 0 if n &lt; k .</formula><p>This can be written as ?P A Q where P = Q = diag((2n + 1) 1 2 ) and</p><formula xml:id="formula_115">A nk = ? ? ? ? ? 1 if n &gt; k 0 if n &lt; k 1 ? n 2n+1 if n = k . Finally, A = D + T ?1 where D = ? diag( n 2n+1</formula><p>) and T is the matrix with 1 on the main diagonal and ?1 on the subdiagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HiPPO-LagT. The HiPPO-LagT matrix is</head><formula xml:id="formula_116">A nk = ? ? ? ? ? ? 1 if n &gt; k 0 if n &lt; k 1 2 if n = k .</formula><p>This can be written as ?P (D + T ?1 )Q where P = Q = I, D = ? 1 2 I, and T is the same tridiagonal matrix as in the HiPPO-LegS case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Implementation Details</head><p>In this section we provide several implementation details that are useful for implementing LSSLs in practice.</p><p>Recall that one of the main primitives of LSSLs is the matrix-vector multiplication y = Ax (Section 3.1, Appendix B), where A is the state matrix A discretized with step size ?t using the bilinear method (Appendix C.1.3). In Appendix E.3.1, we describe how this MVM can be performed with simpler MVM primitives which we call the "forward difference" and "backward difference".</p><p>However, if these MVM primitives are implemented in a specialized way for particular classes of A matrices (i.e., not using atoms in a standard autograd framework), then we also need to calculate several additional gradients by hand. Appendix E.3.2 shows that calculating gradients to A, ?t, x during backpropagation can actually be reduced to those same forward/backward difference primitives.</p><p>Finally, in the case when A is the structured class of matrices in Theorem 11, Appendix E.2 shows how to efficiently calculate those primitives using a black-box tridiagonal solver. Our code 2 implements all the algorithms in this section, with bindings to the cuSPARSE library for efficient tridiagonal solving on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.1 Matrix-vector Multiplication by the Bilinear Discretization</head><p>Bilinear Discretization The discrete state-space system is given by <ref type="formula">(4)</ref> and <ref type="formula" target="#formula_2">(5)</ref>, re-written here for convenience</p><formula xml:id="formula_117">x t = Ax t?1 + Bu t y t = Cx t + Du t</formula><p>where A is a function of A, ? t and A is a function of A, B, ? t . In particular, we define A to be the matrix discretized using the bilinear method (Appendix C.1.3), and the system can be written explicitly: We will call these functions the forward difference and backward difference maps, respectively. (The Euler and backward Euler discretizations (Appendix C.1.2) are also known as the "forward difference" and "backward difference" methods, which in the case of linear systems reduces down to the maps F and B.) Similarly, in order to compute ?L(y) ??t we require ?y ??t . We need the result </p><formula xml:id="formula_118">x t = I ? ?tA 2 ?1 I + ?tA 2 x t?1 + ?tBu t y t = Cx t + Du t</formula><formula xml:id="formula_119">?Y ?1 ?x = ?Y ?1 ?Y ?x Y ?1 for an invertible matrix Y [41,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.3 Computing the Forward/Backward Difference for Tridiagonal Inverse Matrices</head><p>Theorem 11 uses the classes of matrices A = P (D + T ?1 )Q for diagonal D, P, Q and tridiagonal T . We describe how the forward and backward difference MVMs can be performed efficiently for this class of matrices by reducing to a black-box tridiagonal solver.   Backward difference. We will explicitly rewrite the inverse of the matrix G = I + ?t ? P (D + T ?1 )Q. The core observation is to multiply G by a choice selection of matrices to cancel out the T ?1 term:</p><p>T P ?1 GQ ?1 = T P ?1 Q ?1 + ?tT D + ?tI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rearranging yields</head><p>G ?1 = Q ?1 (T P ?1 Q ?1 + ?tT D + ?tI) ?1 T P ?1 .</p><p>Now note that the matrix in the middle is tridiagonal. Hence we have reduced MVM by G ?1 , i.e. the backward difference problem, to a series of diagonal and tridiagonal MVMs (easy), and a tridiagonal inverse MVM (a.k.a. a tridiagonal solve).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Experiments and Experiment Details</head><p>We provide additional experiments and ablations in Appendix F.1. Appendix F.2 describes our training methodology in more detail for each dataset. The hyperparameters for all reported results are in <ref type="table" target="#tab_0">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Additional Experiments</head><p>Missing Data on CharacterTrajectories. <ref type="table" target="#tab_8">Table 8</ref> has results for a setting considered in previous work involving irregularly-sampled time series. LSSL is competitive with the best prior methods, some of which were specialized to handle this setting.</p><p>A and ?t ablations. <ref type="table" target="#tab_0">Tables 9 and 10</ref> show results on SpeechCommands-Raw and a smaller model on sCIFAR, ablating that learning either the A or ?t parameters provides a consistent performance increase. Finally, <ref type="figure" target="#fig_5">Fig. 2</ref> plots the ?t values at the beginning and end of training on the SpeechCommands-Raw dataset, confirming that training ?t does noticeably change their values to better model the data. In particular, the ?t values spread over time to cover a larger range of timescales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Methodology</head><p>We describe our training procedure on each dataset for our model and any relevant baselines.</p><p>General All models and datasets used the Adam optimizer with a LR decay scheduler that reduced LR by 5x upon validation plateau for 10 or 20 epochs. We fixed the batch size to 50 for the MNIST/CIFAR datasets and 32 for other datasets, reducing if necessary to fit in memory. For all models, we chose the hyperparameters that achieved the highest validation accuracy/RMSE (values in <ref type="table" target="#tab_0">Table 11</ref>).</p><p>Error Bars We note that the results in Section 5 do not include standard deviations for formatting reasons, since most of the baselines were best results reported in previous papers without error bars. As Section 6 noted, the LSSL was actually quite stable in performance and not particularly sensitive to hyperparameters. We note that for every result in Section 5, the LSSL with error bars was at least one standard deviation above the baseline results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.1 Sequential and Permuted MNIST</head><p>The model architecture of LSSL(-f) was fixed to the small architecture with 200K parameters (Appendix B). Following <ref type="bibr" target="#b43">[44]</ref>, we fixed the learning rate scheduler to decay on plateau by with a factor of 0.2, and the number of epochs to 200. We searched hyperparameters over the product of the following learning rate values: {0.001, 0.002, 0.004, 0.01}, and dropout values: {0.1, 0.2}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.2 Sequential CIFAR</head><p>The model architecture of LSSL(-f) was fixed to the large architecture with 2M parameters (Appendix B). We searched over the product of the following learning rate values: {0.001, 0.002, 0.004, 0.01, 0.02}, and dropout values: {0.2, 0.3, 0.4}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.3 BIDMC Healthcare</head><p>The BIDMC tasks aim at predicting three vital signs of a patient, respiratory rate (RR), heart rate (HR), and oxygen saturation (SpO2), based on PPG and ECG signals. The clinical data is provided by the Beth Israel Deaconess Medical Center. The PPG and ECG signals were sampled at 125Hz and have a sequence length of 4000.</p><p>For this dataset, we fixed the small LSSL(-f) model (Appendix B). Following <ref type="bibr" target="#b46">[47]</ref>, we changed the scheduler to a multistep scheduler that decays on fixed epochs, and trained for 500 epochs.</p><p>For our methods, we searched over the product of the following learning rate values: {0.004, 0.01, 0.02}, and dropout values: {0.1, 0.2}.</p><p>Baseline parameters. For CKConv, we searched over ? 0 ? [10, 50] following the guidelines of Romero et al. <ref type="bibr" target="#b43">[44]</ref> (best value ? 0 = 20). Since we tuned the sensitive ? 0 , we fixed the learning rate to 0.001 and dropout to 0.1 which was the default used in <ref type="bibr" target="#b43">[44]</ref>.</p><p>The transformer model we used was a vanilla transformer with a hidden dimension of 256, 8 attention heads, 4 layers, and a feedforward dimension of 1024. We used a learning rate of 0.001 and a dropout of 0. We tried a few variants, but no transformer model was effective at all. ?t , which can be interpreted as the timescale at which they operate (Section 2). The plots confirm that LSSL does modify the dt values in order to more appropriately model the speech data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.4 CelebA</head><p>For these larger datasets, we reduced the size of the order N and did not tie it to H. These experiments were computationally heavy and we did not do any tuning (i.e., <ref type="table" target="#tab_0">Table 11</ref> are the only runs). The model size was picked to train in a reasonable amount of time, and the learning rate for the first attribute was picked based on general best hyperparameters for other datasets, and then reduced for subsequent experiments on the other attributes.</p><p>Baseline parameters. For ResNet-18, we used the standard implementation with a learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.5 Speech Commands</head><p>For Speech Commands, we use the same dataset and preprocessing code from Kidger et al. <ref type="bibr" target="#b30">[31]</ref>, Romero et al. <ref type="bibr" target="#b43">[44]</ref>. We consider the two settings from Kidger et al. <ref type="bibr" target="#b30">[31]</ref>: SC-Raw uses very long time-series raw speech signals of 16000 timesteps each, while SC-MFCC uses standard MFCC features of 161 timesteps.</p><p>For our models trained over the raw data, we searched over the product of the following learning rate values: {0.002, 0.004, 0.01}, and dropout values: {0.1, 0.2}. For our models trained over the MFCC features, we searched over the product of the following learning rate values: {0.0001, 0.001, 0.002, 0.004, 0.01}, and dropout values: {0.1, 0.2, 0.3, 0.4}.</p><p>Baseline parameters. To get more results for the strongest baselines on very long sequences in the literature, we ran the UniCORNN <ref type="bibr" target="#b46">[47]</ref> baseline on both Raw and MFCC variants, and the Neural Rough Differential Equations <ref type="bibr" target="#b36">[37]</ref> baseline on the Raw variant.</p><p>For UniCORNN trained over the raw data, we searched over multiple hyperparameters. Specifically, we searched over alpha: {0, 10, 20, 30, 40}, ?t values: {0.00001, 0.0001, 0.001, 0.01}, and learning rate values: {0.0001, 0.0004, 0.001, 0.004}. However, since the method was not able to generalize to the validation set for any hyperparameter combination, we used the authors' reported hyperparameters for the Eigenworms dataset as it also contains very long sequences (? 18000). In particular, we used a learning rate of 0.02, hidden dimension of 256, 3 layers with dt values [0.0000281, 0.0343, 0.0343], dropout of 0.1, and alpha of 0.</p><p>For UniCORNN trained over the MFCC features, we used the authors' reported hyperparameters for the MNIST dataset (again due to similarly sized sequence lengths), and further tuned the learning rate over the values: {0.0001, 0.001, 0.005, 0.01, 0.02}, ?t values: {0.01, 0.1}, and alpha values: {10, 20, 30}.</p><p>The best model used a learning rate of 0.02, hidden dimension of 256, 3 layers with dt values of 0.19, dropout of 0.1, and alpha of 30.65.</p><p>For NRDE on SC-Raw, we used depth 2, step size 4, hidden dimension 32, and 3 layers. Our results were better than unofficial numbers reported in correspondence with the authors, so we did not tune further. <ref type="table" target="#tab_4">(Table 5)</ref> The convergence table compared against logs directly from the corresponding baseline's SoTA models <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47]</ref>, which were either released publicly or found in direct correspondence with the authors. To generate the wall clock numbers, we ran the baseline models on the same hardware as our models and extrapolated to the target epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.6 Convergence Speed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Hyperparameters</head><p>Best hyperparameters for all datasets are reported in <ref type="table" target="#tab_0">Table 11</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(Three views of the LSSL) A Linear State Space Layer layer is a map ut ? R ? yt ? R, where each feature ut ? yt is defined by discretizing a state-space model A, B, C, D with a parameter ?t. The underlying state space model defines a discrete recurrence through combining the state matrix A and timescale ?t into a transition matrix A. (Left) As an implicit continuous model, irregularly-spaced data can be handled by discretizing the same matrix A using a different timescale ?t. (Center) As a recurrent model, inference can be performed efficiently by computing the layer timewise (i.e., one vertical slice at a time (ut, xt, yt), (ut+1, xt+1, yt+1), . . .), by unrolling the linear recurrence. (Right) As a convolutional model, training can be performed efficiently by computing the layer depthwise in parallel (i.e., one horizontal slice at a time (ut) t?[L] , (yt) t?[L] , . . .), by convolving with a particular filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma 3 . 2 .</head><label>32</label><figDesc>(Infinitely) deep stacked LSSL layers of order N = 1 with position-wise non-linear functions can approximate any non-linear ODE?(t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Corollary 4 . 1 .</head><label>41</label><figDesc>For ? corresponding to the classical OPs, hippo(?) is 3-quasiseparable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>LSSL small (? 200K parameters): 6 layers, H = 128, N = 128, M = 1. ? LSSL large (? 2M parameters): 4 layers, H = 256, N = 256, M = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(in continuous-time) by applying Picard iteration to the non-linear term (Appendix C.2.3, Theorem 4). ? Deep linear RNNs approximate the dynamics (14) with both Picard iteration in the depth direction to linearize the non-linear term, and discretization (gates) in the time direction to discretize the equation (Appendix C.2.4, Corollary C.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 .</head><label>2</label><figDesc>Letz(t) = e ?At x(t) (and z 0 = z(t 0 ) = x(t 0 ) = x 0 ). Note that? (t) = e ?At [?(t) ? Ax(t)]= e ?At f (t, x(t))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Corollary C. 3 .</head><label>3</label><figDesc>Consider a deep, linear RNN of the form x ( )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>? Appendix D. 3 proves</head><label>3</label><figDesc>Corollary 4.1, including a formal definition of quasiseparable matrices (i.e., how LSSL matrices are defined) in Definition 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Theorem 5 (</head><label>5</label><figDesc><ref type="bibr" target="#b16">[17]</ref>,Theorem 4.4). For any N ? N matrix A with constant recurrence width, any vector x ? R n , Ax can be computed with?(N ) operations over R.Theorem 6 ([17], Theorem 7.1). For any N ? N matrix A with constant recurrence width, any vector x ? R n , A ?1 x can be computed with?(N ) operations over R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>N ? 1 j=0</head><label>1</label><figDesc>P[i, j]X j . D.2 Proof of Theorem 1 This section proves Theorem 1, which is restated formally in Corollary D.4. Appendix D.2.1 proves some results relating orthogonal polynomials to recurrence width (Appendix D.1.3). Appendix D.2.2 proves Corollary D.4. Appendices D.2.3 and D.2.4 provides examples showing how Corollary D.4 can be specialized to exactly recover the HiPPO-LegT, HiPPO-LagT, HiPPO-LegS methods<ref type="bibr" target="#b23">[24]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 12</head><label>2</label><figDesc>follows from Corollary D.6 and (29). From the following recurrence relations [1, Chapter 12]:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>1 2 1 2</head><label>11</label><figDesc>P n?1 (z) + (2n ? 5) P n?3 (z) + . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>1 2</head><label>1</label><figDesc>if k &lt; n and n ? k is odd, 0 is otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 2</head><label>1</label><figDesc>if k &lt; n and n ? k is odd, (2n + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>1 2</head><label>1</label><figDesc>if k &lt; n and n ? k is even n if n = k 0 otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>J ?,? m (z) ?(z)dz = 2 ?+?+1 2n + ? + ? + 1 ? ?(n + ? + 1)?(n + ? + 1) ?(n + ? + ? + 1)n! ? n,m ,where ?(?) is the gamma function. Let ? ?,? n = 2 ?+?+1 2n + ? + ? + 1 ? ?(n + ? + 1)?(n + ? + 1) ?(n + ? + ? + 1)n! be our normalization constant. We note that the normalized Jacobi polynomials p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>2 ? 2 ?</head><label>22</label><figDesc>and (36) in (34) allows us to write ? ?z J ?,? n (z) as a sum of J ?,? k (n + ?) ?(n + ? + ? + 1) ? + ? + 1)?(k + ? + ? + 1) ?(k + ? + 1) (n + ?) ?(n + ? + ? + 1) n?1 k=0 (?1) n?k (2k + ? + ? + 1)?(k + ? + ? + 1) ?(k + ? + 1) J ?,? k (z) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>D 11 [ 1 ?</head><label>111</label><figDesc>+ ? + 1) ?(n + ? + ? + 1),D 12 [k, k] = (2k + ? + ? + 1)?(k + ? + ? + 1) ?(k + ? + 1) ? ?,? k ,D 21 [n, n] = (?1) n ? (?,? n ? ?(n + ? + 1) ?(n + ? + ? + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>1 ?</head><label>1</label><figDesc>(4.1.1) and (4.1.4) in<ref type="bibr" target="#b52">[53]</ref>, it is known that p ?,? n (1) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Corollary E. 3 .</head><label>3</label><figDesc>Suppose that A is semiseparable instead of quasiseparable, and suppose q is a small constant. Then the cost of Lemma E.2 is O(N log 2 (N )) operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>?</head><label></label><figDesc>Parameters: O(HN ) in the matrices B, C and O(N 2 ) in the matrix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Theorem 11 .</head><label>11</label><figDesc>The class of N ? N matrices S N = {P (D + T ?1 )Q} with diagonal D, P, Q and tridiagonal T includes the original HiPPO-LegS, HiPPO-LegT, and HiPPO-LagT matrices [24].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>Thus it suffices to compute the maps F (A, ?t, x) := (I + ?tA)x and B(A, ?t, x) := (I + ?tA) ?1 x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>E. 3 . 2</head><label>32</label><figDesc>Gradients through the Forward/Backward Difference Primitives In this section we will let y = F (A, ?t, x) or y = B(A, ?t, x) denote the computation of interest, L(y) denote a generic loss function, and dx, dy, . . . denote gradients to x, y, . . . (e.g., dx = ?L(y) ?x ). Derivatives of backward difference. First we have the standard ?L(y) ?x = ?L(y) ?y ?y ?x = ?L(y) ?y (I + ?tA) ?1 . This corresponds to matrix-vector multiplication by (I + ?A) ?T . In other words, it can be computed by the primitive B(A T , ?t, dy).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>1 x</head><label>1</label><figDesc>equation (59)]. Then ?y ??t = ?(I + ?tA) ?1 ??t x = ?(I + ?tA) ?1 ?(I + ?tA) ??t (I + ?tA) ?1 x = ?(I + ?tA) ?1 A(I + ?tA) ?tA) ?1 A (I + ?tA) ?We can summarize this as follows. Let y = B(A, ?t, x) = (I + ?tA) ?1 x and dy = ?L(y)/?y (as a column vector). Then y = B(A, ?t, x) dx = B(A T , ?t, dy) d?t = ?dx T Ay. Derivatives of forward difference. The forward case is simpler. Let y = F (A, ?t, x) = (I + ?tA)x. Then ?y ?x = I + ?tA and ?y ??t = Ax. Thus y = F (A, ?t, x) dx = (I + ?tA) T dy = F (A T , ?t, dy) d?t = dy T Ax.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Forward difference.</head><label></label><figDesc>It is straightforward to compute F (A, ?t, x) = (I + ?t ? P (D + T ?1 )Q)x = x + ?t ? P DQx + ?t ? P T ?1 Qxin terms of multiplication by diagonal matrices x ? Dx and tridiagonal solving x ? T ?1 x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 2 :</head><label>2</label><figDesc>We visualize the 32 largest and smallest ?t values at the start and end of training for the first layer of our state-of-the-art LSSL model on the Speech Commands Raw dataset. The plots visualize 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>(Pixel-by-pixel image classification.) (Top) our methods. (Middle) recurrent baselines. (Bottom) convolutional + other baselines.</figDesc><table><row><cell>Model</cell><cell cols="3">sMNIST pMNIST sCIFAR</cell></row><row><cell>LSSL</cell><cell>99.53</cell><cell>98.76</cell><cell>84.65</cell></row><row><cell>LSSL-fixed</cell><cell>99.50</cell><cell>98.60</cell><cell>81.97</cell></row><row><cell>LipschitzRNN</cell><cell>99.4</cell><cell>96.3</cell><cell>64.2</cell></row><row><cell>LMUFFT [12]</cell><cell>-</cell><cell>98.49</cell><cell>-</cell></row><row><cell>UNIcoRNN [47]</cell><cell>-</cell><cell>98.4</cell><cell>-</cell></row><row><cell cols="2">HiPPO-RNN [24] 98.9</cell><cell>98.3</cell><cell>61.1</cell></row><row><cell>URGRU [25]</cell><cell>99.27</cell><cell>96.51</cell><cell>74.4</cell></row><row><cell>IndRNN [34]</cell><cell>99.0</cell><cell>96.0</cell><cell>-</cell></row><row><cell>Dilated RNN [8]</cell><cell>98.0</cell><cell>96.1</cell><cell>-</cell></row><row><cell>r-LSTM [56]</cell><cell>98.4</cell><cell>95.2</cell><cell>72.2</cell></row><row><cell>CKConv [44]</cell><cell>99.32</cell><cell>98.54</cell><cell>63.74</cell></row><row><cell>TrellisNet [4]</cell><cell>99.20</cell><cell>98.13</cell><cell>73.42</cell></row><row><cell>TCN [3]</cell><cell>99.0</cell><cell>97.2</cell><cell>-</cell></row><row><cell>Transformer [56]</cell><cell>98.9</cell><cell>97.9</cell><cell>62.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>(Vital signs prediction.) RMSE for predicting respiratory rate (RR), heart rate (HR), and blood oxygen (SpO2). * indicates our own runs to complete results for the strongest baselines.</figDesc><table><row><cell>Model</cell><cell>RR</cell><cell>HR</cell><cell>SpO2</cell></row><row><cell>LSSL</cell><cell cols="3">0.350 0.432 0.141</cell></row><row><cell>LSSL-fixed</cell><cell cols="3">0.378 0.561 0.221</cell></row><row><cell>UnICORNN [47]</cell><cell>1.06</cell><cell>1.39</cell><cell>0.869*</cell></row><row><cell>coRNN [47]</cell><cell>1.45</cell><cell>1.81</cell><cell>-</cell></row><row><cell>CKConv</cell><cell cols="2">1.214* 2.05*</cell><cell>1.051*</cell></row><row><cell>NRDE [37]</cell><cell>1.49</cell><cell>2.97</cell><cell>1.29</cell></row><row><cell>IndRNN [47]</cell><cell>1.47</cell><cell>2.1</cell><cell>-</cell></row><row><cell>expRNN [47]</cell><cell>1.57</cell><cell>1.87</cell><cell>-</cell></row><row><cell>LSTM</cell><cell>2.28</cell><cell>10.7</cell><cell>-</cell></row><row><cell>Transformer</cell><cell>2.61*</cell><cell>12.2*</cell><cell>3.02*</cell></row><row><cell>XGBoost [55]</cell><cell>1.67</cell><cell>4.72</cell><cell>1.52</cell></row><row><cell cols="2">Random Forest [55] 1.85</cell><cell>5.69</cell><cell>1.74</cell></row><row><cell>Ridge Regress. [55]</cell><cell>3.86</cell><cell>17.3</cell><cell>4.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>:</cell><cell>(Sequential</cell></row><row><cell cols="3">CelebA Classification.)</cell></row><row><cell></cell><cell cols="2">LSSL-f ResNet</cell></row><row><cell>Att.</cell><cell>78.89</cell><cell>81.35</cell></row><row><cell cols="2">MSO 92.36</cell><cell>93.92</cell></row><row><cell cols="2">Smil. 90.95</cell><cell>92.89</cell></row><row><cell>WL</cell><cell>90.57</cell><cell>93.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>(Raw Speech Classification; Timescale Shift.) (Top): Raw signals (length 16000); 1 ? f indicates test-time change in sampling rate by a factor of f . (Bottom): Pre-processed MFCC features used in prior work (length 161). denotes computationally infeasible.</figDesc><table><row><cell></cell><cell cols="7">LSSL LSSL-f CKConv UnICORNN N(C/R)DE ODE-RNN [45] GRU-ODE [16]</cell></row><row><cell>1 ? 1</cell><cell cols="2">95.87 90.64</cell><cell>71.66</cell><cell>11.02</cell><cell>16.49</cell><cell></cell></row><row><cell>1 ? 1 2</cell><cell cols="2">88.66 78.01</cell><cell>65.96</cell><cell>11.07</cell><cell>15.12</cell><cell></cell></row><row><cell cols="2">MFCC 93.58</cell><cell>92.55</cell><cell>95.3</cell><cell>90.64</cell><cell>89.8</cell><cell>65.9</cell><cell>47.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>(Modeling and Computational Benefits of LSSLs.) In each benchmark category, we compare the number of epochs (ep.) it takes a LSSL-f to reach the previous SoTA (PSoTA) results as well as a near-SoTA target. We also report the wall clock time it took to reach PSoTA relative to the previous best model.</figDesc><table><row><cell></cell><cell cols="2">Permuted MNIST</cell><cell cols="3">BDIMC Heart Rate</cell><cell>Speech Commands RAW</cell></row><row><cell></cell><cell cols="2">98% Acc. PSoTA Time</cell><cell cols="3">1.5 RMSE PSoTA Time</cell><cell>65% Acc. PSoTA Time</cell></row><row><cell cols="2">LSSL-fixed 16 ep.</cell><cell cols="2">104 ep. 0.19? 9 ep.</cell><cell>10 ep.</cell><cell cols="2">0.07? 9 ep.</cell><cell>10 ep.</cell><cell>0.14?</cell></row><row><cell>CKConv</cell><cell>118 ep.</cell><cell>200 ep. 1.0?</cell><cell></cell><cell></cell><cell></cell><cell>188 ep.</cell><cell>280 ep. 1.0?</cell></row><row><cell cols="2">UnICORNN 75 ep.</cell><cell></cell><cell>116 ep.</cell><cell cols="2">467 ep. 1.0?</cell></row></table><note>and thus is related to the coefficients x(t) defined by LSSL-f (Section 2, Section 4.1, [24], Appendix D). Consequently, LSSL may be interpreted as automatically learning MFCC-type features in a trainable basis.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Complexity of various sequence models in terms of length (L), batch size (B), and hidden dimension (H). Measures are parameter count, training computation, memory requirement, and inference computation for 1 sample and time-step.</figDesc><table><row><cell></cell><cell>Convolution</cell><cell>RNN</cell></row><row><cell cols="2">Parameters LH 2</cell><cell>H 2</cell></row><row><cell>Training</cell><cell cols="2">BLH 2 + L log(L)(H 2 + BH) BLH 2</cell></row><row><cell>Memory</cell><cell>BLH + LH 2</cell><cell>BLH</cell></row><row><cell>Parallel</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Inference</cell><cell>LH 2</cell><cell>H 2</cell></row><row><cell></cell><cell>Attention</cell><cell>LSSL-naive</cell></row><row><cell cols="2">Parameters H 2</cell><cell>HN + N 2</cell></row><row><cell>Training</cell><cell>B(L 2 H + LH 2 )</cell><cell>HN 3 + LHN 2 + BL log(L)HN</cell></row><row><cell>Memory</cell><cell>B(L 2 + HL)</cell><cell>HN 2 + LHN + BLH</cell></row><row><cell>Parallel</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Inference</cell><cell>L 2 H + H 2 L</cell><cell>HN 2</cell></row><row><cell></cell><cell>LSSL-fixed</cell><cell>LSSL</cell></row><row><cell cols="2">Parameters HN</cell><cell>HN</cell></row><row><cell>Training</cell><cell>BL log(L)HN</cell><cell>BH(N log 2 N + L log L) + BL log(L)H</cell></row><row><cell>Memory</cell><cell>LHN + BLH</cell><cell>BHL</cell></row><row><cell>Parallel</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Inference</cell><cell>HN 2</cell><cell>HN</cell></row></table><note>E.1.2 Summary of Computation Speed for LSSLs and other Mechanisms</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>? Training: O(HN 3 ) to invert compute the matrix A for all H features. O(LHN 2 ) to compute the Krylov matrix C, CA, . . .. O(BL log(L)HN to multiply by B and convolve with u. ? Memory: O(HN 2 ) to store A. O(LHN ) to store the Krylov matrix. O(BLH) to store the inputs/outputs ? Inference: O(HN 2 ) to for MVM by A. Memory: O(LHN ) to store the Krylov matrix (but cached, so no backprop). O(BLH) for inputs/outputs. ? Inference: O(HN 2 ) to for MVM by A. LSSL ? Parameters: O(HN ) for A, B, C, ?t. ? Training: BH ??(N + L) to compute Krylov, O(BL log(L)H) for the convolution.</figDesc><table><row><cell>? Memory: O(BHL) to store Krylov (and inputs/outputs).</cell></row><row><cell>? Inference: O(HN ) to multiply x t [H, N ] by A[H, N, N ]</cell></row><row><cell>LSSL-fixed</cell></row></table><note>? Parameters: O(HN ) in the matrices C.? Training: O(BL log(L)H) to convolve with u.?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Test accuracies for irregularly sampled time series on the CharacterTrajectories dataset. p% denotes percent of data that was randomly dropped.</figDesc><table><row><cell>Model</cell><cell>0%</cell><cell>30%</cell><cell>50%</cell><cell>70%</cell></row><row><cell cols="2">GRU-ODE [16] -</cell><cell>92.6</cell><cell>86.7</cell><cell>89.9</cell></row><row><cell>GRU-?t [31]</cell><cell>-</cell><cell>93.6</cell><cell>91.3</cell><cell>90.4</cell></row><row><cell>GRU-D [9]</cell><cell>-</cell><cell>94.2</cell><cell>90.2</cell><cell>91.9</cell></row><row><cell cols="2">ODE-RNN [45] -</cell><cell>95.4</cell><cell>96.0</cell><cell>95.3</cell></row><row><cell>NCDE [31]</cell><cell>-</cell><cell>98.7</cell><cell>98.8</cell><cell>98.6</cell></row><row><cell>CKCNN [44]</cell><cell cols="3">99.53 98.83 98.60</cell><cell>98.14</cell></row><row><cell>LSSL</cell><cell>99.30</cell><cell cols="3">98.83 98.83 98.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>A and ?t ablations on sCIFAR.</figDesc><table><row><cell cols="2">Learn ?t Fixed ?t</cell></row><row><cell>Learn A 82.70</cell><cell>80.34</cell></row><row><cell>Fixed A 80.61</cell><cell>80.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>A and ?t ablations on SC-Raw. Learn ?t Fixed ?t Learn A 96.07 95.20 Fixed A 91.59 90.51</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To be precise, the measures that correspond to orthogonal polynomials<ref type="bibr" target="#b51">[52]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Available at https://github.com/HazyResearch/state-spaces</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Arjun Desai, Ananya Kumar, Laurel Orr, Sabri Eyuboglu, Dan Fu, Mayee Chen, Sarah Hooper, Simran Arora, and Trenton Chang for helpful feedback on earlier drafts. We thank David Romero and James Morrill for discussions and additional results for baselines used in our experiments. This work was done with the support of Google Cloud credits under HAI proposals 540994170283 and 578192719349. AR and IJ are supported under NSF grant CCF-1763481. KS is supported by the Wu Tsai Neuroscience Interdisciplinary Graduate Fellowship. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Salesforce, Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The Mobilize Center is a Biomedical Technology Resource Center, funded by the NIH National Institute of Biomedical Imaging and Bioengineering through Grant P41EB027060. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mathematical methods for physicists : a comprehensive guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">J?rgen</forename><surname>George Brown) Arfken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">E</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<imprint>
			<publisher>Academic Press</publisher>
			<pubPlace>George B. Arfken, Hans J. Weber, Frank E. Harris; Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note>7th ed. edition, 2013. ISBN 0-12-384654-4</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01576</idno>
		<title level="m">Quasi-recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Numerical methods for ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Butcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolette</forename><surname>Goodwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Wiley Online Library</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Antisymmetricrnn: A dynamical system view on recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An introduction to orthogonal polynomials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chihara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
	<note>ISBN 9780486479293</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallelizing legendre memory unit training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narsimha</forename><surname>Chilkuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Catformer: Designing stable transformers via sensitivity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gru-ode-bayes: Continuous modeling of sporadically-observed time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaak</forename><surname>Edward De Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A two-pronged progress in structured dense matrix vector multiplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Christopher De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Puttagunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1060" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On a new class of structured matrices. Integral Equations and Operator Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gohberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lipschitz recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>N Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Hodgkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic causal modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Penny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1273" to="1302" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximation of dynamical systems by continuous time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Ken-Ichi Funahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix computations</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2013" />
			<publisher>JHU press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving the gating mechanism of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Closed-form continuous-depth models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Liebenwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tschaikowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Teschl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13898</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Liquid time-constant networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Piotr Aleksander Sok??, and Il Memming Park. Gated recurrent units viewed through the lens of continuous time dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<imprint>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rnns incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning long-term dependencies in irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04418</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (IndRNN): Building a longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>978-1-4673-8391-2</idno>
		<ptr target="http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15" />
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural rough differential equations for long time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Murphy Yuezhen Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Horesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12933</idno>
		<title level="m">Recurrent neural networks in the eye of differential equations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Computing with quasiseparable matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Pernet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on International Symposium on Symbolic and Algebraic Computation</title>
		<meeting>the ACM on International Symposium on Symbolic and Algebraic Computation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The matrix cookbook, version 20121115</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kb Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">3274</biblScope>
			<pubPlace>Denmark, Kongens Lyngby, Denmark</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The pytorch-kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert-Jan</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruintjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flexconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08059</idno>
		<title level="m">Continuous kernel convolutions with differentiable kernel sizes</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Ckconv: Continuous kernel convolution for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>David W Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02611</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5321" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Coupled oscillatory recurrent neural network (cornn): An accurate and (gradient) stable architecture for learning long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unicornn: A recurrent model for learning very long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weak supervision as an efficient approach for automated seizure detection in electroencephalography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lee-Messer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The Temple University hospital seizure detection corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinit</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Von Weltin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Riley</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meysam</forename><surname>Golmohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iyad</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Picone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Lian</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-71041-7_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-71041-7_3" />
	</analytic>
	<monogr>
		<title level="j">Orthogonal Polynomials and Related Approximation Results</title>
		<imprint>
			<biblScope unit="page" from="47" to="140" />
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A computational introduction to number theory and algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Shoup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Orthogonal Polynomials. Number v.23 in American Mathematical Society colloquium publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szeg?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">9780821889527</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Orthogonal Polynomials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szeg?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Colloquium publications</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1975" />
			<biblScope unit="volume">9780821810231</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Can recurrent neural networks warp time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Time series extrinsic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wei Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-021-00745-9</idno>
		<ptr target="https://doi.org/10.1007/s10618-021-00745-9" />
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning longer-term dependencies in RNNs with auxiliary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Legendre memory units: Continuous-time representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Kaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15544" to="15553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Linear state-space control systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Inverting modified matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">106</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Memorandum report</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Performance recovery in digital implementation of analogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2207" to="2223" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
