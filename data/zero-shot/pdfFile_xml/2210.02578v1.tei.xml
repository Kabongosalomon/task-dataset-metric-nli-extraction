<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AOE-Net: Entities Interactions Modeling with Adaptive Attention Mechanism for Temporal Action Proposals Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AICV Lab</orgName>
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<settlement>Fayetteville</settlement>
									<region>Arkansas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Truong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AICV Lab</orgName>
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<settlement>Fayetteville</settlement>
									<region>Arkansas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashu</forename><surname>Yamazaki</surname></persName>
							<email>kyamazak@uark.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AICV Lab</orgName>
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<settlement>Fayetteville</settlement>
									<region>Arkansas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
							<email>bhiksha@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AICV Lab</orgName>
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<settlement>Fayetteville</settlement>
									<region>Arkansas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AOE-Net: Entities Interactions Modeling with Adaptive Attention Mechanism for Temporal Action Proposals Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Springer Nature 2021 L A T E X template ? These authors contributed equally to this work</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>temporal action proposal</term>
					<term>temporal action detection</term>
					<term>human perceiving process</term>
					<term>attention mechanism</term>
					<term>human</term>
					<term>objects</term>
					<term>environment</term>
					<term>interaction</term>
					<term>video understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action proposal generation (TAPG) is a challenging task, which requires localizing action intervals in an untrimmed video. Intuitively, we as humans, perceive an action through the interactions between actors, relevant objects, and the surrounding environment. Despite the significant progress of TAPG, a vast majority of existing methods ignore the aforementioned principle of the human perceiving process by applying a backbone network into a given video as a black-box. In this paper, we propose to model these interactions with a multi-modal representation network, namely, Actors-Objects-Environment Interaction Network (AOE-Net). Our AOE-Net consists of two modules, i.e., perceptionbased multi-modal representation (PMR) and boundary-matching module (BMM). Additionally, we introduce adaptive attention mechanism (AAM) in PMR to focus only on main actors (or relevant objects) and model the relationships among them. PMR module represents each video snippet by a visual-linguistic feature, in which main actors and surrounding environment are represented by visual information, whereas relevant objects are depicted by linguistic features through an image-text model. BMM module processes the sequence of visual-linguistic features 1 arXiv:2210.02578v1 [cs.CV] 5 Oct 2022 Springer Nature 2021 L A T E X template 2</p><p>Actors-Objects-Environment Network as its input and generates action proposals. Comprehensive experiments and extensive ablation studies on ActivityNet-1.3 and THUMOS-14 datasets show that our proposed AOE-Net outperforms previous stateof-the-art methods with remarkable performance and generalization for both TAPG and temporal action detection. To prove the robustness and effectiveness of AOE-Net, we further conduct an ablation study on egocentric videos, i.e. EPIC-KITCHENS 100 dataset. Our source code is publicly available at https://github.com/UARK-AICV/AOE-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given an untrimmed video, TAPG targets localizing temporal segments with specific starting and ending timestamps for each action or activity appearing in the video. TAPG has emerged as one of the most important problems in video analysis and understanding <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. More specifically, TAPG is a key module for other downstream tasks including temporal action detection (TAD) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, video captioning <ref type="bibr" target="#b13">[14]</ref>, action recognition <ref type="bibr" target="#b14">[15]</ref>, etc. In general, TAPG approaches can be divided into two main categories i.e. anchor-based approaches and boundary-based approaches. Inspired by anchor-based object detection in 2D images, anchor-based TAPG methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> pre-define a set of anchor segments and try to fit them into groundtruth action segments in videos. Even though a regression network has been applied to refine the proposals, anchor-based TAPG methods cannot fit all groundtruth actions with diverse lengths by a finite number of anchors. Boundary-based TAPG methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> address the previous limitations by first separately localizing starting and ending timestamps of exiting actions and then fusing them by a follow-up action evaluation module.</p><p>Despite good achievements on benchmarking datasets, boundary-based approaches <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> still have some limitations, in which the most major one is the overlooked video representation. In such designs, a video is split into consecutive snippets (or clips, chunks) of ? frames; then, a 3D convolutional backbone network <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> is simply applied to the entire spatial domain of each snippet to extract its visual representation. However, not all spatial regions in a snippet are relevant nor contribute to the formation of an action. Specifically, as shown in <ref type="figure">Fig. 1(a)</ref>, an actor itself rather than spatial environment influences the action i.e. jogging can be created anywhere regardless of environment. To address those limitations, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> recently propose to separately represent each snippet by both local actors features and global surrounding environment features. Both features are combined by a self-attention module to flexibly balance between local and global visual representation. Although the improvements <ref type="figure">Fig. 1</ref>: Most existing TAPG methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> apply a 3D backbone network to entire spatial domain. However, as shown in (a), actors contribute more importance to an action than environment itself. The SOTA in TAPG <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> extract both local humans features and global environment feature; however, they are unable to either distinguish between main actors who actually commit actions and inessential actors (b) or address egocentric videos where actors are not visible in the scene (c). Our proposed AOE-Net, as illustrated in (d), consists of the global visual environment, local visual main actors features, and linguistic relevant objects features. reported <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> are very promising, those paradigms are unable to discriminate main actors who actually commit actions from the inessential actors, as shown in <ref type="figure">Fig. 1(b)</ref>. Additionally, both AEN and ABN may not be helpful in many videos where actions are not dependent on the presence of humans, i.e., egocentric videos, as shown in <ref type="figure">Fig. 1(c)</ref>.</p><p>Intuitively, besides actors and the environment, we, as human beings, also perceive an action through the presence of relevant objects and their interactions with the surroundings. However, unlike actors, relevant objects are often tiny with few pixels (e.g., less than 20 ? 20 pixels). This causes the problem of vanishing information if we obtain objects from visual feature maps extracted by some typical CNN-based backbone. A possible solution is to leverage "vision and language" methods <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> to represent objects by linguistic features. By this way, information about the presence of every object is fully preserved. We leverage CLIP <ref type="bibr" target="#b25">[26]</ref>, which is a powerful model to associate both vision and language, to extract linguistic features from relevant objects existing in video snippets. As a result, relevant objects are represented by linguistic features whereas environment and main actors are represented by visual features. <ref type="figure">Fig. 1(d)</ref> illustrates our intuition for video representation.</p><p>In this paper, we propose a novel Actors-Objects-Environment Interaction Network (AOE-Net) as a simulation of human perception in modeling the video by visual features from main actors and environment as well as linguistic features from relevant objects. Our AOE-Net consists of two main modules, i.e., (i) perception-based multi-modal representation (PMR) to extract visuallinguistic (V-L) feature and model actors-objects-environment relations in each snippet, (ii) boundary-matching module (BMM) to localize and generate action proposals. To select only main actors along with choosing relevant objects and extract mutual relationships among each of these entities, we propose a novel adaptive attention mechanism (AAM). Our contributions are summarized as follows:</p><p>? We propose a novel network, AOE-Net, which follows the human perception process to understand human actions. ? We introduce a novel and effective attention module, AAM, which simultaneously selects main actors (or relevant objects) and eliminates inessential actor(s) (or objects), then, extracts semantic relations between main actors (or relevant objects). ? Our proposed AOE-Net achieves the SOTA performance on common benchmarking datasets of ActivityNet-  <ref type="figure">Fig. 2</ref>: The architecture of our proposed PMR. Given a ?-snippet s i , the V-L feature is obtained by four modules: (i) actors beholder to extract local visual action feature f a ; (ii) environment beholder to extract global visual environment feature f e ; (iii) objects beholder to extract linguistic object feature f o , and (iv) actors-objects-environment interaction beholder to model V-L feature as the interaction between actors, objects and the environment.</p><p>? Extensive ablation studies and qualitative analysis of AAM are also provided to investigate its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Temporal Action Proposal Generation (TAPG) As stated above, prior works can be categorized into two groups: anchor-based and boundary-based. Anchor-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b26">27]</ref> are inspired by anchor-based object detection methods <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>, i.e., they pre-define a set of fixed segments and learn to fit them into groundtruth action segments in the video. Among them, <ref type="bibr" target="#b26">[27]</ref> uses space-time interest points and dictionary learning. <ref type="bibr" target="#b7">[8]</ref> makes use of C3D <ref type="bibr" target="#b30">[31]</ref> to build a binary classification task to generate proposal segments. TURN <ref type="bibr" target="#b18">[19]</ref> divides a video into units and employs unit-level features with a temporal regression. In some of those anchor-based approaches, a regression network is also applied to refine the proposals. However, the groundtruth proposals vary a lot in terms of duration, which discourages the performance of anchor-based methods. Boundary-based methods <ref type="bibr">[1-7, 32, 33]</ref> resolve this problem by initially localizing the starting and ending timestamps of all actions appearing in the video and then matching them by an action evaluation module, which estimates the actionness score of every possible pair of boundaries. Among them, <ref type="bibr" target="#b31">[32]</ref> adopts a watershed algorithm to group contiguous high-score as proposals. <ref type="bibr" target="#b0">[1]</ref> first predicts each temporal point as either starting or ending point of an action and then evaluates proposals. <ref type="bibr" target="#b9">[10]</ref> combines both anchor-based and boundary-based approaches to reorganize the candidate set. <ref type="bibr" target="#b32">[33]</ref> employs a bilinear matching module to perform TAPG at two distinct granularities. Generally, the boundary-based methods provide superior performance over anchor-based methods.</p><p>Our AOE-Net belongs to the second category. Different from existing boundary-based methods, AOE-Net is based on V-L feature by leveraging the human perception principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Module</head><p>Attention Models (Atts) have a long history <ref type="bibr" target="#b33">[34]</ref> and have become an important concept in neural networks <ref type="bibr" target="#b34">[35]</ref>. Atts can be divided into two main groups: Soft-Attention Models (Soft-Atts) and Hard-Attention Models (Hard-Atts). <ref type="bibr" target="#b35">[36]</ref> was one of the first Soft-Atts that was applied to machine translation. Because of its differentiable architecture, which helps the whole model learn in an endto-end fashion, Soft-Atts has become an essential component in a large number of applications (e.g., speech <ref type="bibr" target="#b36">[37]</ref>, NLP <ref type="bibr" target="#b37">[38]</ref>, computer vision <ref type="bibr" target="#b38">[39]</ref>). Because selfattention networks <ref type="bibr" target="#b39">[40]</ref> are able to learn the relations between input elements regardless of their quantity, their popularity is increasing in not only language models but also in computer vision. Hard-Atts was first introduced in <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b41">[42]</ref> for digit and object classifications, respectively. Hard-Atts aims to mask out irrelevant elements of the inputs by sampling the input elements with probabilities to reduce the distractions. This is an advanced benefit over Soft-Atts; however, Hard-Att in <ref type="bibr" target="#b40">[41]</ref> is indifferentiable. Recently, <ref type="bibr" target="#b42">[43]</ref> proposes a Hard-Att that can be trained by normal gradient back-propagation, with a fundamental observation that the L2-norm values of more important features are usually higher than those of less important features in a feature map.</p><p>In this work, we propose an adaptive attention model (AAM), which leverages both the differentiable Hard-Att <ref type="bibr" target="#b42">[43]</ref> and the self-attention network <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>Given an input video V = {v i } N i=1 , where N is the number of frames, we follow the standard settings from existing works to divide V into a sequence of ??frame snippets s i | T i=1 . Each snippet s i consists of ? consecutive frames, therefore, V has a total of T = N ? snippets. Let ?(.) be an encoding function to extract the visual feature f i of a ?-frame snippet s i ; the video V can be represented as F as follows:</p><formula xml:id="formula_0">F = {f i } T i=1 , where f i = ?(s i )<label>(1)</label></formula><p>Different from the existing works <ref type="bibr">[1-5, 5, 44-47]</ref>, which simply define ?(.) as a pre-trained backbone network (e.g., C3D <ref type="bibr" target="#b19">[20]</ref>, 2Stream <ref type="bibr" target="#b21">[22]</ref>, Slow-fast <ref type="bibr" target="#b22">[23]</ref>), we model ?(.) by the proposed PMR, which is capable of representing visual information of the snippet in both global and local perspectives, using both visual and linguistic information.</p><p>Given the feature sequence F, the boundary-matching module (BMM) has a role of localizing action proposals. In this section, we introduce PMR in Sub-Sec. 3.1. Then, we present the boundary-matching module in Sub-Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Perception-based Multi-modal Representation (PMR)</head><p>PMR aims to extract features based on the principle of how a human perceives an action (i.e., identify the main actors at each temporal period, recognize relevant objects and understand interactions between main actors, relevant objects, and the environment) to specify when the action starts and ends.</p><p>In this paper, we are interested in discovering two modalities of vision and language to extract V-L feature. PMR consists of four main components: (i) environment beholder; (ii) actors beholder; (iii) objects beholder; and (iv) actors-objects-environment interaction beholder. The overall architecture of PMR is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Environment Beholder:</head><p>This component has the role of capturing the global visual information of an input ?-frame snippet. To extract the spatio-temporal information of the snippet, we adopt a 3D network pre-trained on action recognition benchmarking datasets as a backbone feature extractor. First, the snippet is processed through all convolutional blocks of the 3D network to obtain a feature map F M at the final block; then, an average pooling operator is employed to produce a spatio-temporal feature vector f e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Actors Beholder:</head><p>This component semantically extracts visual main actors representation f a . In most cases, an action cannot happen if a human (main actor) is absent notwithstanding environments ( <ref type="figure">Fig. 1(a)</ref>). However, when an action occurs, it does not necessarily signal that every actor in the scene has committed the action ( <ref type="figure">Fig. 1(b)</ref>). Herein, the actors beholder first localizes all existing actors in a ?-frame snippet. To do so, we apply a human detector onto the middle frame assuming that the actors would not move fast enough to be mislocated with a small ?. We denote B = {b i } N B i=1 as a set of detected human bounding boxes, where N B ? 0. Afterwards, each of the detected bounding boxes, b i , is aligned onto feature map F M , which is obtained by the 3D network backbone from environment beholder, using RoIAlign <ref type="bibr" target="#b47">[48]</ref>. Then, each bounding box feature is average-pooled into a single feature vector f a i . Finally, we obtain a set of actor features F a = {f a i } N B i=1 . To adaptively select an arbitrary number of main actors and extract their mutual relationships, we apply our proposed AAM (described in Sub-Sec. 3.2), which is elaborately explained in Sub-Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Objects Beholder:</head><p>Different from the environment and actors, objects may be tiny with a few pixels and therefore may vanish in the feature map F M . Hence, in this objects beholder, we propose to use linguistic information from relevant objects,  which is considerably more informative than visual information. We leverage CLIP <ref type="bibr" target="#b25">[26]</ref> as a pre-trained model to extract linguistic information. CLIP <ref type="bibr" target="#b25">[26]</ref> is trained with a large number of image and description pairs, thus, CLIP effectively learns the correlation between the global scene information and local scene elements. Many scene elements are presented as small objects in the scene and they are hardly captured by an object detector. With CLIP, scene elements can be inferred by globally encoding the entire scene information. Thus, once the entire scene is captured, the small objects of scene elements are obtained accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask-RCNN CLIP</head><p>For example, given an image of people playing tennis shown in <ref type="figure" target="#fig_1">Fig. 3</ref> as below, it is unfeasible to detect a small object such as a tennis ball using an object detector. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref> (left), Mask-RCNN <ref type="bibr" target="#b48">[49]</ref> is only able to detect humans and tennis racket while the tennis ball is not captured. Whereas, CLIP already encoded tennis scene elements including tennis ball when modeling tennis games. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref> (right), CLIP captures tennis ball and other related objects such as basket, court, fence, etc. In this example, we choose top K = 40 detected objects by CLIP. The most relevant objects selected by AMM are shown in bold red.</p><p>Object text extraction is the first step of this component as illustrated in <ref type="figure">Fig. 4</ref>. However, our task just focuses on human activities and their related objects. Therefore, we utilize the corpus of ActivityNet Captioning dataset <ref type="bibr" target="#b13">[14]</ref> to construct the object text vocabulary</p><formula xml:id="formula_1">T = {T i } D i=1</formula><p>. ActivityNet Captioning dataset <ref type="bibr" target="#b13">[14]</ref> annotates the same set of videos in ActivityNet-1.3 <ref type="bibr" target="#b11">[12]</ref>. In its training split, there are a total of 37,447 sentences to densely describe every event in each video, these captions is composed by a vocabulary of up to 10,648 words. In order to create a vocabulary which majorly contains objects and human activities, we eliminate stop words, pronouns, numbers, and infrequent words (which appears 5 times or lower in the whole dataset). Afterwards, we remove words that do not present in the vocabulary used by CLIP <ref type="bibr" target="#b25">[26]</ref>. Fortunately, thanks to the byte pair <ref type="bibr" target="#b48">[49]</ref> encoding used in CLIP <ref type="bibr" target="#b25">[26]</ref>, there are very few words that are removed after in this step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 4:</head><p>Illustration of object text extraction where Encoding, Embedding, and CLIP are pre-trained models from <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b25">[26]</ref>, respectively.</p><p>To this end, the vocabulary for our objects beholder consists of D = 3, 544 words is extracted from the ActivityNet Captioning dataset <ref type="bibr" target="#b13">[14]</ref>.</p><p>Each word T i ? T is encoded by a Transformer network <ref type="bibr" target="#b39">[40]</ref> into a text feature T f i . Let W t be a text projection matrix pre-trained by CLIP, the embedding text vocabulary is computed as</p><formula xml:id="formula_2">T e = W t ?T f , where T f = {T f i } D i=1</formula><p>. Let W i be an image projection matrix pre-trained by CLIP, a middle frame I of the ?-frame snippet is first encoded by Vision Transformer <ref type="bibr" target="#b49">[50]</ref> to extract visual feature I f , and then embedded by W i , i.e., I e = W i ? I f . The pairwise cosine similarities between embedded I e and T e is then computed. Top K similarity scores are chosen as output objects text represented by feature</p><formula xml:id="formula_3">F o = {T f i } K i=1</formula><p>. Ablation study on K will be discussed in Sub-Sec 4.5.3. Similar to the actors beholder, we apply the proposed AAM (described in Sub-Sec. 3.2) to select relevant objects from F o , then model the semantic relations among them, and finally obtain linguistic feature f o .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Actors-Objects-Environment (AOE) Beholder:</head><p>This component aims to model the relations between global visual environment feature f e , local visual of main actors features f a , and linguistic relevant objects features f o . Firstly, we stack three types of features together as</p><formula xml:id="formula_4">F aoe = [f a , f o , f e ].</formula><p>Then, we employ the self-attention model <ref type="bibr" target="#b39">[40]</ref> followed by an average pooling layer to fuse the stack of features F aoe into f i . f i is a V-L feature that represents the input snippet s i through both visual (environment and actors modalities) and linguistic (objects modality) ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Attention Mechanism (AAM)</head><p>Given M actors (or objects) obtained in the input snippet, only a few of those, i.e.,M main actors (or relevant objects), actually contribute to an action. BecauseM is unknown and continuously changes throughout the input video, we propose AAM that inherits the merits from adaptive hard attention <ref type="bibr" target="#b50">[51]</ref> to select an arbitrary number of main actors (or objects) and a soft selfattention mechanism <ref type="bibr" target="#b39">[40]</ref> to extract relationships among them. Take actors Data: Feature vector f e and features set F a represent environment and all actors that appear in an input snippet, respectively. Result: Feature vector f a represents main actors. if h a i &gt; ? then <ref type="bibr">12:</ref> append f a i toF a 13:</p><formula xml:id="formula_5">1:f e ? M LP ?e (f e ) 2: setF a ,</formula><formula xml:id="formula_6">end if 14: end for 15: f a ? self attention(F a )</formula><p>beholder as an instance, AAM is described by the pseudocode in Algorithm 1 and illustrated in <ref type="figure" target="#fig_2">Fig. 5</ref>.</p><p>To begin, the environment feature f e and actors features F a are embedded into the same dimensional space by a multi-layer perceptrons (MLPs) parameterized by ? e and ? a , respectively:</p><formula xml:id="formula_7">f e = M LP ?e (f e )<label>(2)</label></formula><formula xml:id="formula_8">F a = {f a i } M i=1 wheref a i = M LP ?a (f a i )<label>(3)</label></formula><p>Then,f e is combined with each featuref a i ofF a by element-wise addition (i.e., ?) to form a collaborative feature. Afterwards, we can compute the attention score h a i corresponding tof a i using the L2-norm of its corresponding collaborative feature. These computational steps can be presented through the following equation:</p><formula xml:id="formula_9">h a i =||f a i ?f e || 2<label>(4)</label></formula><p>It is proven in <ref type="bibr" target="#b50">[51]</ref> that features with the greater L2-norm values carry more meaningful information and better contribute to later modules.</p><p>Next, we re-scale all L2-norm values by softmax function to be summed up to 1.0, because L2-norm values are unbounded:</p><formula xml:id="formula_10">H a = {h a i } M i=1 , where h a i = e h a i ? M i=1 e h a i<label>(5)</label></formula><p>To obtain the features of an arbitrary number of main actors, we create an adaptive threshold based on the total number of actors ? = 1 |F a | and retrieve only features f a i ? F a with corresponding score higher than ? :</p><formula xml:id="formula_11">F a = {f a i | h a i ? ? }<label>(6)</label></formula><p>After that, we fuse a set of main actors feature vectorsF a into a single feature vector f a by leveraging the self-attention Transformer Encoder proposed in <ref type="bibr" target="#b39">[40]</ref>.</p><p>In the case of objects beholder, the input actors features F a is replaced by the objects features F o .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Boundary-Matching Module (BMM)</head><p>BMM is responsible for localizing action boundary and generating action proposals in videos. In our AOE-Net, BMM module is adopted from previous works i.e. BSN <ref type="bibr" target="#b0">[1]</ref>, BMN <ref type="bibr" target="#b2">[3]</ref>, ABN <ref type="bibr" target="#b6">[7]</ref>, AEN <ref type="bibr" target="#b5">[6]</ref>, AEI <ref type="bibr" target="#b51">[52]</ref> because of its standard and simple design. BMM takes the output V-L features sequence F = {f i } T i=1 from PMR module as its input. Our BMM contains three components: semantic modeling, temporal estimation (TE), and proposal estimation (PE) as illustrated in <ref type="figure" target="#fig_3">Fig. 6</ref>. The first component models the semantic relationship between snippets. The TE component assesses each snippet</p><formula xml:id="formula_12">s i | T i=1</formula><p>to evaluate probabilities of action starting (P S i ) and action ending (P E i ) that exist in s i . Meanwhile, the PE component evaluates every interval <ref type="bibr">[i, j]</ref> in the video to estimate its actionness score P A i,d , where d = j ? i. The detailed architecture of BMM is provided in <ref type="table" target="#tab_3">Table 1</ref>. The semantic modeling component is implemented by two 1-D Conv. layers and outputs a feature map O 2 ? R 128?T . The later components, TE and PE, take O 2 as their input and generate O T ? R 2?T and O P ? R 1?D?T , respectively. The output O T presents probabilities of action starts (P S ? R T ) and action ends (P E ? R T ). The output O P contains actionness scores P A ? R D?T .  At the inference stage, we search through P S and P E to select temporal locations i whose P S i or P E i are local maximums to form sets of potential starting and ending temporal locations, respectively. Then, starting and ending locations (s, e) (e.g. s ? e ? T ) are paired and become a candidate proposal with the score s = P S s ? P E e ? P A s,e?s . Based on the timestamps and scores of candidate proposals, we finally apply NMS <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> to produce the final set of temporal action proposals.</p><formula xml:id="formula_13">F : F ? T O 1 : 256 ? T 1DConv. 128 ? 3/1, ReLU O 1 : 256 ? T O 2 : 128 ? T 1DConv. 256 ? 3/1, ReLU O 2 : 128 ? T O 3 : 256 ? T 1DConv. 2 ? 3/1 , Sigmoid O 3 : 256 ? T O T : 2 ? T Matching layer O 2 : 128 ? T O 5 : 128?32?D? T 3DConv. 512 ? 32 ? 1 ? 1/(32, 0, 0) , ReLU O 5 : 128?32?D?T O 6 : 512 ? 1 ? D ? T squeeze O 6 : 512 ? 1 ? D ? T O 7 : 512 ? D ? T 2DConv. 128 ? 1 ? 1/(0, 0) , ReLU O 7 : 512 ? D ? T O 8 : 128 ? D ? T 2DConv. 128 ? 3 ? 3/(1, 1) , ReLU O 8 : 128 ? D ? T O 9 : 128 ? D ? T 2DConv. 2 ? 1 ? 1/(0, 0) , Sigmoid O 9 : 128 ? D ? T O P : 1 ? D ? T -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Methodology</head><p>Training Labels Generation from Groundtruth: We follow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> to generate the ground truth labels for training process including starting labels, ending labels for L s , L e and duration labels for L act .</p><p>The starting and ending labels are generated for every snippet of the video, which are called L S = {l s n } T n=1 and L E = {l e n } T n=1 , respectively. The boundary timestamps (starting and ending) of every action instance a i = (s i , e i ) are rescaled into T -snippet range by multiplying them with T ?fps L where fps is the frame rate of the video and the action instance a i ? A, A = {a i } M i=1 . After rescaling, the action instance a i becomes a new action instance a ? i = (s ? i , e ? i ). For every snippet t n ? T , we denote a temporal region r n = [t n ? 1, t n + 1]. Analogously, for every pair of boundaries (s ? i , e ? i ) of action a ? i , we denote regions</p><formula xml:id="formula_14">r s i = [s ? i ? 3 2 , s ? i + 3 2 ]</formula><p>and r e i = [e ? i ? 3 2 , e ? i + 3 2 ] as their corresponding starting region and ending region. By this formulation, we have two sets of regions</p><formula xml:id="formula_15">R S = {r s i } M i=1 and R E = {r e i } M i=1</formula><p>for starting and ending boundaries, respectively. Finally, starting label l s n and ending label l e n of a snippet t n are calculated by the following functions:</p><formula xml:id="formula_16">l s n = ? ? ? 1, M i=1 |rn?r s i | |r s i | ? 0.5 0, otherwise l e n = ? ? ? 1, M i=1 |rn?r e i | |r e i | ? 0.5 0, otherwise</formula><p>The duration labels for a video are gathered into a matrix L D ? {0, 1} D?T where D is the maximum length of proposals being considered in number of snippets, as suggested in <ref type="bibr" target="#b2">[3]</ref>, we set D = T in all of our experiments. With an element at position (t i , t j ) stands for a proposal action a p = (t s = tj ?T tv , t e = (tj +ti)?T tv ), it will be assigned by 1 if its Interaction-over-Union with any ground truth action in A = {a i } M i=1 reaches a local maximum, or 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions:</head><p>To train our AOE-Net with the groundtruth labels, we define the loss function L AOE as in Eq. 7 where L s , L e , and L act are loss functions corresponding to starting boundary, ending boundary and actionness score.</p><formula xml:id="formula_17">L AOE = L s (P S , L S ) + L e (P E , L E ) + L act (P A , L A )<label>(7)</label></formula><p>We use weighted binary log-likelihood loss L wb for L s and L e , which is defined as follows:</p><formula xml:id="formula_18">L wb (P, L) = N i=1 L i N + log P i + (1 ? L i ) N ? log(1 ? P i )<label>(8)</label></formula><p>where N + and N ? are the number of positives and negatives in groundtruth labels, respectively. Conversely, L act (P, L) is defined as follows:</p><p>L act (P, L) = L wb (P, L) + ?L 2 (P, L)</p><p>, where L 2 is the mean squared error loss and ? is set to 10.</p><p>To reduce time cost in the training phase of our proposed AOE-Net, actors features F a , objects features set F o and environment feature f e are extracted in advance. Then, AAM and AOE Interaction beholder of PMR module is trained with BMM module in an end-to-end framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>Datasets Our experiments on TAPG and TAD are carried out using both ActivityNet-1.3 <ref type="bibr" target="#b11">[12]</ref> and THUMOS-14 <ref type="bibr" target="#b12">[13]</ref> datasets. The former features 20K videos and 200 activities that have been annotated, whereas the latter has 414 videos and 20 types of actions. We follow prior works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> for videos preprocessing with the snippet length set to ? = 16 in all experiments. To prove the effectiveness of our proposed AOE-Net on egocentric videos, we also conduct an experiment on TAPG task of EPIC-KITCHENS 100 dataset <ref type="bibr" target="#b54">[55]</ref>, which consists of 100 video hours, 20M frames, 90K actions in 700 variable-length videos captured in 45 environments using head-mounted cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>In TAPG, we use two common metrics, i.e., AR@AN and AUC, to evaluate the proposed AOE-Net as well as compare it with SOTA approaches. The former metric is the average recall (AR) calculated at a specific average number of proposals (AN) preserved by each video. The latter one is the area under the AR versus the AN curve score. AR@100 and AUC are the most commonly used metrics in ActivityNet-1.3. In THUMOS-14, however, just AR@AN is utilized to compare approaches; nonetheless, multiple AN are chosen from a list of <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">100,</ref><ref type="bibr">200,</ref><ref type="bibr">500,</ref><ref type="bibr">1000]</ref>.</p><p>In TAD, we use mean Average Precision (mAP) to benchmark approaches. Following the common settings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b55">56]</ref>, we evaluate TAD methods in ActivityNet-1.3 with tIoU thresholds of {0.5, 0.75, 0.95}, and average mAP. Whereas, TAD methods in THUMOS-14 are evaluated with tIoU thresholds of {0.3, 0.4, 0.5, 0.6, 0.7}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>To extract visual features from videos, we use a C3D <ref type="bibr" target="#b19">[20]</ref> network pre-trained on Kinetics-400 <ref type="bibr" target="#b14">[15]</ref> as the backbone network in all experiments on both ActivityNet-1.3 <ref type="bibr" target="#b11">[12]</ref> and THUMOS-14 <ref type="bibr" target="#b12">[13]</ref> (unless stated otherwise). The dimensions of the features extracted from the C3D backbone are 2048.  <ref type="table">Table 2</ref>: TAPG comparisons on ActivityNet-1.3 <ref type="bibr" target="#b11">[12]</ref> in terms of AR@100 and AUC on validation set and AUC on testing set.</p><p>In the objects beholder, to extract object text, we use CLIP <ref type="bibr" target="#b25">[26]</ref> that was pre-trained on a large-scale dataset of 400M image-text pairs crawled from the Internet. The text feature and image feature are encoded by Transformer <ref type="bibr" target="#b39">[40]</ref> and Vision Transformer <ref type="bibr" target="#b49">[50]</ref> networks, respectively. In the actors beholder, to detect humans, we use a Faster-RCNN model <ref type="bibr" target="#b27">[28]</ref> that has been pre-trained on the COCO dataset <ref type="bibr" target="#b63">[64]</ref>. Adam optimizer was used to train our AOE-Net, and the initial learning rate is set to 0.0001 for ActivityNet-1.3 and 0.001 for THUMOS-14.</p><p>On ActivityNet-1.3, Soft-NMS (SNMS) <ref type="bibr" target="#b52">[53]</ref> is used in post-processing for all experiments in TAPG and TAD. On THUMOS-14, following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, both Soft-NMS <ref type="bibr" target="#b52">[53]</ref> and NMS <ref type="bibr" target="#b53">[54]</ref> are utilized in post-processing of TAPG, whereas only NMS is applied in TAD. In the following experimental results, we emphasize the best performance in bold and the second-best performance in underline. <ref type="table">Table 2</ref> presents TAPG comparison on both validation and testing sets of ActivityNet-1.3 <ref type="bibr" target="#b11">[12]</ref>. The experimental results demonstrate that our approach AOE-Net with C3D <ref type="bibr" target="#b19">[20]</ref> feature outperforms the existing methods in terms of AR@100 and AUC by an adequate margin. <ref type="table" target="#tab_6">Table 3</ref> shows the TAPG comparison on THUMOS-14. Compared to the existing TAPG methods, our AOE-Net performs very competitive on AR@ANs metrics with both SNMS and NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance and Comparison on TAPG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Venue &amp; Year Feature @50 @100 @200 @500 @1000 Average   <ref type="bibr" target="#b51">[52]</ref> and followed closely by ABN <ref type="bibr" target="#b5">[6]</ref>, both of which also incorporate local actors and global environment. This experiment strongly supports our observation and motivation on using the human perception principle to analyze human actions in untrimmed videos. Beside solely evaluating AOE-Net on TAPG and TAD tasks, the effects of different backbone features to our AOE-Net also worth an investigation. The performance of our proposed AOE-Net network on different features, i.e., C3D <ref type="bibr" target="#b19">[20]</ref>, 2Stream <ref type="bibr" target="#b21">[22]</ref> and Slowfast <ref type="bibr" target="#b22">[23]</ref>, with the features dimensions are 2048, 2314 and 400, respectively, are reported in the bottom part of <ref type="table">Table 2</ref> on TAPG task of ActivityNet-1.3 dataset <ref type="bibr" target="#b11">[12]</ref>. As demonstrated, we notice that the performance with C3D <ref type="bibr" target="#b19">[20]</ref> features are state-of-the-art, while the performance with SlowFast <ref type="bibr" target="#b22">[23]</ref> features are closely behind. Whereas, the performance with 2Stream <ref type="bibr" target="#b21">[22]</ref> features is the worst in three types of backbone features.</p><p>In TAPG, generalizability is also a significant criterion to evaluate a method. Following the same experiment setup in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b44">45]</ref>, we conduct this study on ActivityNet-1.3 with two subsets, i.e., Seen: "Sports, Exercises, and Recreation" and Unseen: "Socializing, Relaxing, and Leisure". Our AOE-Net is trained on Unseen+Seen and Seen training sets, separately, and then  evaluated on the Seen and Unseen validation sets. <ref type="figure" target="#fig_4">Fig. 7</ref> provides the performance comparison and visualization between AOE-Net with other SOTA methods. In each chart on the right, the performance of AOE-Net is shown in the last columns, which demonstrates that AOE-Net is superior to other SOTA methods. <ref type="figure" target="#fig_4">Fig. 7</ref> also shows that our AOE-Net achieves good performances on Seen validation set with an acceptable drop on Unseen validation set on both training configurations, suggesting that our AOE-Net is highly generalizable to unseen action types.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance and Comparison on TAD</head><p>For a fair comparison, we follow the experiment settings in <ref type="bibr">[1, 3-6, 33, 46, 47]</ref> to produce labels for action proposals produced by our AOE-Net. On AcitivityNet-1.3, we adopt the top-1 video-level classification results generated by the method in <ref type="bibr" target="#b66">[67]</ref> for our proposals. Whereas on THUMOS-14, we instead label our action proposals with either UntrimmedNet <ref type="bibr" target="#b67">[68]</ref> (top-2 classification results) or P-GCN <ref type="bibr" target="#b65">[66]</ref>. <ref type="table" target="#tab_9">Table 4</ref> shows TAD performance comparison between AOE-Net and other SOTA methods on ActivityNet-1.3 validation set. The results emphasize that our method outperforms SOTA methods on multiple tIoU thresholds. The experiment results on THUMOS-14 test set in <ref type="table" target="#tab_11">Table 5</ref> demonstrate that our AOE-Net is superior to other SOTA methods on most of the metrics with both classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We further conduct a rich ablation study to show the effectiveness of each component in the proposed AOE-Net as well as the robustness of AOE-Net to egocentric videos. We also report the network efficiency and AOE-Net performance with different settings of hyper-parameter K. Additional ablation study will be included in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Contribution of each beholder</head><p>We examine TAPG performance on THUMOS-14 with different network settings as given in <ref type="table" target="#tab_12">Table 6</ref>. While the performance of each individual beholder is shown in the Exps.#1-3, different combinations of features are given in Exps.#4-7. This emphasizes the important contribution of actors and objects in understanding human action. Comparisons between Exps.(#4 vs. #6) and (#5 vs. #7) highlight the strong impact of AAM.    <ref type="table">Table 7</ref>: TAPG compare between AAM with attention <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>In Exp.#1 and Exp.#3, as Environment Beholder is not presented, AAM consequently cannot be applied because it requires environment feature as one of its input. Therefore, we replace AAM by a simple soft self-attention layer  followed by an average pooling operation to fuse multiple actors together. Likewise, in Exp.#4 and Exp.#5 we also perform the above replacement strategy to emphasize the effectiveness of AAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Effectiveness of AAM</head><p>We continue studying the effectiveness of the proposed AAM in TAPG task on both ActivityNet-1.3 and THUMOS-14 by comparing AAM with different attention mechanisms, i.e., soft self-attention <ref type="bibr" target="#b39">[40]</ref> (Soft), hard attention <ref type="bibr" target="#b50">[51]</ref> (Hard) as shown in <ref type="table">Table.</ref>7. For soft self-attention mechanism, we simply remove the actors hard attention part at the beginning of our AAM, which is defined in Eq. 2-6, and directly feed the input set of actors features F a (or objects features F o ) into a self-attention mechanism.</p><p>In contrast, for hard-attention mechanism, we replace the self-attention part at the end of our AAM by a simple average pooling operation to average the selected actors featuresF a (or selected objects featuresF o ) into a single representation f a (or f o ).</p><p>With the higher performances on both datasets shown in <ref type="table">Table 7</ref>, AAM proves its appealing advantages over soft self-attention and hard attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Performance of AOE-Net with different number of objects</head><p>The number of input objects K for objects beholder (Sub sec. 3.1.3) is also a hyper-parameter that may affect the performance of our AOE-Net. If we use a large K, the overall model may receive more noisy information due to the increasingly incorrect detected objects. Whereas, if we use a small K, the objects beholder will not present enough significant information with a few objects. Thus, the contribution of objects beholder to understand actions is insufficient.</p><p>In this ablation study, we benchmark our AOE-Net on various number of objects K with TAPG task and ActivityNet-1.3 <ref type="bibr" target="#b11">[12]</ref> dataset. The comparison is reported in <ref type="table" target="#tab_16">Table 9</ref>. <ref type="table" target="#tab_16">Table 9</ref> shows that as we increase K, the TAPG performance of AOE-Net also tend to improve. However, when K &gt; 20, the performance starts fluctuating and is not robust due to more wrongly detected objects in each snippet. Therefore, we conclude that K = 20 gives the best trade-off between performance and robustness.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Robustness of AOE-Net to egocentric videos</head><p>To benchmark the robustness of AOE-Net on egocentric videos, we use EPIC-KITCHENS 100 <ref type="bibr" target="#b54">[55]</ref> to benchmark TAPG task. <ref type="table" target="#tab_14">Table 8</ref> provides the TAPG comparison between our AOE-Net with BMN <ref type="bibr" target="#b2">[3]</ref>. Even actors are not shown in the egocentric videos, our AOE still obtains good TAPG performance with a big improvement compare to BMN. This proves the effectiveness of the objects beholder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5">Network efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Analysis of AAM</head><p>Qualitative Results of AAM with Actors Beholder: <ref type="figure" target="#fig_5">Fig. 8</ref> shows qualitative performances of AAM in selecting main actors in the set of detected ones. The videos are retrieved from ActivityNet-1.3 <ref type="bibr" target="#b11">[12]</ref>. In the case of multiple actors detected ( <ref type="figure" target="#fig_5">Fig. 8(a)</ref> and <ref type="figure" target="#fig_5">Fig. 8(c)</ref>), our proposed AAM can effectively select main actors in the scene and remove the insignificant actors. This aims to eliminate redundant information as well as select the most relevant information to feed into the boundary-matching module. <ref type="figure" target="#fig_5">Fig. 8(b)</ref> illustrates the scenario where the environment is tedious and may not contribute to perceive the action. However, the local information at the bounding box around the main actor can help highlight the action. In <ref type="figure" target="#fig_5">Fig. 8(b)</ref>, AAM one again shows its merit when selecting the main actor who actually commits the action. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of AAM affected by human detector:</head><p>The human detector we used is Faster-RCNN <ref type="bibr" target="#b27">[28]</ref> trained on COCO dataset <ref type="bibr" target="#b63">[64]</ref>. In practice, the human detector is not completely perfect in the videos due to motion blurs or low resolutions. Therefore, the AAM is also affected by the quality of detected human bounding boxes. In <ref type="figure" target="#fig_6">Fig. 9</ref>, we visualize frames of 4 videos where the human detector poorly produces human bounding boxes. In <ref type="figure" target="#fig_6">Fig. 9(a)</ref>, the green bounding box is localized around two athletes in the pool beside two separate bounding boxes for each athlete. Although the green bounding box is incorrect because it contains two humans (even three if we count the one behind), it is intuitively better than the individual boxes of each athlete because it contains richer information of the scene. This proves that our AAM effectively learnt to select the best bounding boxes detected regardless of its quality in terms of human detection. In <ref type="figure" target="#fig_6">Fig. 9(b)</ref>, (c) and (d), we notice that there are badly detected bounding boxes which is just a body part of the humans instead of a whole. However, AAM could learn to eliminate these bad bounding boxes to only select the correct ones.</p><p>From the above observations, we can see that AAM can learn to avoid selecting bad bounding boxes which do not fully contain the humans. Oppositely, AAM also learns to select some badly detected bounding boxes but contains more meaningful information than the correct ones.</p><p>However, we conclude that relying on the human detector in providing locations to attend on is preventing AAM to achieve its highest potential. Therefore, in the future, it would be more beneficial if we have a better module to localize interesting spatial locations in the video frames instead of the human detector. Qualitative Analysis of Objects Beholder <ref type="figure">Fig. 10</ref> visualizes how AOE-Net can take advantage of Objects Beholder. In this figure, we showcase two video for two distinct categories of (A) visible actors and (B) non-visible actors. in (A), the actors are visible and commits the action of tightrope walking, therefore, our AOE-Net can take advantage of all the beholders. Whereas in (B), the actors are not visible in the video frame and commits the action of cooking, therefore, our AOE-Net can only relies on Objects Beholder. In each illustration of (A) and (B), we visualize either when the action does not happen (A.i and B.i) and when the action is happening (A.ii and B.ii).</p><p>As we can see in <ref type="figure">Fig. 10.(A)</ref>, the objects detected in non-action case and action case are very different. Specifically in <ref type="figure">Fig. 10(A.i)</ref>, the non-action scene is captured through objects like "City", "Building", "Tower", etc. Whereas in <ref type="figure">Fig. 10(A.</ref>ii), the action scene includes "Rooftop", "Tightrope", "Roof", and "Hanging", etc. Likewise, in <ref type="figure">Fig. 10.(B)</ref>, the objects detected in each cases of non-action and action are very different. On one hand, in <ref type="figure">Fig. 10(B.i)</ref>, detected objects are "Kitchen", "Cooker", "Oven", and "Stove" etc. On the other hand, in <ref type="figure">Fig. 10(B.</ref>ii), the action scene objects consist of "Passata", "Chorizo", "Pan", and "Salsa", etc.  <ref type="figure">Fig. 11</ref>: Qualitative results in TAPG on ActivityNet-1.3 <ref type="bibr" target="#b11">[12]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C -Non-Human Case</head><p>The qualitative results of our AOE-Net in TAPG of ActivityNet-1.3 <ref type="bibr" target="#b11">[12]</ref> comparing with previous SOTA works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> are illustrated in <ref type="figure">Fig. 11</ref>. In this figure, we showed two medium cases and a hard case and case of egocentric video. In each video, the proposals from all methods with higher scores than 0.4 are selected to show in the qualitative examples.</p><p>In videos of simple cases <ref type="figure">(Fig. 11-A)</ref>, the actors who are receiving pierces (or trimming in video(b)) is easily observed from the whole frame, however, the piercing action is a bit difficult to be identified because it's from the hand of the doctor (or the taylor in video (b)), who is outside of the video view. Therefore, BMN <ref type="bibr" target="#b2">[3]</ref> and DBG <ref type="bibr" target="#b3">[4]</ref> completely failed to propose the exact action intervals. Likewise, ABN <ref type="bibr" target="#b5">[6]</ref> is tricked by the video and proposes an interval from the beginning of the first groundtruth action of piercing until the credit cut. On the other hand, our proposed AOE-Net can propose intervals that match with the groundtruth actions. This explains a lot for the contributions of both actors beholder and objects beholder, which provide more informative features than previous works to give good results.</p><p>In the video of hard case( <ref type="figure">Fig. 11-A)</ref>, the actors are hockey players, who appear very small in the video frames. Therefore, the "hockey playing" activity, which appear at the beginning of this video, is very difficult to be distinguished to "celebrating" activity, which takes place right after the former. This is explanable because we need to carefully observe the movements of hockey players carefully to see this difference. Therfore, all BMN and DBG failed to recognize the groundtruth action interval. Meanwhile, ABN can propose an interval that covers the scene of the field but not necessesarily the groundtruth action. On the other hand, our proposed AOE-Net can propose an interval that closely matches the groundtruth action. This again, explains the contributions of our actors and objects beholders.</p><p>In the non-human case ( <ref type="figure">Fig. 11-C)</ref>, an actor shows their hands doing cooking on a pan in the interval of <ref type="bibr">[13.9-99.17]</ref>, while in [0.0-13.9] and [99. <ref type="bibr">17-116.64]</ref> the advertisements are displayed. As the actor only shows their hands in the video frames to commit the action, they cannot be detected by the Actors Beholder. However, thanks to our Objects Beholder, the advertisement intervals at the beginning and the end of the video are easily perceived, hence the true action interval is detected in between. Contrarily, a previous SOTA model, BMN <ref type="bibr" target="#b2">[3]</ref>, mis-perceived the advertisement intervals as true actions and classifies them as separate action intervals, or mistakenly combines them with the true action interval in between.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>In this paper, we attempt to simulate the human perceiving ability and proposed a novel AOE-Net to locate actions in untrimmed videos. Our AOE-Net contains two modules: PMR and BMM. PMR extracts visual-linguistic representation of each snippet with four beholders. Environment beholder and actors beholder capture global and local visual features of environment and main actors, respectively. Objects beholder extracts linguistic feature from relevant objects. The last beholder aims to model the relations between main actors, relevant objects and environment. To focus on an arbitrary number of main actor(s) or relevant objects, we introduced AAM. The qualitative and quantitative results conducted on ActivityNet-1.3 and THUMOS-14 datasets on both TAPG and TAD tasks evidently suggest that our proposed AOE-Net outperforms SOTA methods. To prove the effectiveness of AOE-Net, we provided ablation studies to show the contribution of each beholder, the effectiveness of proposed AAM, network efficiency, as well as the robustness of AOE-Net when performing on egocentric videos with EPIC-KITCHENS 100 dataset. We further investigate the performance of AOE-Net with various backbone network configurations. These results prove that replicating human perceiving ability in video understanding is a promising track to follow and further explore in the future.</p><p>There are several potential future directions from this research. First, while main actors and relevant objects provide important impact to both TAPG and TAD tasks, it would be of great interest to investigate the impact of human body parts (e.g. hands, legs) and the interaction between them with objects in localizing human activities in untrimmed videos. Finally, integrating our method with human tracking (i.e. main actors tracking) might result in even better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Notations</head><p>The following is a table that summarizes and briefly describes important symbols that are used throughout this manuscript:</p><formula xml:id="formula_20">Symbol Description V</formula><p>A sequence of all frames from the input video. N Total number of frames in the input video. ?</p><p>Length of a snippet, a sub-set of consecutive frames. T Total number of snippets from the input video. s i</p><p>The snippet at index i ? T . I</p><p>The frame in the center of snippet s i . B</p><p>A set of human bounding boxes detected in center frame I of snippet s i . ?(.)</p><p>An encoding function to encode a snippet s i into a feature vector. Loss function to optimize for starting points classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ls</head><p>Labels of starting points in the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Le</head><p>Loss function to optimize for ending points classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Le</head><p>Labels of ending points in the input video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lact</head><p>Loss function to optimize for the actions classification and regression. L D Labels for the actions classification and regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L wb</head><p>Weighted binary cross-entropy loss. L 2</p><p>Mean squared error loss. <ref type="table" target="#tab_3">Table 11</ref>: Descriptions of symbols used in our paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>3.2 and illustrated in Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>An example of objects detected by Mask-RCNN [48] (left) vs. CLIP (right), where the most relevant objects selected by our AMM are highlighted in bold red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Illustration of proposed AAM. We choose actors features F a and environment feature f e as an example. AAM aims to select main actors features, followed by fusing arbitrary main actors features, to obtain visual main actors representation f a .Algorithm 1 AAM to extract the representation of main actors in a snippet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>The overall architecture of our proposed AOE-Net, consisting of perception-based multi-model representation module (PMR) and boundarymatching module (BMM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Generalizability evaluation and comparisons on ActivityNet-1.3 in terms of AR@100 and AUC. Methods are trained on Unseen+Seen and Seen training sets, respectively; and are evaluated on Seen (first two charts) and Unseen (last two charts) validation sets. Top: Detailed performance of individual experiment setting of various methods. Bottom: Visualized generalizability comparison between our proposed AOE-Net and other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Visualization of main actors selection resulting by AAM on ActivityNet-1.3 [12]. (a), (b), and (c) are three different videos. The background is blacked out, the bounding boxes of main actors are outlined by green line and the bounding boxes of insignificant actors are outlined by grey line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Visualization of AAM on ActivityNet-1.3 [12] on cases where the human detector poorly generates human bounding boxes. The background is blacked out, the bounding boxes of main actors are outlined by green line and the bounding boxes of insignificant actors are outlined by grey line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1.3 and THUMOS-14 in both TAPG and TAD tracks with a large margin compared to previous works. ? We investigate the robustness of AOE-Net while working on egocentric videos of EPIC-KITCHENS 100, in which the main actor is absent from the video views. ? We provide ablation studies on the contribution of each entity type</figDesc><table><row><cell>-frame snippet</cell><cell>ce nt er fra m e</cell><cell></cell><cell>Actors Beholder</cell><cell></cell><cell>Actors Features</cell><cell>Actors-Objects-Environment Interaction Beholder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 2 3</cell><cell>Visual Main Actor Feature</cell><cell>V-L Feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AAM</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RoI Align</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Actor Localizer</cell><cell cols="2">Environment Beholder</cell><cell>Visual</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Environment</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feature</cell></row><row><cell></cell><cell>3D Network Backbone</cell><cell>Feature Map</cell><cell cols="2">Average Pooling</cell><cell>Stack</cell><cell>Self Attention Mechanism</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Objects Beholder</cell><cell>Objects</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Man</cell><cell>Features</cell></row><row><cell></cell><cell></cell><cell>Object Text Extractor</cell><cell>Woman Camera Shooting Photograph</cell><cell></cell><cell>AAM</cell><cell>Linguistic Relevant Objects Feature</cell></row></table><note>(i.e., main actors, relevant objects, and environment) as well as various combinations among them.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Top-40 Detected Objects badminton racket baskets ... court opponent tennis racquet ball ... fence athlete</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>H a to empty list F a stores selected main actors, H a stores scores of every actor3:  for each f a i in F a do</figDesc><table><row><cell>4:f</cell><cell>a i ? M LP ?a (f a i )</cell></row><row><cell cols="2">5: 6: 7: end for h a i ?||f a i ?f e || 2 append h a i to H a 8: H a ? sof tmax(H a )</cell><cell>?: element-wise addition</cell></row><row><cell cols="2">9: ? ? 1 |h a | 10: for each h a i in H a do</cell></row><row><cell>11:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The detailed architecture of BMM with three components. F is the input feature obtained from PMR. T and D are the temporal length of the video and maximum duration of proposals in terms of the number of snippets.</figDesc><table><row><cell>Layers</cell><cell>Input</cell><cell>Output</cell></row><row><cell>1DConv.</cell><cell></cell><cell></cell></row><row><cell>256 ? 3/1, ReLU</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>TAPG comparisons on THUMOS-14 in terms of AR@AN, where SNMS represents Soft-NMS [53]. On SNMS, AOE-Net obtains the second best on all AR@ANs, except AR@100 where it is competitive to the best ones (50.26 vs. 50.67). On NMS, AOE-Net obtains the best on AR@100 and the second best on AR@200 and AR@500 with very close gap with the SOTA, 57.49 vs. 57.74 and 62.40 vs. 62.74, respec- tively. Notably, the performance on TAPG in both datasets of our AOE-Net are a very competitive with AEI-B</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>TAD comparisons on ActivityNet-1.3 in terms of mAP@tIoU and mAP, where the proposals are combined with video-level classification results generated by<ref type="bibr" target="#b66">[67]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>TAD comparisons on THUMOS-14 in term of mAP@tIoU using two different classifiers, i.e., UntrimmedNet<ref type="bibr" target="#b67">[68]</ref> and P-GCN<ref type="bibr" target="#b65">[66]</ref>.</figDesc><table><row><cell>Exp</cell><cell>Act.</cell><cell>Env.</cell><cell>Setting Obj. AAM Soft-Att</cell><cell>@50</cell><cell cols="3">TAPG Performance @100 @200 @500</cell><cell>@1000</cell></row><row><cell>#1</cell><cell></cell><cell></cell><cell></cell><cell>25.96</cell><cell>35.14</cell><cell>43.48</cell><cell>52.37</cell><cell>58.47</cell></row><row><cell>#2</cell><cell></cell><cell></cell><cell></cell><cell>38.94</cell><cell>47.80</cell><cell>54.93</cell><cell>61.92</cell><cell>65.96</cell></row><row><cell>#3</cell><cell></cell><cell></cell><cell></cell><cell>18.06</cell><cell>26.68</cell><cell>37.14</cell><cell>49.28</cell><cell>56.99</cell></row><row><cell>#4</cell><cell></cell><cell></cell><cell></cell><cell>40.87</cell><cell>49.09</cell><cell>56.24</cell><cell>63.53</cell><cell>67.29</cell></row><row><cell>#5</cell><cell></cell><cell></cell><cell></cell><cell>42.60</cell><cell>49.86</cell><cell>56.87</cell><cell>63.76</cell><cell>67.60</cell></row><row><cell>#6</cell><cell></cell><cell></cell><cell></cell><cell>43.79</cell><cell>49.67</cell><cell>56.73</cell><cell>63.49</cell><cell>67.36</cell></row><row><cell>#7</cell><cell></cell><cell></cell><cell></cell><cell>44.56</cell><cell>50.26</cell><cell>57.30</cell><cell>64.32</cell><cell>68.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>TAPG comparisons on different network settings. Act., Env., Obj. denote actors, environment, objects beholders.</figDesc><table><row><cell>Attention</cell><cell>THUMOS-14</cell><cell></cell><cell cols="2">ActivityNet-1.3</cell></row><row><cell></cell><cell cols="4">@50 @100 @200 @500 @1000 AR @100 AUC (val) AUC (test)</cell></row><row><cell>Hard [51]</cell><cell>43.74 49.24 56.63 63.46 67.25</cell><cell>77.11</cell><cell>69.02</cell><cell>69.56</cell></row><row><cell>Soft [40]</cell><cell>42.60 49.86 56.87 63.76 67.60</cell><cell>76.93</cell><cell>69.06</cell><cell>69.23</cell></row><row><cell>AAM</cell><cell>44.56 50.26 57.30 64.32 68.19</cell><cell>77.67</cell><cell>69.71</cell><cell>70.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>TAPG comparison between our AOE-Net with BMN [3] on egocentric videos<ref type="bibr" target="#b54">[55]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>TAPG performance of our AOE-Net on ActivityNet-1.3<ref type="bibr" target="#b11">[12]</ref> with various settings of K</figDesc><table><row><cell></cell><cell cols="4">#Params FLOPs Inference time (s)</cell></row><row><cell></cell><cell>(M)</cell><cell>(G)</cell><cell>GPU</cell><cell>CPU</cell></row><row><cell>BMN [3]</cell><cell>4.9</cell><cell>71.22</cell><cell>0.128</cell><cell>4.15</cell></row><row><cell>DBG [4]</cell><cell>2.9</cell><cell>47.52</cell><cell>0.03</cell><cell>-</cell></row><row><cell>GTAD [5]</cell><cell>5.6</cell><cell>150.28</cell><cell>0.14</cell><cell>-</cell></row><row><cell>ABN [6]</cell><cell>6.9</cell><cell>87.88</cell><cell>0.07</cell><cell>0.21</cell></row><row><cell>AEI [52]</cell><cell>6.9</cell><cell>90.62</cell><cell>0.08</cell><cell>0.21</cell></row><row><cell>AOE-Net</cell><cell>8.8</cell><cell>94.02</cell><cell>0.12</cell><cell>0.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Network efficiencies of AOE-Net and several of previous works.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>reports the efficiency of AOE-Net and previous SOTA with #param-</cell></row><row><cell>eters in millions (M) , computational cost (GFLOPs), inference time on a</cell></row><row><cell>3-minute video with either an Intel Core i9-9920X CPU or a single NVIDIA</cell></row><row><cell>RTX 2080 Ti.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>i) Non-action Case ii) Action Case Top-40 Detected Objects40 Detected Objects Input framesInput frames AAM Selections City Building Tower AAM Selections Rooftop Hanging Tightrope Roof A -Video with Visible Actors Input frames Input frames AAM Selections Stove Cooker Kitchen Oven AAM Selections Passata Pan Salsa B -Video with Non-Visible Actor</head><label></label><figDesc>Qualitative results to illustrate the effectiveness of Objects Beholder with AAM in (A) videos with visible actors and (B) videos with non-visible actor. In each case, we illustrate two instances of having action and without action. The input frames are shown on the left, its objects detected by CLIP are shown in the middle, and the most relevant objects selected by AAM are shown in the right.</figDesc><table><row><cell cols="3">Roof Higher Rooftop Jumper Tightrope ? Top-Clouds Raise Bungee Flying Hanging Rises ? Foreground Scraper Roofs Footage ? View Building City Tower Skies ? Kitchen Cooker Cupboard Stove Oven Dishwasher Microwave Extractor Saucepan Washer ? ? Top-40 Detected Objects Passata Chorizo Toast Saucepan Spaghetti Tortilla Salsa Pot Pan Nutella ? ? Top-40 Detected Objects *empty* p=0.55 p=0.69 36.91 45.65 57.31 *empty* Fig. 10: 4.7 Qualitative Analysis of AOE-Net i) Non-action Case ii) Action Case a) GT BMN ABN Our AOE DBG b) GT BMN ABN Our AOE DBG p=0.64 16.51 23.31 A -Medium Cases GT BMN ABN Our AOE DBG B -Hard Case GT 13.9 p=0.43 0.0 23.56 BMN p=0.47 0.0</cell><cell>67.02</cell><cell>Chorizo p=0.41 102.5 99.17 116.64</cell></row><row><cell></cell><cell></cell><cell>74.4</cell><cell cols="2">p=0.42</cell><cell>99.2</cell></row><row><cell>Our AOE</cell><cell>11.81</cell><cell>p=0.56</cell><cell></cell><cell>98.02</cell></row><row><cell></cell><cell>12.99</cell><cell>p=0.68</cell><cell></cell><cell>113.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>F M A feature map extracted by a backbone network. F a A set of actors features in snippet s i . F o A set of objects features in snippet s i . T Vocabulary used in Objects Beholder. D Total number of words in vocabulary T . T e A set of embedded features for every word in mathcalT . I e Embedded feature representing center frame I of s i . K A hyper-parameter, defines maximum number of words in T to be selected. F o Top K embedded features in T e that is best matched with I. f e Encoded feature of f e , used in AAM. F a Encoded feature of every actors feature in F a , used in AAM. H a A set of scores of every actor feature in F a . ? An adaptive threshold to filter out actor features that has lower scores. F a A set of main actor features that are selected. f e Output feature vector of Environment Beholder. f a Output feature vector of Actors Beholder. f o Output feature vector of Objects Beholder. F aoe A stack of f a , f o , f e to serve Actors-Objects-Environment Beholder. f</figDesc><table /><note>i Output feature vector of Actors-Objects-Environment Beholder. Ls</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Actors-Objects-Environment Network</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This material is based upon work supported by the National Science Foundation under Award No OIA-1946391 and NSF 1920920.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BSN++: complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="11499" to="11506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abn: Agent-aware boundary networks for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="126431" to="126445" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Agent-environment network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-K</forename><surname>Vo-Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2160" to="2164" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cascaded Boundary Regression for Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.01180" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1705" to="01180" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Motion-appearance co-memory networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS Challenge: Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densecaptioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. NIPS&apos;14</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vision and language: from visual perception to content creation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA TSIP</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NeurIPS</publisher>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An attentive survey of attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mithal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Polatkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIST</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1875" to="1886" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An attentive survey of attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Polatkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mithal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02874</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS. Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Saccader: Improving accuracy of hard attention models for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Differential attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7680" to="7688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Tsi: Temporal scale invariant network for action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Relaxed transformer decoders for direct action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1162.https://aclanthology.org/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale. CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning visual question answering by bootstrapping hard attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aei: Actors-environment interaction with adaptive attention for temporal action proposals generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd British Machine Vision Conference 2021, BMVC 2021, Virtual Event</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="850" to="855" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">IJVC</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="539" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Msr asia msm at activitynet challenge 2017: Trimmed action recognition, temporal action proposals and densecaptioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Srg: Snippet relatedness-based temporal action proposal generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Self-supervised learning for semi-supervised temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1905" to="1914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Temporal context aggregation network for temporal action proposal refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="485" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Boundary adjusted network based on cosine similarity for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Accurate temporal action proposal generation with relation-aware pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10810" to="10817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">CUHK &amp; ETHZ &amp; SIAT submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1608.00797</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="539" to="555" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
