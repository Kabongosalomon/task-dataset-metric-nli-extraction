<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES You Only Need 90K Parameters to Adapt Light: a Light Weight Transformer for Image Enhancement and Exposure Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziteng</forename><surname>Cui</surname></persName>
							<email>cui@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gu</surname></persName>
							<email>lin.gu@riken.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghan</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
							<email>gaopeng@pjlab.org.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
							<email>zhengkjiang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>qiaoyu@pjlab.org.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><forename type="middle">Ai</forename><surname>Laboratory</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riken</forename><surname>Aip</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tencent</forename><forename type="middle">Youtu</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">STUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES You Only Need 90K Parameters to Adapt Light: a Light Weight Transformer for Image Enhancement and Exposure Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Challenging illumination conditions (low light, under-exposure and over-exposure)   in the real world not only cast an unpleasant visual appearance but also taint the computer vision tasks. After camera captures the raw-RGB data, it renders standard sRGB images with image signal processor (ISP). By decomposing ISP pipeline into local and global image components, we propose a lightweight fast Illumination Adaptive Transformer (IAT) to restore the normal lit sRGB image from either low-light or under/overexposure conditions. Specifically, IAT uses attention queries to represent and adjust the ISP-related parameters such as colour correction, gamma correction. With only ?90k parameters and ?0.004s processing speed, our IAT consistently achieves superior performance over State-of-The-Art (SOTA) on the benchmark low-light enhancement and exposure correction datasets. Competitive experimental performance also demonstrates that our IAT significantly enhances object detection and semantic segmentation tasks under various light conditions. Our code and pre-trained model is available at this url.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure" target="#fig_6">Figure 1</ref><p>: Nature lay hid in Night, apply our IAT, and all was light, middle figure shows our results compared with other SOTA methods on LOL-V1 dataset <ref type="bibr" target="#b63">[64]</ref>. and downstream vision tasks (e.g., semantic segmentation and object detection). Images taken under inadequate illumination ( <ref type="figure" target="#fig_6">Fig.1 left top)</ref> suffer from limited photon counts and undesirable in-camera noise. On the other hand, outdoor scenes are often exposed to strong light such as direct sunlight ( <ref type="figure" target="#fig_6">Fig.1 left down)</ref>, making image saturated due to the limited range of sensors and non-linearity in the camera image pipeline. To make it worse, both the under and over exposure may exist together, i.e. spatial-variant illumination cast by shadow could make the contrast ratio to be 1000:1 or higher.</p><p>Multiple techniques such as low-light enhancement <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b76">77]</ref>, exposure correction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b71">72]</ref> have been proposed to adapt to the difficult light condition. Low-light enhancement methods restore the details while suppressing the accompanying noises. Exposure correction methods focus on adjusting the under/over exposure image to reconstruct a clear image against short/long exposure time. While many efforts elaborate on improving human oriented visual perceptual, there also several methods enhance the high-level tasks by boosting their robustness against low light <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b52">53]</ref> and over-exposure conditions <ref type="bibr" target="#b43">[44]</ref>. As shown in <ref type="figure" target="#fig_6">Fig.1</ref>, we aim for a unified lightweight framework that improves both the visual appearance and consequent recognition tasks under challenging real-world light condition.</p><p>While sRGB images are most common to everyday life, many existing light adaptive methods operate on raw-RGB images which linearly proportion to the actual scene irradiance. To directly process the sRGB images, we specifically considers the image signal processor (ISP) in camera that renders sRGB from raw-RGB image. We propose a novel two-branches transformer based model to handle this issue, a pixel-wise local branch f coupled with global ISP branch g. In local branch f , we map the input image to latent feature space and replace transformer's attention block to depth-wise convolution for light-weight design. In global branch g, we use transformer's attention queries to control and adjust the global ISP-related parameters (i.e. colour transform matrix, gamma value). In addition, the learned queries could dynamic change under different light condition at same time (i.e. over-exposure and under exposure).</p><p>Extensive experiments are conducted on several real-world and synthetic datasets, i.e.,</p><p>image enhancement dataset LOL <ref type="bibr" target="#b63">[64]</ref> and photo retouching dataset MIT-Adobe FiveK <ref type="bibr" target="#b5">[6]</ref>, low-light detection dataset EXDark <ref type="bibr" target="#b40">[41]</ref> and low-light segmentation dataset ACDC <ref type="bibr" target="#b51">[52]</ref>. Results show that our IAT achieve state-of-the-art performance across a range of low-level and high-level tasks. More importantly, our IAT model is of only 0.09M parameters, much smaller than current SOTA transformer-based models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63]</ref>. Besides, our average inference speed is 0.004s per image, faster than the SOTA methods taking around 1s per image. Our contribution could be summarised as follow:</p><p>? We have proposed a fast and light-weight framework, Illumination Adaptive Transformer (IAT), to adapt to challenging light conditions in the real world, which could both handle the low-light enhancement and exposure correction tasks.</p><p>? We have proposed a novel transformer-style structure to estimate ISP-related parameters to fuse the target sRGB image, wherein the learnable attention quires are utilised to attend the whole image, also we replace the layer norm to a new light normalisation, for better handling low-level vision tasks.</p><p>? Extensive experiments on several real-world datasets on 3 low-level tasks and 3 highlevel tasks demonstrate the superior performance of IAT over SOTA methods. IAT is light weight and mobile-friendly with only 0.09M model parameters and 0.004s processing time per image. We will release the source code upon publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Enhancement against Challenging Light Condition</head><p>Earlier low-light image enhancement solutions mainly rely on RetiNex theory <ref type="bibr" target="#b34">[35]</ref> or histogram equalization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b55">56]</ref>. Since LLNet <ref type="bibr" target="#b41">[42]</ref> utilised a deep-autoencoder structure, CNN based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b76">77]</ref> have been widely used in this task and gain SOTA results on the benchmark enhancement datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b63">64]</ref>. Similar to low-light enhancement, traditional exposure correction algorithms <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b71">72]</ref> also use image histograms to adjust image intensities. The strategy then tends to correct exposure errors by adjusting the tone curve via a trained deep learning model <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b70">71]</ref>. Very recently, Afifi et al. <ref type="bibr" target="#b1">[2]</ref> propose a coarse-to-fine neural network to correct photo exposure, after that Nsampi et al. <ref type="bibr" target="#b46">[47]</ref> introduce attention mechanism into this task.</p><p>Beyond low-level vision, low-light/ strong-light scenario also deteriorates the performance of high level vision <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b77">78]</ref>. Several methods based on data synthesis <ref type="bibr" target="#b77">[78]</ref>, self-supervised learning <ref type="bibr" target="#b15">[16]</ref> and domain adaptation <ref type="bibr" target="#b52">[53]</ref> have been proposed to support high level vision tasks under challenging illumination conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision Transformers</head><p>Transformer <ref type="bibr" target="#b60">[61]</ref> was firstly proposed in NLP area to capture long-range dependencies by global attention. ViT <ref type="bibr" target="#b17">[18]</ref> made the first attempt in vision task by splitting the image into tokens before sending into transformer model. Since then, Transformer based models have gained superior performances in many computer vision tasks, including image/video classification <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40]</ref>, object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b75">76]</ref>, semantic segmentation <ref type="bibr" target="#b65">[66]</ref>, vision-language model <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b74">75]</ref> and so on.</p><p>In low-level vision area, transformer-based models has also made much progress on several sub-directions, such as image super-resolution <ref type="bibr" target="#b36">[37]</ref>, image restoration <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b72">73]</ref>, image colorization <ref type="bibr" target="#b33">[34]</ref> and bad weather restoration <ref type="bibr" target="#b59">[60]</ref>. Very recently, MAXIM <ref type="bibr" target="#b58">[59]</ref> use MLPbased model <ref type="bibr" target="#b54">[55]</ref> in low-level vision area which also shows MLP's potential on low-level vision tasks. However, existing transformer &amp; MLP models require much computational cost (e.g. 115.63M for IPT <ref type="bibr" target="#b8">[9]</ref>, 14.14M for MAXIM <ref type="bibr" target="#b58">[59]</ref>), making it hard to implement on mobile and edge devices. Extreme lightweight of our method (0.09M) is particular important in low-level vision and computational photography.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Illumination Adaptive Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>For a sRGB image I i taken from light condition L i , the input photons under light condition L i would project through the lens on capacitor cluster, to pass by the in-camera process <ref type="bibr" target="#b64">[65]</ref> and render with image signal processor (ISP) pipeline G(?) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>. Our goal is to match input sRGB I i to the target sRGB image I t (taken under light condition L t ). Existing deep-learning based methods tend to build an end-to-end mapping between I i and I t <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> or estimate some high-level representation to assist enhancement task (i.e. illumination map <ref type="bibr" target="#b61">[62]</ref>, colour transform function <ref type="bibr" target="#b32">[33]</ref>, 3D look-up table <ref type="bibr" target="#b73">[74]</ref>). However, the actual lightness degradation happens in raw-RGB space, and the processes in camera ISP involves more elaborated non-linear operations such as white balance, colour space transform, gamma correction, etc. Therefore, much of research conducts image enhancement <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b64">65]</ref> directly on raw-RGB data rather than sRGB images.</p><p>To this end, Brooks et al. <ref type="bibr" target="#b4">[5]</ref> inverse each steps in ISP pipeline (i.e. gamma correction, tone mapping, camera colour transformation) to transform input sRGB image to "unprocessed" raw-RGB data. After that, Afifi and Brown <ref type="bibr" target="#b0">[1]</ref> apply an encoder-decoder structure to edit the illumination of sRGB image from input light I i to target light I t as following:</p><formula xml:id="formula_0">I t = G(F(I i )),<label>(1)</label></formula><p>where F is an unknown reconstruction function maps I i to the corresponding raw-RGB data D = F(I i ), and G is camera rendering function that transform D back to target sRGB image I t . Here <ref type="bibr" target="#b0">[1]</ref> use the network encoder f to represent F, before adding several individual decoders g t upon encoder f . The function maps f (I i ) to target I t illumination conditions is represented below:</p><formula xml:id="formula_1">I t = g t ( f (I i )),<label>(2)</label></formula><p>For the sake of lightweight network design, inspired by the DETR <ref type="bibr" target="#b6">[7]</ref> which controls different object proposals via transformer queries, here we use different queries to control the ISP-related parameters in g t (?). This re-configures parameters to make the image I i adaptive to target light condition L t . In training stage, the queries is dynamically updated in each iteration to match the target image I t . Here we simplify the ISP procedures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> into the equation 3 below. The simplification details could be found in the supplementary.</p><formula xml:id="formula_2">g t (?) = (max( ? c j W c i ,c j (?), ?)) ? , c i , c j ? {r, g, b}.<label>(3)</label></formula><p>W c i ,c j is a 3 ? 3 joint colour transformation matrix, considering the white balance and colour transform matrix. We adopt 9 queries to control W c i ,c j 's parameters. ? denotes the gamma correction parameter which we use a single query to control. ? is a very small value to prevent numerical instability. Here we set ? = 1e ?8 in our experiments.</p><p>For process F, we apply a pixel-wise least squares model f . Our f consists of two individual branches to predict multiply map M and add map A. We then apply a least squares to process input sRGB image: f (I i ) = I i M + A. Here M and A has the same size with I i to complete pixel-level multiplicative and additive adjustment. Finally, the equation of our IAT model follows:</p><formula xml:id="formula_3">I t = (max( ? c j W c i ,c j (I i M + A)), 0) ? .<label>(4)</label></formula><p>The non-linear operations are decomposed into a local pixel-wise components f and a global ISP components g. Thus, we design two individual transformer style branches: local adjustment branch and global ISP branch, to estimate the local pixel-wise components and global ISP components respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Structure</head><p>Given an input sRGB image I i ? R H?W ?3 under light condition L i , where H ? W denotes the size dimension and 3 denotes the channel dimension ({r, g, b}). As shown in <ref type="figure" target="#fig_0">Fig.2</ref>, we propose our Illumination Adaptive Transformer (IAT) to transfer the input RGB image I i to a target RGB I t ? R H?W ?3 under the proper uniform light L t .</p><p>Local Branch. In the local branch, we focus on estimating the local components M, A to correct the effect of illumination. Instead of adopting a U-Net <ref type="bibr" target="#b50">[51]</ref> style structure, which downsamples the images first before upsampling, we aim to maintain the input resolution through the local branch to preserve the informative details. Therefore, we propose a transformer-style architecture for the local branch. Compared to popular U-Net style structures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43]</ref>, our structure could deal with arbitrary resolution images without resizing them.</p><p>At first, we expand the channel dimension via a 3?3 convolution and pass them to two independent branches stacked by Pixel-wise Enhancement Module (PEM). For the lightweight design in the local branch, we replace self-attention with depth-wise convolution as suggested in the previous works <ref type="bibr">[</ref>  further save computation cost. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref> (a), our PEM first encodes the position information by 3?3 depth-wise convolution before enhancing local details with PWConv-DWConv-PWConv. Finally, we adopt two 1?1 convolutions to enhance token representation individually. Specially, we design new kind of normalisation names light normalisation, to replace transformer's Layer Normalisation <ref type="bibr" target="#b3">[4]</ref>. As shown in <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>, light normalisation learns to scale a and bias b via two learnable parameters before fusing the channels via the learnable matrix. The matrix is initialised as an identity matrix. For better convergence, we adopt Layer Scale <ref type="bibr" target="#b57">[58]</ref> which multiplies the features by a small number k 1 /k 2 .</p><formula xml:id="formula_4">1/2?'-/ 1/2?'-/ (`, de, p) 3?3 DWConv LN FC GELU FC FC FC P ] \ FC _)")#1 $'(1%f g'$$' ' c z{ z| "/'&amp;'c"/ -'1'$/(/1 %&amp;%( 1 %&amp;%( 0 %&amp;%( 10 }' Q%f/"??%2/ ?&amp;?'&amp;_/$/&amp;( q),#"/ c b")c'" Q1/,%_(%)&amp; q),#"/ 1/2?'-/</formula><p>We stack 3 PEMs in each branch and then connect the output features with the input features through element-wise addition. This skip connection <ref type="bibr" target="#b23">[24]</ref> helps maintain the original image details. Finally, we decrease the channel dimension by a 3?3 convolutions and adopt ReLU/ Tanh function to generate the local components M/ A in Eq. 4.</p><p>Global ISP Branch. Global ISP branch accounts for part of the ISP pipeline <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> (i.e. gamma correction, colour matrix transform, white balance) when transferring the target RGB image I t . Specifically, the value of each pixel in the target image is determined by a global operation defined in Eq.3.</p><p>Inspired by Detection Transformer DETR <ref type="bibr" target="#b6">[7]</ref>, we design global component queries to decode and predict the W, ? to generate sRGB image I t . This transformer structure allows capturing global interactions between context and individual pixels. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we first stack two convolutions as a lightweight encoder, which encodes the features in a high dimension with lower resolution, on the one hand lower resolution would save computational cost which contribute to the light-weight design, on the other hand higher feature representation would be helpful to extract image's global-level features. Then the generated features are passed to the Global Prediction Module (GPM), <ref type="figure" target="#fig_1">Fig. 3 (b)</ref> shows the detailed structure of GPM, different from original DETR <ref type="bibr" target="#b6">[7]</ref> model, our global component queries Q are initialised as zeros without extra multi-head self-attention. Q is global component learnable embedding that attends keys K and values V generated from encoded features. The positional encoding for K and V is from a depth-wise convolution, which is friendly with different input resolutions. After feed forward network (FFN) <ref type="bibr" target="#b17">[18]</ref> with two linear layers, we add two extra parameters with special initialisation to output colour matrix and gamma. This initialisation makes sure the colour matrix is identity matrix W and the gamma value g is one in the beginning, thus contributing to stable training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our proposed IAT model on benchmark datasets and experimental settings for both low-level and high-level vision tasks under different illumination conditions. Three low-level vision tasks include: (a) image enhancement (LOL (V1 &amp; V2-real) <ref type="bibr" target="#b63">[64]</ref>), (b) image enhancement (MIT-Adobe FiveK <ref type="bibr" target="#b5">[6]</ref>), (c) exposure correction <ref type="bibr" target="#b1">[2]</ref>. Three high-level visions tasks include: (d) low-light object detection (e) low-light semantic segmentation ( f ) various-light object detection. The number of PEM number in local branch are both set to 3, while the channel number in PEM is set to 16.</p><p>For all low-level vision experiments: {(a), (b), (c)}, the IAT model are trained on a single GeForce RTX 3090 GPU with batch size 8. We use Adam optimizer to train our IAT model while the initial learning rate and weight decay are separately set to 2e ?4 and 1e ?4 . A cosine learning schedule has also been adopted to avoid over-fitting. For data augmentation, horizontal and vertical flips have been used to acquire better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Low-level Image Enhancement.</head><p>For (a) and (b) image enhancement task, we evaluate our IAT framework on benchmark datasets: LOL (V1 &amp; V2-real) <ref type="bibr" target="#b63">[64]</ref> and MIT-Adobe FiveK <ref type="bibr" target="#b5">[6]</ref>.</p><p>LOL <ref type="bibr" target="#b63">[64]</ref> has two versions: LOL-V1 consists of 500 paired normal-light images and low-light images. 485 pairs are used for training and the other 15 pairs are for testing. LOL-  <ref type="bibr" target="#b63">[64]</ref> datasets, best and second best results are marked in red and blue respectively, noted here <ref type="bibr" target="#b21">[22]</ref> is non-deep learning method and <ref type="bibr" target="#b20">[21]</ref> is self-supervised learning method.  V2-real consists of 789 paired normal-light images and low-light pairs. 689 pairs are used for training and the other 100 pairs are for testing. The loss function between input image I i and target image I t for LOL dataset training is a mixed loss function <ref type="bibr" target="#b59">[60]</ref> consisting of smooth L1 loss and VGG loss <ref type="bibr" target="#b30">[31]</ref>. In LOL-V1 training, the images are cropped into 256 ? 256 to train 200 epochs and then fine-tune on 600 ? 400 resolution for 100 epochs. In LOL-V2real training, the image resolution is maintained at 600 ? 400 and trained for 200 epochs. Both LOL-V1 and LOL-V2-real testing the image resolution is maintained at 600 ? 400. We compare our method with SOTA methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b76">77]</ref>.</p><p>For image quality analysis, we evaluate the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). For the model efficiency analyze, we report three metrics: FLOPs, model parameters and test time, as shown in the last column of MIT-Adobe FiveK <ref type="bibr" target="#b5">[6]</ref> dataset contains 5000 images, each was manually enhanced by five different experts (A/B/C/D/E). Following the previous settings <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b61">62]</ref>, we only use experts C's adjusted images as ground truth images. For MIT-Adobe FiveK <ref type="bibr" target="#b5">[6]</ref> dataset training, we use a single L1 loss function to optimize IAT model. Our method is compared with SOTA enhancement methods [13, 28, 29, 45, 51, 62, 62, 74] on FiveK dataset. The image quality results (PSNR, SSIM) and model parameters are reported in <ref type="table">Table.</ref> 2. Our IAT also gain satisfactory result in both quality and efficiency. Qualitative results of LOL <ref type="bibr" target="#b63">[64]</ref> and FiveK <ref type="bibr" target="#b5">[6]</ref> has been shown in <ref type="figure" target="#fig_2">Fig.4</ref>. More results could be found in supplementary material.  For the (c) exposure correction task, we evaluate IAT on the benchmark dataset proposed by <ref type="bibr" target="#b1">[2]</ref>. The dataset contains 24,330 sRGB images, divided into 17,675 training images, 750 validation images, and 5905 test images. Images in <ref type="bibr" target="#b1">[2]</ref> are adjusted by MIT-Adobe FiveK <ref type="bibr" target="#b5">[6]</ref> dataset with 5 different exposure values (EV), ranging from under-exposure to over-exposure condition. Same as <ref type="bibr" target="#b5">[6]</ref>, test set has 5 different experts' adjust results (A/B/C/D/E). Following the setting of <ref type="bibr" target="#b1">[2]</ref>, the training images are cropped to 512 ? 512 patches and the test image is resized to have a maximum dimension of 512 pixels. We compare the test images with all five experts' results. Here we use L1 loss function for exposure correction training. The evaluation result is shown in <ref type="table">Table.</ref> 3. Our comparison methods include both traditional image processing methods (Histogram Equalization <ref type="bibr" target="#b19">[20]</ref>, LIME <ref type="bibr" target="#b21">[22]</ref>) and deep learning methods (DPED <ref type="bibr" target="#b28">[29]</ref>, DPE <ref type="bibr" target="#b12">[13]</ref>, RetinexNet <ref type="bibr" target="#b63">[64]</ref>, Deep-UPE <ref type="bibr" target="#b61">[62]</ref>, Zero-DCE <ref type="bibr" target="#b20">[21]</ref>, MSEC <ref type="bibr" target="#b1">[2]</ref>). Evaluation metrics are same as <ref type="bibr" target="#b1">[2]</ref>, including PSNR, SSIM and perceptual index (PI). <ref type="table" target="#tab_5">Table. 3</ref> shows that our IAT model has gained best result on all evaluation indexs. Compared to the second best result MSEC <ref type="bibr" target="#b1">[2]</ref>, IAT has much fewer parameters (0.09M v.s. 7M) and less evaluation time (0.004s per image v.s. 0.5s per image). Qualitative result has been shown in <ref type="figure" target="#fig_2">Fig.4</ref> and more visual results are given in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">High-level Vision</head><p>For high-level vision tasks: {(d), (e), ( f )}, we use IAT to restore the image before feeding to the subsequent recognition algorithms based on mmdetection and mmsegmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>. For a fair comparison, we run all of the experiments in the same setting: same input size, same data augmentation methods (expand, random crop, multi-size, random flip...), same training epochs and same initial weights. We train the recognition algorithm on the datasets enhanced by IAT. We compare our methods with original datasets as well as datasets enhanced by other enhancement methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>For object detection task in (d) EXDark dataset <ref type="bibr" target="#b40">[41]</ref> and ( f ) TYOL dataset <ref type="bibr" target="#b25">[26]</ref>. EXDark includes 7,363 real-world low-light images, ranging from twilight to extreme dark environment with 12 object categories. We take 80% images of each category for training and the other 20% for testing. TYOL includes 1680 images with 21 classes. We take 1365 images for training and other for evaluation. For both datasets, we perform object detection with YOLO-V3 <ref type="bibr" target="#b49">[50]</ref>, all the input images have been cropped and resized to 608 ? 608 pixel size, we use SGD optimizer to train YOLO-V3 with batch size 8 for 25 epochs to EXDark and 45 epochs to TYO-L, the initial learning rate is 1e ?3 and weight decay is 1e ?4 . The detection metric mAP and per-image evaluation time is shown in <ref type="table">Table.</ref> 4. Our IAT model gains best results in both accuracy and speed compared to the baseline model and other enhancement methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>For semantic segmentation in (e) ACDC dataset <ref type="bibr" target="#b51">[52]</ref>, we take 1006 night images in the ACDC dataset and then adopt DeepLab-V3+ <ref type="bibr" target="#b10">[11]</ref> to train on the ACDC-night train set and test on ACDC-night val set. The DeepLab-V3+ <ref type="bibr" target="#b10">[11]</ref> model is initialed by an Cityscape dataset <ref type="bibr" target="#b14">[15]</ref> pre-train model, we tuned the pre-train model by SGD optimizer with batch size 8 for 20000 iters, initial learning rate is set to 0.05, momentum and weight decay are separately set to 0.9 and 5e ?4 . We show the segmentation metric mIOU and per-image evaluation time in <ref type="table">Table.</ref> 4, we found that all the enhancements methods invalid in this setting, this may because the lightness condition in ACDC <ref type="bibr" target="#b51">[52]</ref> is various and exceeds the generalisation ability of the enhancement model. For this problem, we propose to joint training our IAT model with following segmentation network (as well as detection network), which would solves this problem and improve the semantic segmentation/ object detection results in low-light conditions, detailed analyse please refer to Sec. 8 of supplementary material 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel lightweight transformer framework IAT, by adapting ISP-related parameters to adapt to challenging light conditions. Despite its superior performance on several real-world datasets for both low-level and high-level tasks, IAT is extremely light with a fast speed. The lightweight and mobile-friendly IAT has the potential to become a standing tool for the computer vision community.</p><p>However, one mian drawback of the IAT module is that, the image signal processor (ISP) has been simplified due to the light-weight demand, we think that more detailed ISPrelated parts could be concerned and interpolate to the IAT module. In further, we'd also like to implement IAT on 3D human relighting task, to solve more complex lighting problems under 3D condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>Corner symbol '*' in the author name means the corresponding author. This work supported by JST Moonshot R&amp;D Grant Number JPMJMS2011 and JST, ACT-X Grant Number JP-MJAX190D, Japan. This work also supported by National Natural Science Foundation of China (Grant No. 62206272) and Shanghai Committee of Science and Technology (Grant No. 21DZ1100100). <ref type="bibr" target="#b0">1</ref> For more experimental details and ablation analyse, please refer to the supplementary material. For the global part g of the IAT module, here we simplify the ISP procedures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref> as the following equation:</p><formula xml:id="formula_5">G(?) = Gamma(W ccm (W wb (?))).<label>(5)</label></formula><p>White balance (WB) function is an essential part in ISP pipeline. WB algorithm estimates the per channel gain on the image, to maintain the object's colour constancy under various different light colour. WB is usually represented as a 3 ? 3 diagonal von Kris matrix W wb in camera imaging pipeline <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref>. After that, camera color matrix (CCM) W ccm converts the white-balanced data from camera internal color space cRGB to sRGB colour space <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. At last gamma correction aims to match non-linearity of humans perception on dark regions. A standard gamma curve is usually represent as an exponential function with the exponential parameter ?, so we build our global branch g t (?) following the equation:</p><formula xml:id="formula_6">g t (?) = (max( ? c j W c i ,c j (?), ?)) ? , c i , c j ? {r, g, b},<label>(6)</label></formula><p>where the W c i ,c j is a joint colour transform function consist of white balance matrix and CCM and ? is the gamma correction's exponential value, ? is a minimum number to keep non-negative. Final as we discussed in Sec.3.1, the input image I i would separately pass by local branch f and global branch g to generate the prediction result? t = g t ( f (I i )).</p><p>We also evaluate to train the model with corresponding raw-RGB data as additional supervision. Since it's hard to directly get raw-RGB data from the currect dataset, we then adopt the Invertible ISP <ref type="bibr" target="#b66">[67]</ref> to generate corresponding raw-RGB data I raw from the input image I i , we use pre-train weights in <ref type="bibr" target="#b66">[67]</ref> to generate I raw . In the training stage, we additional add a loss function L raw for raw-RGB supervision, the total loss function shown as follow:</p><formula xml:id="formula_7">L total =L rgb + ? ? L raw =L 1 (g t ( f (I i ), I t ) + ? ? L 1 ( f (I i ), I raw ).<label>(7)</label></formula><p>L total is the total loss function that consist of two parts: the first part L rgb is L1 loss function between predict result g t ( f (I i )) with ground truth image I t , while the second part L raw is the L1 loss function between middle representation f (I i ) and raw-RGB image I raw for raw-RGB part supervision, and ? is a balance parameter where we set it to 0.1 in our experiments. We make the comparison experiments on exposure correction dataset <ref type="bibr" target="#b1">[2]</ref>, the training and experiments' settings are follow the settings in Sec.4.2, only difference is the training strategy with or without raw-RGB supervision. The comparison results are shown in <ref type="table" target="#tab_7">Table 5</ref>, we can find that with the additional supervision of raw-RGB data, most of evaluation metrics on exposure correction dataset <ref type="bibr" target="#b1">[2]</ref> would be improved.  For high-level vision tasks under challenging lighting conditions,shown in <ref type="figure" target="#fig_4">Fig.5</ref>, current high-level vision frameworks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b49">50]</ref> usually well-trained on large scale normal-light datasets (i.e. MS COCO <ref type="bibr" target="#b37">[38]</ref>, ImageNet <ref type="bibr" target="#b16">[17]</ref>), so directly take low-light/ strong-light data as input would cause the lightness in-consistency, on the other hand, using image enhancement methods (Sec.4.3 in main text) to pre-process images may cause target inconsistency (human vision v.s. machine vision) <ref type="bibr" target="#b15">[16]</ref>, since the goal of image restoration is image quality (i.e. PSNR, SSIM) and the goal of detection/ segmentation is machine-vision accuracy (i.e. mAP, mIOU).</p><p>An example is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, by attaching IAT to the downstream task module, our IAT could conduct object detection and semantic segmentation with the downstream frameworks. During training, we aim to minimise the downstream framework's loss function (i.e. object detection loss L ob j between detection predictiont and ground truth t) by jointly optimising the whole network's parameters (see Eq. 8). Compared to the subsequent high-level module, the time-complexity and model storage of our IAT main structure could be ignored (i.e. IAT main structure v.s. YOLO-V3 <ref type="bibr" target="#b49">[50]</ref>, 417KB v.s. 237MB). min i?I,d?D L ob j (t,t)</p><formula xml:id="formula_8">I t (x) = I(I 1 (x)),t = D(I t (x))<label>(8)</label></formula><p>We make the comparison experiments on low-light detection dataset EXDark <ref type="bibr" target="#b40">[41]</ref> and low-light semantic segmentation dataset ACDC <ref type="bibr" target="#b51">[52]</ref>. For object detection task we adopt the YOLO-V3 <ref type="bibr" target="#b49">[50]</ref> object detector and for segmentation task we adopt DeepLabV3+ <ref type="bibr" target="#b10">[11]</ref> segmentation framework, the training and experiments' settings are follow the settings in Sec.4.3.</p><p>Experimental results are shown in <ref type="table">Table.</ref> 6, "original" means to take the original lowlight images for training and evaluation, "pre-enhancement" means to pre-enhancement the EXDark <ref type="bibr" target="#b40">[41]</ref> and ACDC <ref type="bibr" target="#b51">[52]</ref> datasets with IAT model trained on LOL-V1 dataset <ref type="bibr" target="#b63">[64]</ref> ("IAT (LOL)") and MIT-Adobe FiveK dataset <ref type="bibr" target="#b5">[6]</ref> ("IAT (MIT5K)"). The "joint training" means  to joint train IAT with the following high-level framework, and IAT model is separately random initialize ("IAT (none)"), initialize with LOL pre-train weights ("IAT (LOL)") and initialize with MIT-Adobe FiveK weights ("IAT (MIT5K)"), from Table. <ref type="bibr" target="#b5">6</ref> we could see that joint-training IAT with the high-level frameworks would further improve high-level visual performance, on both of object detection and semantic segmentation task.</p><p>9 Ablation Studies 9.1 Contribution of each part.</p><p>To evaluate each part's contribution in our IAT model, we make an ablation study on the lowlight enhancement task of LOL-V2-real <ref type="bibr" target="#b63">[64]</ref> dataset, and the low-light object detection task of EXDark <ref type="bibr" target="#b40">[41]</ref> dataset. We report the PSNR and SSIM results of the enhancement task and the mAP result of the detection task. We compare our normalization with LayerNorm <ref type="bibr" target="#b3">[4]</ref> and ResMLP's normalization <ref type="bibr" target="#b56">[57]</ref>, and then evaluate different parts' contributions of the global branch (predict matrix and predict gamma value). The ablation results are shown in <ref type="table">Table.</ref> 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Blocks &amp; Channels Ablation.</head><p>To evaluate the scalability of our IAT model, we try the different block numbers and channel numbers in the local branch. We try different PEM numbers to generate M and A. The PSNR results on LOL-V2-real <ref type="bibr" target="#b63">[64]</ref> dataset has been shown in <ref type="table">Table.</ref>8. It shows that keeping the same PEM number to generate M and A would be helpful to IAT's performance. Keeping the same block number to generate M and A, we then evaluate with similar parameters to answer whether the local branch should be "short and thick" or "long and thin". The local branch's block number and channel number are respectively set to 2/24 and 4/12 for comparison. The results of PSNR, SSIM and model parameters are reported in <ref type="table">Table.</ref> 9.  10 Additional Qualitative Results.</p><p>In this section we show more qualitative results on low-level vision tasks: image enhancement (LOL (V1 &amp; V2-real) <ref type="bibr" target="#b63">[64]</ref>, MIT-Adobe FiveK <ref type="bibr" target="#b5">[6]</ref>) and exposure correction <ref type="bibr" target="#b1">[2]</ref>. <ref type="figure" target="#fig_6">Fig. 1</ref> shows the image enhancement results on LOL-V1 dataset <ref type="bibr" target="#b63">[64]</ref> compare with RCT <ref type="bibr" target="#b32">[33]</ref> and MBLLEN <ref type="bibr" target="#b42">[43]</ref>, <ref type="figure" target="#fig_0">Fig. 2</ref> shows the image enhancement results on LOL-V2-real dataset <ref type="bibr" target="#b63">[64]</ref> compare with MBLLEN <ref type="bibr" target="#b42">[43]</ref> and KIND <ref type="bibr" target="#b76">[77]</ref>. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the image enhancement results on MIT-Adobe FiveK dataset <ref type="bibr" target="#b5">[6]</ref> compare with Deep-UPE <ref type="bibr" target="#b61">[62]</ref> and Deep-LPF <ref type="bibr" target="#b44">[45]</ref>. We could see that IAT can generate higher quality images which closer to reference target image I t . Meanwhile IAT also take much fewer parameters and less inference time.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Image Enhancement Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Exposure Correction Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Structure of our Illumination Adaptive Transformer (IAT), the black line refers to the parameters generation while the yellow line refers to image processing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Detailed structure of Pixel-wise Enhancement Module (PEM) and Global Prediction Module (GPM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Results on enhancement dataset<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b63">64]</ref>) and exposure correction dataset<ref type="bibr" target="#b1">[2]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>8</head><label></label><figDesc>Joint Training with High-level Framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Joint training Enhancement Module with High-level Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4</head><label>4</label><figDesc>shows the exposure correction results on<ref type="bibr" target="#b1">[2]</ref> dataset, we show both under-exposure and over-exposure results of our IAT, and compare to five experts' results. IAT also generate high quality images, and have ability to handle under/over-exposure at same time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Qualitative comparison results on LOL-V1<ref type="bibr" target="#b63">[64]</ref> dataset, compare with enhancement methods MBLLEN<ref type="bibr" target="#b42">[43]</ref> and RCT<ref type="bibr" target="#b32">[33]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative comparison results on LOL-V2-real<ref type="bibr" target="#b63">[64]</ref> dataset, compare with enhancement methods MBLLEN<ref type="bibr" target="#b42">[43]</ref> and KIND<ref type="bibr" target="#b76">[77]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison results on MIT-Adobe FiveK<ref type="bibr" target="#b5">[6]</ref> dataset, compare with enhancement methods Deep-UPE<ref type="bibr" target="#b61">[62]</ref> and Deep-LPF<ref type="bibr" target="#b44">[45]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison results of both under-exposure and over-exposure images on exposure correction dataset<ref type="bibr" target="#b1">[2]</ref>, left is input image, second row is output of our IAT, right are 5 experts' results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on LOL (V1 &amp; V2)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>PSNR?</cell><cell>18.57</cell><cell>21.57</cell><cell>23.80</cell><cell>21.76</cell><cell>23.04</cell><cell>23.63</cell><cell>25.21</cell><cell>25.32</cell></row><row><cell>SSIM?</cell><cell>0.701</cell><cell>0.843</cell><cell>0.880</cell><cell>0.871</cell><cell>0.893</cell><cell>0.875</cell><cell>0.922</cell><cell>0.920</cell></row><row><cell>#Params.?</cell><cell>-</cell><cell>1.3M</cell><cell>3.3M</cell><cell>-</cell><cell>1.0M</cell><cell>0.8M</cell><cell>0.6M</cell><cell>0.09M</cell></row></table><note>Experimental results on MIT-Adobe FiveK [6] dataset.Metric White-Box [28] U-Net [51] DPE [13] DPED [29] D-UPE [62] D-LPF [45] 3D LUT [74] IAT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table.1. We list different model's test time on their corresponding code platform (M means Matlab, T means TensorFlow, P means PyTorch). As shown inTable 1, IAT (local) only uses the local network to train the model and IAT refers to the whole framework. Our IAT gains SOTA result on both image quality and model efficiency, especially less than 100? FLOPs and parameters usage compare to the current SOTA methods MAXIM<ref type="bibr" target="#b58">[59]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on exposure correction dataset<ref type="bibr" target="#b1">[2]</ref>. Note here HE and LIME<ref type="bibr" target="#b21">[22]</ref> are non-deep learning methods. PSNR, SSIM and PI results, reported by competing works, are from<ref type="bibr" target="#b1">[2]</ref>.PSNR?  SSIM? PSNR? SSIM? PSNR? SSIM? PSNR? SSIM? PSNR? SSIM? HE* [20] 16.14 0.685 16.28 0.671 16.52 0.696 16.63 0.668 17.30 0.688 16.58 0.682 2.405 LIME* [22] 11.15 0.590 11.83 0.610 11.52 0.607 12.64 0.628 13.61 0.653 12.15 0.618 2.432 DPED [29] (Sony) 17.42 0.675 18.64 0.701 18.02 0.683 17.55 0.660 17.78 0.663 17.88 0.676 2.806 DPE [13] (S-FiveK) 16.93 0.678 17.70 0.668 17.74 0.696 17.57 0.674 17.60 0.670 17.51 0.677 2.621</figDesc><table><row><cell cols="2">Method PSNR? SSIM? RetinexNet [64] Expert A 10.76 0.585 11.61 0.596 11.13 0.605 11.99 0.615 12.67 0.636 11.63 0.607 3.105 Expert B Expert C Expert D Expert E Avg PI?</cell></row><row><cell>Deep-UPE [62]</cell><cell>13.16 0.610 13.90 0.642 13.69 0.632 14.80 0.649 15.68 0.667 14.25 0.640 2.405</cell></row><row><cell>Zero-DCE [21]</cell><cell>11.64 0.536 12.56 0.539 12.06 0.544 12.96 0.548 13.77 0.580 12.60 0.549 2.865</cell></row><row><cell>MSEC [2]</cell><cell>19.16 0.746 20.10 0.734 20.20 0.769 18.98 0.719 18.98 0.727 19.48 0.739 2.251</cell></row><row><cell>IAT (local)</cell><cell>16.61 0.750 17.52 0.822 16.95 0.780 17.02 0.773 16.43 0.789 16.91 0.783 2.401</cell></row><row><cell>IAT</cell><cell>19.90 0.817 21.65 0.867 21.23 0.850 19.86 0.844 19.34 0.840 20.34 0.844 2.249</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on low-light detection dataset EXDark<ref type="bibr" target="#b40">[41]</ref>, low-light semantic segmentation dataset ACDC<ref type="bibr" target="#b51">[52]</ref> and various light detection dataset TYOL<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="6">(d) EXDark Detection [41] (e) ACDC Segmentation [52] ( f ) TYOL Detection [26] mAP? time(s)? mIOU? time(s)? mAP? time(s)?</cell></row><row><cell>base-line</cell><cell>76.4</cell><cell>0.033</cell><cell>63.3</cell><cell>0.249</cell><cell>88.4</cell><cell>0.023</cell></row><row><cell>MBLLEN [43]</cell><cell>76.3</cell><cell>0.086</cell><cell>63.0</cell><cell>0.332</cell><cell>95.3</cell><cell>0.105</cell></row><row><cell>DeepLPF [45]</cell><cell>76.3</cell><cell>0.138</cell><cell>61.9</cell><cell>0.807</cell><cell>94.5</cell><cell>0.223</cell></row><row><cell>Zero-DCE [21]</cell><cell>76.9</cell><cell>0.042</cell><cell>61.9</cell><cell>0.300</cell><cell>95.2</cell><cell>0.030</cell></row><row><cell>IAT</cell><cell>77.2</cell><cell>0.040</cell><cell>62.1</cell><cell>0.280</cell><cell>95.8</cell><cell>0.027</cell></row><row><cell cols="3">4.2 Exposure Correction.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison experiments of with (w) and without (w/o) raw-RGB supervision on exposure correction dataset<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="10">Expert A PSNR? SSIM? PSNR? SSIM? PSNR? SSIM? PSNR? SSIM? PSNR? SSIM? Expert B Expert C Expert D Expert E</cell></row><row><cell>w/o raw-RGB</cell><cell>19.90</cell><cell>0.817</cell><cell>21.65</cell><cell>0.867</cell><cell>21.23</cell><cell>0.850</cell><cell>19.86</cell><cell>0.844</cell><cell>19.34</cell><cell>0.840</cell></row><row><cell>w raw-RGB</cell><cell>19.98</cell><cell>0.822</cell><cell>22.03</cell><cell>0.885</cell><cell>21.16</cell><cell>0.843</cell><cell>19.94</cell><cell>0.852</cell><cell>19.48</cell><cell>0.841</cell></row><row><cell cols="6">7 Analyse on Module Structure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison experiments on low-light detection dataset EXDark<ref type="bibr" target="#b40">[41]</ref> and low-light semantic segmentation dataset ACDC<ref type="bibr" target="#b51">[52]</ref>.</figDesc><table><row><cell></cell><cell>original</cell><cell cols="5">pre-enhancement IAT (LOL) IAT (MIT5K) IAT (none) IAT (MIT5K) IAT (LOL) joint training</cell></row><row><cell>EXDark (mAP?)</cell><cell>76.4</cell><cell>77.2</cell><cell>76.9</cell><cell>77.1</cell><cell>77.6</cell><cell>77.8</cell></row><row><cell>ACDC (mIOU?)</cell><cell>63.3</cell><cell>62.1</cell><cell>61.3</cell><cell>61.5</cell><cell>62.1</cell><cell>63.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Experiments on LOL-V2-real<ref type="bibr" target="#b63">[64]</ref> dataset (SSIM, PSNR) and EXDark<ref type="bibr" target="#b40">[41]</ref> dataset (mAP), shows each part's contribution of IAT.</figDesc><table><row><cell>Local Layer [57]'s Our Global Global PSNR? Branch Norm Norm Norm (matrix) (gamma) ? ? 18.80 ? ? 19.61 (+0.81) 0.776 (+0.014) 75.8 (+0.0) SSIM? mAP? 0.762 75.8 ? ? 20.01 (+1.21) 0.786 (+0.024) 76.3 (+0.5) ? ? ? 21.95 (+3.15) 0.811 (+0.049) 76.5 (+0.7) ? ? ? 22.76 (+3.96) 0.805 (+0.043) 76.7 (+0.9) ? ? ? ? 23.50 (+4.70) 0.824 (+0.062) 77.1 (+1.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Blocks Number. 22.85 22.34 3 22.24 23.50 22.67 4 22.42 23.00 23.48</figDesc><table><row><cell>M</cell><cell>A</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>2</cell><cell></cell><cell>22.10</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Channel Number. Long and Thin (12:4) 22.60 0.807 86.22 Short and Thick (24:2) 22.70 0.815 101.03 Ours (16:3) 23.50 0.824 91.15</figDesc><table><row><cell>#Channel:#Block</cell><cell>PSNR? SSIM?</cell><cell>#Param.? (K)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">STUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep white-balance editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning multi-scale photo exposure correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Auto white-balance correction for mixed-illuminant scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Layer normalization. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input / output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<editor>Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extreme-quality computational imaging via degradation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep photo enhancer: Unpaired learning for image enhancement from photographs with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ching</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man-Hsin</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multitask aet with orthogonal tangent regularity for dark object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziteng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised lowlight image enhancement network using attention module and identity invariant loss. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le-Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>Prentice-Hall, Inc., USA</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">013168728</biblScope>
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zero-reference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the connection between local attention and dynamic depth-wise convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flexisp: A flexible camera image processing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bop: Benchmark for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Glentbuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Zabulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crafting object detection in very low light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2021-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exposure: A white-box photo post-processing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dslr-quality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning the image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Wandell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A software platform for manipulating the camera imaging pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Hakki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Karaimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representative color transform for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-Min</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeong Jun</forename><surname>Koh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Colorization transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An alternative technique for the computation of the designator in the retinex theory of color vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Uniformer: Unifying convolution and self-attention for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09450</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Benchmarking low-light image enhancement and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Getting to know low-light images with the exclusively dark dataset. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Seng</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adedotun</forename><surname>Kin Gwn Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumik</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mbllen: Low-light image/video enhancement using cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongsoon</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Db-gan: Boosting object recognition under strong lighting conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kei</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimasa</forename><surname>Kobori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deeplpf: Deep local parametric filters for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Marza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive dynamic range imaging: optical control of pixel exposures over space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Branzoi</forename><surname>Nayar</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2003.1238624</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Computer Vision</title>
		<meeting>Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning exposure correction via consistency modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyun</forename><surname>Ntumba Elie Nsampi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Donggeun Yoo, and In So Kweon. Distort-andrecover: Color enhancement using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Yolo in the dark: Domain adaptation method for merging multiple models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukihiro</forename><surname>Sasagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Nagahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Nighttime visibility enhancement by increasing the dynamic range and suppression of light effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aashish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR46437.2021.01180</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11972" to="11981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An allmlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Ilya O Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dosovitskiy</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24261" to="24272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Maxim: Multi-axis mlp for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Transformer-based restoration of images degraded by adverse weather conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeya Maria Jose</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transweather</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14813</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A physics-based noise formation model for extreme low-light raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Invertible image signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Snr-aware low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="17714" to="17724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Learning to adapt to light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Fu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi-Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Shi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2202.08098" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">From fidelity to perceptual quality: A semi-supervised approach for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deepexposure: Learning to expose photos with asynchronously reinforced adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runsheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Automatic exposure correction of consumer photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Restormer: Efficient transformer for high-resolution image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09881</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning imageadaptive 3d lookup tables for high performance photo enhancement in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.3026740</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.03930</idno>
		<title level="m">Tip-adapter: Training-free clip-adapter for better visionlanguage modeling</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Monodetr: Depth-aware transformer for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanzhuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13310</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Kindling the darkness: A practical low-light image enhancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on multimedia</title>
		<meeting>the 27th ACM international conference on multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Optical flow in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6749" to="6757" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
