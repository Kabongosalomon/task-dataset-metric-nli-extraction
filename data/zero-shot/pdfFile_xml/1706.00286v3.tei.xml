<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING TO COMPUTE WORD EMBEDDINGS ON THE FLY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
							<email>bahdanau@iro.umontreal.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bosc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">Grefenstette</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>London</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Jagiellonian University</orgName>
								<address>
									<settlement>Krakow MILA</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Facebook AI Research MILA</orgName>
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING TO COMPUTE WORD EMBEDDINGS ON THE FLY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the "long tail" of this distribution requires enormous amounts of data. Representations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling. * Work partly done at ElementAI. ? Equal contribution. ? CIFAR Associate Fellow ? CIFAR Senior Fellow</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Natural language yields a Zipfian distribution <ref type="bibr" target="#b30">(Zipf, 1949)</ref> which tells us that a core set of words (at the head of the distribution) are frequent and ubiquitous, while a significantly larger number (in the long tail) are rare. Learning representations for rare words is a well-known challenge of natural language understanding, since the standard end-to-end supervised learning methods require many occurrences of each word to generalize well.</p><p>The typical remedy to the rare word problem is to learn embeddings for some proportion of the head of the distribution, possibly shifted towards the domain-specific vocabulary of the dataset or task at hand, and to treat all other words as out-of-vocabulary (OOV), replacing them with an unknown word "UNK" token with a shared embedding. This essentially heuristic solution is inelegant, as words from technical domains, names of people, places, institutions, and so on will lack a specific representation unless sufficient data are available to justify their inclusion in the vocabulary. This forces model designers to rely on overly large vocabularies, as observed by <ref type="bibr" target="#b17">(Mi et al., 2016;</ref><ref type="bibr" target="#b22">Sennrich et al., 2015)</ref>, which are parametrically expensive, or to employ vocabulary selection strategies <ref type="bibr" target="#b16">(L'Hostis et al., 2016)</ref>. In both cases, we face the issue that words in the tail of the Zipfian distribution will typically still be too rare to learn good representations for through standard embedding methods. Some models, such as in the work of <ref type="bibr" target="#b13">Ling et al. (2015)</ref>, have sought to deal with the open vocabulary problem by obtaining representations of words from characters. This is successful at capturing the semantics of morphological derivations (e.g. "running" from "run") but puts significant pressure on the encoder to capture semantic distinctions amongst syntactically similar but semantically unrelated words (e.g. "run" vs. "rung"). Additionally, nothing about the spelling of named entities, e.g. "The Beatles", tells you anything about their semantics (namely that they are a rock band).</p><p>In this paper we propose a new method for computing embeddings "on the fly", which jointly addresses the large vocabulary problem and the paucity of data for learning representations in the long tail of the Zipfian distribution. This method, which we illustrate in <ref type="figure">Figure 1</ref>, can be summarized as follows: instead of directly learning separate representations for all words in a potentially unbounded vocabulary, we train a network to predict the representations of words based on auxiliary data. Such auxiliary data need only satisfy the general requirement that it describe some aspect of the semantics of the word for which a representation is needed. Examples of such data could be dictionary definitions, Wikipedia infoboxes, linguistic descriptions of named entities obtained from Wikipedia articles, or something as simple as the spelling of a word. We will refer to the content of auxiliary data as "definitions" throughout the paper, regardless of the source. Several sources of auxiliary data can be used simultaneously as input to a neural network that will compute a combined representation. These representations can then be used for out-of-vocabulary words, or combined with withinvocabulary word embeddings directly trained on the task of interest or pretrained from an external data source <ref type="bibr" target="#b18">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b20">Pennington et al., 2014)</ref>. Importantly, the auxiliary data encoders are trained jointly with the objective, ensuring the preservation of semantic alignment with representations of within-vocabulary words. In the present paper, we will focus on a subset of these approaches and auxiliary data sources, restricting ourselves to producing out-of-vocabulary words embeddings from dictionary data, spelling, or both.</p><p>The obvious use case for our method would be datasets and tasks where there are many rare terms such as technical writing or bio/medical text <ref type="bibr" target="#b6">(Del?ger et al., 2016)</ref>. On such datasets, attempting to learn global vectors-for example GloVe embeddings <ref type="bibr" target="#b20">(Pennington et al., 2014</ref>)-from external data, would only provide coverage for common words and would be unlikely to be exposed to sufficient (or any) examples of domain-specific technical terms to learn good enough representations. However, there are no (or significantly fewer) established neural network-based baselines on these tasks, which makes it harder to validate baseline results. Instead, we present results on a trio of well-established tasks, namely reading comprehension, recognizing textual entailment, and a variant on language modelling. For each task, we compare baseline models with embeddings trained directly only on the task objective to those same models with our on the fly embedding method. Additionally, we report results for the same models with pretrained GLoVe vectors as input which we do not update. We aim to show how the gap in results between the baseline and the data-rich GLoVe-based models can be partially but substantially closed merely through the introduction of relatively small amounts of auxiliary definitions. Quantitative results show that auxiliary data improves performance. Qualitative evaluation indicates our method allows models to draw and exploit connections defined in auxiliary data, along the lines of synonymy and semantic relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Arguably, the most popular approach for representing rare words is by using word embeddings trained on very large corpora of raw text. <ref type="bibr" target="#b18">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b20">Pennington et al., 2014)</ref>. Such embeddings are typically explicitly or implicitly based on word co-occurence statistics. Being a big step forward from the models that are trained from scratch only on the task at hand, the approach can be criticized for being extremely data-hungry 1 . Obtaining the necessary amounts of data may be difficult, e.g. in technical domains. Besides, auxiliary training criteria used in the pretraining approaches are not guaranteed to yield representations that are useful for the task at hand. <ref type="bibr" target="#b7">(Dhingra et al., 2017)</ref> proposed to represent OOV words by fixed random vectors. While this has shown to be effective for machine comprehension, this method does not account for word semantics at all, and therefore, does not cover the same ground as the method that we propose.</p><p>There have been a number of attempts to achieve out-of-vocabulary generalization by relying on the spelling. <ref type="bibr" target="#b13">(Ling et al., 2015)</ref> used a bidirectional LSTM to read the spelling of rare words and showed that this can be helpful for language modeling and POS tagging. We too will investigate spelling as a  <ref type="figure">Figure 1</ref>: An example of how we deal with out-of-vocabulary words (indicated in bold). We obtain representations by retrieving external information (e.g. a dictionary definition) and embedding it, for example, with another LSTM-RNN, instead of using a catch-all "UNK" representation for out-of-vocabulary items.</p><p>source of auxiliary data. In this respect, the approach presented here subsumes theirs, and can be seen as a generalization to other types of definitions.</p><p>The closest to our work is the study by <ref type="bibr" target="#b10">Hill et al. (2016)</ref>, in which a network is trained to produce an embedding of a dictionary definition that is close to the embedding of the headword. The network is shown to be an effective reverse dictionary and a crossword solver. Our approach is different in that we train a dictionary reader in an end-to-end fashion for a specific task, side-stepping the potentially suboptimal auxiliary ranking cost that was used in that earlier work. Their method also relies on the availability of high-quality pretrained embeddings which might not always be feasible. Another related work by <ref type="bibr" target="#b14">Long et al. (2016)</ref> uses dictionary definitions to provide initialization to a database embedding method, which is different from directly learning to use the definitions like we do. Concurrently with this work <ref type="bibr" target="#b26">Weissenborn et al. (2017)</ref> studied dynamic integration of background knowledge from a commonsense knowledge base. In another concurrent work <ref type="bibr" target="#b15">Long et al. (2017)</ref> build a new dataset for named entity prediction and show that external knowledge can be very useful for this task.</p><p>At a higher level our approach belongs to the a broad family of methods for conditioning neural networks on external knowledge. For example, <ref type="bibr" target="#b12">Larochelle et al. (2008)</ref> propose to add classes to a classifier by representing them using their "descriptions". By description they meant, for example, a canonical picture of a printed character, that would represent all its possible handwritten versions. Their idea to rely on descriptions is similar to our idea to rely on definitions, however we focus on understanding complex inputs instead of adding new output classes.</p><p>Enhancing word embeddings with auxiliary data from knowledge bases (including wordnet) has a long tradition <ref type="bibr" target="#b29">(Xu et al., 2014;</ref><ref type="bibr" target="#b8">Faruqui et al., 2015)</ref>. Our work differs from previous approaches in essential ways. First, we use a textual form and are not restricted to knowledge represented as a graph. Second, we learn in an end to end fashion, allowing the model to pick useful information for the task of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ON THE FLY EMBEDDINGS</head><p>In general, a neural network processes a language input by replacing its elements x i , most often words, with the respective vectors e(x i ), often called embeddings <ref type="bibr" target="#b1">(Bengio et al., 2003)</ref>. Embeddings are typically either trained from scratch or pretrained. When embeddings are trained from scratch, a restricted vocabulary V train = {w 1 , . . . , w n } is defined, usually based on training set frequency. Words not in V train are replaced by a special token UNK with a trainable embedding e(UNK ). Unseen test-time words w / ? V train are then represented by e(UNK ), which effectively means the specific meaning of this word is lost. Even if w had been included in V train but was very rare, its learned embedding e(w) would likely not be very informative.</p><p>The approach proposed in this work, described in <ref type="figure">Figure 1</ref>, is to use definitions from auxiliary data, such as dictionaries, to compute embeddings of rare words on the fly, as opposed to having a persistent embedding for each of them. More specifically, this involves fetching a definition d(w) = (x 1 , . . . , x k ) and feeding it into a network f that produces an embedding e d (w) = f (e (x 1 ), . . . , e (x k )). We will refer to e d (w) as a definition embedding produced by a definition reader f . One can either use the same embeddings e = e when reading the dictionary or train different ones. Likewise, one can either stick to a shared vocabulary V dict = V train , or consider two different ones. When a word x i ? V dict is encountered, it is replaced by UNK and the respective trainable embedding e (UNK ) is used. For the function f we consider three choices: a simple mean pooling (</p><formula xml:id="formula_0">MP) e d (w) = k i=1 e (x i )/k, a mean pooling (MP-L) with a linear transfor- mation e d (w) = k i=1 V e (x i )/k,</formula><p>where V is a trainable matrix, and lastly, using the last state of an LSTM <ref type="bibr" target="#b11">(Hochreiter &amp; Schmidhuber, 1997)</ref>, e d (w) = LST M (e (x 1 ), . . . , e (x k )). Many words have multiple dictionary definitions. We combine embeddings for multiple definitions using mean pooling. We include all definitions whose headwords match w or any possible lemma of a lower-cased w 2 . To simplify the notation, the rest of the paper assumes that there is only one definition for each word.</p><p>While the primary purpose of definition embeddings e d (w) is to inform the network about the rare words, they might also contain useful information for the words in V train . When we use both, we combine the information coming from the embeddings e(w) and the definition embeddings e d (w) by computing e c (w) = e(w) + W e d (w), where W is a trainable matrix, or just by simply summing the two, e c (w) = e(w) + e d (w). Alternatively it is possible to just use e c (w) = e(w) for w from V train and e c (w) = e d (w) otherwise. When no definition is available for a word w, we posit that e d (w) is a zero vector. A important observation that makes an implementation of the proposed approach feasible is that the definitions d(x i ) of all words x i from the input can be processed in parallel 3 .</p><p>To keep things simple, we limited the scope and complexity of this first incarnation of our approach as follows. First, we do not consider definitions of word combinations, such as phrasal verbs like "give up" and geographical entities like "San Francisco". Second, our definition reader could itself better handle the unknown words w / ? V dict by using their definition embeddings e d (w) instead e (UNK ), thereby implementing a form of recursion. We will investigate both in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">QUESTION ANSWERING</head><p>We used the Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b21">(Rajpurkar et al., 2016</ref>) that consists of approximately 100000 human-generated question-answer pairs. For each pair, a paragraph from Wikipedia is provided that contains the answer as a continuous span of words.</p><p>Our basic model is a simplified version of a coattention network proposed in <ref type="bibr" target="#b28">(Xiong et al., 2016)</ref>. First, we represent the context of length n and the question of length m as matrices C ? R n,d and Q ? R m,d by running them through an LSTM and a linear transform. Next, we compute the affinity scores L = CQ T ? R n,m . By normalizing L with row-wise and column-wise softmaxes we construct context-to-question and question-to-context attention maps A C and A Q . These are used to construct a joint question-document representation U 0 as a concatenation along the feature axis of the matrices C, A C Q and A C A T Q C. We transform U 0 with a bidirectional LSTM and another ReLU <ref type="bibr" target="#b9">(Glorot et al., 2011)</ref> layer to obtain the final context-document representation U 2 . Finally, two linear layers followed by a softmax assign to each position of the document probabilities of it being the beginning and the end of the answer span. We refer the reader to the work of <ref type="bibr" target="#b28">Xiong et al. (2016)</ref> for more details. Compared to their model, the two main simplifications that we applied is skipping the iterative inference procedure and using usual ReLU instead of the highway-maxout units.</p><p>Our baseline is a model with the embeddings trained purely from scratch. We found that it performs best with a small vocabulary of 10k most common words from the training set. This can be explained by a rather moderate size of the dataset: all the models we tried tended to overfit severely. In our preliminary experiments, we found that a smaller V train of 3k words is better for the models using auxiliary data, and that combining the information from definitions and word embeddings is helpful. Unless otherwise specified, we use summation preceded by a linear transformation for composing word embedding with definition embeddings: e c (w) = e(w) + W e d (w).</p><p>We tried different models (MP and LSTM) for reading the dictionary and used an LSTM for reading the spelling. In addition to using either the spelling or the dictionary definitions, we tried mixing the dictionary definitions with the spelling. When both dictionary and spelling were used, we found that using a LSTM for reading the spelling and MP-L for reading dictionary definitions works best. As mentioned in Section 3 our dictionary lookup procedure involves lowercasing and lemmatization. In order to estimate the contribution of these steps we add to the comparison a model that fetches the spelling of a lemmatized and lowercased version of each word. The last model in our comparison is trained with the GLoVe embeddings. Except for GloVe embeddings, all vectors, such as word embeddings and LSTM states, have d=200 dimensions in all models.</p><p>The results are reported in <ref type="table" target="#tab_0">Table 1</ref>. We report the exact match ratio as computed by the evaluation tools provided with the dataset, which is basically the accuracy of selecting the right answer. We report the average over multiple runs on the development set. Looking at the results one can see that adding any external information results in a significant improvement over the baseline model (B) (3.7 -10.5 points). When the dictionary alone is used, mean pooling (D3) performs similarly to LSTM (D4). For the model with mean pooling, we verified that the matrix W in computing e c (w) is necessary for best results (see model D2), and back-propagating through the process of reading definition is helpful (see model D1). We thereby establish that our method is preferable to the one by <ref type="bibr" target="#b14">Long et al. (2016)</ref>, which prescribes mean pooling of available embeddings without end-to-end training.</p><p>We found that adding the spelling (S) helps more than adding a dictionary (D) (3 points difference), possibly due to relatively lower coverage of our dictionary. However, the model that uses both (SD) has a 1.1 point advantage over the model that uses just the spelling (S), demonstrating that combining several forms of auxiliary data allows the model to exploit the complementary information they provide. The model with GLoVe embeddings (G) is still ahead with a 1.1 point margin, but the gap has been shrunk.</p><p>Finally, we evaluated the models S, SL and SD on the test set. The parameters for test evaluation were selected based on the development set results 5 . The test set results confirm the benefit of using dictionary definitions (SD has a 1 point advantage over S). Lastly, the model that uses lemmatization and lowercasing (SL) to retrieve spelling does not outperform S, which shows that the advantage To understand how our model successfully uses the dictionary and what prevents it from using it better, we conducted a qualitative investigation on selected examples. Namely, we considered examples on which SD selected the correct answer span and S did not, and from them we chose the ones with the largest difference of log-likelihoods that S and SD assigned to the correct answer. <ref type="figure" target="#fig_1">Figure 2</ref> shows the attention maps A C for both models for one of the examples. We can see that S has no clue that "overseas" may be an answer to an question about location, whereas SD is aware of that, presumably thanks to the definition "overseas -&gt; in a foreign country". In a similar manner we observed SD being able to match "direction" and "eastwards", "where" and "outdoors". Another pattern that we saw is that the model with the dictionary was able to answer questions of the form "which scientist" or "which actress" better. Both words "scientist" and "actress" were not frequent enough to make it to V train , but the definitions "actress -&gt; a female actor" "scientist -&gt; a person with advanced knowledge of one or more sciences" apparently provided enough information about these words that the model could start matching them with named entities in the passage.</p><p>Furthermore, we compared the models G and SD in a similar way. We found that often SD simply was missing a definition. For a example, it was not able to match "XBox" and "console", "Mark Twain" and "author", "most-watched" and "most watched". We also saw cases where the definition was available but was not used, seemingly because the key word in the definition was outside V train = V dict . For example, "arrow" was defined as "a projectile with a straight thin shaft", and the word "projectile" was quite rare in the training corpus. As a consequence, the model had no chances to understand that an arrow is a weapon and match the word "arrow" in the context with the word "weapon" in the question. Likewise, "Earth" was defined as "planet", but "planet" was outside V train . Finally, we saw cases where inferring important aspects of meaning from the dictionary would be non-trivial, for example, guessing that "historian" is a "profession" from the definition "a person who is an authority on history and who studies it and writes about it" would involve serious common sense reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ENTAILMENT PREDICTION</head><p>We used the Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref>, which consists of around 500k pairs of sentences (hypothesis and premise) and the task is to predict the logical relation (contradiction, neutral or entailment) between them. In addition we used the Multi-Genre Natural Language Inference (MultiNLI) corpus <ref type="bibr" target="#b27">(Williams et al., 2017)</ref>, which effectively is a more recent and more diverse version of SNLI of approximately the same size. A key distinction of MultiNLI from SNLI is the availability of a "matched" and a "mismatched" development and test set. The matched test and development sets contain data from the same domains as the training set, whereas the mismatched ones were intentionally collected using data from different domains.</p><p>We implemented a variant (replacing TreeLSTM by biLSTM) of Enhanced Sequential Inference Model (ESIM) <ref type="bibr" target="#b5">(Chen et al., 2016)</ref>   Similarly to the SQuAD experiments, we found that the baseline model performs best with a larger vocabulary (5k words for SNLI, 20k words for MultiNLI) than the model that uses auxiliary information (3k words). Differently from SQuAD, we found it helpful to use a different vocabulary V dict = V train . We built V dict by collecting the 11000 words that occur most often in the definitions, where each definition is weighted with the frequency of its headword in the training data. While in theory it would still be possible to share word embeddings between the main model and the definition reader even when they have different vocabularies, we opted for the simpler option of using separate word embeddings e and e . Since with separate word embeddings having a subsequent linear transformations would be redundant, we simply add the result of mean pooling k i=1 e (x i )/k to e(w). We use an LSTM for reading the spelling, but unlike the SQuAD experiments, we found that simply adding the last hidden state of the spelling LSTM to e(w) worked better. We also tried to use a LSTM for reading definitions but it did no better than the simpler mean pooling. Our last model used pretrained GloVe embeddings. 300-dimensional ESIM performed best for the baseline and GloVe models, whereas using just 100 dimensions worked better for the models using auxiliary information. All runs were repeated 3 times and scores are averaged.</p><p>Results on SNLI and MultiNLI are presented in <ref type="table" target="#tab_2">Table 2</ref>. Using dictionary definitions allows us to bridge ?40% of the gap between training from scratch and using embeddings pretrained on 840 billion words, and this improvement is consistent on both datasets. Compared to the SQuAD results, an important difference is that spelling was not as useful on SNLI and MultiNLI. We also note that we tried using fixed random embeddings for OOV words as proposed by <ref type="bibr" target="#b7">(Dhingra et al., 2017)</ref>, and that this method did not bring a significant advantage over the baseline.</p><p>In order to gain some insights on the performance of our entailment recognition models, in <ref type="figure" target="#fig_2">Figure 3a</ref> we plot a t-SNE (van der Maaten &amp; Hinton, 2008) visualization of word embeddings computed for  the words from BLESS dataset <ref type="bibr" target="#b0">(Baroni &amp; Lenci, 2011)</ref>. Specifically, we used embeddings produced by the definition encoder of our best SNLI model using Wordnet definitions. The BLESS dataset contains three categories of words: fruits, tools and vehicles. One can see that fruit words tend to be separated from tool and vehicle words. <ref type="figure" target="#fig_2">Figure 3b</ref> shows that, as expected, dictionary-enabled models significantly outperform baseline models for sentences containing rare words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LANGUAGE MODELLING</head><p>Experiments in the previous sections used datasets of moderate size. To get an idea of how useful different auxiliary data sources will be for datasets of various sizes, we also apply our approach to the One Billion Words (OBW) language modelling task <ref type="bibr" target="#b4">(Chelba et al., 2014)</ref>. In the first round of experiments, we use only 1% of the training data (? 10 7 words) and in the second we train our models on the whole training set (? 10 9 words). Similarly to prior work on using the spelling <ref type="bibr" target="#b13">(Ling et al., 2015)</ref> we restrict the softmax output layer to only predict probabilities of the 10k most common words, however, we do not impose such a constraint when the model processes the words from the input context. We want to assess thereby if having a definition of an observed word helps the model to predict the following ones from a restricted vocabulary.</p><p>Our baseline model is an LSTM with 500 units and with trainable input embeddings for the 10k most frequent input words. This covers around 90.24% of all word occurrences. We consider computing embeddings of the less frequent input words from their dictionary definitions, GloVe vectors and spellings. These sources of auxiliary information were available for 63.35%, 97.43% and 100% of the rest of occurrences respectively. In order to compare how helpful these sources are when they are available, we run additional set of experiments with "restricted" inputs. Specifically, we only use auxiliary information for a word if it has both a GloVe embedding and a dictionary definition. When the word does not have any of these, we replace it with "UNK". We report results for three variants of dictionary-enabled models. The first variant (dict1) uses the same LSTM for reading the text and the definitions. The second one (dict2) has two separate LSTMs but the word embeddings are shared 6 . The third variant (dict+spelling) adds spelling to our best dictionary model. Lastly, we trained a model that used the lowercased lemma of the word as the definition.</p><p>The training was early-stopped using a development set. We report the test perplexities in <ref type="table" target="#tab_3">Table 3</ref>. Similarly to our other experiments, using external information to compute embeddings of unknown words helps in all cases. We observe a significant gain even for dict1, which is remarkable as this model has the same architecture and parameters as the baseline. We note that lemma+lowercase performs worse than any model with the dictionary, which suggests that dictionary definitions are used in a non-trivial way. Adding spelling consistently helps more than adding dictionary definitions. In our experiments with restricted inputs ((R) in <ref type="table" target="#tab_3">Table 3</ref>), spelling and dict2 show similar performance, which suggests that this difference is mostly due to the complete coverage of spelling. Using both dictionary and spelling is consistently slightly better than using just spelling, and the improvement is more pronounced in the restricted setting. Using GloVe embeddings results in the best perplexity. Switching to the full training set shrank all the gaps.</p><p>To zoom in on how the models deal with rare words, we look at the perplexities of the words that appear right after out-of-vocabulary words (PPL after OOV). We can see that the ranking of different models mostly stays the same, yet the differences in performance become larger. For example, on the 1% version of the dataset, in the restricted setting, adding definitions to the spelling helps to bridge half of the 6 points PPL after OOV gap between spelling and GloVe. This is in line with our expectations that when definitions are available they should be helpful for handling rare words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>We showed how different sources of auxiliary information, such as the spelling and a dictionary of definitions can be used to produce on the fly useful embeddings for rare words. While it was known before that adding the spelling information to the model is helpful, it is often hard or not possible to infer the meaning directly from the characters, as confirmed by our entailment recognition experiments. Our more general approach offers endless possibilities of adding other data sources and learning end-to-end to extract the relevant bits of information from them. Our experiments with a dictionary of definitions show the feasibility of the approach, as we report improvements over using just the spelling on question answering and semantic entailment classification tasks. Our qualitative investigations on the question answering data confirms our intuition on where the improvement comes from. It is also clear from them that adding more auxiliary data would help, and that it would probably be also useful to add definitions not just for words, but also for phrases (see "Mark Twain" from Section 4.1). We are planning to add more data sources (e.g. first sentences from Wikipedia articles) and better use the available ones (WordNet has definitions of phrasal verbs like "come across") in our future work.</p><p>An important question that we did not touch in this paper is how to deal with rare words in the auxiliary information, such as dictionary definitions. Based on our qualitative investigations (see the example with "arrow" and "weapon" in Section 4.1), we believe that better handling rare words in the auxiliary information could substantially improve the proposed method. It would be natural to use on the fly embeddings similarly to the ones that we produce for words from the input, but the straight-forward approach of computing them on request would be very computation and memory hungry. One would furthermore have to resolve cyclical dependencies, which are unfortunately common in dictionary data (when e.g. "entertainment" is defined using "diverting" and "diverting" is defined using "entertainment"). In our future work we want to investigate asynchronous training of on the fly embeddings and the main model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have shown that introducing relatively small amounts of auxiliary data and a method for computing embeddings on the fly using that data bridges the gap between data-poor setups, where embeddings need to be learned directly from the end task, and data-rich setups, where embeddings can be pretrained and sufficient external data exists to ensure in-domain lexical coverage. A large representative corpus to pretrain word embeddings is not always available and our method is applicable when one has access only to limited auxiliary data. Learning end-to-end from auxiliary sources can be extremely data efficient when these sources represent compressed relevant information about the word, as dictionary definitions do. A related desirable aspect of our approach is that it may partially return the control over what a language processing system does into the hands of engineers or even users: when dissatisfied with the output, they may edit or add auxiliary information to the system to make it perform as desired. Furthermore, domain adaptation with our method could be carried out simply by using other sources of auxiliary knowledge, for example definitions of domain-specific technical terms in order to understand medical texts. Overall, the aforementioned properties of our method make it a promising alternative to the existing approaches to handling rare words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The attention maps A C of the models with (on the left) and without the dictionary (on the right). The rows correspond to words of the context and the columns to the words of the question. One can see how with the help of the dictionary the model starts considering "overseas" as a candidate answer to "where". P ? R m,d matrices by encoding them using a bidirectional LSTM. Analogously, alignment matrices A H and A P are computed by normalizing affinity scores. These alignment matrices are used to form joint hypothesis-premise representations. For the hypothesis we compute and concatenate H, A H P , H ? A H P and H A H P , yielding a h ? R n,4d sentence embedding, and proceed similarly for the premise. The resulting sentence representations are then processed in parallel by a bidirectional LSTM and the final states of the LSTMs are concatenated and processed by a single layer Tanh MLP to predict entailment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) t-SNE projection of word embeddings computed on the fly. (b) Prediction accuracy of different ESIM models trained on SNLI, as a function of the mean log rank of words in the input. As expected, the dictionary-enabled model clearly outperforms the baseline on sentences contaning rare words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Exact match (EM) ratio for different models on SQuAD development and tests set. "dict" stands for dictionary, "MP" stands for mean pooling.</figDesc><table><row><cell>model</cell><cell cols="2">EM dev EM test</cell></row><row><cell>baseline (B)</cell><cell>52.58</cell><cell>-</cell></row><row><cell>dict, MP, sum, no back-prop (D1)</cell><cell>56.27</cell><cell>-</cell></row><row><cell>dict, MP, sum (D2)</cell><cell>57.03</cell><cell>-</cell></row><row><cell>dict, MP, transform and sum (D3)</cell><cell>58.9</cell><cell>-</cell></row><row><cell>dict, LSTM (D4)</cell><cell>58.78</cell><cell>-</cell></row><row><cell>spelling (S)</cell><cell>61.94</cell><cell>62.9</cell></row><row><cell>spelling+lemmas (SL)</cell><cell>62.4</cell><cell>62.6</cell></row><row><cell>spelling+dict (SD)</cell><cell>63.06</cell><cell>64.08</cell></row><row><cell>GloVe (G)</cell><cell>64.19</cell><cell>-</cell></row></table><note>of SD over S is not due to the normalization procedures and that SD must be using the dictionary definitions in a non-trivial way.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on SNLI and MultiNLI. X/Y means X percent accuracy on the development set and Y percent accuracy on the test set. "dict" stands for "dictionary".</figDesc><table><row><cell></cell><cell>SNLI</cell><cell></cell><cell cols="2">MultiNLI</cell></row><row><cell></cell><cell></cell><cell cols="2">matched</cell><cell cols="2">mismatched</cell></row><row><cell cols="6">baseline 83.39/82.84 69.05/68.55 67.22/68.57</cell></row><row><cell cols="6">spelling 83.78/82.89 69.76/68.89 70.48/69.76</cell></row><row><cell>dict</cell><cell cols="5">84.88/84.39 71.39/71.45 71.65/70.7</cell></row><row><cell>GloVe</cell><cell cols="5">87.20/86.39 74.63/74.58 73.32/73.92</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.90</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.88</cell><cell></cell></row><row><cell>frigate motorcycle</cell><cell></cell><cell></cell><cell>0.86</cell><cell></cell></row><row><cell cols="2">axe screwdriver chisel fork hammer corkscrew hatchet knife rake saw sieve spade spoon wrench coconut shovel cherry lemon cranberry lime peach glider plum ambulance battleship apricot grapefruit grape banana apple pear pineapple strawberry bomber bus fighter helicopter scooter train truck jet car ferry van tool tanker yacht fruit vehicle (a)</cell><cell>Accuracy</cell><cell>0.84 0.82 0.80 0.76 0.78</cell><cell cols="2">ESIM baseline ESIM dict ESIM spelling ESIM GloVe</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>6</cell><cell>7 Mean log rank</cell><cell>8</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on 1% of OBW and full OBW training set. We report 2 variants of perplexities on the test set. The experiments where coverage is restricted are indicated by (R). See Section 4.3 for explanations.</figDesc><table><row><cell></cell><cell></cell><cell>1%</cell><cell></cell><cell>100%</cell></row><row><cell>model</cell><cell cols="4">PPL PPL after OOV PPL PPL after OOV</cell></row><row><cell>baseline</cell><cell>73.07</cell><cell>102.09</cell><cell>46.77</cell><cell>70.76</cell></row><row><cell cols="2">lemma+lowercase 70.57</cell><cell>86.24</cell><cell>45.02</cell><cell>56.45</cell></row><row><cell>spelling</cell><cell>67.06</cell><cell>55.04</cell><cell>39.77</cell><cell>26.27</cell></row><row><cell>glove</cell><cell>63.83</cell><cell>44.26</cell><cell>39.15</cell><cell>24.20</cell></row><row><cell>dict1</cell><cell>69.14</cell><cell>68.38</cell><cell>42.04</cell><cell>36.50</cell></row><row><cell>dict2</cell><cell>68.08</cell><cell>65.77</cell><cell>41.34</cell><cell>34.47</cell></row><row><cell>dict2+spelling</cell><cell>66.23</cell><cell>52.06</cell><cell>39.56</cell><cell>25.22</cell></row><row><cell>spelling (R)</cell><cell>68.24</cell><cell>63.65</cell><cell>41.34</cell><cell>33.61</cell></row><row><cell>glove (R)</cell><cell>66.69</cell><cell>57.21</cell><cell>41.22</cell><cell>32.35</cell></row><row><cell>dict2 (R)</cell><cell>68.06</cell><cell>65.62</cell><cell>41.34</cell><cell>34.53</cell></row><row><cell cols="2">dict2+spelling (R) 67.43</cell><cell>60.58</cell><cell>40.89</cell><cell>32.48</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The GLoVe embeddings used are computed using 840 billion words of English text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">EXPERIMENTSWe worked on extractive question answering, semantic entailment classification and language modelling. For each task, we picked a baseline model and a architecture from the literature which we knew would provide sensible results, to explore how augmenting it with on the fly embeddings would affect performance. We explored two complementary sources of auxiliary data. First, we used word definitions from WordNet<ref type="bibr" target="#b19">(Miller, 1995)</ref>. While WordNet is mostly known for its structured information about synonyms, it does contain natural language definitions for all its 147306 lemmas (this also includes multi-word headwords which we do not consider in this work) 4 . Second, we experimented with the character-level spelling of words as auxiliary data. To this end, in order to fit in with our use of dictionaries, we added fake definitions of the form "Word" ? "W", "o", "r", "d".In order to measure the performance of models in "data-rich" scenarios where a large amount of unlabelled language data is available for the training of word representations, we used as pretrained word embeddings 300-dimensional GLoVe vectors trained on 840 billion words<ref type="bibr" target="#b20">(Pennington et al., 2014)</ref>. We compared our auxiliary data-augmented on the fly embedding technique to baselines and models with fixed GLoVe embeddings to measure how well our technique closes the gap between a data-poor and data-rich scenario.2  We used the wordnet-based lemmatizer from NLTK.3  The same applies for all the words from a mini-batch of examples. By composing huge batches of up 10000 definitions from a mini-batch of examples, we were able to process them all in a reasonable time on GPUs. 4 Advantages of using WordNet include its free availability and the ease of parsing, e.g., we used the NLTK<ref type="bibr" target="#b2">(Bird, 2006)</ref> interface to extract the definitions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We were limited in how many models could be evaluated on the test set by the fact that test set evaluation is done by SQuAD authors manually.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We also tried learning separate embeddings in both cases, as well as building a separate vocabulary but this did not help.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank the developers of Theano <ref type="bibr" target="#b23">(Theano Development Team, 2016)</ref> and Blocks (van Merri?nboer et al., 2015)   for their great work. We thank NVIDIA for giving access to their DGX-1 computers used in this work. Stanis?aw Jastrz?bski was supported by Grant No. DI 2014/016644 from Ministry of Science and Higher Education, Poland. We also thank NSERC, IBM, CIFAR and Samsung for provided funding. We thank ?aglar G?l?ehre and Alexandre Lacoste for useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How we blessed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<idno>978-1-937284-16-9</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=2140490.2140491" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v3/bengio03a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Interactive presentation sessions</title>
		<meeting>the COLING/ACL on Interactive presentation sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<ptr target="http://www.isca-speech.org/archive/interspeech_2014/i14_2635.html" />
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2635" to="2639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Enhancing and combining sequential and tree LSTM for natural language inference. ArXiV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/1609.06038</idno>
		<ptr target="http://arxiv.org/abs/1609.06038" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of the bacteria biotope task at bionlp shared task 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louise</forename><surname>Del?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bossy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estelle</forename><surname>Chaix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mouhamadou</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th BioNLP Shared Task Workshop</title>
		<meeting>the 4th BioNLP Shared Task Workshop<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Arnaud Ferr?, Philippe Bessieres, and Claire N?dellec</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Comparative Study of Word Embeddings for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00993</idno>
		<idno>arXiv: 1703.00993</idno>
		<ptr target="http://arxiv.org/abs/1703.00993" />
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N/N15/N15-1184.pdf" />
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-05" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">275</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to understand phrases by embedding the dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Library/AAAI/2008/aaai08-103.php" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Third AAAI Conference on Artificial Intelligence<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07-13" />
			<biblScope unit="page" from="646" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Lu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu?s</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram?n</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02096</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging lexical resources for learning entity embeddings in multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P/P16/P16-2019.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">World knowledge for reading comprehension: Rare entity prediction with hierarchical lstms using external descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="836" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vocabulary selection strategies for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L&amp;apos;</forename><surname>Gurvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hostis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Vocabulary manipulation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03209</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/219717.219748</idno>
		<ptr target="http://doi.acm.org/10.1145/219717.219748" />
	</analytic>
	<monogr>
		<title level="j">Communications of ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1162.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D16/D16-1264.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visualizing high-dimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bart Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warde-Farley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00619</idno>
		<title level="m">Chorowski, and Yoshua Bengio. Blocks and fuel: Frameworks for deep learning</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dynamic Integration of Background Knowledge in Neural NLU Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02596</idno>
		<idno>arXiv: 1706.02596</idno>
		<ptr target="http://arxiv.org/abs/1706.02596" />
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<ptr target="https://arxiv.org/abs/1704.05426" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rc-net: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2661829.2662038</idno>
		<ptr target="http://doi.acm.org/10.1145/2661829.2662038" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM &apos;14</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Human behavior and the principle of least effort: An introduction to human ecology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zipf</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
