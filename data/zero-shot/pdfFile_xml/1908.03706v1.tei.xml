<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting temporal consistency for real-time video depth estimation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhouhan</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting temporal consistency for real-time video depth estimation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accuracy of depth estimation from static images has been significantly improved recently, by exploiting hierarchical features from deep convolutional neural networks (CNNs). Compared with static images, vast information exists among video frames and can be exploited to improve the depth estimation performance. In this work, we focus on exploring temporal information from monocular videos for depth estimation. Specifically, we take the advantage of convolutional long short-term memory (CLSTM) and propose a novel spatial-temporal CSLTM (ST-CLSTM) structure. Our ST-CLSTM structure can capture not only the spatial features but also the temporal correlations/consistency among consecutive video frames with negligible increase in computational cost. Additionally, in order to maintain the temporal consistency among the estimated depth frames, we apply the generative adversarial learning scheme and design a temporal consistency loss. The temporal consistency loss is combined with the spatial loss to update the model in an end-to-end fashion. By taking advantage of the temporal information, we build a video depth estimation framework that runs in real-time and generates visually pleasant results. Moreover, our approach is flexible and can be generalized to most existing depth estimation frameworks. Code is available at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Benefiting from the powerful convolutional neural networks (CNNs), some recent methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> have achieved outstanding performance on depth estimation from monocular static images. The success of these methods is based on the deeply stacked network structures and large amount of training data. For instance, the state-of-the-art depth estimation model DORN <ref type="bibr" target="#b1">[2]</ref> has more than one hundred of convolution layers, the high computational cost may hamper it from practical applications. However, in some scenarios such as automatic driving <ref type="bibr" target="#b5">[6]</ref> and robots navigation <ref type="bibr" target="#b6">[7]</ref>, estimating of depths in real-time is required. Directly extend existing methods from static image to video sequence is not feasible because of the excessive computational cost. In addition, sequential frames which contain rich temporal information are usually provided in such scenarios. The existing methods fail to take the temporal information into consideration.</p><p>In this work, we exploit temporal information from videos by making use of the convolutional long short-term memory (CLSTM) and the generative adversarial networks (GANs), and propose a real-time depth estimation framework. We illustrate our proposed framework in <ref type="figure">Fig. 1</ref>. It consists of three main parts: 1) spatial features extraction part; 2) temporal correlations collection part and 3) spatial-temporal loss calculation part. The spatial features extraction part and the temporal correlations collection part compose our novel spatial-temporal CLSTM (ST-CLSTM) structure. The spatial features extraction part first takes as input n continuous frames x 1 , x 2 , ? ? ? , x n and outputs high level features f 1 , f 2 , ? ? ? , f n . The temporal correlations collection part then takes as input the high-level features and outputs depth estimations d 1 , d 2 , ? ? ? , d n . With the cell and gate modules, the CLSTM can make use of the cues acquired from the previous frame to reason the current frame, and thus encode the temporal information. As for spatial-temporal loss calculation, we first calculate the spatial loss between the estimated and the ground-truth depths. In order to further enforce the temporal consistency, we design a new temporal loss by introducing a generative adversarial learning scheme. Specifically, we apply a 3D CNN as the discriminator which takes as input the estimated and ground-truth depth sequences and outputs the temporal loss. The temporal loss is combined with the spatial loss and back propagated through the entire framework to update the weights in an end-to-end fashion.</p><p>To summarize, our main contributions are as follows.</p><p>? We propose a novel ST-CLSTM structure that is able to capture spatial features as well as temporal correlations for video depth estimation. To our knowledge, this is the first time that CLSTM is employed for video depth estimation. ? We design a novel temporal consistency loss by using the generative adversarial learning scheme. Our temporal loss can further enforce the temporal consistency and improve the performance for video depth estimation. ? Our proposed video depth estimation framework can execute in real-time and can be generalized to most existing depth estimation frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Depth estimation Recently, many deep learning based depth estimation methods have been proposed and achieved significant achievements. To name a few, Eigen et al. <ref type="bibr" target="#b3">[4]</ref> employed a multi-scale neural network with two components to generate coarse estimations globally and refine the results locally. Xie et al. <ref type="bibr" target="#b7">[8]</ref> used shortcut connections in their network to fuse low-level and high-level features. Cao et al. <ref type="bibr" target="#b8">[9]</ref> proposed to formulate depth estimation as a classification problem instead of a regression problem. Laina et al. <ref type="bibr" target="#b4">[5]</ref> employed a reverse huber loss to estimate depth distributions and an up-sampling module to overcome the low-resolution problem. Yin et al. <ref type="bibr" target="#b9">[10]</ref> designed a loss term to enforce geometric constraints. To further improve the performance, some methods incorporate conditional random fields in their methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Recently the method DORN <ref type="bibr" target="#b1">[2]</ref> proposed a spacing-increasing discretization (SID) policy and estimated depths with a ordinal regression loss. Although excellent performance has been achieved, the networks are deep and computation is heavy.</p><p>Some other works focus on estimating depth values from videos. Zhou et al. <ref type="bibr" target="#b0">[1]</ref> proposed to use bundle adjustment as well as a super-resolution network to improve depth estimation. Specifically, the bundle adjustment is used to estimate depths and camera poses simultaneously, and the superresolution network is used to recover details. Mahjourian et al. <ref type="bibr" target="#b2">[3]</ref> incorporated a 3D loss with geometric constraints to estimate depths and ego-motions simultaneously. In this work, we propose to estimate depths by exploiting temporal information from videos.</p><p>CLSTM in video analysis Recurrent neural networks (RNNs), especially the long short-term memories (LSTMs) have achieved great success in various computer vision tasks such as language processing <ref type="bibr" target="#b12">[13]</ref> and speech recognition <ref type="bibr" target="#b13">[14]</ref>. With the memory cells, LSTMs can capture short and long term temporal dependencies. However, conventional LSTMs only take as input one-dimensional vectors and thus can not be applied to image sequence processing.</p><p>To overcome this limitation, Shi et al. <ref type="bibr" target="#b15">[15]</ref> proposed convolutional LSTM (CLSTM), which can capture long and short term temporal dependencies while retaining the ability of handling two-dimensional feature maps. Recently, CLSTMs have been used in video processing. In <ref type="bibr" target="#b16">[16]</ref>, Song et al. proposed a Deeper Bidirectional CLSTM (DB-CLSTM) structure which learns temporal characteristics in a cascaded and deeper way for video salient object detection. Liu et al. <ref type="bibr" target="#b17">[17]</ref> proposed a tree-structure based traversal method to model the 3D-skeleton of a human being in spatial-temporal domain. They applied CLSTM to handle the noise and occlusions in 3D skeleton data, which improves the temporal consistency of the results. Jiang et al. <ref type="bibr" target="#b18">[18]</ref> developed a two-layer ConvLSTM (2C-LSTM) to predict video saliency. An object-to-motion convolutional neural network has also been proposed.</p><p>GAN The generative adversarial network (GAN) has been an active research topic since it was proposed by Goodfellow et al. in <ref type="bibr" target="#b19">[19]</ref>. The basic idea of GAN is the training of two adversarial networks, a generator and a discriminator. During the process of adversarial training, both generator and discriminator become more robust. GANs have been widely used in various applications, such as image-to-image translation <ref type="bibr" target="#b20">[20]</ref> and synthetic data generation <ref type="bibr" target="#b21">[21]</ref>. GAN has been mainly used for generating images. One of the first work to apply adversarial training to improve structured output learning might be <ref type="bibr" target="#b22">[22]</ref>, where a discriminator loss is used to distinguish predicted pose and ground-truth pose for pose estimation from monocular images. Recently, GANs have also been adopted in depth estimation. In <ref type="bibr" target="#b23">[23]</ref>, Almalioglu et al. employed GAN to generate sharper and more accurate depth maps.</p><p>In this paper, we design a novel temporal loss by employing GAN. Our temporal loss can enforce the temporal consistency among video frames.  <ref type="figure">Figure 1</ref> -Illustration of our framework. The framework contains three main parts: spatial features extraction; temporal correlations collection; and spatial-temporal loss calculation. The first two parts consist of our ST-CLSTM structure which captures both spatial features and temporal correlations. After the ST-CLSM generates depth estimations, a 3D CNN is introduced to calculate the temporal loss. The spatial and temporal losses are combined to update the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ST-CLSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Method</head><p>In this section, we elaborate on our proposed video depth estimation framework. We first introduce our ST-CLSTM structure; then we present our generative adversarial learning scheme and our spatial and temporal loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">ST-CLSTM</head><p>Our depth estimation framework contains three main components: spatial feature extraction; temporal correlation collection; and spatial-temporal loss calculation, as illustrated in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Spatial feature extraction network</head><p>Spatial feature extraction is the key to the performance and processing speed as it contains the majority of trainable parameters in our depth estimation framework. In our work, we use a modified structure proposed by Hu et al. <ref type="bibr" target="#b24">[24]</ref>.</p><p>We show the details of our spatial feature extraction network in <ref type="figure">Fig. 2</ref>. The network contains an encoder, a decoder and a multi-scale feature fusion module (MFF). The encoder can be any 2D CNN model, such as the VGG-16 <ref type="bibr" target="#b25">[25]</ref>, the ResNet <ref type="bibr" target="#b26">[26]</ref>, the SENet <ref type="bibr" target="#b27">[27]</ref>, among many others. In order to build a real-time depth estimation framework, we apply a shallow ResNet-18 model instead of the SENet-154 as the encoder.</p><p>The decoder employs four up-projection modules to improve the spatial resolution and decreases the number of  <ref type="figure">Figure 2</ref> -Spatial feature extraction network. This network consists of three parts, including an encoder, a decoder and a multi-scale feature fusion module (MFF). In this paper, we employ the relatively shallow model ResNet-18 as the encoder for fast processing. channels of the feature maps. This encoder-decoder structure has been widely used in pixel-level tasks <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b1">2]</ref>. The MFF module is designed to integrate features of different scales. Similar strategies are used in <ref type="bibr" target="#b29">[29]</ref>.</p><p>Note that, in our depth estimation framework, the spatial feature extraction network can be replaced by other depth estimation models. In other words, our proposed depth es- timation framework can be applied to other state-of-the-art depth estimation methods with minimum modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">CLSTM</head><p>As the input frames are continuous in the temporal dimension, taking the temporal correlations of these frames into consideration is intuitive and presumably helpful for improving depth estimation performance. In terms of achieving this goal, both the 3D CNN and the CLSTM are competent. Here, we use the CLSTM, as the it is more flexible than the 3D CNN for online inference. The structure of our proposed CLSTM is shown in <ref type="figure" target="#fig_1">Fig. 3</ref> (b). <ref type="figure" target="#fig_1">Fig. 3 (a)</ref> shows the traditional LSTM. The inputs and the outputs are vectors and the key operation is the Hadamard product. A single LSTM cell at time t can be expressed as:</p><formula xml:id="formula_0">f t = ? (W f ? [h t?1 , x t ] + b f ) , i t = ? (W i ? [h t?1 , x t + b i ]) , C t = tanh (W C ? [h t?1 , x t ] + b C ) , C t = f t ? C t?1 + i t ?C t , o t = ? (W o ? [h t?1 , x t ] + b o ) , h t = o t ? tanh (C t ) ,<label>(1)</label></formula><p>where ? and tanh are sigmoid and hyperbolic tangent activation functions. ? and ? represent the Hadamard product and pointwise multiplication.</p><p>Compared with the traditional LSTM, our proposed CLSTM exhibits two main differences: 1) Operation. Following <ref type="bibr" target="#b15">[15]</ref>, we replace the Hadamard product in LSTM with convolution to handle the extracted 2D feature maps.</p><p>2) Structure. We adjust the structure of CLSTM to deal with depth estimation task. Specifically, our proposed CLSTM cell can be expressed as:</p><formula xml:id="formula_1">f t = ? f t , D t?1 (f t?1 ) * W f + b f , i t = ?([f t , D t?1 (f t?1 )] * W i + b i ), C t = tanh([f t , D t?1 (f t?1 )] * W C + b C ), C t = f t ? C t?1 + i t ?C t , o t = ?([f t , D t?1 (f t?1 )] * W o + b o ), R t = Conv([o t , tanh(C t )]),<label>(2)</label></formula><p>where * is the convolutional operator.</p><formula xml:id="formula_2">W f , W i , W C , W o and b f , b i , b C , b o</formula><p>denote the kernels and bias terms at the corresponding convolution layers. After we extract the spatial features of video frames, we feed the feature map of the previous frame f t?1 into a convolution layer D t?1 to compress the number of channels from c to 8. Then we concatenate f t?1 with the feature map of current frame f t to formulate a feature map with c + 8 channels. Next, we feed the concatenated feature map to CLSTM to update the information stored in memory cell. Finally, we concatenate the information in the updated memory cell C t and the feature map of output gate, then feed them to a refine structure R t that consists of two convolution layers to obtain the final estimation result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spatial-temporal loss</head><p>As shown in <ref type="figure">Fig. 1</ref>, the output of our ST-CLSTM is the estimated depth. We design two loss functions to train our ST-CLSTM model: a spatial loss to maintain the spatial features and a temporal loss to capture the temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Spatial loss</head><p>We follow <ref type="bibr" target="#b24">[24]</ref> and design a similar loss function as our spatial loss, which can be expressed as:</p><formula xml:id="formula_3">L spatial = l depth + ?l grad + ?l normal ,<label>(3)</label></formula><p>where ? and ? are weighting coefficients. It is composed of three terms. The l depth is applied to penalize inaccurate depth estimations. Most existing depth estimation methods simply apply the 1 or 2 loss. As pointed in <ref type="bibr" target="#b30">[30]</ref>, a problem of this type of loss is that the value tends to be larger as the ground-truth depth getting further. We apply a logarithm 1 loss which is expressed as:</p><formula xml:id="formula_4">F (x, y) = ln(||x ? y|| 1 + 1.0).<label>(4)</label></formula><p>Consequently, our l depth is defined as:  where n is the number of pixels; d i and g i are the estimated and ground-truth depth of pixel i respectively. l grad is designed to penalize the errors around edges. It is defined as:</p><formula xml:id="formula_5">l depth = 1 n n i=1 F (d i , g i ),<label>(5)</label></formula><formula xml:id="formula_6">l grad = 1 n n i=1 (F ( x (d i ), x (g i )) + F ( y (d i ), y (g i ))),<label>(6)</label></formula><p>where x and y represent the spatial derivative along the x-axis and y-axis respectively. The last item l normal is designed to measure the angle between two surface normals, and thus is sensitive to small depth structures. It is expressed as:</p><formula xml:id="formula_7">l normal = 1 n n i=1 1 ? ? d i ? ? g i ? d i ? ? d i ? g i ? ? g i ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_8">? d i = [? x (d i ), ? y (d i ), 1]</formula><p>and ? denotes inner product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Temporal loss</head><p>Our proposed ST-CLSTM is able to exploit the temporal correlations among consecutive video frames. In order to further enforce the consistency among frames, we apply the generative adversarial learning scheme and design a temporal consistency loss. Specifically, after our ST-CLSTM produces depth estimations, we introduce a threedimensional convolutional neural network (3D CNN) which takes as input the estimated depth sequence and output a score. This score represents the probability of the depth sequence comes from our ST-CLSTM rather than the groundtruths. The 3D CNN is then act as a discriminator. We train the discriminator by maximizing the probability of assigning the correct label to both the estimated and ground-truth depth sequences. Our ST-CLSTM acts as the generator. The discriminator tries to distinguish the generator's output (labelled as 'fake') from the ground truth depth sequence (labelled as 'real'). Upon convergence we wish that the generator's output can appear as close as possible to the ground truth so as to confuse the discriminator. During the training of discriminator, we train the generator simultaneously. The objective of our generative adversarial learning is expressed as follows: min</p><formula xml:id="formula_9">G max D V (G, D) = E z?? [log(D(z))] + E x?? [log(1 ? D(G(x)))],<label>(8)</label></formula><p>where x = [x 1 , ...x n ] are the input RGB frames and z = [d 1 , ...d n ] are the ground-truth depth frames. ? and ? are the distributions of input RGB frames and ground-truth depths respectively. Since our discriminator is a binary classifier, we train it using the cross entropy loss. The cross entropy loss then acts as our temporal loss function. During the training of our ST-CLSTM, we combine our temporal loss with the aforementioned spatial loss as follows:</p><formula xml:id="formula_10">L = L spatial + ?L temporal ,<label>(9)</label></formula><p>where ? is a weighting coefficient. We empirically set it to 0.1.</p><p>The detailed structure of our 3DCNN is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. It is composed of 4 convolution blocks, a global average pooling layer and a fully-connected layer. Each convolution block contains a 3D convolution layer, followed by a batch normalization layer, a ReLU layer and a max pooling layer. The first 3D convolution layer and all the max pooling layers have a stride of 2. In practice, as plotted in <ref type="figure" target="#fig_2">Fig. 4</ref>, our 3DCNN takes as input concatenated RGB and depth frames to enforce the consistency between the video frame and the corresponding depth. In order to increase the robustness of our discriminator, in our generated input depth sequences, we randomly mix some ground-truth depth frames with a certain probability.</p><p>Note that, the adversarial training here is mainly to enforce temporal consistency, instead of improving the depth accuracy of single frame's depth as in <ref type="bibr" target="#b31">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we evaluate our proposed depth estimation framework on the indoor NYU Depth V2 dataset and the outdoor KITTI dataset, and compare against a few existing depth estimation approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation metrics</head><p>Spatial Metrics We evaluate the performance of our framework using the commonly applied metrics defined as follows: 1) Mean relative error (Rel): 1</p><formula xml:id="formula_11">N N i=1 ||di?gi|| 1 gi ; 2)</formula><p>Root mean squared error (RMS):</p><formula xml:id="formula_12">1 N N i=1 (d i ? g i ) 2 ; 3) Mean log 10 error (log10): 1 N N i=1 || log 10 d i ? log 10 g i || 1 ; 4) Accuracy with threshold t: Percentage of d i such that max( di gi , gi di ) = ? &lt; t ? [1.25, 1.25 2 , 1.25 3 ]</formula><p>. N denotes the total number of pixels. d i and g i are estimated and groundtruth depths of pixel i, respectively.</p><p>Temporal Metrics Maintaining temporal consistency means keeping the changes and motions among adjacent frames of estimation results consistent with that of corresponding ground truths. In order to quantitatively evaluate the temporal consistency, we introduce two metrics: temporal change consistency (TCC) and temporal motion consistency (TMC). They are defined as:</p><formula xml:id="formula_13">TCC(D, G) = n?1 i=1 SSIM(abs(d i ? d i+1 ), abs(g i ? g i+1 )) n ? 1 ,<label>(10)</label></formula><formula xml:id="formula_14">TMC(D, G) = n?1 i=1 SSIM(of low(d i , d i+1 ), of low(g i , g i+1 )) n ? 1 ,<label>(11)</label></formula><p>where D = d 1 , d 2 , ? ? ? , d n and G = g 1 , g 2 , ? ? ? , g n are estimation depth maps of n consecutive frames and the corresponding ground truths. of low denotes real time T V ?L 1 optical flow <ref type="bibr" target="#b32">[32]</ref>. SSIM is structural similarity <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation details</head><p>We train our proposed framework for 20 epochs. The initial learning rate of the ST-CLSTM is set to 0.0001 and decrease by a factor of 0.1 after every five epochs. Our spatial feature extraction network in the ST-CLSTM is pretrained on the ImageNet dataset. As for our 3D CNN, the initial learning rate is set to 0.1 for the NYU Depth V2 dataset and 0.01 for the KITTI dataset. The parameters of our 3D CNN are randomly initialized. During the generative adversarial training, before we start to update our 3D CNN parameters, we first train our ST-CLSTM for one epoch for the NYU Depth V2 dataset, and two epochs for the KITTI dataset, to make sure that our ST-CLSTM is able to generate plausible depth estimations.</p><p>Following <ref type="bibr" target="#b24">[24]</ref>, we employ three data augmentation methods including: 1) randomly flip the RGB image and depth map horizontally with a probability of 50%; 2) rotate the RGB image and depth map by a random degree c ?  The ST-CLSTM is the key component in our proposed depth estimation framework as it captures both spatial and temporal information. In this section, we evaluate the performance of our ST-CLSTM on both indoor and outdoor datasets. The results are reported in <ref type="table">Table 1</ref>. We denote the baseline approach that captures no temporal information as 2DCNN. Specifically, we replace the CLSTM in our ST-CLSTM structure with 3 convolution layers. The number of channels are 128, 128 and 1 respectively. Since the temporal information exists among consecutive frames, the number of input frames influences the performance of our ST-CLSTM. We first evaluate the performance of our ST-CLSTM on the NYUD Depth V2 dataset with different number of input frames and show the results in the first 4 rows in <ref type="table">Table 1</ref>. We can see that with the number of frame increases, the performance increases, as our ST-CLSTM captures more temporal information. We use 5 input frames in our experiments considering the computation cost.</p><p>We can see from <ref type="table">Table 1</ref> that our ST-CLSTM is able to capture the temporal information and improve the depth estimation performance on both indoor and outdoor datasets. In this section, we evaluate the performance of our generative adversarial learning scheme which further enforces the temporal consistency among video frames. The evaluation results on the NYU Depth V2 and the KITTI dataset are reported in <ref type="table">Table 2</ref>. For each dataset, we show the results of our ST-CLSTM without and with generative adversarial learning, denoted as ST-CLSTM and GAN respectively. We can see from <ref type="table">Table 2</ref> that our generative adversarial learning and temporal loss can enforce the temporal consistency and further improve the performance of our ST-CLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Benefit of generative adversarial learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Improvement of temporal consistency</head><p>The major contribution of our work is to exploit temporal information for accurate depth estimation. The aforementioned experiments have revealed that our proposed ST-CLSTM and generative adversarial learning scheme are able to better capture the temporal information and improve the depth estimation performance. In this section, we show the improvement of our proposed framework in the temporal dimension with both visual effects and temporal consistency metrics.</p><p>We show the estimated depths of four consecutive frames with one frame gap between each frame in <ref type="figure" target="#fig_3">Fig. 5</ref>. We first show the RGB frames and the ground-truth depth maps in the first two rows, then we show the depth estimations of the baseline method (2DCNN) and our proposed framework in the last three rows.</p><p>We highlight a front area and a background area in blue and red dotted windows respectively, and we maximize the blue dotted window for better visualization. Since the four frames are consecutive, the ground-truth depths in these four frames change smoothly. However, the baseline method fails to maintain the smoothness. The estimated depths vary largely. Our ST-CLSTM captures the temporal correlations and produces visually better performance as demonstrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. For all the frames, the edges of objects are sharper and the backgrounds are smoother. With our proposed generative adversarial learning scheme, the temporal consistency is enforced and the performance is further improved. The details are well maintained in all the frames. For instance, the bars of the chair in the red dotted window. 1 3D CNN can capture the change and motion information between consecutive frames, as it convolves the input along both the spatial and temporal dimensions. To confuse the 3D CNN discriminator, the change and motion of estimation results must keep consistent with that of corresponding ground truths. We sampled 654 sequences from test set with a length of 16 frames each and report the average TCC and TMC in <ref type="table">Table 3</ref>, from which we can see that the 3D CNN discriminator does not only improve the estimation accuracy, but also better enforces the temporal consistency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Comparisons with state-of-the-art results</head><p>In this section, we evaluate our approach on the NYU Depth V2 dataset and the KITTI dataset and compare with some state-of-the-art results. The results are reported in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table" target="#tab_6">Table 5</ref> respectively. We can see that with our captured temporal information, we outperform most stateof-the-art methods which often use more complicated network structures. The aim of our work is to exploit temporal information for real-time depth estimation. We apply a shallow ResNet18 model as our backbone. The performance of our approach can be improved with deeper backbone networks. We leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Speed analysis</head><p>One of the contributions of our work here is that our model can execute in real-time for practical applications. In this section, we evaluate the processing time of our model. Specifically, we feed our model videos with spatial resolution of 304 ? 228. We test 600 frames for five epochs and report the mean values. We load the videos in two different ways: 1) Serial mode (S-mode). We load the video frames one by one. 2) Parallel+serial mode (PS-mode). We feed 120 frames to our spatial extraction network at one time to obtain the spatial features, then we feed the spatial features to our CLSTM one by one.</p><p>We implement our model with the PyTorch <ref type="bibr" target="#b47">[47]</ref>, and perform the inference on a computer with 8GB RAM, Intel i7-4790 CPU and GTX1080Ti GPU. We report the processing time of one frame, and the frame rate in <ref type="table">Table 6</ref>. We can see that compared with the baseline (2D CNN) method, our ST-CLSTM method shows negligible drop of processing speed. Moreover, when we adopt the PS-mode for data loading, our processing speed increases dramatically. As the frame rate of common video formats is less than 30fps, our model is sufficiently fast to work in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this work, we have proposed a novel ST-CLSTM structure by combining a shallow 2D CNN and a CLSTM. Our ST-CLSTM is able to capture both spatial features and temporal correlations among video frames for depth estimation. We have also designed a novel temporal loss by introducing the generative adversarial learning scheme. Our temporal loss is able to further enforce temporal consistencies among video frames. Experiments on bench-mark indoor and outdoor datasets reveal that our proposed framework can effectively capture temporal information and achieve outstanding performance. Moreover, our proposed framework is able to execute in real-time for realworld applications, and can be easily generalized to most existing depth estimation frameworks.   <ref type="table">Table 6</ref> -Processing speed of different models and data loading modes. The resolution of input frame is 304 ? 228.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 -</head><label>3</label><figDesc>LSTM and CLSTM. (a) LSTM; (b) CLSTM. In LSTM, both the inputs and the outputs are vectors. In our proposed CLSTM, the inputs are feature maps and the the outputs are the estimated depths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 -</head><label>4</label><figDesc>Structure of the 3DCNN discriminator model in adversarial learning. It contains four convolution blocks, a global average pooling layer and a fully connected layer. It takes as input concatenated RGB-D video frames and output a binary label which indicates the input source.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[? 5 ?</head><label>5</label><figDesc>, 5 ? ]; 3) scale the brightness, contrast and saturation values of the RGB image by a random ratio r ? [0.6, 1.4].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 4 .</head><label>4</label><figDesc>Benefit of ST-CLSTM 0.585 0.059 0.819 0.961 0.990 3 ST-CLSTM 0.134 0.581 0.058 0.824 0.965 0.991 4 ST-CLSTM 0.133 0.577 0.057 0.831 0.963 0.990 5 ST-CLSTM 0.132 0.572 0.057 0.833 0.966 0.991 KITTI 1 2DCNN 0.111 4.385 0.048 0.871 0.962 0.987 5 ST-CLSTM 0.104 4.139 0.045 0.883 0.967 0.988Table 1-Experiment results of our ST-CLSTM. The first 4 rows are the results on the NYU Depth V2 dataset and the last 2 rows are the results on the KITTI dataset. # denotes the number of input frames. ? i means ? &lt; 1.25 i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 -</head><label>5</label><figDesc>Visual results of depth estimation on the NYU Depth V2 dataset. The top five rows are: RGB inputs, ground truth, the results of baseline, ST-CLSTM and ST-CLSTM+GAN. For better visualization, we present the corresponding zoom-in regions of ground truth and estimations results on the four bottom rows. Here, both ST-CLSTM and ST-CLSTM+GAN are trained with 5 frames inputs. From the results on the last row, we can see that the estimation results generated by ST-CLSTM+GAN exhibit better temporal consistency than that of 2DCNN and ST-CLSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>NYU Depth V2 contains 464 videos taken from indoor scenes. We apply the same train/test split as in Eigen et al. [4] which contains 249 videos for training, and 654 samples from the rest 215 videos for test. During training, we resize the image from 640 ? 480 to 320 ? 240 and then crop patches of 304 ? 228 for training. KITTI contains 61 outdoor video scenes captured by cameras and depth sensors mounted on a driving car. We apply the same train/test split as in Eigen et al. [4] which contains 32 videos for training, and 697 samples from the rest 29 videos for test. During training, we randomly crop patches of size 480?320 from the original images as inputs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.585 0.059 0.819 0.961 0.990 0.846 0.956 ST-CLSTM 0.132 0.572 0.057 0.833 0.966 0.991 0.866 0.962 3D-GAN 0.131 0.571 0.056 0.833 0.965 0.991 0.870 0.965Table 3 -Experiment results on NYU Depth V2.</figDesc><table><row><cell>Model</cell><cell>Rel</cell><cell>RMS log10 ? 1</cell><cell>? 2</cell><cell>? 3</cell><cell>TCC TMC</cell></row><row><cell>Baseline</cell><cell cols="2">0.139 0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 -</head><label>4</label><figDesc>Comparisons with state-of-the-arts on the NYU Depth V2 dataset. We show our results in the last row.</figDesc><table><row><cell>Method</cell><cell>Rel</cell><cell>RMS?</cell><cell>log10</cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell><cell>backbone</cell></row><row><cell cols="2">DepthTransfer [34] 0.350</cell><cell>1.200</cell><cell>0.131</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Make3D [35] 0.349</cell><cell>1.214</cell><cell>-</cell><cell cols="3">0.447 0.745 0.897</cell><cell>-</cell></row><row><cell cols="2">Liu et al. [36] 0.335</cell><cell>1.060</cell><cell>0.127</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Li et al. [37] 0.232</cell><cell>0.821</cell><cell>0.094</cell><cell cols="3">0.621 0.886 0.968</cell><cell>-</cell></row><row><cell cols="2">Liu et al. [38] 0.230</cell><cell>0.824</cell><cell>0.095</cell><cell cols="3">0.614 0.883 0.971</cell><cell>-</cell></row><row><cell cols="2">Wang et al. [11] 0.220</cell><cell>0.824</cell><cell>-</cell><cell cols="3">0.605 0.890 0.970</cell><cell>-</cell></row><row><cell cols="2">Liu et al. [12] 0.213</cell><cell>0.759</cell><cell>0.087</cell><cell cols="3">0.650 0.906 0.976</cell><cell>-</cell></row><row><cell cols="2">Eigen et al. [39] 0.158</cell><cell>0.641</cell><cell>-</cell><cell cols="3">0.769 0.950 0.988</cell><cell>-</cell></row><row><cell cols="2">Chakrabarti et al. [40] 0.149</cell><cell>0.620</cell><cell>-</cell><cell cols="3">0.806 0.958 0.987</cell><cell>VGG19</cell></row><row><cell cols="2">Li et al. [41] 0.143</cell><cell>0.635</cell><cell>0.063</cell><cell cols="3">0.788 0.958 0.991</cell><cell>VGG16</cell></row><row><cell cols="2">Ma &amp; Karaman [42] 0.143</cell><cell>-</cell><cell>-</cell><cell cols="3">0.810 0.959 0.989</cell><cell>ResNet-50</cell></row><row><cell cols="2">Laina et al. [5] 0.127</cell><cell>0.573</cell><cell>0.055</cell><cell cols="3">0.811 0.953 0.988</cell><cell>ResNet50</cell></row><row><cell cols="2">Pad-net [43] 0.120</cell><cell>0.582</cell><cell>0.055</cell><cell cols="3">0.817 0.954 0.987</cell><cell>ResNet50</cell></row><row><cell>DORN [2]</cell><cell>0.115</cell><cell>0.509</cell><cell>0.051</cell><cell cols="3">0.828 0.965 0.992</cell><cell>ResNet101</cell></row><row><cell cols="2">Ours 0.131</cell><cell>0.571</cell><cell>0.056</cell><cell cols="3">0.833 0.965 0.991</cell><cell>ResNet18</cell></row><row><cell>Method</cell><cell>Rel</cell><cell>RMS</cell><cell>log10</cell><cell>? 1</cell><cell>? 2</cell><cell>? 3</cell><cell>backbone</cell></row><row><cell cols="3">Make3D [35] 0.280 8.734</cell><cell>-</cell><cell cols="3">0.601 0.820 0.926</cell><cell>-</cell></row><row><cell cols="3">Eigen et al. [4] 0.190 7.156</cell><cell>-</cell><cell cols="3">0.692 0.899 0.967</cell><cell>-</cell></row><row><cell cols="3">Liu et al. [12] 0.217 6.986</cell><cell>-</cell><cell cols="3">0.647 0.882 0.961</cell><cell>-</cell></row><row><cell cols="3">LRC [44] 0.114 4.935</cell><cell>-</cell><cell cols="3">0.861 0.949 0.976</cell><cell>ResNet-50</cell></row><row><cell cols="3">Kuznietsov et al. [45] 0.113 4.621</cell><cell>-</cell><cell cols="3">0.862 0.960 0.986</cell><cell>ResNet-50</cell></row><row><cell cols="3">Mahjourian et al. [3] 0.159 5.912</cell><cell>-</cell><cell cols="4">0.784 0.923 0.970 DispNet [46]</cell></row><row><cell cols="3">Zhou et al. [1] 0.143 5.370</cell><cell>-</cell><cell cols="3">0.824 0.937 0.974</cell><cell>VGG-19</cell></row><row><cell cols="3">Ours 0.101 4.137</cell><cell>0.043</cell><cell cols="3">0.890 0.970 0.989</cell><cell>ResNet-18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 -</head><label>5</label><figDesc>Comparisons with state-of-the-art methods on the KITTI dataset. The first five rows are the results of static image depth estimation methods. The following two rows are the results of video depth estimation methods, and the last row are our results.</figDesc><table><row><cell>Model</cell><cell>Dataload</cell><cell cols="2">Time (ms per frame) Speed (fps)</cell></row><row><cell>Baseline</cell><cell>S-mode</cell><cell>28.90</cell><cell>34.60</cell></row><row><cell>ST-CLSTM</cell><cell>S-mode</cell><cell>30.22</cell><cell>33.09</cell></row><row><cell cols="2">ST-CLSTM PS-mode</cell><cell>5.72</cell><cell>174.83</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Readers may refer to the demonstration video: https://youtu. be/B705k8nunLU</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We would like to thank Huawei Technologies for the donation of GPU cloud computing resources. This work was in part supported by the National Natural Science Foundation of China (61871460, 61876152), Fundamental Research Funds for the Central Universities (3102019ghxm016) and Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University (CX201816).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation with bundle adjustment, super-resolution and clip loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaess</surname></persName>
		</author>
		<idno>abs/1812.03368</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
	<note>in International conference on 3D vision</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Poseidon: Face-from-depth for driver pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venturelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4661" to="4670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1611.03673</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3174" to="3182" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?ernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int</title>
		<meeting>IEEE Int</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Conf. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6645" to="6649" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting video saliency with object-to-motion cnn and two-layer convolutional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1709.06316</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised reverse domain adaptation for synthetic medical images via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2572" to="2581" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial PoseNet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GANVO: Unsupervised deep monocular visual odometry and depth estimation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Almalioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Saputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Gusmao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics Automation</title>
		<meeting>IEEE Int. Conf. Robotics Automation</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter. Conf. App. Comp. Vis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1043" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on fourier domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt</title>
		<meeting>IEEE Conf. Comp. Vis. Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking monocular depth estimation with adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Symp. German Assoc. Pattern Recogn</title>
		<meeting>Annual Symp. German Assoc. Pattern Recogn</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2658" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3372" to="3380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics Automation</title>
		<meeting>IEEE Int. Conf. Robotics Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pad-net: multitasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
