<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Attention-controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-set Sample Weighting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Christmas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Huber</surname></persName>
							<email>p.huber@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
							<email>wuxiaojun@jiangnan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of IoT Engineering</orgName>
								<orgName type="institution">Jiangnan University</orgName>
								<address>
									<postCode>214122</postCode>
									<settlement>Wuxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Attention-controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-set Sample Weighting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic Attention-Controlled CSR (DAC-CSR), for robust facial landmark detection on unconstrained faces. Our DAC-CSR divides facial landmark detection into three cascaded sub-tasks: face bounding box refinement, general CSR and attention-controlled CSR. The first two stages refine initial face bounding boxes and output intermediate facial landmarks. Then, an online dynamic model selection method is used to choose appropriate domain-specific CSRs for further landmark refinement.</p><p>The key innovation of our DAC-CSR is the fault-tolerant mechanism, using fuzzy set sample weighting, for attentioncontrolled domain-specific model training. Moreover, we advocate data augmentation with a simple but effective 2D profile face generator, and context-aware feature extraction for better facial feature representation. Experimental results obtained on challenging datasets demonstrate the merits of our DAC-CSR over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial Landmark Detection (FLD), also known as face alignment, is a prerequisite for many automatic face analysis systems, e.g. face recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, expression analysis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> and 2D-3D inverse rendering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref>. Facial landmarks provide accurate shape information with semantic meaning, enabling geometric image normalisation and feature extraction for use in the remaining stages of a face processing pipeline. This is crucial for high-fidelity face image analysis. As the technology of FLD for constrained faces has already been well developed, the current trend is to address FLD for unconstrained faces in the presence of extreme variations in pose, expression, illumination and partial occlusion <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>More recently, unconstrained FLD has seen huge progress owing to the state-of-the-art Cascaded Shape Re- gression (CSR) architecture <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref>. The key to the success of CSR is to construct a strong regressor from a set of weak regressors arranged in a cascade. This architecture greatly improves the performance of FLD in terms of generalisation capacity and accuracy. However, in the light of very recent studies <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, the capacity of CSR appears to be saturating, especially for unconstrained faces with extreme appearance variations. For example, the FLD error of state-of-the-art CSR-based methods increases from around 3% (error in percent of the inter-ocular distance) on the Labelled Face Parts in the Wild (LFPW) <ref type="bibr" target="#b1">[2]</ref> dataset to 6.5% on the more challenging Caltech Occluded Faces in the Wild (COFW) <ref type="bibr" target="#b3">[4]</ref> dataset. This degradation has three main reasons: 1) The modelling capacity of the existing CSR architecture is limited. 2) CSR is sensitive to the positioning of face bounding boxes used for landmark initialisation.</p><p>3) The volume of available training data is insufficient. Can these limitations be overcome, especially for unconstrained faces exhibiting extreme appearance variations? We offer an encouraging answer by presenting a new Dynamic Attention-Controlled CSR (DAC-CSR) architecture with a dynamic domain selection mechanism and a novel training strategy which benefits from training data augmentation and fuzzy set training sample weighting. <ref type="figure" target="#fig_0">Fig. 1</ref> depicts a simplified overview of the proposed DAC-CSR architecture. Its innovation is in linking three types of regressor cascades performing in succession: 1) face bounding box refinement for better landmark initialisation, 2) an initial landmark update using a general CSR, and 3) a final landmark refinement by dynamically selecting an attention-controlled domain-specific CSR that is optimised to improve landmark location estimates. The new architecture decomposes the task at hand into three cascaded subtasks that are easier to handle.</p><p>In contrast to previous multi-view models, e.g. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46]</ref>, the key innovation of our DAC-CSR is its in-built faulttolerant mechanism. The fault tolerance is achieved by means of an innovative training strategy for attentioncontrolled model training of the set of domain-specific CSRs performing the final shape update refinement. Rather than using samples from just a single domain, each domainspecific regressor cascade is trained using all the training samples. However, their influence is controlled by a domain-specific fuzzy membership function which weighs samples from the relevant domain more heavily than all the other training samples. An annealing schedule of domainspecific fuzzy membership functions progressively sharpens the relative weighting of in-domain and out-of-domain training samples in favour of the in-domain set for successive stages of each domain-specific cascade.</p><p>Each test sample progresses through the system of cascades. Prior to each of the domain-specific cascade stages, the domain of attention is selected dynamically based on the current shape estimate. The proposed training strategy guarantees that each domain-specific cascaded regressor can cope with out-of-domain test samples and is endowed with the capacity to update the shape in the correct direction even if the current domain has been selected subject to labelling error. This is the essence of error tolerance of the proposed system.</p><p>An important contributing factor to the promising performance of our DAC-CSR is training data augmentation. Our innovation here is to use a 2D face model for synthesising extreme profile face poses (out of plane rotation) with realistic background. Furthermore, we propose a novel contextaware feature extraction method to extract rich local facial features in the context of global face description.</p><p>The proposed framework has been evaluated on benchmarking databases using standard protocols. The results achieved on the database containing images with extreme poses (AFLW <ref type="bibr" target="#b23">[24]</ref>) are significantly better than the stateof-the-art performance reported in the literature.</p><p>The paper is organised as follows. In the next section we present a brief review of related literature. The preliminaries of CSR are presented in Section 3. The proposed DAC-CSR is introduced in Section 4.1. The discussion of its training is confined to Section 4.2, which defines the domain-specific fuzzy membership functions and their annealing schedule. On-line dynamic domain selection is the subject of Section 4.3 and the proposed feature extraction scheme can be found in Section 4.4. Section 5 addresses the problem of training set augmentation. The experimental evaluation carried out and the results achieved are described in Section 6. The paper is drawn to conclusion in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Facial landmark detection can trace its history to the nineteen nineties. The representative FLD methods making the early milestones include Active Shape Model (ASM) <ref type="bibr" target="#b7">[8]</ref>, Active Appearance Model (AAM) <ref type="bibr" target="#b6">[7]</ref> and Constrained Local Model (CLM) <ref type="bibr" target="#b9">[10]</ref>. These algorithms and their extensions have achieved excellent FLD results in constrained scenarios <ref type="bibr" target="#b16">[17]</ref>. As a result, the current trend is to develop a more robust FLD for unconstrained faces that are rich in appearance variations. The leading algorithms for unconstrained FLD are CSR-based approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref>. In contrast to the classical methods such as ASM, AAM and CLM that rely on a generative PCA-based shape model, CSR directly positions facial landmarks on their optimal locations based on image features. The shape update is achieved in a discriminative way by constructing a mapping function from robust shaperelated local features to shape updates. The secret of the success of CSR is the architecture that cascades a set of weak regressors in series to form a strong regressor.</p><p>There have been a number of improvements to the performance of CSR-based FLD. One category of these improvements is to enhance some components of the existing CSR architecture. For example, the use of more robust shape-related local features, e.g. Scale-Invariant Feature Transform (SIFT) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>, Sparse Auto-Encoder (SAE) <ref type="bibr" target="#b15">[16]</ref>, Local Binary Features (LBF) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref> and Convolutional Neural Networks (CNN-) based features <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>, has been suggested. Another example is to use more powerful regression methods as weak regressors in CSR, such as random forests <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref> and deep neural networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. Lately, 3D face models have been shown to positively impact FLD in challenging benchmarking datasets, especially in relation to faces with extreme poses <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>Multi-view models: Another important approach is to adopt advanced CSR architectures, such as the use of multiple CSR models or constructing multi-view models. Feng et al. <ref type="bibr" target="#b15">[16]</ref> constructed multiple CSR models by randomly selecting subsets from the original training set and fusing multiple outputs to produce the final FLD result. A similar idea has also been used in <ref type="bibr" target="#b40">[41]</ref>. As an alternative, a multiview FLD system employs a set of view-specific models that are able to achieve more accurate landmark detection for faces exhibiting specific views <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>However, the use of multiple models or multi-view models is not without difficulties. One has to either estimate the view of a test image to select an appropriate model, or apply all view-specific models to a test image and then choose the best result as the final output. For the former, implementing a model selection stage for unconstrained faces is hard in practice. An erroneously selected view-specific model may result in FLD failure. For the latter strategy, it is timeconsuming to apply all the trained models to a test image. Also, the ranking of the outputs of different view-based models is non-trivial. In contrast to previous studies, our DAC-CSR addresses these issues by improving the faulttolerance properties of a trained domain-specific model and by using an online dynamic model selection strategy.</p><p>Data augmentation: For a learning-based approach such as CSR, the availability of a large volume of training samples is essential. However, it is a tedious task to manually annotate facial landmarks for a large quantity of training data. To address this problem, data augmentation is widely used in CSR-based FLD. Traditional methods include random perturbation of initial landmarks, image flipping, image rotation, image blurring and adding noise to the original face images. However, none of these methods are able to inject new out-of-plane rotated faces to an existing training dataset. Recently, to augment a training set by samples with rich pose variations, the use of 3D face models has been suggested. For instance, Feng et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref> used a 3D morphable face model to synthesise a large number of 2D faces. However, the synthesised virtual faces lack realistic appearance variations especially in terms of background and expression changes. To mitigate this problem, they advocated a cascaded collaborative regression strategy to train a CSR from a mixture of real and synthesised faces. To generate realistic face images with pose variations, Zhu et al. fit a 3D shape model to 2D face images and generate profile face views from the reconstructed 3D shape information <ref type="bibr" target="#b46">[47]</ref>. However, these 3D-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47]</ref> require 3D face scans for model construction, which are expensive to capture. Also, it is difficult in practice to fit a 3D face model to 2D images. In this paper, we propose a simple but efficient 2D-based method to generate virtual faces with out-of-plane pose variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cascaded Shape Regression (CSR)</head><p>Given an input face image I and the corresponding face bounding box b = [x 1 , y 1 , x 2 , y 2 ] T (coordinates of the upper left and lower right corners) of a detected face in the image, the goal of FLD is to output the face shape in the form of a vector, s = [x 1 , y 1 , ..., x L , y L ] T , consisting of the coordinates of L pre-defined facial landmarks with semantic meaning such as eye centres and nose tip. To this end, we first initialise the face shape, s , by putting the mean shape into the bounding box. Then a trained CSR ? = {?(1), ?(2), ..., ?(N )} is used to update the initial shape estimate, where ? is a strong regressor consisting of N weak regressors. A weak regressor can be obtained using any regression method, such as linear regression, ran-dom forests and neural networks. In this paper, we use ridge regression as a weak regressor, i.e. ? = {A, e}:</p><formula xml:id="formula_0">? : ?s = A ? f (I, s ) + e,<label>(1)</label></formula><p>where A ? R 2L?N f is a projection matrix, N f is the dimensionality of a shape-related feature vector extracted using f (I, s ), and e ? R 2L is an offset. For the shape-related feature extraction, we apply local descriptors, e.g. HOG, to the neighbourhoods of all the facial landmarks of the current shape estimate and concatenate the extracted features into a long vector. The use of a weak regressor results in an update to the current shape estimate:</p><formula xml:id="formula_1">s ? s + ?s.<label>(2)</label></formula><p>A trained CSR applies all the weak regressors in cascade to progressively update the shape estimate and obtain the final FLD result from an input image. Given a training dataset</p><formula xml:id="formula_2">T = {I i , b i , s * i } I i=1</formula><p>with I samples including face images, face bounding boxes and manually annotated facial landmarks, we first obtain the initial shape estimates, {s i } I i=1 , of all the training samples using the face bounding boxes provided. Then the shape update between the current shape estimate and ground-truth shape of the ith training sample can be calculated using ?s * i = s * i ? s i . The first weak regressor is obtained using ridge regression by minimising the loss:</p><formula xml:id="formula_3">arg min A,e I i=1 ||A ? f (I i , s i ) + e ? ?s * i || 2 2 + ?||A|| 2 F ,<label>(3)</label></formula><p>where ? is the weight of the regularisation term. This is a typical least-square estimation problem with a closed-form solution <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref>. Last, this trained weak regressor is used to update the current shape estimates of all the training samples, which forms the training data for the second weak regressor. This procedure is repeated until all the N weak regressors are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dynamic Attention-controlled CSR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture</head><p>The architecture of the proposed DAC-CSR method has three cascaded stages: face bounding box refinement, general CSR and domain-specific CSR, as shown in <ref type="figure">Fig. 2</ref>. In fact, our DAC-CSR can be portrayed as a strong regres-</p><formula xml:id="formula_4">sor ? = {? b , ? g , ? d }, where ? b is a weak regressor for face bounding box refinement, ? g = {? g (1), ..., ? g (N g )} is a classical CSR with N g weak regressors, ? d = {? d (1), ..., ? d (M )</formula><p>} is a strong regressor with M domainspecific CSRs and each of them has N d weak regressors</p><formula xml:id="formula_5">? d (m) = {? d (m, 1), ..., ? d (m, N d )}. Face Box Refinement b General Shape Regression g (n) (n?{1, 2, ?, N g }) Shape Initialisation dense face description Domain-specific Shape Regression d (m,n) (n?{1, 2, ..., N d })</formula><p>... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Prediction based on the Current</head><p>Shape Estimate (m?{1, 2, ..., M} )</p><formula xml:id="formula_6">n=1 n?n+1 d (1,n) d (2,n) d (M,n)</formula><p>dense face description sparse face description dense face description sparse face description n?n+1 n=1 <ref type="figure">Figure 2</ref>. The proposed DAC-CSR has three stages in cascade: face bounding box refinement, general CSR and domain-specific CSR.</p><p>Face bounding box refinement: We define the weak regressor for the first step as ? b = {A b , e b }:</p><formula xml:id="formula_7">? b : ?b = A b ? f b (I, b) + e b ,<label>(4)</label></formula><p>where f b (I, b) extracts dense local features from the image region inside the original face bounding box and ?b is used to adjust the original bounding box. The training of this weak regressor is the same as the procedure introduced in Section 3 for classical CSR. The only difference here is that we use face bounding box differences instead of shape differences for the regressor learning in Eq. (3). The ground-truth face bounding box for a training sample is computed by taking the minimum enclosing rectangle around the ground-truth face shape.</p><p>General CSR: The initial shape estimate, s , for general CSR is obtained by translating and scaling the mean shape so that it exactly fits into the refined bounding box, touching all four sides. Then the general CSR progressively updates the initial shape estimate, s ? s + ?s , using all the weak regressors in ? g = {? g (1), ..., ? g (N g )}, as indicated in Algorithm 1. The nth weak regressor is defined as ? g (n) = {A g (n), e g (n)}:</p><formula xml:id="formula_8">? g (n) : ?s = A g (n) ? f c (I, s ) + e g (n),<label>(5)</label></formula><p>where f c (I, s ) is a context-aware feature extraction function that combines both dense face description and shaperelated sparse local features. The training of this stage is the same as the classical CSR introduced in Section 3. Domain-specific CSR: Suppose this stage has M domain-specific CSRs corresponding to M sub-domains, each having N d weak regressors. The nth weak regressor of the mth domain-specific CSR is defined as:</p><formula xml:id="formula_9">? d (m, n) : ?s = A d (m, n) ? f c (I, s ) + e d (m, n),<label>(6)</label></formula><p>where m = 1, ..., M , N = 1, ..., N d . Given the current shape estimate s output by the previous general CSR, a domain predictor is used to select a domain-specific CSR for input : image I, face bounding box b and a trained DAC-CSR model ? = {? b , ? g , ? d } output: facial landmarks s 1 refine the face bounding box b using ? b ; 2 estimate the current face shape, s , using the refined face bounding box ; 3 for n ? 1 to N g do <ref type="bibr" target="#b3">4</ref> apply the nth general weak regressor ? g (n) to update the current shape estimate; 5 end 6 for n ? 1 to N d do <ref type="bibr" target="#b6">7</ref> predict the label (m) of the sub-domain of the current shape estimate using Eq. (11) ; <ref type="bibr" target="#b7">8</ref> apply the nth weak regressor ? d (m, n) in the mth domain-specific CSR to update the current shape; 9 end Algorithm 1: FLD using our DAC-CSR. the current shape update (Section 4.3). It should be noted that we use a dynamic domain selection strategy, which updates the label for the domain-specific model selection after each shape update, as shown in Algorithm 1. As a result of the proposed domain-specific CSR training described in Section 4.2, this mechanism makes our DAC-CSR tolerant to domain prediction errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Offline Domain-specific CSR Training</head><p>Given a training dataset T with I samples, as introduced in Section 3, the first two stages, i.e. face bounding box refinement and general CSR, are trained directly using T . To train a domain-specific CSR, we first create M subsets {T 1 , ..., T M } from the original training set, where T m ? T . To this end, we normalise all the current shape estimates, output by the previous general CSR, to the interval [0, 1]. Then PCA is used to obtain the first K shape eigenvectors. All the current shape estimates are projected to the sub-domain 3 [01] sub-domain 4 <ref type="bibr" target="#b10">[11]</ref> sub-domain 2 <ref type="bibr" target="#b9">[10]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K-dimensional subspace to obtain the projected coefficients</head><formula xml:id="formula_10">{c i } I i=1 , where c i = [c i,1 , ..., c i,K ] T .</formula><p>Then the original domain is partitioned into M = 2 K + 1 overlapping subdomains, as demonstrated in <ref type="figure" target="#fig_2">Fig. 3</ref> for K = 2. For the M th sub-domain, it includes the training samples satisfying K k=1</p><formula xml:id="formula_11">(c i,k ?c k ) 2 (?(k)) 2 ? 1,</formula><p>where c k and ?(k) are the mean and standard deviation of the kth element of the coefficient vectors. For other sub-domains, each includes the training samples in a specific region of a K-dimensional coordinate system. To be more specific, for each coefficient c i , a subdomain membership word g(c i ) is generated by:</p><formula xml:id="formula_12">g(c i ) = 1 + K k=1 b c (c i,k )2 k?1 ,<label>(7)</label></formula><p>where b c (c i,k ) is a coding function that converts the kth element in a coefficient vector to a bit:</p><formula xml:id="formula_13">b c (c i,k ) = 1 if c i,k ? c(k) 0 otherwise .<label>(8)</label></formula><p>Then the mth sub-domain, 1 &lt; m &lt; 2 K , includes the training samples with their membership words g(c i ) = m. Our domain split strategy results in M sub-domains with overlapping boundaries. This is different from previous studies using multi-view models such as <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46]</ref>, in which the intersection of any two different subsets is empty, i.e.</p><formula xml:id="formula_14">T i ? T j = , ?i = j.</formula><p>The advantage of our domain split strategy is that it improves the fault-tolerance ability of each trained domainattention model, because of the overlap of two different sub-domains. For a test sample, a domain predictor may output an inaccurate label for model selection due to the rough shape estimate provided from the previous general CSR. But, the inaccurately selected domain-specific model is still able to refine the current shape estimate. To further improve this refinement capacity, we propose a fuzzy training strategy. For each domain-specific CSR, we use all the training samples from the original training set to train a specific regressor, but weight more heavily the training samples of the specific domain by increasing their fuzzy set membership values in the objective function. More specifically, to train the nth weak regressor of the mth domain-specific CSR, the objective function is defined as:</p><formula xml:id="formula_15">arg min A d ,e d I i=1 w i ||A d ? f c (I i , s i ) + e d ? ?s * i || 2 2 + ?||A d || 2 F ,<label>(9)</label></formula><p>where w i is a fuzzy set membership value defined by:</p><formula xml:id="formula_16">w i = 1 ? h(n) if {I i , b i , s * i } ? T m h(n) otherwise ,<label>(10)</label></formula><p>where h(n) is a decreasing function which progressively reduces the weights of the training samples not belonging to the mth sub-domain and increases the weights of the training samples of the mth sub-domain. This is a standard weighted least-square estimation problem with a closedform solution. It should be noted that our fuzzy domainspecific model learns a weak regressor that is able to refine a face shape estimate from any sub-domain, and with better capacity to refine face shapes from a specific domain. This capability is exhibited even when using a domain split strategy without overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Dynamic Domain Selection in Testing</head><p>Given a new test image with a detected face bounding box, the trained DAC-CSR model ? = {? b , ? g , ? d } first applies the face bounding box refiner ? b and general CSR ? g to obtain the intermediate face shape estimate s . Then a specific domain-attention weak regressor is selected to further update the current shape estimate.</p><p>To select an appropriate weak regressor, the current shape estimate s is projected into the PCA space learned at training time to obtain the coefficient vector c, and the label of the sub-domain is obtained using:</p><formula xml:id="formula_17">p(c) = 2 K + 1 if K k=1 (c i,k ?c k ) 2 (?(k)) 2 ? 1 g(c) otherwise .<label>(11)</label></formula><p>Note that, here, the sub-domains are not overlapped. This is different from the domain split strategy used in the training stage. However, this domain prediction function is only based on the current shape information and may provide inaccurate labels for model selection. To address this issue and further improve the fault-tolerance capacity of our DAC-CSR, a dynamic domain selection strategy is used. As discussed in the last section, a trained domainspecific CSR is able to improve the current shape estimate even if selected in error by the domain prediction mechanism. Hence the updated shape estimate produced by the nth weak regressor can be a basis for selecting a more appropriate domain in the next step of the shape updating process. We re-run the domain prediction before performing the next weak regressor and choose the (n + 1)st weak regressor of a newly selected domain-specific model for cur-  <ref type="figure">Figure 4</ref>. A comparison of synthesised 2D faces using (a) a 3D morphable model <ref type="bibr" target="#b14">[15]</ref>, (b) 3D-based face profiling <ref type="bibr" target="#b46">[47]</ref>, and (c) our 2D-based method. rent shape update, as summarised in Algorithm 1. This dynamic model selection strategy is repeated after each shape update in our domain-specific CSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Context-aware Feature Extraction</head><p>Feature extraction is crucial for constructing a robust mapping from feature space to shape updates. In classical CSR-based approaches, shape-related local features are created by concatenating all the extracted local features around each landmark into a long vector. Although this sparse shape-related feature extraction method provides a good description of the texture information of different facial parts, it does not offer a good representation of the contextual information of faces. In our DAC-CSR, we use a contextaware feature extraction method. To be more specific, we use both a dense local description of the whole face region and sparse shape-related local features for weak regressor training <ref type="figure" target="#fig_2">(Fig. 3)</ref>. Note that, for the first bounding box refinement step, we use only the dense local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">2D Profile Face Generation</head><p>For a learning-based approach, a large number of annotated face images are crucial for training. As discussed in Section 2, traditional data augmentation methods are not able to inject new out-of-plane pose variations, and the use of 3D face models is very expensive. To mitigate this issue, we propose a simple 2D-based method that can generate virtual faces with out-of-plane pose variations. A comparison between our proposed 2D-based profile face generator and two 3D-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47]</ref> is shown in <ref type="figure">Fig. 4</ref>.</p><p>To warp a face image to another pose, we first build <ref type="figure">Figure 5</ref>. The mesh generated for 2D image warpping.</p><p>a PCA-based shape model that is equivalent to the shape model used in ASM <ref type="bibr" target="#b7">[8]</ref> and AAM <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref>. Then we choose the corresponding shape eigenvector controlling yaw rotations (usually the first one) to change the pose of the current face shape. To this end, we first calculate the coefficient of the shape of a face image projected by the selected shape eigenvector. A new face shape with pose variations is generated by adjusting the projected coefficient. The 2D shape model used is constructed using a face dataset rich in pose variations. Note, we only generate pose-varying face shapes with the same rotation direction of the original shape, i.e. left or right. Then we expand the face shape with more external facial landmarks and compute a 2D mesh of the original and new shapes using Delaunay triangulation, as shown in <ref type="figure">Fig. 5</ref>. Last, a piece-wise affine warp is used to map the texture from the original face shape to a new one <ref type="bibr" target="#b26">[27]</ref>. Moreover, the synthesised faces can be flipped about their vertical axis to obtain more faces with pose variations in the other direction (right or left), which is similar to <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets and Implementation Details</head><p>Datasets:</p><p>In our experiments, we use two challenging face datasets, including the Annotated Facial Landmarks in the Wild (AFLW) dataset <ref type="bibr" target="#b23">[24]</ref> and the Caltech Occluded Faces in the Wild (COFW) dataset <ref type="bibr" target="#b3">[4]</ref> to evaluate the performance of our DAC-CSR architecture.</p><p>The AFLW dataset has 25993 unconstrained faces with large-scale pose variations up to ?90 ? . Each AFLW face image has up to 21 landmarks of visible facial parts. AFLW does not have a standard protocol for FLD evaluation; hence we follow the protocol used in Cascaded Compositional Learning (CCL) <ref type="bibr" target="#b45">[46]</ref>. This is the first work to use the whole AFLW dataset to benchmark an FLD algorithm. It reports the currently best results on AFLW. CCL used 24386 images from AFLW and manually annotated all the missing landmarks in the original dataset. The annotation system opted for 19 landmarks per image without the two ear landmarks (ID-13 and ID-17). CCL has two protocols: AFLW-full and AFLW-frontal, as shown in <ref type="table" target="#tab_0">Table 1</ref>. AFLW-full splits the 24386 images into 20000/4386 for training/testing. The AFLW-frontal protocol selects 1165 1  <ref type="bibr" target="#b3">[4]</ref> frontal images from the 4386 test images to evaluate an FLD algorithm on frontal faces. The COFW dataset has 1345 training and 507 test images, which are all unconstrained faces. Each COFW face has 29 manually annotated landmarks. COFW is a challenging benchmark containing major occlusions.</p><p>Implementation Details: In our experiments, we only used one weak regressor for face bounding box refinement. The numbers of weak regressors for general CSR and domain-specific CSR were set to 2 and 3 respectively. We set the number of sub-domains to M = 5 using 2 PCA shape coefficients, i.e. K = 2. The value of the regularisation term in the ridge regression training was assigned to ? = 10000, and the decreasing schedule controlling fuzzy membership values was set to h(n) = (0.3, 0.2, 0.1) for n = <ref type="figure" target="#fig_0">(1, 2, 3)</ref>. To extract a dense face description, we resized the face region to 100 ? 100 and extracted HOG features using a cell size of 10 and block size of 2. To extract sparse shape-related local features, we computed the HOG descriptor in the neighbourhood of each facial landmark. The radius was set to 1/7 of the maximum of the height and width of the current shape estimate. Each local image patch was resized to 30 ? 30 and the cell size was set to 10. In addition, the central 15?15 image patch was used to extract multi-scale HOG features using a cell size of 5.</p><p>To augment training data, we applied our 2D-based method to generate virtual face images with new poses. Each training image in COFW was augmented using 9 new poses. For AFLW, we only synthesised new faces for semifrontal training images. We also flipped all the training images about the vertical axis, added Gaussian blur with ? = 1 pixel and performed random perturbations of the initial face bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation on AFLW</head><p>The Cumulative Error Distribution (CED) curve of our DAC-CSR using the AFLW-full protocol is shown in <ref type="figure">Fig. 6</ref>. The error was calculated using the Euclidean distance between the detected and ground-truth landmarks, normalised by face size <ref type="bibr" target="#b45">[46]</ref>. Our DAC-CSR achieves much better results on the AFLW-full protocol than the current best result reported for CCL <ref type="bibr" target="#b45">[46]</ref>.  <ref type="figure">Figure 6</ref>. A CED curve comparison of our DAC-CSR with stateof-the-art methods, including SDM <ref type="bibr" target="#b37">[38]</ref>, ERT <ref type="bibr" target="#b21">[22]</ref>, RCPR <ref type="bibr" target="#b3">[4]</ref>, CFSS <ref type="bibr" target="#b44">[45]</ref>, LBF <ref type="bibr" target="#b28">[29]</ref>, LBF + GRF <ref type="bibr" target="#b18">[19]</ref> and CCL <ref type="bibr" target="#b45">[46]</ref>, on the AFLW dataset (better viewed in colour). In this experiment, 20000 images were used for training and 4386 images were used for testing, following the AFLW-full protocol in <ref type="bibr" target="#b45">[46]</ref>. <ref type="table" target="#tab_1">Table 2</ref>. A comparison of our DAC-CSR with state-of-the-art methods on AFLW, measured in terms of the average error, normalised by face size. The protocol is the same as in <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>AFLW-full AFLW-frontal SDM <ref type="bibr" target="#b37">[38]</ref> 4.05% 2.94% RCPR <ref type="bibr" target="#b3">[4]</ref> 3.73% 2.87% ERT <ref type="bibr" target="#b21">[22]</ref> 4.35% 2.75% LBF <ref type="bibr" target="#b28">[29]</ref> 4.25% 2.74% LBF + GRF <ref type="bibr" target="#b18">[19]</ref> 3.15% N.A. CFSS <ref type="bibr" target="#b44">[45]</ref> 3.92% 2.68% CCL <ref type="bibr" target="#b45">[46]</ref> 2.72% 2.17% Our DAC-CSR 2.27% 1.81% set and the frontal face subset protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Evaluation on COFW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Comparison to State-of-the-art</head><p>The CED curves of our DAC-CSR and a set of state-ofthe-art methods on the COFW dataset are shown in <ref type="figure">Fig. 7</ref>.</p><p>In addition, a more detailed comparison is presented in <ref type="table">Ta</ref>  <ref type="figure">Figure 7</ref>. A comparison between our DAC-CSR and state-ofthe-art methods, including SDM <ref type="bibr" target="#b37">[38]</ref>, RCPR <ref type="bibr" target="#b3">[4]</ref>, RCRC <ref type="bibr" target="#b15">[16]</ref>, CDRN <ref type="bibr" target="#b41">[42]</ref> and DRDA <ref type="bibr" target="#b41">[42]</ref>, on COFW.  <ref type="bibr" target="#b36">[37]</ref> 6.03% 4.14% 4 (GPU) Our DAC-CSR 6.03% 4.73% 10 compared to the two cutting-edge deep-neural-networkbased algorithms, DRDA <ref type="bibr" target="#b41">[42]</ref> and RAR <ref type="bibr" target="#b36">[37]</ref>. In addition, the speed of our DAC-CSR on an Intel i7-4790 CPU is up to 10 FPS, which is faster than RAR with GPU acceleration (NVIDIA Titan Z). As the current bottleneck for unconstrained FLD is not the speed, e.g. LBF can perform FLD at up to 3000 FPS, the key aim of our DAC-CSR is to provide a more robust FLD algorithm for faces with extreme appearance variations, as exhibited in the AFLW-full evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Self Evaluation</head><p>In this part, we investigate the contributions of the proposed DAC-CSR architecture and our 2D-based data augmentation method to the accuracy of FLD on COFW. To this end, we compare the classical CSR method trained on the original training set (CSR) with the classical CSR trained on the augmented dataset using faces synthesised by our 2Dbased face generation method (CSR+SYN), our DAC-CSR trained on the original dataset (DAC-CSR) and our DAC-CSR trained on the augmented dataset (DAC-CSR+SYN). The CED curves of these settings are shown in <ref type="figure" target="#fig_4">Fig. 8</ref>. In fact, the architecture of classical CSR is the same as SDM <ref type="bibr" target="#b37">[38]</ref>. They also have similar CED curves (comparing <ref type="figure">Fig. 7</ref> with <ref type="figure" target="#fig_4">Fig. 8</ref>). As indicated by <ref type="figure" target="#fig_4">Fig. 8</ref>, the new DAC-CSR architecture trained on the original dataset performs better than CSR with our 2D-based data augmentation method (DAC-CSR vs CSR+SYN). However, the best result is achieved when the new DAC-CSR architecture is used jointly with our 2D-based data augmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented a new DAC-CSR architecture for robust FLD in unconstrained faces. The proposed method achieved superior FLD results on the challenging AFLW dataset and delivered competitive performance on the COFW dataset. This is due to the proposed versatile faulttolerant mechanism using fuzzy domain-specific model training and the online dynamic model selection strategy. In addition, a simple but effective data augmentation method based on 2D face synthesis was proposed. Compared with the classical CSR method, both the new DAC-CSR architecture and the 2D-based data augmentation method proved beneficial for the FLD performance on unconstrained faces.</p><p>We believe that our contributions can be further extended, e.g. using deep-neural-network-based approaches. We leave for future work the exploration of methods that combine our DAC-CSR architecture and data augmentation method with other FLD algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The pipeline of our proposed DAC-CSR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The proposed domain split strategy (K = 2, c k = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>A self-evaluation of our proposed DAC-CSR on COFW. The meaning of each term is introduced in Section 6.3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>A summary of the evaluation protocols used in our experiments</figDesc><table><row><cell>Protocol</cell><cell>Training Set</cell><cell>Test Set</cell><cell cols="2"># Landmarks Normalisation</cell><cell>Setting</cell></row><row><cell>AFLW-full</cell><cell cols="2">20000 from AFLW 4386 from AFLW</cell><cell>19</cell><cell>face size</cell><cell>CCL [46]</cell></row><row><cell cols="3">AFLW-frontal 20000 from AFLW 1165 from AFLW</cell><cell>19</cell><cell>face size</cell><cell>CCL [46]</cell></row><row><cell>COFW</cell><cell>1345 from COFW</cell><cell>507 from COFW</cell><cell>29</cell><cell>eye distance</cell><cell>standard</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>compares our DAC-CSR with state-of-the-art methods on AFLW using both the AFLW-full and AFLWfrontal protocols. The results obtained with our DAC-CSR show the best normalised average error on both the full test</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell>in Total)</cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>(4386 Fraction of Test Faces</cell><cell>0 0.2 0.4 0.6</cell><cell>0</cell><cell>0.02 Error Normalised by Face Size 0.04 0.06 0.08 SDM ERT RCPR CFSS LBF GRF+LBP CCL Our DAC-CSR 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison on COFW. The error was measured on 29 landmarks and normalised by the inter-ocular distance.</figDesc><table><row><cell>Method</cell><cell>Error</cell><cell cols="2">Failure Speed (FPS)</cell></row><row><cell>ESR [5]</cell><cell>11.2%</cell><cell>36%</cell><cell>4</cell></row><row><cell>RCPR [4]</cell><cell>8.5%</cell><cell>20%</cell><cell>3</cell></row><row><cell>HPM [18]</cell><cell>7.5%</cell><cell>13%</cell><cell>0.03</cell></row><row><cell>RCRC [16]</cell><cell>7.3%</cell><cell>12%</cell><cell>22</cell></row><row><cell>CCR [15]</cell><cell>7.03%</cell><cell>10.9%</cell><cell>69</cell></row><row><cell>DRDA [42]</cell><cell>6.46%</cell><cell>6%</cell><cell>N.A.</cell></row><row><cell>RAR</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In our experiments, 1314 frontal faces were selected using the list provided by<ref type="bibr" target="#b45">[46]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the EPSRC Programme Grant 'FACER2VM' (EP/N007743/1), the National Natural Science Foundation of China (61373055, 61672265) and the Natural Science Foundation of Jiangsu Province (BK20140419, BK20161135).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inverse rendering of faces with a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aldrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1080" to="1093" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Report on the FG 2015 video person recognition evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face alignment by Explicit Shape Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2887" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1407</biblScope>
			<biblScope unit="page" from="484" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active shape models -&apos;smart snakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machince Vision Conference</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">View-based active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="657" to="664" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature Detection and Tracking with Constrained Local Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Mahine Vision Conference</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="929" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">M3csr: Multi-view, multi-scale and multi-component cascade shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pantic. Variational gaussian process auto-encoder for ordinal prediction of facial action units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Taipei, Taiwan. Oral</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascaded collaborative regression for robust facial landmark detection trained using a mixture of synthetic and real images with dynamic weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3425" to="3440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random Cascaded-Regression Copse for Robust Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic face annotation by multilinear AAM with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2586" to="2589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Occlusion Coherence: Localizing Occluded Faces with a Hierarchical Deformable Part Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Growing regression forests by classification: Applications to object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="552" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<title level="m">Efficient 3D Morphable Face Model Fitting. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fitting 3D Morphable Face Models using local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?tsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1195" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3D Morphable Face Models and Their Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="185" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Annotated Facial Landmarks in the Wild: A Large-scale, Realworld Database for Facial Landmark Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active Appearance Models Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated 3d face reconstruction from multiple images using quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piotraschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: Database and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dictionary Integration using 3D Morphable Face Models for Pose-invariant Collaborative-representationbased Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00284</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Convolutional Network Cascade for Facial Point Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mnemonic Descent Method: A Recurrent Process Applied for End-To-End Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust face alignment using a mixture of invariant experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tambe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection via recurrent attentiverefinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2664" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learn to Combine Multiple Hypotheses for Accurate Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Computer Vision -Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Random Subspace Supervised Descent Method for Regression Problems in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1816" to="1820" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Occlusion-Free Face Alignment: Deep Regression Networks Coupled With De-Corrupt AutoEncoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Auto-Encoder Networks (CFAN) for Real-Time Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8690</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unconstrained Face Alignment via Cascaded Compositional Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face Alignment Across Large Poses: A 3D Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Discriminative 3D morphable model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops onAutomatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
