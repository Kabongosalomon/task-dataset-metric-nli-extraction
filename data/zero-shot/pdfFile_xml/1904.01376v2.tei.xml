<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EASY TRANSFER LEARNING BY EXPLOITING INTRA-DOMAIN STRUCTURES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Lab. of Mobile Computing and Pervasive Device</orgName>
								<orgName type="department" key="dep2">Inst. of Computing Tech</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
							<email>yqchen@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Lab. of Mobile Computing and Pervasive Device</orgName>
								<orgName type="department" key="dep2">Inst. of Computing Tech</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
							<email>han.yu@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
							<email>huangmeiyu@qxslab.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Qian Xuesen Lab. of Space Tech</orgName>
								<orgName type="institution">China Academy of Space Tech</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<email>qyang@cse.ust.hk</email>
							<affiliation key="aff3">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EASY TRANSFER LEARNING BY EXPLOITING INTRA-DOMAIN STRUCTURES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transfer Learning</term>
					<term>Domain Adaptation</term>
					<term>Cross-domain Learning</term>
					<term>Non-parametric Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning aims at transferring knowledge from a welllabeled domain to a similar but different domain with limited or no labels. Unfortunately, existing learning-based methods often involve intensive model selection and hyperparameter tuning to obtain good results. Moreover, cross-validation is not possible for tuning hyperparameters since there are often no labels in the target domain. This would restrict wide applicability of transfer learning especially in computationallyconstraint devices such as wearables. In this paper, we propose a practically Easy Transfer Learning (EasyTL) approach which requires no model selection and hyperparameter tuning, while achieving competitive performance. By exploiting intra-domain structures, EasyTL is able to learn both nonparametric transfer features and classifiers. Extensive experiments demonstrate that, compared to state-of-the-art traditional and deep methods, EasyTL satisfies the Occam's Razor principle: it is extremely easy to implement and use while achieving comparable or better performance in classification accuracy and much better computational efficiency. Additionally, it is shown that EasyTL can increase the performance of existing transfer feature learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The success of multimedia applications depends on the availability of sufficient labeled data to train machine learning models. However, it is often expensive and time-consuming to acquire massive amounts of labeled data. Transfer learning (TL), or domain adaptation <ref type="bibr" target="#b11">[12]</ref> is a promising strategy to enhance the learning performance on a target domain with few or none labels by leveraging knowledge from a well-labeled source domain. Since the source and target domains have dif-   ferent distributions, numerous methods have been proposed to reduce the distribution divergence <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref>. Unfortunately, despite the great success achieved by existing TL methods, it is notoriously challenging to apply them to a real situation since we cannot determine the best TL model and their optimal hyperparameters. The reasons are threefold. Firstly, most traditional and deep TL methods are parametric methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5</ref>] that have to go through an intensively expensive and time-consuming process to tune a lot of hyperparameters (e.g. <ref type="figure" target="#fig_1">Fig. 1</ref>; MEDA, DANN, and CDAN in <ref type="table" target="#tab_0">Table 1</ref>). Secondly, cross-validation, which is the most common strategy to select models and tune hyperparameters, is not available in TL since there are often no labeled data in the target domain <ref type="bibr" target="#b11">[12]</ref>. Thirdly, although the recent AutoML methods can automatically tune the hyperparameters via tree pruning, boosting, or neural architecture search <ref type="bibr" target="#b14">[15]</ref>, they are unable to handle the different distributions between domains in TL and typically take a long time to converge.</p><p>These challenges seriously restrict the real application of TL, especially on small devices that require instant local computing with limited resources such as wearables. Is it possible to develop an easy but powerful TL algorithm to circumvent model selection and parameter tuning, but have competitive performance, i.e. satisfying Occam's Razor principle <ref type="bibr" target="#b15">[16]</ref>?</p><p>In this paper, we make the first attempt towards addressing this challenge by proposing a practically Easy Transfer Learning (EasyTL) approach. EasyTL is able to perform knowledge transfer across domains without the need for model selection and hyperparameter tuning ( <ref type="table" target="#tab_0">Table 1)</ref>. By exploiting intra-domain stuctures, EasyTL learns both nonparametric transfer features by intra-domain alignment and transfer classifier by intra-domain programming. Thus, it is able to avoid negative transfer <ref type="bibr" target="#b11">[12]</ref>. Furthermore, EasyTL can also increase the performance of existing TL methods by serving as their final classifier via intra-domain programming.</p><p>We conduct extensive experiments on public TL datasets. Both visual domain adaptation and cross-domain sentiment analysis demonstrated significant superiority of EasyTL in classification accuracy and computational efficiency over state-of-the-art traditional and deep TL methods. In short, EasyTL has the following characteristics:</p><p>? Easy: EasyTL is extremely easy to implement and use. Therefore, it eliminates the need for model selection or hyperparameter tuning in transfer learning. ? Accurate: EasyTL produces competitive results in several popular TL tasks compared to state-of-the-art traditional and deep methods. ? Efficient: EasyTL is significantly more efficient than other methods. This makes EasyTL more suitable for resource-constrained devices such as wearables. ? Extensible: EasyTL can increase the performance of existing TL methods by replacing their classifier with intra-domain programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>EasyTL significantly differs from existing work in the following three aspects: Transfer learning. Existing TL methods can be summarized into two main categories: (a) instance reweighting, which reuses samples from the source domain according to some weighting technique; and (b) feature transformation, which performs subspace learning or distribution adaptation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>. Unfortunately, these methods are all parametric approaches. They depend on extensive hyperparameter tuning through cross-validation for the feature transformation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>, or the prediction model <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>, or both <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. Most methods require multiple iterations of training <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>. The L2T framework <ref type="bibr" target="#b24">[25]</ref> is similar to EasyTL in spirit, but L2T is still based on model iteration and parameter tuning. Deep TL methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5]</ref> require heavy hyperparameter tuning. In TL, cross-validation is often not available since there are almost no labeled data in the target domain <ref type="bibr" target="#b11">[12]</ref>. In contrast, EasyTL is a non-parametric TL approach that directly learns from intra-domain structures, which requires no model selection and hyperparameter tuning and much more efficient than existing methods.</p><p>Non-parametric learning. Nearest-neighbor (NN) classifier is the most common non-parametric method. However, NN computes the distance between each sample in two domains, which is more likely to be influenced by domain shift. Naive Bayes NN (NBNN) classifier is used for domain adaptation <ref type="bibr" target="#b17">[18]</ref>, which still requires hyperparameter tuning and iterations. Nearest Centroid (NC) classifier is based on the distance between each target sample to the class center of the source domain. EasyTL uses a linear programming to get the softmax probabilities (float weights), while NC is basically 0/1 weights. This means that EasyTL could not only consider the relationship between sample and center, but also the relations of other samples. Open set DA <ref type="bibr" target="#b13">[14]</ref> has similar idea with EasyTL, while it solves a binary programming and requires other classifiers.</p><p>Automated machine learning. Recent years have witnessed the advance of Automated Machine Learning (Au-toML) <ref type="bibr" target="#b14">[15]</ref>. AutoML produces results without model selection and parameter tuning. However, no existing AutoML frameworks can handle TL tasks since they assume the training and test data are in the same distribution. AutoDIAL <ref type="bibr" target="#b9">[10]</ref> is similar to EasyTL, which includes automatic domain alignment layers for deep networks. However, AutoDIAL still requires many parameters in the network to be tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EASY TRANSFER LEARNING</head><p>We follow the most common TL settings in existing work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. We are given a labeled source domain</p><formula xml:id="formula_0">? s = {(x s i , y s i )} ns i=1 and an unlabeled target domain ? t = {x t j } nt j=1 . The feature space X s = X t , label space Y s = Y t , but marginal distribution P s (x s ) = P t (x t ) with conditional dis- tribution Q s (y s |x s ) = Q t (y t |x t ).</formula><p>The goal is to predict the labels y t ? Y t for the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>It is challenging to design a TL method that requires no model selection and hyperparameter tuning. On one hand, a simple NN (Nearest Neighbor) classifier may suffice, while NN suffers in handling the distribution divergence between domains. On the other hand, existing TL methods such as GFK <ref type="bibr" target="#b5">[6]</ref> and BDA <ref type="bibr" target="#b19">[20]</ref> could reduce the distribution divergence, while they require tuning a lot of hyperparameters. Combining both of them without considering domain structures may easily result in negative transfer <ref type="bibr" target="#b11">[12]</ref>, which dramatically hurts the performance of TL. Currently, there is no such effort.</p><p>In this work, we propose a practically Easy Transfer Learning (EasyTL) approach to learn non-parametric transfer features and classifier with competitive performance to existing heavy methods. In short, to be consistent with Occam's Razor principle <ref type="bibr" target="#b15">[16]</ref>. In light of the recent advance in representation learning <ref type="bibr" target="#b6">[7]</ref>, incorporating knowledge of locality can greatly improve the representations quality. Therefore, Instead of learning sample-wise distance, EasyTL focuses on exploiting the intra-domain structure. The main part of EasyTL is a novel non-parametric Intra-domain programming classifier, while remains open for adopting existing methods for Intra-domain alignment. Intra-domain programming is able to learn discriminative transfer information from domains. <ref type="figure" target="#fig_2">Fig. 2</ref> shows the procedure of EasyTL. In the following sections, we first introduce the proposed intra-domain programming. Then, we show how to adapt a non-parametric feature learning method for intra-domain alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Intra-domain Programming</head><p>Intra-domain programming directly learns a transfer classifier for TL problem and provides reliable likelihood information for intra-domain alignment. We introduce the concept of the Probability Annotation Matrix, based on which we can build the non-parametric transfer classifier. Formally, let c ? {1, ? ? ? , C} denote the class label, a matrix M ? R C?nt with its element 0 ? M cj ? 1 is a probability annotation matrix. Here, the entry value M cj of M indicates the annotation probability of x t j belonging to class c. <ref type="figure" target="#fig_4">Fig. 3</ref> illustrates the main idea of M. There are 4 classes and n t target samples with example activation values. Similar to the widely-adopted softmax classifier in neural networks, the highest probability value for x t 1 is 0.4, which indicates that it belongs to C 4 . The same goes for x t 2 and x t 3 , etc. The real probability annotation values are to be learned by our proposed approach.</p><p>Instead of learning y t directly, the algorithm focuses on learning the probability annotation matrix M. In this way, the cost function can be formalized as:</p><formula xml:id="formula_1">J = nt j C c D cj M cj ,<label>(1)</label></formula><p>where the distance value D cj is an entry in a distance matrix D. D cj denotes the distance between x t j and the c-th class center of the source domain ?  The class center h c on ? s can be calculated as:</p><formula xml:id="formula_2">D cj = ||x t j ? h c || 2 .<label>(2)</label></formula><formula xml:id="formula_3">h c = 1 |? (c) s | ns i x s i ? I(y s i = c),<label>(3)</label></formula><p>where I(?) is an indicator function which evaluates to 1 if the condition is true, and 0 otherwise. Consider the constraints to minimize the cost function in Eq. (1). Firstly, note that the value of M cj is a probability measuring the confidence of x t j belonging to class c, such that the sum of the probability of one particular sample x t j belonging to all existing classes is 1. This is ensured by the following constraint:</p><formula xml:id="formula_4">C c M cj = 1, ?j ? {1, ? ? ? , n t }.<label>(4)</label></formula><p>Secondly, since ? s and ? t have the same label space (i.e. Y s = Y t ), there must be at least one sample for any given class c. This is ensured by the following constraint:</p><p>M cj = arg max r M rj , r ? {1, ? ? ? , C}, ?c ? {1, ? ? ? , C}, ?j.</p><p>(5) In fact, the ideal state of M cj should be a binary value (0 or 1), i.e. M cj = 1 iff x j belongs to class c, otherwise M cj = 0. Therefore, we use the following formulation to replace Eq. (5) in the computation without affecting the results:</p><formula xml:id="formula_5">nt j M cj ? 1, ?c ? {1, ? ? ? , C}.<label>(6)</label></formula><p>Learning objective. Combining the cost function in Eq. (1) and the constraints in Eq. (4) and Eq. (6), the final learning objective becomes:</p><formula xml:id="formula_6">min J = nt j C c D cj M cj . s.t. ? ? ? ? ? 0 ? M cj ? 1 C c M cj = 1, ?j ? {1, ? ? ? , n t } nt j M cj ? 1, ?c ? {1, ? ? ? , C}<label>(7)</label></formula><p>Solving Eq. (7) requires us to solve a linear programming (LP) problem. There are existing solution packages such as PuLP 1 for solving the above linear programming 1 https://pypi.python.org/pypi/PuLP/1.1 problem efficiently. Then, M can be obtained. Eventually, the label of x t j is given by the softmax function:</p><formula xml:id="formula_7">y t j = arg max r M rj C c M cj for r ? {1, ? ? ? , C}. (8)</formula><p>It is noticeable that this classifier does not involve any parameters to tune explicitly. This is significantly different from well-established classifiers such as SVM that needs to tune numerous hyperparameters. In fact, intra-domain programming can be used alone for TL problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Intra-domain Alignment</head><p>Intra-domain alignment serves as the transfer feature learning methods for EasyTL. Although EasyTL can be used directly in TL with good performance, it can also be combined with transfer feature learning to eliminate feature distortions in the original space. On the other hand, existing transfer feature learning methods can also be extended using the previous intra-domain programming to enhance their performance. Recall our inspiration that learning the structure of locality will help learn useful representations <ref type="bibr" target="#b6">[7]</ref>. Therefore, if we perform personalized transfer feature learning within each subspace, we could further reduce the domain divergence. For simplicity and efficiency, we only perform feature learning from the source domain to the subspace of the target domain.</p><p>Inspired by the non-parametric feature learning method CORAL <ref type="bibr" target="#b16">[17]</ref>, the intra-programming process of EasyTL can be formulated as:</p><formula xml:id="formula_8">z r = x r ? (cov(x s ) + E s ) ? 1 2 ? (cov(x t ) + E t ) 1 2 if r = s x r if r = t<label>(9)</label></formula><p>where cov(?) is the covariance matrix. E s and E t are identity matrices with equal sizes to ? s and ? t , respectively. We can treat this step as a re-coloring process of each subspace <ref type="bibr" target="#b16">[17]</ref>. Eq. (9) aligns the two distributions by re-coloring whitened source features with the covariance of target distributions.</p><p>Remark: Other than CORAL, EasyTL can also choose other popular methods for feature learning such as BDA <ref type="bibr" target="#b19">[20]</ref> and GFK <ref type="bibr" target="#b5">[6]</ref>. We choose CORAL for its computational efficiency and that it contains no other parameters to tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Computational Complexity</head><p>We use the big-O notation to analyze the complexity of EasyTL. Intra-domain alignment takes at most O(Cn 3 s ). The complexity of intra-domain programming is O(n 3 t C 3 ) given that it is a linear programming problem (Eq. (7)) <ref type="bibr" target="#b10">[11]</ref>. EasyTL can be made more efficient with the low-rank representations and other fast computing algorithms.</p><p>As an intuitive comparison of classifiers, we compare intra-domain programming with the well-established SVM. A single round of SVM training and prediction takes O((n s + Algorithm 1 EasyTL: Easy Transfer Learning Input: Feature matrix x s , x t for ? s and ? t , respectively; and label vector y s for ? s Output: Predicted label vector y t for target domain.</p><p>1: (Optional) Perform intra-domain alignment via Eq. (9) 2: Solve Eq. <ref type="formula" target="#formula_6">(7)</ref> to obtain the probability annotation matrix M and compute y t using Eq. (8) <ref type="bibr">3:</ref> return Label vector y t for ? t n t ) 3 ) computations <ref type="bibr" target="#b0">[1]</ref>. This implies that the time complexity of intra-domain programming is comparable to a single round of SVM. However, SVM still needs a couple of rounds for hyperparameter tuning (e.g. kerneltype, constraints) before getting optimal performance. Therefore, EasyTL is theoretically more efficient than SVM. We will experimentally show the efficiency of EasyTL in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We use four popular TL datasets: 1) Amazon Review State-of-the-art traditional and deep comparison methods: NN (Nearest Neighbor), SVM with linear kernel <ref type="bibr" target="#b16">[17]</ref>, TCA (Transfer Component Analysis) <ref type="bibr" target="#b12">[13]</ref>, GFK (Geodesic Flow Kernel) <ref type="bibr" target="#b5">[6]</ref>, SA (Subspace Alignment) <ref type="bibr" target="#b3">[4]</ref>, CORAL (CORrelation ALignment) <ref type="bibr" target="#b16">[17]</ref>, BDA (Balanced Distribution Adaptation) <ref type="bibr" target="#b19">[20]</ref>, JGSA (Joint Geometrical and Statistical Alignment) <ref type="bibr" target="#b25">[26]</ref>, D-GFK <ref type="bibr" target="#b23">[24]</ref>, ResNet50, DANN (Domain-adversarial Neural Networks) <ref type="bibr" target="#b4">[5]</ref>, JAN (Joint Adaptation Networks) <ref type="bibr" target="#b8">[9]</ref>, and CDAN (Conditional Adversarial Adaptation Networks) <ref type="bibr" target="#b7">[8]</ref>. The source code of EasyTL is available at http://transferlearning.xyz/code/ traditional/EasyTL. Specifically, we use EasyTL(c) to denote intra-domain programming since it can serve as a TL method alone, while EasyTL denotes the full method. By following the standard protocol in <ref type="bibr" target="#b16">[17]</ref>, we adopt linear SVM for traditional TL methods. For the amazon dataset, we use the 400dimensional features <ref type="bibr" target="#b21">[22]</ref>. For image datasets, we use the 2048-dimensional ResNet50 finetuned features <ref type="bibr" target="#b21">[22]</ref>. For Office-Caltech datasets, we adopt the SURF features <ref type="bibr" target="#b23">[24]</ref>.  Deep TL methods are only used in image datasets whose results are obtained from existing work. Classification accuracy is acting as the evaluation metric <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results: Accuracy and Efficiency</head><p>The results on Amazon Review is shown in <ref type="table" target="#tab_2">Table 3</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows the results on ImageCLEF DA (IDs 1?6) and Office-Home (IDs 7?18). Results on Office-Caltech are in the code page due to space limit. We also report the average rank <ref type="bibr" target="#b2">[3]</ref>, parameter and running time (train+test) of several methods in  tuning process while deep methods have to run the network multiple times to get optimal parameters. 2) Moreover, in real applications, it is rather difficult and almost impossible to search parameters since there are often little or none labeled data in the target domain. It seems that adversarial learning provides little contribution on TL compared to the power of ResNet in representation learning, and it is rather difficult to train an adversarial network.</p><p>3) The results imply that the "2stage" procedure (finetune+EasyTL) is better than "1-stage" methods (deep TL) even if they are end-to-end. All methods have their advantages and disadvantages. 4) EasyTL tends to perform consistently well on rather balanced datasets (Amazon Review and Image-CLEF DA). While on the unbalanced Office-Caltech dataset, the performance of EasyTL is limited. The research of improving EasyTL for unbalanced datasets remains as future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Extensibility</head><p>We evaluate the extensibility of EasyTL by extending existing TL methods with the classifier of EasyTL. We use PCA, TCA <ref type="bibr" target="#b12">[13]</ref>, GFK <ref type="bibr" target="#b5">[6]</ref>, and CORAL <ref type="bibr" target="#b16">[17]</ref> for intra-domain alignment and compare the performance of +SVM and +EasyTL in <ref type="figure" target="#fig_6">Fig. 4</ref>. The results show that: 1) By using feature learning methods other than CORAL, EasyTL can still achieve comparable performance to existing approaches. 2) More importantly, EasyTL does not rely on any particular feature learning methods to achieve good performance. 3) EasyTL can increase the performances of existing TL methods. It clearly implies the extensibility of EasyTL in transfer learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we propose EasyTL as the first non-parametric transfer learning approach by exploiting intra-domain structures features to learn non-parametric transfer features and classifiers. Thus, it requires no model selection and hyperparameter tuning. EasyTL is easy, accurate, efficient, and extensible, and can be applied directly in the resource-constrained devices such as wearables. Experiments demonstrate the superiority of EasyTL in accuracy and efficiency over several popular traditional and deep TL methods. EasyTL can also increase the performance of other TL methods. EasyTL opens up some new research opportunities in TL: Practical Transfer Learning. Instead of pursuing good performance in accuracy, we do hope that EasyTL could open up a new door in research on practical transfer learning that focuses more on the actual usage of the methods: parameter tuning and model validation.</p><p>Automated Domain Selection. Currently, EasyTL focuses on how to transfer. In real applications, another important problem is what to transfer. Some existing work focused on this issue, but they also require heavy parameter tuning. We will further explore this part by extending EasyTL.</p><p>Multi-label Transfer Learning. EasyTL can be used in multi-label transfer learning by changing the value of probability annotation matrix. This makes EasyTL suitable for multi-label tasks. Existing multi-label transfer methods have to modify their classifiers. EasyTL can be more advantageous for these applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Corresponding author: Y. Chen. J. Wang and Y. Chen are also affiliated with University of Chinese Academy of Sciences, Beijing 100190, China. 978-1-5386-1737-3/18/$31.00 2019 IEEE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>A brief illustration of EasyTL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The procedure of EasyTL. The colored boxes and circles denote samples from the source and target domains. The red line denotes the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(c) s . We denote h c as the c-th class center of ? (c) s . Then, D cj can be computed by the Euclidean distance:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>An example of the probability annotation matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><ref type="bibr" target="#b1">[2]</ref> is a cross-domain sentiment analysis dataset that contains positive and negative reviews of four kinds of products: Kitchen appliance (K), DVDs (D), Electronics (E), and Books (B).2) Office-Caltech [6] contains 10 common classes of images in Amazon (A), DSLR (D), Webcam (W), and Caltech (C). 3) Image-CLEF DA [9] contains 12 categories of images belonging to 3 domains: Caltech (C), ImageNet (I), and Pascal (P). 4) Office-Home [19] contains 15,500 images of 65 categories from 4 domains: Art (Ar), Clipart (Cl), Product (Pr), and Real-world (Rw). Within each dataset, any two domains can be source and target to construct TL tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Extending existing methods with EasyTL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between EasyTL and recent TL methods on Office-Home [19] dataset</figDesc><table><row><cell>Method</cell><cell cols="4">MEDA [23] DANN [5] CDAN [8] EasyTL</cell></row><row><cell>Accuracy</cell><cell>60.3</cell><cell>57.6</cell><cell>62.8</cell><cell>63.3</cell></row><row><cell>Hyperparameter</cell><cell>?, ?, p</cell><cell>?</cell><cell>?</cell><cell>None</cell></row><row><cell>Network</cell><cell></cell><cell>ResNet50</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Accuracy (%) on Image-CLEF DA and Office-Home datasets using ResNet features</figDesc><table><row><cell>ID</cell><cell>Task</cell><cell cols="12">ResNet 1NN SVM TCA GFK BDA CORAL DANN JAN CDAN EasyTL(c) EasyTL</cell></row><row><cell>1</cell><cell>C ? I</cell><cell>78.0</cell><cell>83.5</cell><cell>86.0</cell><cell cols="3">89.3 86.3 90.8</cell><cell>83.0</cell><cell>87.0</cell><cell>89.5</cell><cell>91.2</cell><cell>85.5</cell><cell>91.5</cell></row><row><cell>2</cell><cell>C ? P</cell><cell>65.5</cell><cell>71.3</cell><cell>73.2</cell><cell cols="3">74.5 73.3 73.7</cell><cell>71.5</cell><cell>74.3</cell><cell>74.2</cell><cell>77.2</cell><cell>72.0</cell><cell>77.7</cell></row><row><cell>3</cell><cell>I ? C</cell><cell>91.5</cell><cell>89.0</cell><cell>91.2</cell><cell cols="3">93.2 93.0 94.0</cell><cell>88.7</cell><cell>96.2</cell><cell>94.7</cell><cell>96.7</cell><cell>93.3</cell><cell>96.0</cell></row><row><cell>4</cell><cell>I ? P</cell><cell>74.8</cell><cell>74.8</cell><cell>76.8</cell><cell cols="3">77.5 75.5 75.3</cell><cell>73.7</cell><cell>75.0</cell><cell>76.8</cell><cell>78.3</cell><cell>78.5</cell><cell>78.7</cell></row><row><cell>5</cell><cell>P ? C</cell><cell>91.2</cell><cell>76.2</cell><cell>85.8</cell><cell cols="3">83.7 82.3 83.5</cell><cell>72.0</cell><cell>91.5</cell><cell>91.7</cell><cell>93.7</cell><cell>91.0</cell><cell>95.0</cell></row><row><cell>6</cell><cell>P ? I</cell><cell>83.9</cell><cell>74.0</cell><cell>80.2</cell><cell cols="3">80.8 78.0 77.8</cell><cell>71.3</cell><cell>86.0</cell><cell>88.0</cell><cell>91.2</cell><cell>89.5</cell><cell>90.3</cell></row><row><cell>-</cell><cell>AVG</cell><cell>80.7</cell><cell>78.1</cell><cell>82.2</cell><cell cols="3">83.2 81.4 82.5</cell><cell>76.7</cell><cell>85.0</cell><cell>85.8</cell><cell>88.1</cell><cell>85.0</cell><cell>88.2</cell></row><row><cell>7</cell><cell>Ar ? Cl</cell><cell>34.9</cell><cell>45.3</cell><cell>45.3</cell><cell cols="3">38.3 38.9 38.9</cell><cell>42.2</cell><cell>45.6</cell><cell>45.9</cell><cell>46.6</cell><cell>51.6</cell><cell>52.8</cell></row><row><cell>8</cell><cell>Ar ? Pr</cell><cell>50.0</cell><cell>60.1</cell><cell>65.4</cell><cell cols="3">58.7 57.1 54.8</cell><cell>59.1</cell><cell>59.3</cell><cell>61.2</cell><cell>65.9</cell><cell>68.1</cell><cell>72.1</cell></row><row><cell>9</cell><cell>Ar ? Rw</cell><cell>58.0</cell><cell>65.8</cell><cell>73.1</cell><cell cols="3">61.7 60.1 58.2</cell><cell>64.9</cell><cell>70.1</cell><cell>68.9</cell><cell>73.4</cell><cell>74.2</cell><cell>75.9</cell></row><row><cell>10</cell><cell>Cl ? Ar</cell><cell>37.4</cell><cell>45.7</cell><cell>43.6</cell><cell cols="3">39.3 38.7 36.2</cell><cell>46.4</cell><cell>47.0</cell><cell>50.4</cell><cell>55.7</cell><cell>53.1</cell><cell>55.0</cell></row><row><cell>11</cell><cell>Cl ? Pr</cell><cell>41.9</cell><cell>57.0</cell><cell>57.3</cell><cell cols="3">52.4 53.1 53.1</cell><cell>56.3</cell><cell>58.5</cell><cell>59.7</cell><cell>62.7</cell><cell>62.9</cell><cell>65.9</cell></row><row><cell>12</cell><cell>Cl ? Rw</cell><cell>46.2</cell><cell>58.7</cell><cell>60.2</cell><cell cols="3">56.0 55.5 50.2</cell><cell>58.3</cell><cell>60.9</cell><cell>61.0</cell><cell>64.2</cell><cell>65.3</cell><cell>67.6</cell></row><row><cell>13</cell><cell>Pr ? Ar</cell><cell>38.5</cell><cell>48.1</cell><cell>46.8</cell><cell cols="3">42.6 42.2 42.1</cell><cell>45.4</cell><cell>46.1</cell><cell>45.8</cell><cell>51.8</cell><cell>52.8</cell><cell>54.4</cell></row><row><cell>14</cell><cell>Pr ? Cl</cell><cell>31.2</cell><cell>42.9</cell><cell>39.1</cell><cell cols="3">37.5 37.6 38.2</cell><cell>41.2</cell><cell>43.7</cell><cell>43.4</cell><cell>49.1</cell><cell>45.8</cell><cell>46.9</cell></row><row><cell>15</cell><cell>Pr ? Rw</cell><cell>60.4</cell><cell>68.9</cell><cell>69.2</cell><cell cols="3">64.1 64.6 63.1</cell><cell>68.5</cell><cell>68.5</cell><cell>70.3</cell><cell>74.5</cell><cell>73.5</cell><cell>74.7</cell></row><row><cell>16</cell><cell>Rw ? Ar</cell><cell>53.9</cell><cell>60.8</cell><cell>61.1</cell><cell cols="3">52.6 53.8 50.2</cell><cell>60.1</cell><cell>63.2</cell><cell>63.9</cell><cell>68.2</cell><cell>62.2</cell><cell>63.8</cell></row><row><cell>17</cell><cell>Rw ? Cl</cell><cell>41.2</cell><cell>48.3</cell><cell>45.6</cell><cell cols="3">41.7 42.3 44.0</cell><cell>48.2</cell><cell>51.8</cell><cell>52.4</cell><cell>56.9</cell><cell>50.2</cell><cell>52.3</cell></row><row><cell>18</cell><cell>Rw ? Pr</cell><cell>59.9</cell><cell>74.7</cell><cell>75.9</cell><cell cols="3">70.5 70.6 68.2</cell><cell>73.1</cell><cell>76.8</cell><cell>76.8</cell><cell>80.7</cell><cell>76.0</cell><cell>78.0</cell></row><row><cell>-</cell><cell>AVG</cell><cell>46.1</cell><cell>56.4</cell><cell>56.9</cell><cell cols="3">51.3 51.2 49.8</cell><cell>55.3</cell><cell>57.6</cell><cell>58.3</cell><cell>62.8</cell><cell>61.3</cell><cell>63.3</cell></row><row><cell>-</cell><cell>Avg rank [3]</cell><cell>12</cell><cell>7</cell><cell>6</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>8</cell><cell>5</cell><cell>4</cell><cell>2</cell><cell>3</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Accuracy (%) on Amazon Review dataset</figDesc><table><row><cell cols="6">Method 1NN TCA GFK SA BDA CORAL JGSA EasyTL(c) EasyTL</cell></row><row><cell cols="2">B ? D 49.6 63.6 66.4 67.0 64.2</cell><cell>71.6</cell><cell>66.6</cell><cell>78.4</cell><cell>79.8</cell></row><row><cell cols="2">B ? E 49.8 60.9 65.5 70.8 62.1</cell><cell>65.1</cell><cell>75.0</cell><cell>77.5</cell><cell>79.7</cell></row><row><cell cols="2">B ? K 50.3 64.2 69.2 72.2 65.4</cell><cell>67.3</cell><cell>72.1</cell><cell>79.2</cell><cell>80.9</cell></row><row><cell cols="2">D ? B 53.3 63.3 66.3 67.5 62.4</cell><cell>70.1</cell><cell>55.5</cell><cell>79.5</cell><cell>79.9</cell></row><row><cell cols="2">D ? E 51.0 64.2 63.7 67.1 66.3</cell><cell>65.6</cell><cell>67.3</cell><cell>77.4</cell><cell>80.8</cell></row><row><cell cols="2">D ? K 53.1 69.1 67.7 69.4 68.9</cell><cell>67.1</cell><cell>65.6</cell><cell>80.4</cell><cell>82.0</cell></row><row><cell cols="2">E ? B 50.8 59.5 62.4 61.4 59.2</cell><cell>67.1</cell><cell>51.6</cell><cell>73.0</cell><cell>75.0</cell></row><row><cell cols="2">E ? D 50.9 62.1 63.4 64.9 61.6</cell><cell>66.2</cell><cell>50.8</cell><cell>73.1</cell><cell>75.3</cell></row><row><cell cols="2">E ? K 51.2 74.8 73.8 70.4 74.7</cell><cell>77.6</cell><cell>55.0</cell><cell>84.6</cell><cell>84.9</cell></row><row><cell cols="2">K ? B 52.2 64.1 65.5 64.4 62.7</cell><cell>68.2</cell><cell>58.3</cell><cell>75.2</cell><cell>76.5</cell></row><row><cell cols="2">K ? D 51.2 65.4 65.0 64.6 64.3</cell><cell>68.9</cell><cell>56.4</cell><cell>73.8</cell><cell>76.3</cell></row><row><cell cols="2">K ? E 52.3 74.5 73.0 68.2 74.0</cell><cell>75.4</cell><cell>51.7</cell><cell>82.0</cell><cell>82.5</cell></row><row><cell>AVG</cell><cell>51.3 65.5 66.8 67.3 65.5</cell><cell>69.1</cell><cell>60.5</cell><cell>77.8</cell><cell>79.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The results demonstrate that EasyTL outperforms all comparison methods in both sentiment and image data. EasyTL(c) can also achieve competitive performance than most methods. Note that except 1NN and EasyTL, other methods all require hyperparameter tuning. It clearly indicates the superiority of EasyTL in accuracy and efficiency.There are more insightful findings. 1) On the larger and more challenging Office-Home dataset, the performance of EasyTL(c) is only slightly worse than CDAN, while it outperforms all other comparison methods. Compared to deep TL methods (DANN, JAN, and CDAN), although EasyTL takes finetuned features as inputs, it only requires one distinct fine-</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Average rank, parameter, and running time</figDesc><table><row><cell>Method</cell><cell>TCA</cell><cell>GFK</cell><cell cols="5">CORAL DANN CDAN EasyTL(c) EasyTL</cell></row><row><cell>Avg rank</cell><cell>9</cell><cell>10</cell><cell>8</cell><cell>4</cell><cell>2</cell><cell>3</cell><cell>1</cell></row><row><cell cols="3">Parameter d, ?, C, ? d, C, ?</cell><cell>C, ?</cell><cell>?</cell><cell>?</cell><cell>None</cell><cell>None</cell></row><row><cell>Time (s)</cell><cell>119</cell><cell>127.4</cell><cell>126.5</cell><cell cols="2">&gt;1000 &gt;1000</cell><cell>23.8</cell><cell>59.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Time complexity analysis of support vector machines in libsvm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdiansah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wardoyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="28" to="34" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dem?ar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5067" to="5075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the complexity of linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNN</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Taking human out of learning applications: A survey on automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Quanming</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1810" to="13306" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Occam&apos;s razor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="294" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Frustratingly easy nbnn domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Balanced distribution adaptation for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1129" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stratified transfer learning for cross-domain activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PerCom</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Everything about transfer learning and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://transferlearning.xyz" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning discriminative geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transfer learning via learning to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint geometrical and statistical alignment for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
