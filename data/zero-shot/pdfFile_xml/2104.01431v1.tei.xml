<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aggregated Contextual Transformations for High-Resolution Image Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Baining</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">Aggregated Contextual Transformations for High-Resolution Image Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image synthesis</term>
					<term>image inpainting</term>
					<term>object removal</term>
					<term>generative adversarial networks (GAN)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image inpainting that completes large free-form missing regions in images is a promising yet challenging task. State-of-the-art approaches have achieved significant progress by taking advantage of generative adversarial networks (GAN). However, these approaches can suffer from generating distorted structures and blurry textures in high-resolution images (e.g., 512 ? 512). The challenges mainly drive from (1) image content reasoning from distant contexts, and (2) fine-grained texture synthesis for a large missing region. To overcome these two challenges, we propose an enhanced GAN-based model, named Aggregated COntextual-Transformation GAN (AOT-GAN), for high-resolution image inpainting. Specifically, to enhance context reasoning, we construct the generator of AOT-GAN by stacking multiple layers of a proposed AOT block. The AOT blocks aggregate contextual transformations from various receptive fields, allowing to capture both informative distant image contexts and rich patterns of interest for context reasoning. For improving texture synthesis, we enhance the discriminator of AOT-GAN by training it with a tailored mask-prediction task. Such a training objective forces the discriminator to distinguish the detailed appearances of real and synthesized patches, and in turn facilitates the generator to synthesize clear textures. Extensive comparisons on Places2, the most challenging benchmark with 1.8 million high-resolution images of 365 complex scenes, show that our model outperforms the state-of-the-art by a significant margin in terms of FID with 38.60% relative improvement. A user study including more than 30 subjects further validates the superiority of AOT-GAN. We further evaluate the proposed AOT-GAN in practical applications, e.g., logo removal, face editing, and object removal. Results show that our model achieves promising completions in the real-world. We release code and models in https://github.com/researchmm/AOT-GAN-for-Inpainting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I MAGE inpainting aims at filling missing regions in images with plausible contents <ref type="bibr" target="#b0">[1]</ref>. An effective image inpainting algorithm plays an important role in numerous applications, such as object removal, photograph restoration and diminished reality <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Despite of the great benefits of this technology, image inpainting meets grand challenges in simultaneously recovering reasonable contents and clear textures for large free-form missing regions in high-resolution images.</p><p>Early works attempt to solve the problem by either diffusionbased or patch-based algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. A diffusionbased algorithm propagates contextual information from boundaries into holes in the isophotes direction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>. A patch-based algorithm synthesizes missing contents by copying and pasting similar patches from uncorrupted image contexts or external database <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>. These approaches can work well in complet-? This work has been submitted to the IEEE for possible publication.</p><p>Copyright may be transferred without notice, after which this version may no longer be accessible. ? ?2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. ? This work is conducted when Yanhong Zeng was a research intern in Microsoft Research. ? Y. <ref type="bibr">Zeng</ref>    <ref type="figure">Fig. 1</ref>: An illustration of image inpainting. Given an image (a) (resolution: 512 ? 512) with large irregular holes, our model is able to reconstruct more plausible structures and clearer textures of the window in (d) compared with GatedConv <ref type="bibr" target="#b6">[7]</ref> (b) and HiFill <ref type="bibr" target="#b7">[8]</ref> (c). We present enlarged patches by red boxes next to the images.</p><p>ing narrow holes in stationary backgrounds. However, they often fail to hallucinate plausible contents in semantic inpainting due to the lack of powerful reasoning for missing contents and textures in complex scenes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Significant progress has been made by the emergence of deep feature learning <ref type="bibr" target="#b14">[15]</ref> and adversarial training <ref type="bibr" target="#b15">[16]</ref> for image inpainting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. These deep image inpainting models are typically built upon generative adversarial networks (GAN) <ref type="bibr" target="#b15">[16]</ref>. Through a joint optimization by reconstruction losses and adver-sarial losses <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>, existing deep models have shown promising results in semantic inpainting. However, directly applying these deep models in higher-resolution images (e.g., 512 ? 512) often results in distorted structures and blurry textures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Such a problem hinders these algorithms from practical applications, where users' photos are often larger than 256 ? 256. The difficulties mainly drive from simultaneously inferring reasonable contents from distant image contexts and generating fine-grained image presentation for each pixel over a large region. We discuss these two challenges respectively as below.</p><p>Challenge 1: Context reasoning. To infer reasonable contents for missing regions, deep image inpainting models leverage features from distant image contexts. For example, contextual attention modules are proposed to model relations between hole regions and image contexts by patch-wise matching <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>. However, the patch-wise matching often results in distorted structures due to the issue of repeating patterns <ref type="bibr" target="#b21">[22]</ref>. The other line of works stack serialized dilated convolution layers to capture distant contexts. However, as widely discussed in previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>, serialized dilated convolutions tend to encode features of predefined gridding patterns rather than capturing rich patterns of interest for context reasoning. As shown in <ref type="figure">Fig. 1</ref>, the state-ofthe-art approaches, GatedConv <ref type="bibr" target="#b6">[7]</ref> (b) and HiFill <ref type="bibr" target="#b7">[8]</ref> (c), fail in completing the structures of the window in results.</p><p>Challenge 2: Fine-grained texture synthesis. To synthesize fine-grained textures, state-of-the-art models utilize adversarial training by designing a game-theoretical minimax game between a generator and a discriminator. The majority of them inherit the discriminator from PatchGAN <ref type="bibr" target="#b23">[24]</ref> due to its great success in image translation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. The discriminator of PatchGAN aims to distinguish the patches of a real image from those of an inpainted result. However, they ignore the fact that the patches outside missing regions are shared by both real and inpainted images, and blindly push the discriminator to distinguish these identical patches. Such a problematic learning objective weakens the discriminator, and in turn hinders the generator from synthesizing realistic fine-grained textures. As shown in <ref type="figure">Fig. 1</ref>, GatedConv <ref type="bibr" target="#b6">[7]</ref> (b) and HiFill <ref type="bibr" target="#b7">[8]</ref> (c) tend to generate unrealistic textures in the missing regions.</p><p>To overcome the above two challenges, we propose a novel GAN-based model for high-resolution image inpainting, named Aggregated COntextual-Transformation GAN (AOT-GAN). It consists of (1) a generator that exploits aggregated contextual transformations of distant image contexts for enhancing context reasoning, and (2) a discriminator trained by a tailored maskprediction task to facilitate the synthesis of fine-grained textures.</p><p>To enhance context reasoning for large missing regions, we build the generator of AOT-GAN by repeating a carefully-designed building block, namely AOT block. Specifically, the AOT blocks adopt the split-transform-merge strategy. First, an AOT block splits the kernel of a standard convolution into multiple subkernels. Second, it applies each sub-kernel on the incoming features with different dilation rates. Finally, the AOT block aggregates different transformations from all the sub-kernels as output ( <ref type="figure" target="#fig_2">Fig. 3-(b)</ref>). Through these three steps, informative distant image contexts are leveraged by using various dilation rates, while rich patterns of interest are captured by aggregating multiple transformation results, leading to better context reasoning for missing regions. To the best of our knowledge, AOT block is the first highly-modularized building block tailored for highresolution image inpainting.</p><p>To facilitate the synthesis of fine-grained textures, we design a novel mask-prediction task to train the discriminator. Most existing approaches follow PatchGAN <ref type="bibr" target="#b10">[11]</ref> to force the discriminator to predict all patches in inpainted images as fake, while ignoring the fact that those outside missing regions indeed come from real images. Therefore, these approaches may struggle to generate realistic fine-grained textures. To overcome the above limitation, we instead train the discriminator to predict a downsampled patchlevel inpainting mask. In other words, for an inpainted image, the discriminator is expected to segment the synthesized patches from real ones. Such a learning objective leads to a stronger discriminator and in turn improve the generator to synthesize finegrained realistic textures.</p><p>We summarize our contributions as follows: ? We propose to learn aggregated contextual transformations for high-resolution image inpainting, which allows capturing both informative distant contexts and rich patterns of interest for context reasoning. ? We design a novel mask-prediction task to train the discriminator tailored for image inpainting. Such a design forces the discriminator to distinguish the detailed appearances of real and synthesized patches, which in turn facilitates the generator to synthesize fine-grained textures. ? We conduct extensive evaluations on the most challenging benchmark, Places2 <ref type="bibr" target="#b25">[26]</ref>. Results show that our model outperforms the state-of-the-art by a significant margin in terms of FID with 38.60% relative improvement. The remainder of the paper is organized as follows. We review related works in Section 2, and introduce our approach in Section 3. Extensive experiments are presented in Section 4, followed by the conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Image inpainting aims at filling missing regions in images with plausible contents <ref type="bibr" target="#b0">[1]</ref>. Due to its significant practical values for image editing applications (e.g., object removal and photograph restoration), image inpainting has become an active research topic for decades <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref>. Existing approaches can fall into two categories, i.e., non-learning based or learning-based methods. In this section, we discuss these approaches as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Non-learning based Image Inpainting</head><p>Prior to the prevalence of deep learning, approaches for image inpainting are typically divided into two categories, diffusionbased and patch-based algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>. We introduce both of them in this section.</p><p>A diffusion-based algorithm propagates contextual pixels from boundaries to the holes along the isophotes direction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Specifically, many boundary conditions are imposed by the use of Partial Differential Equations during the pixel propagation. Through leveraging the prior knowledge of these boundary conditions, diffusion-based approaches have shown promising results in filling in narrow sketches. However, these approaches typically introduce diffusion-related blurs thus can fail in completing large missing regions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Inspired by patch-based methods for texture synthesis <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, more and more patch-based filling approaches are proposed for image inpainting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>. A patch-based approach typically synthesizes missing contents by copying and pasting similar patches from known image contexts or external database. To improve . AOT-GAN is built upon a generative adversarial network (GAN), which consists of a generator and a discriminator. Specifically, the generator is highly-modularized by stacking multiple-layers of a carefully-designed block, i.e., AOT block, for enhancing context reasoning. The discriminator is trained by a tailored mask-prediction task, which aims at predicting downsampled patch-level inpainting masks. Our model is jointly optimized by a reconstruction loss, an adversarial loss <ref type="bibr" target="#b15">[16]</ref>, a perceptual loss <ref type="bibr" target="#b28">[29]</ref> and a style loss <ref type="bibr" target="#b29">[30]</ref>. Details can be found in Section 3.</p><p>the patch-based algorithm, many efforts have been made to find the optimal patch size, patch offset, filling orders, and matching algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>.  <ref type="bibr" target="#b1">[2]</ref>. These approaches can work well especially in the stationary background inpainting with repeating patterns. However, these approaches can fall short in completing large missing regions of complex scenes for semantic inpainting. This is because patch-based approaches heavily rely on patch-wise matching by low-level features (e.g., isophotes). Such a technology is unable to synthesize contents for which similar patches do not exist in known image contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning-based Image Inpainting</head><p>Significant progress has been made by the emergence of deep feature learning <ref type="bibr" target="#b14">[15]</ref> and adversarial training <ref type="bibr" target="#b15">[16]</ref> for image inpainting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref>. Compared with non-learning based approaches, deep image inpainting models are able to synthesize more plausible contents for complex scenes. To infer missing contents for a large missing region, deep image inpainting models take advantage of features from distant image contexts. Context Encoder <ref type="bibr" target="#b13">[14]</ref>, a seminal deep inpainting model, adopts a fully-connected layer in a compact latent feature space for global image contexts encoding. Early works that follow the architecture of Context Encoder have shown promising results on images of human faces, street views, etc. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. However, these models can only deal with low-resolution images (e.g., 128 ? 128) and missing regions of fixed shapes (e.g., center squares) due to the usage of fully-connected layers. To solve this problem, Iizuka et al. propose to build models upon fully convolutional networks (FCN) <ref type="bibr" target="#b33">[34]</ref>. To capture distant contexts based on FCN, a contextual attention module is proposed to find patches of interest from contexts by patch-wise matching <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref>. Zeng et al. futher adopt a non-local module named attention transfer network to fill missing regions in feature pyramids <ref type="bibr" target="#b20">[21]</ref>. Despite the effective long-range relation modeling, non-local modules tend to repeat patterns in missing regions and distort structures <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. To avoid the issue of repeating patterns, the other line of works stack serialized dilated convolution layers to capture distant contexts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40]</ref>. Dilated convolutions use kernels that are spread out, allowing to compute each output pixel with a much larger receptive field than standard convolutions <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. By stacking the dilated convolutions, the model can effectively "see" a larger area of the input image for context reasoning. However, as widely acknowledged, serialized dilated convolutions tend to encode features of predefined gridding patterns rather than patterns of interest for context reasoning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Inspired by recent works of style transfer <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, increasing numbers of deep inpainting models exploit a style loss <ref type="bibr" target="#b29">[30]</ref> and a perceptual loss <ref type="bibr" target="#b28">[29]</ref> for the synthesis of fine-grained textures. Specifically, the joint optimization of a style loss and a perceptual loss aims at minimizing the perceptual distance between the deep features of inpainted results and the original images. Although promising results have been shown, it remains challenging for these models to generate clear textures <ref type="bibr" target="#b43">[44]</ref>.</p><p>Significant progress has been made by adopting the framework of generative adversarial networks (GAN) for semantic inpainting <ref type="bibr" target="#b15">[16]</ref>. A GAN-based model typically consists of a generator network and a discriminator network. The discriminator is trained to distinguish inpainted images from real ones, while the generator is optimized to synthesize realistic images to fool the discriminator. Through the game-theoretical minmax game between the generator and the discriminator, GAN-based inpainting models are able to generate clearer textures. To further improve the discriminator network, Iizuka et al. propose a join training by a global and a local discriminator <ref type="bibr" target="#b33">[34]</ref>. The global discriminator looks at the entire image and the local discriminator focuses on the local consistency. However, the local discriminator can only deal with missing regions of fixed shapes due to the usage of fully-connected layers in its network. To solve this problem, Yu et al. inherit the discriminator from PatchGAN [24] due to its great success in image translation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref>. The discriminator of PatchGAN aims to distinguish patches of real images from those of inpainted results. Moreover, they apply spectral normalization to each layer of the discriminator to stabilize the training of GAN <ref type="bibr" target="#b45">[46]</ref>. However, PatchGAN-based models usually ignore the fact that patches outside missing regions are shared by both real and inpainted images, and blindly pushing the discriminator to distinguish these identical patches can weaken the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Inferring reasonable contents and generating clear textures for a large free-form missing region are vital yet challenging to high-resolution image inpainting. In this section, we present the details of the proposed Aggregated COntextual-Transformation GAN (AOT-GAN), which enhances the context reasoning and texture synthesis for high-resolution image inpainting. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, AOT-GAN is built upon a generative adversarial network (GAN), which consists of a generator network and a discriminator network. Specifically, the generator is constructed by stacking multiple layers of a carefully-designed building block, i.e. AOT block. The AOT block is able to capture both informative distant contexts and rich patterns of interest for enhancing context reasoning. The discriminator is improved by a tailored maskprediction task, which can in turn facilitates the generator to synthesize clear textures. AOT-GAN is trained by a joint optimization of a reconstruction loss, an adversarial loss, a perceptual loss and a style loss. Through such a joint optimization, AOT-GAN is able to synthesize reasonable contents and clear textures for missing regions of high-resolution images.</p><p>In this section, we introduce the details of AOT block in Section 3.1 and the design of mask-prediction task for the discriminator in Section 3.2. The overall optimization objectives are introduced in Section 3.3, followed by the implementation details in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Aggregated Contextual Transformations</head><p>In this section, we present a simple and highly-modularized generator network, which is enhanced for context reasoning. In particularly, context reasoning for a large free-form missing region is one of the grand challenges to high-resolution image inpainting <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45]</ref>. For one thing, to ensure a coherent structure with surrounding image contexts, deep image inpainting models need to propagate information from distant contexts to missing regions. For another, as objects in complex scenes have various scales and angles of view, capturing as rich as possible patterns of interest is important for context reasoning. The AOT block is designed to capture both informative distant image contexts and rich patterns of interest for context reasoning in high-resolution image inpainting.</p><p>The overview of the generator network of AOT-GAN is depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>. Specifically, the generator consists of an encoder, a stack of the building blocks, i.e. AOT blocks, and a decoder. The generator takes as inputs a masked image and a binary mask indicating missing regions, and it outputs a completed image. First, we stack several layers of standard convolutions as the encoder for feature encoding from the input. Second, features from the encoder are processed by the stack of AOT blocks. Through the backbone built by AOT blocks, the generator is able to capture informative distant image contexts as well as rich patterns of interest for filling missing regions. Finally, we use multiple layers of transposed convolutions to decode features from AOT blocks to a completed image as the final result. We discuss more details of the design of AOT blocks as below.</p><p>AOT blocks adopt the split-transformation-merge strategy by three steps <ref type="bibr" target="#b46">[47]</ref>. (i) Splitting: as shown in <ref type="figure" target="#fig_2">Fig. 3-(b)</ref>, an AOT block splits the kernel of a standard convolution into multiple subkernels, each of which has fewer output channels. For example, splitting a kernel with 256 output channels into four sub-kernels makes each sub-kernel has 64 output channels. (ii) Transforming:   <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref> and the proposed AOT block (b). The numbers inside orange blocks denote (#input channels, filter sizes, dilation rates, #output channels).</p><formula xml:id="formula_0">(a) Residual</formula><p>each sub-kernel performs a different transformation of the input feature x 1 by using a different dilation rate. Specifically, a dilated kernel is spread out by introducing zeros between consecutive positions, which has achieved a great success in the field of semantic segmentation <ref type="bibr" target="#b40">[41]</ref>. Using a larger dilation rate enables the sub-kernel to "see" a larger area of the input image, while a sub-kernel that uses a small dilation rate focuses on the local patterns of a smaller receptive field. (iii) Aggregating: the contextual transformations from different receptive fields are finally aggregated by a concatenation followed by a standard convolution for feature fusion. Such a design allows the AOT block to predict each output pixel through different views. Through the above three steps, an AOT block is able to aggregate multiple contextual transformations for enhancing context reasoning. Furthermore, the stack of AOT blocks largely enriches the combinations of different pathways in the generator network, allowing the generator to capture as many as possible patterns of interest for context reasoning. It is worth noting that, compared with a standard residual block used in the state-of-the-art deep inpainting models ( <ref type="figure" target="#fig_2">Fig. 3-(a)</ref>), AOT blocks do not introduce extra model parameters and computation costs in these three steps. Inspired by the great success of ResNet <ref type="bibr" target="#b14">[15]</ref>, deep inpainting models typically incorporate an identical residual connection in their building blocks to ease the training of networks. As shown in <ref type="figure" target="#fig_2">Fig. 3-(a)</ref>, an identical residual connection aggregates the input feature x 1 and the learned residual feature x 2 by element-wise summing in a spatially-invariant way. However, they ignore the discrepancies between the values of input pixels inside and outside missing regions, resulting in the problem of color discrepancy in inpainted images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b43">44]</ref>. To alleviate this problem, we propose to use a novel gated residual connection in the building block. As shown in <ref type="figure" target="#fig_2">Fig. 3-(b)</ref>, a gated residual connection first calculates the spatially-variant gate value g from x 1 by a standard convolution and a sigmoid operation, and then the AOT block aggregates the input feature x 1 and the learned residual feature x 2 by a weighted sum with g, which is denoted as:</p><formula xml:id="formula_1">x 3 = x 1 ? g + x 2 ? (1 ? g).<label>(1)</label></formula><p>Such a spatially-variant feature aggregation encourages updating features inside missing regions while retaining the known features outside missing regions. We conduct both quantitative and qualita-tive experiments to show the improvements by the gated residual connection in Section 4.7.2.</p><p>Relation to residual blocks <ref type="bibr" target="#b14">[15]</ref>. To enlarge the receptive fields, the state-of-the-art inpainting models also improve the residual block by using dilated convolutions as their building blocks ( <ref type="figure" target="#fig_2">Fig. 3-(a)</ref>) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>. Specifically, they use the same dilation rate for the kernel inside one building block. Although distant image contexts can be leveraged by the stack of such an improved residual block, it often suffers from encoding features of predefined gridding patterns instead of patterns of interest for context reasoning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref>. On the contrast, the proposed AOT block is able to capture rich patterns of interest by learning aggregated contextual transformations with various dilation rates.</p><p>Relation to Atrous Spatial Pyramid Pool (ASPP) <ref type="bibr" target="#b40">[41]</ref>. ASPP is a multi-branch module that shares a similar topology of the proposed AOT block. We highlight the difference that, ASPP is proposed for the task of semantic segmentation and it is used at the end of networks for high-level recognition, while AOT block is a basic building block repeating in inpainting models for low-level tasks. Besides, the AOT block incorporates a carefully-designed gated residual connection that cares about the problem of color discrepancy for image inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Soft Mask-Guided PatchGAN (SM-PatchGAN)</head><p>In this section, we present a novel mask-prediction task for enhancing the training of the discriminator of AOT-GAN, which can in turn facilitate the generator to synthesize clear textures for high-resolution image inpainting.</p><p>Synthesizing clear fine-grained textures for a large missing region is another challenge for high-resolution image inpainting. This is because there are various possible results for filling a missing region, whereas deep inpainting models are trained to reconstruct the original images based on reconstruction losses (e.g., L 1 loss) <ref type="bibr" target="#b12">[13]</ref>. With such an optimization objective, deep inpainting models tend to generate an average of all the possible solutions, resulting in blurry textures. To overcome this challenge, we build our model upon a generative adversarial network (GAN) <ref type="bibr" target="#b15">[16]</ref>. Through the joint optimization of reconstruction losses and an adversarial loss, our model, AOT-GAN, is able to generate clearer results that fall near the manifold of natural images.</p><p>In practice, we denote a real image as x, a corresponding binary inpainting mask as m (with value 0 for known pixels and 1 for missing regions), as pixel-wise multiplication, G as the generator, and thus the inpainted results can be denoted as:</p><formula xml:id="formula_2">z = x (1 ? m) + G(x (1 ? m), m) m.<label>(2)</label></formula><p>We inherit the network architecture of the discriminator from PatchGAN <ref type="bibr" target="#b23">[24]</ref> due to its great success in the field of image translation. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the discriminator network consists of several layers of standard convolutions, each of which reduces the spatial size of feature maps by two. The discriminator takes as input an image from inpainted results or real data, and outputs a prediction map. In particular, each pixel of the prediction map indicates the prediction for a N ? N patch in the input image as real or fake.</p><p>To facilitate the synthesis of fine-grained textures, we design a mask-prediction task for the training of the discriminator, which we term a Soft Mask-guided PatchGAN (SM-PatchGAN). Specifically, we down sample the inpainting masks as the ground truths of the mask-prediction task. To simulate the pixel propagation   around the boundaries of missing regions, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, we use a soft patch-level mask for training. The soft mask is obtained by Gaussian filtering. We denote the adversarial loss for the discriminator as:</p><formula xml:id="formula_3">(3) L D adv = E z?pz (D(z) ? ?(1 ? m)) 2 + E x?p data (D(x) ? 1) 2 ,</formula><p>where ? denotes the composition function of the down-sampling and the Gaussian filtering. Correspondingly, the adversarial loss for the generator is denoted as:</p><formula xml:id="formula_4">(4) L G adv = E z?pz (D(z) ? 1) 2 m ,</formula><p>where only predictions of the synthesized patches for missing regions are used to optimize the generator. Through such an optimization, the discriminator is encouraged to segment synthesized patches of missing regions from real ones of contexts outside missing regions. The enhanced discriminator can in turn facilitate the generator to synthesize more realistic textures.</p><p>Relation to PatchGAN <ref type="bibr" target="#b23">[24]</ref>. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, the Patch-GAN discriminator used in previous inpainting models aims at distinguishing patches of inpainted images from those of real ones. In this way, they blindly push the discriminator to predict all patches in inpainted images as fake, while ignoring that patches outside missing regions indeed come from real images. In contrast, the proposed SM-PatchGAN aims at distinguishing synthesized patches of missing regions from real ones of contexts, which can lead to a stronger discriminator.</p><p>Relation to HM-PatchGAN. We consider another possible design of the mask-prediction task, which we term a Hard Maskguided PatchGAN (HM-PatchGAN). As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, the HM-PatchGAN enhances the PatchGAN discriminator by training by hard binary inpainting masks without Gaussian filtering. The HM-PatchGAN ignores the fact that a patch in an inpainted image around boundaries may contain both real and synthesized pixels. We speculate that such a design can weaken the training of the discriminator. To avoid the above issue, the proposed SM-PatchGAN instead uses soft labels by the processing of Gaussian filtering. We conduct extensive ablation study to show the superiority of SM-PatchGAN in Section 4.7.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Optimization</head><p>The principle of choosing optimization objectives for image inpainting is to ensure both per-pixel reconstruction accuracy and visual fidelity of inpainted images. To this end, we carefully select four optimization objectives, i.e., a L 1 loss, a style loss <ref type="bibr" target="#b29">[30]</ref>, a perceptual loss <ref type="bibr" target="#b28">[29]</ref>, and an adversarial loss of the proposed SM-PatchGAN, for AOT-GAN following the majority of existing deep inpainting models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref>. First, we include a L 1 loss to ensure the reconstruction accuracy on pixel-level:</p><formula xml:id="formula_5">L rec = x ? G(x (1 ? m), m) 1 .<label>(5)</label></formula><p>Since the effectiveness of the perceptual loss <ref type="bibr" target="#b28">[29]</ref> and the style loss <ref type="bibr" target="#b29">[30]</ref> for image inpainting have been widely verified <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, we include them to improve the accuracy of perceptual reconstruction. Specifically, a perceptual loss aims at minimizing the L 1 distance between the activation maps of inpainted and real images:</p><formula xml:id="formula_6">L per = i ? i (x) ? ? i (z) 1 N i ,<label>(6)</label></formula><p>where ? i is the activation map from the i-th layers of a pre-trained network (e.g., VGG19 <ref type="bibr" target="#b47">[48]</ref>), N i is the number of elements in ? i . Similarly, the Style loss is defined as the L 1 distance between the Gram matrices of deep features of inpainted and real images:</p><formula xml:id="formula_7">L sty = E i ? i (x) T ? i (x) ? ? i (z) T ? i (z) 1 .<label>(7)</label></formula><p>Finally, we include the adversarial loss of SM-PatchGAN described in Eq. 4 to improve the visual fidelity of inpainted images. The whole AOT-GAN is trained by a joint optimization of these four objectives, and we conclude the overall optimization objective as below:</p><formula xml:id="formula_8">(8) L = ? adv L G adv + ? rec L rec + ? per L per + ? sty L sty .</formula><p>For our experiments, we empirically choose ? adv = 0.01, ? rec = 1, ? per = 0.1 and ? sty = 250 for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>To build the generator network of the proposed AOT-GAN, we stack three layers of convolutions as the encoder, which reduces the spatial size of images by four times in total. Correspondingly, we use three layers of transposed convolutions for the upsampling in the decoder. We stack eight layers of AOT blocks to build the backbone of the generator. For building the discriminator network, we stack four layers of convolutions with stride as two following 70?70 PatchGAN <ref type="bibr" target="#b23">[24]</ref>. Accordingly, for the processing of Gaussian filtering in SM-PatchGAN, we set the kernel size of the Gaussian kernel as 70 ? 70 to simulate the pixel propagation by convolutions in the discriminator network. In particular, to avoid the issue of color shift caused by normalization layers, we remove all normalization layers in the generator network following previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>. For the model training, eight images are randomly sampled and corresponding masks are randomly created as pairs in each mini-batch. We use a fixed learning rate at 1e ? 4 for both the training of the discriminator and the generator. We use Adam optimizer with ? 1 = 0 and ? 2 = 0.9 for training. In practice, we use VGG19 <ref type="bibr" target="#b47">[48]</ref> pretrained on ImageNet <ref type="bibr" target="#b48">[49]</ref> as our pre-trained network for calculating the style loss and the perceptual loss. Code and models are available in https://github.com/researchmm/ AOT-GAN-for-Inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we introduce the details of datasets, baselines and evaluation metrics for image inpainting in Section 4.1-4.3. We conduct both quantitative and the qualitative evaluations in Section 4.4 and Section 4.5, followed by a user study in Section 4.6. Extensive ablation studies are introduced in Section 4.7 to verify the effectiveness of each component of the AOT-GAN. Finally, we show practical applications of our model in Section 4.8 and discuss limitations in Section 4.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use free-form masks provided by Liu et al. <ref type="bibr" target="#b43">[44]</ref> for both training and testing following common settings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b7">8]</ref>. Because the free-form masks are more challenging, closer to real-world applications and also widely adopted by a majority of inpainting approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>. All images are resized to 512 ? 512 for both training and evaluation following a standard high-resolution setting <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>. We introduce three datasets used in this paper with their brief introductions as below:</p><p>? Places2 [26] contains over 1.8 million images from 365 kinds of scenes. It is one of the most challenging datasets for image inpainting due to its complex scenes. We use the splits of training/testing (i.e., 1.8 million/36,500) following the settings used by most inpainting models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref>. ? CELEBA-HQ [50] is a high-quality dataset of human faces.</p><p>The high-frequency details of hairs and skins can help us to evaluate the fine-grained texture synthesis of models. We use 28,000 images for training and 2,000 for testing following a common setting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We list all the state-of-the-art models used for comparisons with their abbreviations and brief introductions as below:   <ref type="bibr" target="#b12">[13]</ref>, PEN-Net <ref type="bibr" target="#b20">[21]</ref>, PConv <ref type="bibr" target="#b43">[44]</ref>, EdgeConnect <ref type="bibr" target="#b16">[17]</ref>, GatedConv <ref type="bibr" target="#b6">[7]</ref> and HiFill <ref type="bibr" target="#b7">[8]</ref> on Places2 <ref type="bibr" target="#b25">[26]</ref>. ? Lower is better. ? Higher is better. Best and second best results are highlighted and underlined.</p><formula xml:id="formula_9">? CA [13</formula><p>Mask CA <ref type="bibr" target="#b12">[13]</ref> PEN-Net <ref type="bibr" target="#b20">[21]</ref> PConv <ref type="bibr" target="#b43">[44]</ref> EdgeConnect <ref type="bibr" target="#b16">[17]</ref> GatedConv <ref type="bibr" target="#b6">[7]</ref> HiFill <ref type="bibr" target="#b7">[8]</ref> AOT-GAN (ours) </p><formula xml:id="formula_10">L 1 (10 ?2 ) ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We list all the objective metrics used for quantitative comparisons and the reasons for using them as below:</p><p>? L 1 error is widely used in previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. It calculates the mean absolute error between inpainted and original images to evaluate per-pixel reconstruction accuracy. ? Peak Signal-to-Noise Ratio (PSNR) is one of the classical image quality assessments that used by many inpainting approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>. ? Structural Similarity Index (SSIM) <ref type="bibr" target="#b52">[53]</ref> compares inpainted results with the original images from the aspects of luminance, contrast and structure. ? Fr?chet Inception Distance (FID) <ref type="bibr" target="#b53">[54]</ref> is a popular deep metric for perceptual rationality <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref>. FID measures the distance between the distributions of real and fake image features. It is worth noting that, deep metrics have been proved to be closer to human perception <ref type="bibr" target="#b54">[55]</ref>. Although the above four objective metrics can reflect the performance of inpainting models partly, a subjective assessment is always the best way to evaluate inpainting models. Therefore, we conduct extensive qualitative evaluations and a user study for a comprehensive comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative Comparisons</head><p>To evaluate the AOT-GAN on benchmarks, we conduct a quantitative comparison on the most challenging benchmark, Places2 <ref type="bibr" target="#b25">[26]</ref>. Specifically, we use all the images of the evaluation set of Places2 following the common setting used in many deep inpainting approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. Each kind of scene in Places2 contains 100 evaluation images. For each testing image, we randomly sample a free-form mask as the testing mask, which has a specific hole-to-image area ratio (e.g., 20-30%). The free-form masks are provided by Liu et al. <ref type="bibr" target="#b43">[44]</ref>. We use the same image-mask pairs for all approaches for fair comparisons. The results of quantitative comparison can be found in <ref type="table" target="#tab_5">Table 1</ref>. It shows that our model outperforms the state-of-the-art methods in terms of all the metrics, especially in FID. Specifically, our model outperforms GatedConv <ref type="bibr" target="#b6">[7]</ref>, the most competitive model, in terms of FID with 38.60% relative improvement in the most challenging setting (i.e., 50-60% ratio of holes). We highlight the improvements by AOT-GAN in terms of FID, as FID has been verified to be closer to human perception <ref type="bibr" target="#b54">[55]</ref>. We can safely draw the conclusion that, AOT-GAN outperforms the state-ofthe-art models in both accurate reconstruction and synthesizing perceptually pleasing contents.</p><p>MNPS <ref type="bibr" target="#b18">[19]</ref> is a multi-stage model designed for high-resolution image inpainting (512 ? 512). Since only the parameters of the MNPS model trained on ImageNet <ref type="bibr" target="#b48">[49]</ref> is available, we compare MNPS and AOT-GAN on ImageNet. For fair comparisons, the , PEN-Net <ref type="bibr" target="#b20">[21]</ref>, PConv <ref type="bibr" target="#b43">[44]</ref>, EdgeConnect <ref type="bibr" target="#b16">[17]</ref>, GatedConv <ref type="bibr" target="#b6">[7]</ref> and HiFill <ref type="bibr" target="#b7">[8]</ref> on Places2 <ref type="bibr" target="#b25">[26]</ref>. Each column shows the results and their enlarged patches marked by red boxes next to them. As shown in these cases, our model is able to reconstruct more plausible structures and clearer textures of various scenes, including valley, street, field wild and alcove. All the images are center-cropped and resized to 512 ? 512. See analysis in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">[Best viewed with zooming-in]</head><p>AOT-GAN is trained on ImageNet by center square inpainting masks, following the same training setting used by MNPS. Images are resized to 512 ? 512 for training. It takes MNPS a long time to fill a missing region in a 512 ? 512 image even on GPUs, due to the hundreds of iterations for post-processing. Therefore, we randomly sample 2,000 images from the testing set of ImageNet for testing and we use a center 224 ? 224 square as inpainting mask, following a common setting <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>. The results in <ref type="table" target="#tab_7">Table 2</ref> show that our single-stage model is able to gain improvements in terms of all the four metrics compared with MNPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Comparisons</head><p>In addition to the quantitative evaluation, we conduct qualitative comparisons with the state-of-the-art deep inpainting models, i.e., CA <ref type="bibr" target="#b12">[13]</ref>, PEN-Net <ref type="bibr" target="#b20">[21]</ref>, PConv <ref type="bibr" target="#b43">[44]</ref>, EdgeConnect <ref type="bibr" target="#b16">[17]</ref>, Gated-Conv <ref type="bibr" target="#b6">[7]</ref> and HiFill <ref type="bibr" target="#b7">[8]</ref> on Places2 <ref type="bibr" target="#b25">[26]</ref>. Specifically, we show the results of various scenes, including valley, street, field wild and alcove, with their enlarged patches in <ref type="figure" target="#fig_6">Fig. 5</ref>. The results show that, most baselines may fail in completing complex scenes with extremely large missing regions. Specifically, CA <ref type="bibr" target="#b12">[13]</ref> , PEN-Net <ref type="bibr" target="#b20">[21]</ref> and PConv <ref type="bibr" target="#b43">[44]</ref> tend to generate blurry textures. EdgeConnect  <ref type="bibr" target="#b43">[44]</ref>, GatedConv <ref type="bibr" target="#b6">[7]</ref>, and real data on CELEBA-HQ <ref type="bibr" target="#b49">[50]</ref> and Places2 <ref type="bibr" target="#b25">[26]</ref>. "Percentage" means the percentage of cases where our result is better than the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Percentage</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CELEBA-HQ Places2</head><p>AOT-GAN &gt; PConv <ref type="bibr" target="#b43">[44]</ref> 95.65% 97.67% AOT-GAN &gt; GatedConv <ref type="bibr" target="#b6">[7]</ref> 86.62% 95.08% AOT-GAN &gt; Real 22.47% 17.39% [17] fills in the holes with checkerboard artifacts. GatedConv <ref type="bibr" target="#b6">[7]</ref> and HiFill <ref type="bibr" target="#b7">[8]</ref> distort the structures in results by unreasonable repeating patterns. With the enhanced generator and discriminator networks, our model is able to reconstruct more plausible contextual structures and generate clearer textures in various kinds of complex scenes. For example, in the results of street in <ref type="figure" target="#fig_6">Fig. 5</ref>, AOT-GAN is able to reconstruct the structure of the window. In the results of valley, AOT-GAN is able to hallucinate plausible structures and synthesize fine-grained textures of valley even with the extremely large missing regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">User Study</head><p>We can find in the quantitative comparison in <ref type="table" target="#tab_5">Table 1</ref> that, PConv <ref type="bibr" target="#b43">[44]</ref> wins second place in terms of L 1 error, PSNR and SSIM. Besides, GatedConv <ref type="bibr" target="#b6">[7]</ref> wins second place in terms of FID. Therefore, we conduct a user study by pair-wise comparisons on real data, our model and the results of these top two best baselines (i.e., PConv <ref type="bibr" target="#b43">[44]</ref> and GatedConv <ref type="bibr" target="#b6">[7]</ref>). In each trial, two images are shown to the volunteers in an anonymous way. Specifically, one inpainted image is sampled from our model and the other is from other corresponding results (i.e., real data, or PConv <ref type="bibr" target="#b43">[44]</ref>, or GatedConv <ref type="bibr" target="#b6">[7]</ref>). We randomly sample 400 cases for the experiments on Places2 and CELEBA-HQ, respectively. Participants are not informed of the locations of missing regions and they are asked to select images with better structures and more realistic textures. More than 30 participants are invited in the user study and we have collected 951 valid votes.</p><p>The results of the user study can be found in <ref type="table" target="#tab_8">Table 3</ref>. We can see that the AOT-GAN outperforms other methods in most cases. Compared with real data, the results of AOT-GAN can surprisingly win 22.47% cases on CELEBA-HQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Ablation Studies</head><p>In this section, we conduct three sets of ablation studies to verify the effectiveness of three components of AOT-GAN, i.e., aggregated contextual transformations, the gated residual connections and the SM-PatchGAN discriminator. As shown in <ref type="table">Table.</ref> 5, each set of ablation studies are conducted based on previous component. Specifically, we conduct both quantitative and qualitative comparisons on CELEBA-HQ with 40-50% mask-to-image ratio inpainting masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">Aggregated Contextual Transformations</head><p>To verify the effectiveness of the aggregated contextual transformations in AOT blocks, we use a model of the same network depth as AOT-GAN as a baseline. In particularly, we remove the components of gated residual connections and SM-PatchGAN. The baseline uses standard residual connections in its building blocks and it is trained by a standard PatchGAN, so that it can be improved only by the aggregated contextual transformations. We conduct extensive comparisons for the design of aggregated contextual transformations by using different numbers of branches and dilation rates based on the baseline.</p><p>The quantitative comparison in <ref type="table" target="#tab_9">Table 4</ref> shows that, compared with a single-branch module, the enhanced models with aggregated contextual transformations gain significant improvements in terms of L 1 error, PSNR, SSIM, and FID. We can also observe that a larger number of branches with more diverse dilation rates is able to achieve larger improvements. We speculate that this is because more branches with more diverse dilation rates can largely enrich the patterns captured by AOT blocks, which is important to high-resolution image inpainting, especially in filling large missing regions.</p><p>According to the quantitative comparisons in <ref type="table" target="#tab_9">Table 4</ref>, we use four branches and the dilation rate of each branch is 1, 2, 4, 8, respectively in our final model. We compare this setting and the single-branch module with rate = 2 (i.e., the residual block used in EdgeConnect <ref type="bibr" target="#b16">[17]</ref>) by qualitative comparisons in <ref type="figure" target="#fig_7">Fig. 6</ref>. We can see that, our model is able to complete the large missing region of the human face with better structures, as well as clearer textures of hairs. We can draw a conclusion that, the design of aggregated contextual transformations is able to enhance context reasoning and result in better structures and textures for highresolution image inpainting.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2">Gated Residual Connections</head><p>The gated residual connections are used to aggregate the input features and the learned residual features in AOT blocks. To verify the effectiveness of the gated residual connection, we compare it with several counterparts for aggregating residual features, i.e., identical residual connections <ref type="bibr" target="#b14">[15]</ref>, GatedConvolution <ref type="bibr" target="#b6">[7]</ref>, and feature concatenation with a 1 ? 1 convolution in AOT blocks. Specifically, GatedConvolution generalizes partial convolution <ref type="bibr" target="#b43">[44]</ref> by providing a learnable dynamic feature selection for each channel at each spatial location. We compare these methods by replacing the gated residual connections in AOT blocks. The quantitative comparison in <ref type="table" target="#tab_11">Table 6</ref> shows that our gated residual connections outperforms others in terms of all the metrics. The visual comparison in <ref type="figure" target="#fig_8">Fig. 7</ref> shows that, leveraging the gated residual connections, our model is able to generate more realistic facial textures than other residual fusion methods. This is because the proposed gated residual connection provides a learnable spatially-variant connection between the input features and the learned residual features. Such a gated connection is able to update  We compare different discriminators in AOT-GAN, i.e., Patch-GAN <ref type="bibr" target="#b23">[24]</ref>, HM-PatchGAN and SM-PatchGAN. Specifically, the generator is constructed by the proposed full AOT blocks. The quantitative comparison in <ref type="table" target="#tab_12">Table 7</ref> shows that both the proposed HM-PatchGAN and SM-PatchGAN outperform PathGAN. Specifically, SM-PatchGAN performs the best especially in terms of FID. Besides, the results of qualitative comparisons in <ref type="figure" target="#fig_9">Fig.  8</ref> show that, compared with PatchGAN and HM-PatchGAN, the SM-PatchGAN is able to synthesize clearer textures of the eyes for the large missing region of the boy's face. We highlight that, the SM-PatchGAN is optimized to distinguish detailed appearances of synthesized patches of missing regions and those of contexts. Such an optimization forces the discriminator to capture realistic textures and in turn facilitates the generator to synthesize clearer textures for high-resolution image inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked</head><p>Image Our Results <ref type="figure">Fig. 9</ref>: Visual results of the proposed AOT-GAN in the application of logo removal. The AOT-GAN is able to remove logos from images in an undetectable way and fill in missing regions with plausible contents. All these images are from QMUL-OpenLogo <ref type="bibr" target="#b50">[51]</ref> and are resized to 832 ? 512. We highlight the inpainting masks covering logos with blue bounding boxes. [Best viewed with zoom-in]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Real Applications</head><p>In this section, we apply AOT-GAN to three practical applications (i.e., logo removal, face editing and object removal), to verify the effectiveness of AOT-GAN in the real-world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1">Logo Removal</head><p>Automatic logo removal technique is very useful in logo design, media content creation, and so on <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. It requires removing logos from images and filling in the missing regions seamlessly. However, since the scales of logos vary largely and the backgrounds are typically complex, it remains challenging for existing inpainting approaches to do well in logo removal. To evaluate the performance of AOT-GAN on logo removal, we use the images of QMUL-OpenLogo <ref type="bibr" target="#b50">[51]</ref> for both training and testing. For training, images are center-cropped and resized to 512 ? 512 and we use rectangles of arbitrary sizes as training masks. For testing, images are resized to keep the shortest side as 512 and we use the bounding boxes of logos as inpainting masks. We show the results of AOT-GAN on QMUL-OpenLogo in <ref type="figure">Fig. 9</ref>. We can find that, AOT-GAN is able to remove logos from images in an undetectable way and fill in missing regions with plausible contents. We attribute the promising results to the enhanced context reasoning and texture synthesis by AOT-GAN. Specifically, both rich patterns of interest and informative distant contexts can be captured by learning aggregated contextual transformations in the generator of AOT-GAN, and the synthesis of clear textures are ensured by the use of SM-PatchGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2">Face Editing</head><p>The application of face editing is more and more popular in phones. Users usually use face editing to remove wrinkles, scars, or beards to make their face photo more attractive. To evaluate the performance of AOT-GAN on face editing, we use the images of CELEBA-HQ <ref type="bibr" target="#b49">[50]</ref> and all images are resized to 512?512 for both training and testing. We show the visual results of face editing by the proposed AOT-GAN in <ref type="figure">Fig. 10</ref>. Results show that, our model is able to complete coherent structures of faces or mouths and generate clear facial textures. For example, in the third case in <ref type="figure">Fig. 10</ref>, AOT-GAN is able to reconstruct the missing structures of the face and the mouth of the woman face, while synthesizing fine-grained textures of the teeth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.3">Object Removal</head><p>Object removal is widely used in many image editing applications. It aims at removing unwanted objects from images and fill in missing regions with plausible backgrounds. To evaluate the performance of AOT-GAN for object removal, we train the model on Places2 <ref type="bibr" target="#b25">[26]</ref> with irregular masks <ref type="bibr" target="#b43">[44]</ref>. For testing, we use the masks provided by users as inpainting masks. All the images are resized to 512 ? 512 for both training and testing. We show the results of object removal by the proposed AOT-GAN in <ref type="figure">Fig.  11</ref>. The visual results show that, AOT-GAN is able to remove unwanted objects in different kinds of complex scenes with the real masks provided by users. For example, in the third case in <ref type="figure">Fig. 11</ref>, AOT-GAN removes the man, reconnects the railings and synthesizes clear textures in the wall. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Limitations</head><p>We have shown that, AOT-GAN has achieved state-of-the-art performance with the enhanced generator and discriminator networks.</p><p>In this section, we discuss two limitations of AOT-GAN as below.</p><p>Customized AOT block. In practice, the number of branches and dilation rates in AOT block are empirically studied and set. When the image size changes, the optimal setting may need to search again. We plan to investigate adaptive mechanisms to improve AOT block in the future <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>Mask selection. In this paper, we use user-specified foreground masks (e.g., <ref type="figure">Fig. 11</ref>) or masks generated by models (e.g., <ref type="figure">Fig.  9</ref>) as our inpainting masks. However, interactive or automatic object segmentation remains a challenging problem and an active research field <ref type="bibr" target="#b59">[60]</ref>. As shown in the failure case in <ref type="figure" target="#fig_1">Fig. 12</ref>, when the inpainting mask can not cover the logo well and it leaves some boundaries outside the mask, it can be hard for AOT-GAN to remove the boundaries. Besides, it tends to propagate the boundaries and results in noticeable artifacts. Therefore, multiple interactive operations and refinements are required to include all affected pixels in the mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we propose to learn aggregated contextual transformations and an enhanced discriminator for high-resolution image inpainting. In comparison to previous approaches, the aggregated contextual transformations show impressive improvements by allowing to capture both informative distant contexts and rich patterns of interest for extremely large missing regions. Besides, the discriminator is improved by a novel mask prediction task, which in turn facilitates the generator to synthesize more realistic textures. We conduct extensive evaluations including a user study to show that AOT-GAN outperforms the state-of-the-art approaches by a significant margin. Besides, we conduct ablation studies to analyze each component of AOT-GAN. We also show the results of AOT-GAN on three practical applications to verify the effectiveness of AOT-GAN in the real-world. As our future work, we plan to extend AOT-GAN to other low-level vision tasks, such as single image super-resolution, image denoising, and image-to-image translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The overview of the proposed Aggregated COntextual-Transformation GAN (AOT-GAN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>An illustration of the Residual block (a) used in the stateof-the-art deep inpainting models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>An illustration of different tasks for the training of the discriminator. PatchGAN aims at distinguishing patches of inpainted images from those of real images, while HM-PatchGAN and SM-PatchGAN aim to segment synthesized patches of missing regions from real ones of contexts according to inpainting masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>? QMUL-OpenLogo [51] contains 27,083 images from 352 logo classes. Each image is annotated by fine-grained bounding box annotations of logos. We use 15,975 training images for training and 2,777 validation images for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative comparisons of AOT-GAN with CA<ref type="bibr" target="#b12">[13]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Visual comparisons on images (512 ? 512) of CELEBA-HQ [50] to verify the effectiveness of the aggregated contextual transformations in AOT block. [Best viewed with zoom-in]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Visual comparisons on images (512 ? 512) of CELEBA-HQ [50] to verify the effectiveness of the gated residual connection in AOT block. [Best viewed with zoom-in]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Visual comparisons on images (512 ? 512) of CELEBA-HQ [50] to verify the effectiveness of the SM-PatchGAN discriminator of AOT-GAN. [Best viewed with zoom-in]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :Fig. 11 :Fig. 12 :</head><label>101112</label><figDesc>Visual results of AOT-GAN in the application of face editing. The proposed AOT-GAN is able to generate plausible facial structures and realistic textures for high-resolution images of faces (i.e., 512 ? 512). [Best viewed with zoom-in] Visual results of the proposed AOT-GAN in the application of object removal. The proposed AOT-GAN shows promising results in filling irregular holes that provided by users in highresolution images (i.e., 512 ? 512). [Best viewed with zoom-in] A failure case. When the inpainting mask can not cover the logo well and it leaves some boundaries outside the mask, AOT-GAN tends to propagate the boundaries and results in noticeable artifacts. [Best viewed with zoom-in]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and H. Chao are with the School of Computer, Sun Yatsen University, and also with Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China. Email: zengyh7@mail2.sysu.edu.cn, isschhy@mail.sysu.edu.cn ? J. Fu and B. Guo are with Microsoft Research.</figDesc><table /><note>Email: jianf@microsoft.com, bainguo@microsoft.com</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Net<ref type="bibr" target="#b20">[21]</ref> is built upon a U-Net<ref type="bibr" target="#b51">[52]</ref> backbone. It proposes to fill missing regions from deep to shallow by a cross-layer non-local module. PConv<ref type="bibr" target="#b43">[44]</ref> adopts a proposed partial convolution layer instead of the standard convolution to deal with the issue of color discrepancy inside and outside holes. EdgeConnect [17] model consists of two stages. The first stage completes the edges of corrupted images, and the second stage completes the color images with the guidance of the completed edges from the first stage. GatedConv<ref type="bibr" target="#b6">[7]</ref> is a two-stage model that incorporates a gated convolution and SN-PatchGAN for image inpainting.</figDesc><table /><note>] is a two-stage coarse-to-fine model. It deploys a patch-based non-local module, i.e., contextual attention module, to effectively model long-range correlations.? PEN-???? HiFill [8] proposes a light-weight model with an efficient Contextual Residual Aggregation mechanism that enables ultra super-resolution image inpainting.? MNPS [19] is a multi-scale neural patch synthesis approach based on the joint optimization of image content and texture constraints. It shows sharper and more coherent results than previous works for high-resolution images.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 1 :</head><label>1</label><figDesc>Quantitative comparisons of the proposed AOT-GAN with state-of-the-art deep inpainting models, i.e., CA</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 2 :</head><label>2</label><figDesc>Quantitative comparison results with MNPS [19] onImageNet<ref type="bibr" target="#b48">[49]</ref>. ? Lower is better. ? Higher is better.</figDesc><table><row><cell></cell><cell>L 1 (10 ?2 ) ?</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>FID?</cell></row><row><cell>MNPS[19]</cell><cell>3.37</cell><cell>21.36</cell><cell>0.821</cell><cell>43.87</cell></row><row><cell>Ours</cell><cell>3.25</cell><cell>22.10</cell><cell>0.840</cell><cell>41.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 3 :</head><label>3</label><figDesc>User study for pair-wise comparisons with PConv</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 4 :</head><label>4</label><figDesc>Quantitative comparisons on CELEBA-HQ<ref type="bibr" target="#b49">[50]</ref> for using different numbers of branches and dilation rates in AOT blocks. ? Lower is better. ?Higher is better.</figDesc><table><row><cell>#branch</cell><cell>rates</cell><cell>L 1 (10 ?2 ) ?</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>FID?</cell></row><row><cell>1</cell><cell>1</cell><cell>3.65</cell><cell>23.16</cell><cell>0.807</cell><cell>14.21</cell></row><row><cell>1</cell><cell>2</cell><cell>3.59</cell><cell>23.29</cell><cell>0.810</cell><cell>12.28</cell></row><row><cell>2</cell><cell>1,2</cell><cell>3.58</cell><cell>23.25</cell><cell>0.812</cell><cell>11.54</cell></row><row><cell>2</cell><cell>2,8</cell><cell>3.36</cell><cell>23.83</cell><cell>0.830</cell><cell>10.56</cell></row><row><cell>3</cell><cell>1,2,8</cell><cell>3.36</cell><cell>23.91</cell><cell>0.831</cell><cell>10.06</cell></row><row><cell>4</cell><cell>1,2,4,8</cell><cell>3.28</cell><cell>24.06</cell><cell>0.834</cell><cell>9.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 :</head><label>5</label><figDesc>Ablation studies for each component of the proposed AOT-GAN. The full model (AOT-GAN) that exploits aggregated contextual transformations, gated residual connections and SM-PatchGAN performs the best. ? Lower is better. ?Higher is better.</figDesc><table><row><cell>Model</cell><cell>AOT block</cell><cell>Gated residual connections</cell><cell>SM-PatchGAN</cell><cell>L 1 (10 ?2 ) ?</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>FID?</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>3.28</cell><cell>24.06</cell><cell>0.834</cell><cell>9.86</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>3.06</cell><cell>24.47</cell><cell>0.849</cell><cell>8.02</cell></row><row><cell>3 (AOT-GAN)</cell><cell></cell><cell></cell><cell></cell><cell>2.98</cell><cell>24.65</cell><cell>0.852</cell><cell>7.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 6 :</head><label>6</label><figDesc>Quantitative comparisons on CELEBA-HQ<ref type="bibr" target="#b49">[50]</ref> to verify the effectiveness of the proposed gated residual connection in AOT blocks. ? Lower is better. ? Higher is better.</figDesc><table><row><cell></cell><cell>L 1 (10 ?2 ) ?</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>FID?</cell></row><row><cell>Identical ResBlock</cell><cell>3.28</cell><cell>24.06</cell><cell>0.834</cell><cell>9.86</cell></row><row><cell>GatedConv. [7]</cell><cell>3.28</cell><cell>23.98</cell><cell>0.835</cell><cell>10.22</cell></row><row><cell>1 ? 1 conv.</cell><cell>3.14</cell><cell>24.46</cell><cell>0.846</cell><cell>9.06</cell></row><row><cell>Ours</cell><cell>3.06</cell><cell>24.47</cell><cell>0.849</cell><cell>8.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 7 :</head><label>7</label><figDesc>Quantitative comparisons on CELEBA-HQ<ref type="bibr" target="#b49">[50]</ref> to verify the effectiveness of the SM-PatchGAN discriminator of AOT-GAN. ? Lower is better. ? Higher is better.</figDesc><table><row><cell></cell><cell>L 1 (10 ?2 ) ?</cell><cell>PSNR?</cell><cell>SSIM?</cell><cell>FID?</cell></row><row><cell>PatchGAN [24]</cell><cell>3.06</cell><cell>24.47</cell><cell>0.849</cell><cell>8.02</cell></row><row><cell>HM-PatchGAN</cell><cell>2.99</cell><cell>24.63</cell><cell>0.853</cell><cell>7.58</cell></row><row><cell>SM-PatchGAN</cell><cell>2.98</cell><cell>24.65</cell><cell>0.852</cell><cell>7.37</cell></row><row><cell cols="5">features inside missing regions, while retaining low-level details</cell></row><row><cell cols="2">of valid pixels outside holes.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4.7.3 Soft Mask-guided PatchGAN (SM-PatchGAN)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0.3 0.4 1 1 0.6 0.7 1 1 1 1 1 1 1 1 1 1 0.3 0.4 1 1 0.6 0.7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is partially supported by NSF of China under Grant 61672548, U1611461.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Space-time completion of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diminished reality based on image inpainting considering background geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kawai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1236" to="1247" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High-quality real-time video inpaintingwith pixmix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Broll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="866" to="879" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contextual residual aggregation for ultra high-resolution image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<meeting><address><addrLine>1, 2, 6, 7, 8, 9</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="7508" to="7517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous structure and texture image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="882" to="889" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inpainting of wide-baseline multiple viewpoint video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2417" to="2428" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Patchbased image inpainting via two-stage low rank approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2023" to="2036" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image completion with structure propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in TOG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>1, 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Edgeconnect: Generative image inpainting with adversarial edge learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="528" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6721" to="6729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image inpainting via generative multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning pyramidcontext encoder network for high-quality image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coherent semantic attention for image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in WACV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Foreground-aware image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A variational model for filling-in gray level and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verdera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Navier-stokes, fluid dynamics, and image and video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="355" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Realtime texture synthesis by patch-based sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="150" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Texture synthesis by nonparametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generative face completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3911" to="3919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5485" to="5493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shift-net: Image inpainting via deep feature rearrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image inpainting with learnable bidirectional attention maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Contextual-based image inpainting: Infer, match, and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pepsi: Fast image inpainting with parallel decoding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sagong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="360" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Smoothed dilated convolutions for improved dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in KDD</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>3, 4, 6, 7, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structureflow: Image inpainting via structure-aware appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Open logo detection challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>AC-SSC</publisher>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="6626" to="6637" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="586" to="595" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Automatic video logo detection and removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Visible watermark removal scheme based on reversible data hiding and image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process. Image Commun</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporally coherent completion of dynamic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
