<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Target Transformed Regression for Accurate Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Target Transformed Regression for Accurate Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate tracking is still a challenging task due to appearance variations, pose and view changes, and geometric deformations of target in videos. Recent anchor-free trackers provide an efficient regression mechanism but fail to produce precise bounding box estimation. To address these issues, this paper repurposes a Transformer-alike regression branch, termed as Target Transformed Regression (TREG), for accurate anchor-free tracking. The core to our TREG is to model pair-wise relation between elements in target template and search region, and use the resulted target enhanced visual representation for accurate bounding box regression. This target contextualized representation is able to enhance the target relevant information to help precisely locate the box boundaries, and deal with the object deformation to some extent due to its local and dense matching mechanism. In addition, we devise a simple online template update mechanism to select reliable templates, increasing the robustness for appearance variations and geometric deformations of target in time. Experimental results on visual tracking benchmarks including VOT2018, VOT2019, OTB100, GOT10k, NFS, UAV123, LaSOT and TrackingNet demonstrate that TREG obtains the state-of-the-art performance, achieving a success rate of 0.640 on LaSOT, while running at around 30 FPS. The code and models will be made available at https://github.com/MCG-NJU/TREG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual object tracking <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref> is an important yet challenging task in computer vision with a wide range of applications, such as robotics, surveillance <ref type="bibr" target="#b37">[38]</ref>, and humancomputer interaction <ref type="bibr" target="#b25">[26]</ref>. It aims to estimate the state of an arbitrary object in video frames, given the target bounding box in an initial frame. Although much progress <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref> had been made in recent years, accurate tracking still remains challenging, due to the fact that the target might be with deformation, pose and viewpoint changes, and even occluded by other objects. In general, tracking a single object can be decomposed into sub-tasks of classification <ref type="bibr">Figure 1</ref>. A comparison of our approach with state-of-the-art tracker. Observed from the visualization results, our TREG produces more precise bounding boxes than the state-of-the-art trackers PrDiMP <ref type="bibr" target="#b10">[11]</ref> and other anchor-free trackers, such as Ocean <ref type="bibr" target="#b41">[42]</ref>, when encountering circumstances of deformation, scale changes and fast movement. and regression, which is to localize the target roughly and regress the precise bounding box, respectively.</p><p>In order to build an accurate tracker, regression branch design is of great importance as it is responsible for generating the precise bounding box. The previous works on box estimation can be roughly grouped into two types: <ref type="bibr" target="#b0">(1)</ref> indirect bounding box estimation and (2) direct bounding box regression. For the first type, early approaches <ref type="bibr" target="#b0">[1]</ref> simply use a multi-scale searching strategy based on the classification branch. Then, ATOM <ref type="bibr" target="#b8">[9]</ref> method presents a specialized IoU prediction network to select and refines the final object box. For the second type, the seminal Siamese trackers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23]</ref> resort to anchor-based mechanism to direct regress the bounding size based on the predefined anchors. More recently, some anchor-free trackers <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref> are more popular owning to its simplicity in design and superior performance, by directly regressing the box size. However, unlike image object detection, these anchor-free trackers are not sufficiently accurate and robust for object tracking, due to the essential illness of tracking problem that we expect a tracker to be trained with a one-shot supervision yet to generalize well to unseen deformations and variations.</p><p>In this paper, our objective is to design a more accurate anchor-free tracker by proposing a customized regression branch to effectively handle object deformation, pose and view changes. Based on the above analysis, we argue that the core problem of accurate anchor-free tracking is how to integrate the target information to regression branch to retain its precise boundary information and handle its variations in time. The existing anchor-free trackers <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref> simply utilize a depth-wise correlation representation to fuse target information or a target guided attention module to modulate the visual representations. Although these techniques provide feasible solutions for target information integration, they may contain insufficient target information for precise regression, and lack flexibility to deal with object variations. Accordingly, we figure out two important factors that need to be considered specifically. First, in order to generate precise object boundaries, we need to keep sufficient target information in regression branch. Second, for handling appearance variations and object deformation, regression branch is expected to be flexible with deformation and adaptive over time.</p><p>Following the above analysis, we devise an anchor-free and target-guided regression branch with a transformeralike design, termed as Target Transformed Regression (TREG), partially inspired by the success of Transformer <ref type="bibr" target="#b31">[32]</ref> in context modeling. Basically, we utilize the cross-attention mechanism in transformer to explicitly model all pairwise interactions between elements of target template and search areas, making these enhanced representations particularly suitable for precise boundary offset regression. Specifically, the feature cells in target template are encoded as key and value at first. Then, for each position in search area, we enhance its visual representation by querying its feature over all the pairs of key-val in the target template. This retrieved target-aware representation is able to enhance the target-relevant information and deal with object deformation to some extent, thanks to the local and dense matching between all elements of target template and search regions. To further improve the effectiveness of our TREG, we establish an online target template queue to maintain the variations of the tracked object, and design a confidence based update strategy to adaptively select reliable target templates. Finally, we place a feed-forward network on top of the target transformed representation to perform object boundary offset regression.</p><p>Combined with the existing online classification branch from DiMP <ref type="bibr" target="#b1">[2]</ref>, we develop a principled anchor-free tracking framework. We perform comprehensive experiments on eight benchmarks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14]</ref> to demonstrate the superior performance of our TREG to previous state-of-the-art methods. The contributions of this work are three-fold:</p><p>? We propose an accurate anchor-free tracker by specifically devising a target transformed regression branch (TREG). The advantage of modeling pair-wise relation between elements in target template and search area enables our TREG to keep precise boundary information and effectively deal with objects variations.</p><p>? We present a simple online target update mechanism by establishing a confidence based template queue, which enables the tracker to be flexible to deal with appearance variations and geometric deformations of object over time.</p><p>? Our TREG outperforms the popular state-of-theart real-time trackers on eight benchmark datasets including VOT2018 <ref type="bibr" target="#b21">[22]</ref>, VOT2019 <ref type="bibr" target="#b20">[21]</ref>, La-SOT <ref type="bibr" target="#b12">[13]</ref>, TrackingNet <ref type="bibr" target="#b29">[30]</ref>, OTB <ref type="bibr" target="#b36">[37]</ref>, GOT10k <ref type="bibr" target="#b18">[19]</ref>, UAV123 <ref type="bibr" target="#b28">[29]</ref> and NFS <ref type="bibr" target="#b13">[14]</ref>, especially achieving a success rate of 0.640 on LaSOT, while running at a real-time speed of around 30 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly introduce recent trackers from the aspect of target regression. Besides, we discuss the transformer mechanism used in tracking.</p><p>Target regression for tracking Target regression is employed in estimating the precise target state. Previous works can be roughly grouped into two types, indirect target estimation and direct bounding box regression. For the former one, some CF-based trackers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref> and SiamFC <ref type="bibr" target="#b0">[1]</ref> employed brutal multi-scale test to estimate the target scale roughly. ATOM <ref type="bibr" target="#b8">[9]</ref> and DiMP [2] employ a specialized IoU prediction network to select and refine the final object box. For the latter one, RPN-based trackers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b34">35]</ref> regress the location shift and size difference between pre-defined anchor boxes and target location. SATIN <ref type="bibr" target="#b14">[15]</ref> and CGACD <ref type="bibr" target="#b11">[12]</ref> detect corners using cross-correlation operation and correlation-guided attention operation respectively. SiamFC++ <ref type="bibr" target="#b38">[39]</ref> directly regresses the offset to box corners. Anchor-free trackers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6]</ref> are more and more popular owning to its simplicity in design and superior performance. However, to use target information, the existing anchor-free trackers employ a targetguided attention or a depth-wise correlation to modulate the search frame representation, which may contain insufficient target information for precise regression, and lack flexibility to deal with object variations. We devise a target transformed regression branch to tackle these issues and acquire superior performance.  <ref type="figure">Figure 2</ref>. TREG Framework. We present an end-to-end anchor-free tracker, termed as TREG, for accurate tracking, which is composed of a backbone to extract common features, classification feature extractors and a regression feature extractor to extract task-specific features, multi-scale classification module and target-aware transformer based regression module to localize the target center and estimate the precise target bounding box respectively. Similarly to FCOT <ref type="bibr" target="#b5">[6]</ref>, We employ a multi-scale online classification component, where the discriminative model generator is proposed in DiMP <ref type="bibr" target="#b1">[2]</ref>, to localize an accurate target center. For regression, a simple online updating target-aware transformer is proposed to yield a robust and precise regressor. Detailed structure of the online target-aware transformer can be found at <ref type="figure">Figure.</ref> 3.</p><p>Transformer mechanism in tracking Generalized transformer mechanism <ref type="bibr" target="#b31">[32]</ref> is a group of neural network layers that aggregates information from the entire input sequence. It introduces attention layers, which scan through each element of a sequence and update it by aggregating information from the whole sequence. In the area of visual tracking, CSR-DCF <ref type="bibr" target="#b26">[27]</ref> constructs an object spatial attention map to constrain correlation filter learning and calculates the channel reliability values of weighted sum correlation response maps. Then RASNet <ref type="bibr" target="#b33">[34]</ref> introduces spatial and channelwise attention to a Siamese network. CGCAD <ref type="bibr" target="#b11">[12]</ref> further proposes a correlation-guided attention for corner detection. However, the pixel-wise correlation-guided spatial attention overlooks the fact that there are some background parts in the target, which may result in high attention weights outside the object region. Consequently, they adopt a complex two-stage structure for estimating one RoI and then detecting the corners, so as to alleviate the issue. Compared with them, our target-aware transformer retains sufficient target information to enhance the regression representation and can be easily deployed for online regression thanks to its simple structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We develop a target transformed regression branch with online updating mechanism for accurate anchor-free tracking. The online target transformed regression component is basically devised complying with the following guidelines: (i) a fully target integration module to yield high-quality visual representation to retain sufficient target information for precise object boundary generation, (ii) pixel-wise context modeling to enhance the target-relevant features and cope with object deformation, and (iii) an efficient online mechanism so as to handle appearance variations in a consecutive sequence.</p><p>We take inspirations from the transformer in context modeling and its variants <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref> in computer vision, and devise a transformer-alike structure for anchor-free regression. Given a query element and a set of key elements, a transformer function adaptively aggregates the key contents to transform the query feature based on attentive weights that measure the compatibility of query-key pairs. Following this technical perspective, a target-aware transformer is designed for incorporating target appearance into our regression features. The feature cells of target template extracted by a backbone and an ROI pooling layer is encoded as key and value elements. Then for each position in search region, we enhance its visual representation by querying its feature over all the pairs of key-value. Then the retrieved representations are fused to original feature so that we can obtain rich target-aware information for regression. Through the local and dense matching between all elements of target template and search regions, we can not only preserve sufficient target information to produce high-quality representation for regression, but also be able to handle with object deformation to some extent. Additionally, we establish an online target template queue to maintain the variations of the tracked object, composed of three static targets augmented by the given template and reliable online targets adaptively selected based on the classification confidence.  (a) Target-aware transformer takes search region feature and pooled targets feature as input, producing transformed regression feature. Two 3D Conv with kernel size of 1 ? 1 ? 1 is performed to targets feature to generate key and value respectively. Similarly, a 2D Conv with kernel size of 1 ? 1 is performed to search region feature to yield query element. " " denotes matrix multiplication, and " " denotes element-wise sum. N orm. denotes scaling by 1/(t ? h ? w). (b) Online template update mechanism is implemented by maintaining a targets template queue with size of 7, which is composed of 3 static targets feature and 4 online updating ones. The static targets is acquired by performing an augmentation to the given target. The online targets memory is updated every n frames. An online samples bar is to reserve n targets feature with classification scores. Then the proper online target is selected by maximize the confidence scores from the samples bar.</p><p>Finally, a feed-forward network is placed on top of the target transformed representation to perform object boundary offset regression directly. By these designs, we construct a simple yet accurate target transformed regression branch.</p><p>Based on our proposed target transformed regression branch, we build an efficient and principled anchor-free tracker (TREG), by combining an online classification branch. The framework of the proposed tracker is shown in <ref type="figure">Figure.</ref> 2. The whole framework is composed of a common backbone to extract features for both classification and regression branches, a classification branch to localize the target center, and a regression branch to estimate precise target state. Similar to FCOT <ref type="bibr" target="#b5">[6]</ref>, we employ a multi-scale online classification component, where the discriminative model generator is proposed in DiMP <ref type="bibr" target="#b1">[2]</ref>, to estimate a robust target center location. Given the target center location and template in online targets queue, the target-aware transformer produces the enhanced visual features, which are used to regress offset to the object boundary directly. We describe the target transformed regression branch and the framework in details in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Target transformed regression</head><p>In general, anchor-free regression is solely based on the appearance representation to predict the geometric offsets of box boundary with respect to object center by using a feed-forward network. In order to bridge the gap between appearance information and geometric structure, we hope to incorporate more detailed structure information (e.g., object boundary) into our visual representation, thus relieving the difficulty of geometric structure prediction. As for the specific anchor-free object tracking, we aim to leverage more detailed target information to enhance the object relevant areas while suppress the distractor from background. However, the depth-wise correlation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b15">16]</ref> yields similarity map with inconspicuous boundary of the target facing with object deformation since the whole target is served as a correlation filter. Meanwhile, the pixel-wise correlationbased attention <ref type="bibr" target="#b11">[12]</ref> imposes other challenge due to retaining insufficient target information. The region outside the object may be assigned with a high attention weight when there exists background parts in target. To overcome these shortcomings, we choose to better exploit the all pair-wise correlation between the elements in target and search area, and aim to leverage this rich and high-order features to keep sufficient target-related information, and as well handle the object variation issues during tracking.</p><p>Target-aware transformer. As outlined in the above analysis, our target transformed regression is inspired and re-purposed from the Transformer architecture <ref type="bibr" target="#b31">[32]</ref>. Ba-sically, we use the search area as a query to enhance its representation with target information. Concretely, the target hidden representation is encoded as key and value elements in a pixel-wise way, providing weighted aggregation response for a query which is a position of search region feature. We define the target-aware feature transformation as:</p><formula xml:id="formula_0">y i = W ( 1 N k j?? k A(t j , x i ) ?(t j )) + x i . (1)</formula><p>Here, i indexes a position in search region with representation x i . j is the index that enumerates all possible positions in target template with feature representation t j . k indexes the template in our target queue and ? k specifies the feature cells in target template for query. The function ? computes a representation of the target signal at the position j which is served as value element. A pairwise function A(t j , x i ) depicts the relations between t j and x i . The implementation form of A(t j , x i ) is as follows:</p><formula xml:id="formula_1">A(t j , x i ) = ?(x i ) T ?(t j ),<label>(2)</label></formula><p>where ? function encodes x i as a query element and ? function encodes t j as a key element. Target-aware information aggregation is with a weighted sum. Then the weighted sum is scaled by 1/N to perform normalization. N is the total number of elements in our target queue, which is t ? h ? w and t is the number of template and h and w is template size. Particularly, the normalization factor 1/(t ? h ? w) can not be substituted with Softmax function as in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b31">32]</ref>. The reason lies in that some positions in background and distractors of the search region are expected to have low dependency with target, while Softmax function will amplify this noise influence as the sum of attention weights between the query and all the keys is always 1. Besides, W represents feature transform to make the queried feature to be the same shape with x i . y i is the target transformed representation with a simple average of original features and retrieved representation. As in <ref type="figure">Figure.</ref> 3, target-aware transformer takes search region feature and ROI-pooled target feature as input, where the spatial size of search region feature is 88 ? 88 and the target size is 5 ? 5 in this work. The functions ? and ? are implemented with a 3D convolution layer with kernel size of 1 ? 1 ? 1 respectively, since an online 3D targets memory is maintained as described in 3.2. Similarly, the functions ? and W are 2D convolution layers with kernel size of 1 ? 1. A target-aware transformer operation is a flexible building block and can be easily inserted to the current anchor-free trackers. There are only 4 convolution layers with kernel size of 1 to be trained offline in the block. Furthermore, we can keep a target queue of variable sizes, which can be easily deployed for online updating.</p><p>Discussion with other anchor-free regression. To better expound the insight of our target-aware transformer, we compare with the regression component of previous anchorfree trackers. In general, there are the two major methods for integrating target information to search region feature in anchor-free regression. First, most of them <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42</ref>] employ depth-wise correlation to produce similarity map based on the whole template. Since the correlation filter simply possesses global information of the target, it is difficult to reflect the object boundary precisely when encountering deformation or similar intricate circumstances. Secondly, CGACD <ref type="bibr" target="#b11">[12]</ref> brings in a complex correlation-guided attention for detecting the corner which imposes other limitations. Pixel-wise correlation is the primary operation to generate target-aware spatial attention. However, it overlooks the fact that there are some background parts in the target, which may result in high attention weights outside the object region. Consequently, they adopt a complex twostage structure for estimating one RoI expected to contain the target and then detecting the corners, so as to alleviate the issue.</p><p>In contrast, our target-aware transformer explicitly models all pair-wise interactions between elements of target template and search area, and uses the retrieved feature representation to enhance the original features. This unique design makes our target transformed representation to attain more detailed target-relevant information than previous methods, and well handle the appearance variation and deformation of object due to our local and dense matching mechanism. The exploration study demonstrates that our target transformed regression is able to yield more accurate tracking results, as discussed in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Online template update</head><p>In order to cope with the target variations in a consecutive sequence, we present an online template update mechanism for regression as visualized in <ref type="figure" target="#fig_0">Figure. 3(b)</ref>. An inherent issue that comes with the online scheme, is that the tracked objects may be imprecise. The tracker is thus confused and tends to drift. It is crucial for accurate regression to make a trade-off between using online targets and static targets. Therefore, an online targets template queue comprising 3 static targets and 4 online targets is maintained. The static targets are acquired by performing a data augmentation to the given template. The online targets memory is updated every n frames, where n is the updating interval. Since the tracked targets are unstable, we design a confidence based updating strategy to adaptively select reliable target templates. When the object is predicted with the maximum confidence among the online targets in the samples bar, its template will be added into target queue. Experimental results demonstrate the effectiveness of online template update mechanism in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation details</head><p>Framework. Following the architecture of <ref type="bibr" target="#b1">[2]</ref>, we employ a ResNet-50 <ref type="bibr" target="#b16">[17]</ref> as the backbone to extract common feature. Then an U P layer, composed of 2 convolutional layers and 2 up-sampling layers are used to generate highresolution feature. Then the Classification head and the regression head extract task-specific features to cope with classification and regression tasks separately. The classification head and the regression head are composed of a convolutional layer and 2 deformable convolutional layers <ref type="bibr" target="#b6">[7]</ref>.</p><p>We employ the multi-scale classification branch similar with <ref type="bibr" target="#b5">[6]</ref>, learning discriminative models proposed in <ref type="bibr" target="#b1">[2]</ref>. At first, Low-resolution score map and high-resolution score map are generated by separate online classifiers. Then these two maps are fused to predict a robust and precise target center. During training, the classification objective is a Gaussian function map centered at the ground-truth target center.</p><p>The regression branch is composed of the presented target-aware transformer and a feed-forward network containing 2 convolutional layers and a deformable convolutional layer to estimate the offset from a point to the target boundary. During the offline training process, we perform regression for positions in the vicinity of the target center, which is an area with a radius of 2 in this work.</p><p>Offline training. Similar with some popular trackers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref>, we use the training splits of LaSOT <ref type="bibr" target="#b12">[13]</ref>, Track-ingNet <ref type="bibr" target="#b29">[30]</ref>, GOT-10k <ref type="bibr" target="#b18">[19]</ref> and COCO <ref type="bibr" target="#b24">[25]</ref>. We train the model for 50 epochs by sampling 40000 videos per epoch, giving a total training time of 100 hours on 8 Nvidia Tesla V100 GPUs. The whole network is trained end-to-end with a mini-batch size of 80. We use ADAM <ref type="bibr" target="#b19">[20]</ref> with learning rate decay of 0.2 at the epoch of 25 and 35. The classification training loss settings are the same with DiMP <ref type="bibr" target="#b1">[2]</ref>. For regression, we use IoU loss and the loss weight is set to 1.</p><p>Online tracking. We perform data augmentation to the first frame with translation, rotation,and blurring, yielding a total of 15 initial online training samples for online classification. And we select 3 samples as the static targets for the online target template queue. During the tracking process, we employ the designed confidence based updating strategy to select reliable samples. We ensure a maximum queue size of 7 by discarding the oldest sample. Online classification is performed as in <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our tracking approach is implemented in Python based on PyTorch. For inference, we test our tracker on a single Nvidia RTX 2080Ti GPU, achieving a tracking speed of   around 30 FPS. The code for training and inference will be made public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Exploration study</head><p>Here, we describe the experiments to analyze the impact of the target transformed regression and the online template updating mechanism for the transfromer proposed in this work. The experiments are performed on the combined UAV123 <ref type="bibr" target="#b28">[29]</ref> and NFS <ref type="bibr" target="#b13">[14]</ref> datasets, as in ATOM <ref type="bibr" target="#b8">[9]</ref>. There are a total of 233 challenging videos. We report two metrics, area-under-the-curve(AUC) score and precision score (Precision). For fair comparison, all the following experiments are performed under identical training setting.</p><p>Target-aware transformer for regression. Our primary contribution is the proposed target transformed regression branch. To evaluate its effectiveness, we take comparison with the following analogous methods for anchor-free tracking. For a fair comparison, We only substitute the target-aware transformer with the following modules, while the other components including classification branch and feed-forward network are the same. And our TREG only utilize the given target as template without the online samples. Baseline: we compare with a baseline approach that removes the target-aware transformer. As shown in Table. 1, the performance drops by 3.5% in terms of Precision and 4.4% in terms of AUC, which indicates that incorporating the target information into regression representation is of vital importance. DW-Corr: then we take comparison with depth-wise correlation based regression method as being commonly used in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39]</ref>. The performance has a obvious drop compared with ours, since it can not handle with target deformation for accurate regression. PCorr-Att: we also compare with the pixelwise correlation-guided spatial attention module proposed in <ref type="bibr" target="#b11">[12]</ref>, since it has high relevance with the target-aware transformer. It generates inferior performance than ours, which demonstrates that retaining sufficient target informa-  <ref type="figure">Figure 5</ref>. Visualization of classification feature when applying our target-aware transformer to classification branch. Red box denotes the ground-truth bounding box and the green one denotes the tracked negative object.</p><p>Online Selected Samples  tion for regression representation is essential for eliminating the interference of the background or distractors. In <ref type="figure">Figure</ref>. 4, we further provide intuitive visualization samples for regression feature to illustrate that the proposed method represents the boundary of targets better than other methods. It can be seen that the boundary of the diver including head and foot are get augmented through our target-aware transformer although the object changes in the sequence.</p><p>Target-aware transformer for classification. We further analyze the appliance of target-aware transformer for classification branch, only substituting the high-resolution classification module with the proposed transformer-alike structure. We can see from <ref type="table">Table.</ref> 1 that the performance of TAT-Cls is not so good as ours TREG. Specifically, the pixelto-pixel matching method is not suitable for discriminating similar objects since it tends to overlook the overall information of targets, which can be derived from <ref type="figure">Figure. 5</ref>. As a consequence, target-aware transformer is not employed for classification in our design.</p><p>Online template update. We investigate the impact of the online transformer mechanism for regression. The results of the investigation are shown in <ref type="table">Table.</ref> 2. In our design, the online targets queue is composed of a static targets  memory and an online updating targets memory. We can derive that the size of the static targets memory is of slight influence. Compared with only using a static target, maintaining an online targets template queue improves the AUC by 0.9% and the Precision by 1.2%. It demonstrates the effectiveness of the online regression mechanism. Furthermore, we observe that the performance drops a little if without the confidence based updating strategy since the online targets may be unreliable, which proves the effectiveness of the scheme. Furthermore, we visualize the online targets queue in <ref type="figure">Figure.</ref> 6. It can be derived that high confidence scores generally correspond to high-quality samples, which proves that the online samples selection strategy is useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the state-of-the-art</head><p>We test the proposed TREG on eight tracking benchmarks, including VOT2018 <ref type="bibr" target="#b21">[22]</ref>, VOT2019 <ref type="bibr" target="#b20">[21]</ref> La-SOT <ref type="bibr" target="#b12">[13]</ref>, TrackingNet <ref type="bibr" target="#b29">[30]</ref>,UAV123 <ref type="bibr" target="#b28">[29]</ref>, GOT10k <ref type="bibr" target="#b18">[19]</ref>, OTB100 <ref type="bibr" target="#b36">[37]</ref> and NFS <ref type="bibr" target="#b13">[14]</ref>, and compare our results with the state-of-the-art trackers.</p><p>OTB-100 OTB100 <ref type="bibr" target="#b36">[37]</ref> is a commonly used benchmark, which evaluates performance on Precision and AUC scores. <ref type="figure">Figure.</ref> 7 presents evaluation results of our TREG on both two metrics on OTB-100 benchmark. Achieving 71.4% and 94.5% in AUC score and precision score, our tracker reaches the state-of-the-art level w.r.t. other trackers in comparison.   LaSOT LaSOT <ref type="bibr" target="#b12">[13]</ref> has 280 videos in its test set. We evaluate our TREG on the test set to validate its long-term capability. The <ref type="figure" target="#fig_5">Figure. 8</ref> shows that our TREG surpasses other real-time trackers with a large margin. Specifically, it achieves the top-ranked performance on AUC criteria of 64.0% and Precision of 74.1%. The remarkable performance suggests that our TREG not only adapts to long-term tracking, but keeps precision of online targets regression.</p><p>VOT2018 Our TREG is tested on the VOT2018 <ref type="bibr" target="#b21">[22]</ref> dataset consisting of 60 videos in comparison with the SOTA trackers. As shown in <ref type="table">Table.</ref> 3, TREG achieves the performance on EAO criteria of 0.496 and Robustness of 0.098, which outperforms all state-of-the-art trackers. The improved Accuracy suggests that our TREG can generate precise bounding boxes.</p><p>VOT2019 Our TREG is tested on the VOT2019 <ref type="bibr" target="#b20">[21]</ref> dataset consisting of 60 videos in comparison with the SOTA trackers. As shown in   GOT10k GOT10k <ref type="bibr" target="#b18">[19]</ref> is a large-scale dataset with over 10000 video segments and has 180 segments for the test set. Apart from generic classes of moving objects and motion patterns, the object classes in the train and test set are zerooverlapped. Our TREG obtain state-of-the-art performance on the test split.</p><p>UAV123 UAV123 <ref type="bibr" target="#b28">[29]</ref> is a large dataset containing 123 Sequences with average sequence length of 915 frames, which is captured from low-altitude UAVs. <ref type="table">Table. 7</ref> shows our results on UAV123 dataset. Our proposed TREG outperforms the previous best reported result in precision. For AUC score, our TREG achieves 66.9%, which is close to PrDiMP.</p><p>NFS NFS dataset <ref type="bibr" target="#b13">[14]</ref> contains a total of 380K frames in 100 videos from real world scenarios. We evaluate our TREG on the 30 FPS version of this dataset. As shown in <ref type="table">Table.</ref> 8, Our TREG outperforms all previous approaches by a significant margin. Our result demonstrates the effectiveness of TREG for accurate regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a target-aware transformer, termed as TREG, to transfer an regression component from detection into tracking. We formulated the process of target integration for regression as a pixel-wise feature transformation guided by the target features. Additionally, we explored an efficient online regression mechanism which maintains an updating targets memory and selects the confident samples. Our approach provided accurate target estimation while being robust against distractor objects in the scene, outperforming previous methods on eight datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Online Target-aware Transformer for Regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of regression feature for different methods. Red box denotes the ground-truth bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Comparison results of trackers on OTB2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Comparison results of trackers on LaSOT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Analysis of online template update mechanism.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-art trackers on VOT2018.</figDesc><table><row><cell></cell><cell cols="7">ATOM SiamRPN++ DiMP50 SiamMask SiamBAN Ocean TREG</cell></row><row><cell></cell><cell>[9]</cell><cell>[23]</cell><cell>[2]</cell><cell>[35]</cell><cell>[5]</cell><cell>[42]</cell><cell></cell></row><row><cell cols="2">Accuracy 0.603</cell><cell>0.599</cell><cell>0.594</cell><cell>0.594</cell><cell>0.602</cell><cell cols="2">0.594 0.603</cell></row><row><cell cols="2">Robustness 0.411</cell><cell>0.482</cell><cell>0.278</cell><cell>0.461</cell><cell>0.396</cell><cell cols="2">0.316 0.221</cell></row><row><cell>EAO</cell><cell>0.292</cell><cell>0.285</cell><cell>0.379</cell><cell>0.287</cell><cell>0.327</cell><cell cols="2">0.350 0.391</cell></row><row><cell cols="8">Table 4. Comparison with state-of-the-art trackers on VOT2019.</cell></row><row><cell></cell><cell cols="7">ATOM SiamRPN++ DiMP PrDiMP CGACD SiamFC++ TREG</cell></row><row><cell></cell><cell>[9]</cell><cell>[23]</cell><cell>[2]</cell><cell>[11]</cell><cell>[12]</cell><cell>[39]</cell><cell></cell></row><row><cell>Prec.(%)</cell><cell>64.8</cell><cell>69.4</cell><cell>68.7</cell><cell>70.4</cell><cell>69.3</cell><cell>70.5</cell><cell>75.0</cell></row><row><cell cols="2">Norm. Prec.(%) 77.1</cell><cell>80.0</cell><cell>80.1</cell><cell>81.6</cell><cell>80.0</cell><cell>80.0</cell><cell>83.8</cell></row><row><cell>Succ.(%)</cell><cell>70.3</cell><cell>73.3</cell><cell>74.0</cell><cell>75.8</cell><cell>71.1</cell><cell>75.4</cell><cell>78.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Comparison with state-of-the-art trackers on TrackingNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table. 4, TREG achieves the performance on EAO criteria of 0.391, Robustness of 0.221 and Accuracy of 0.603, which outperforms all state-of-theart trackers. It suggests that our TREG can generate precise bounding boxes.TrackingNet TrackingNet<ref type="bibr" target="#b29">[30]</ref> provides over 30K videos with more than 14 million dense bounding box annotations. The videos are sampled from YouTube, covering target categories and scenes in real life. We validate TREG on its test set and achieve a remarkable improvement on all three metrics. Our TREG is proved to improve tracking performance on the large scale benchmark.</figDesc><table><row><cell></cell><cell cols="7">ATOM DiMP SiamFC++ SiamRCNN PrDiMP OCEAN TREG</cell></row><row><cell></cell><cell>[9]</cell><cell>[2]</cell><cell>[39]</cell><cell>[33]</cell><cell>[11]</cell><cell>[42]</cell><cell></cell></row><row><cell cols="2">SR0.50 63.4</cell><cell>71.7</cell><cell>69.5</cell><cell>72.8</cell><cell>73.8</cell><cell>72.1</cell><cell>77.8</cell></row><row><cell cols="2">SR0.75 40.2</cell><cell>49.2</cell><cell>47.9</cell><cell>59.7</cell><cell>54.3</cell><cell>-</cell><cell>57.2</cell></row><row><cell>AO</cell><cell>55.6</cell><cell>61.1</cell><cell>59.5</cell><cell>64.9</cell><cell>63.4</cell><cell>61.1</cell><cell>66.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Comparison with state-of-the-art trackers on GOT10k. Comparison with state-of-the-art trackers on UAV123.</figDesc><table><row><cell></cell><cell cols="8">ATOM SiamRPN++ SiamCAR SiamBAN DiMP PrDiMP TREG</cell></row><row><cell></cell><cell>[9]</cell><cell>[23]</cell><cell cols="2">[16]</cell><cell>[5]</cell><cell>[2]</cell><cell>[11]</cell></row><row><cell>Precision(%)</cell><cell>-</cell><cell>80.7</cell><cell>76.0</cell><cell></cell><cell>83.3</cell><cell>-</cell><cell>-</cell><cell>88.4</cell></row><row><cell>AUC(%)</cell><cell>64.4</cell><cell>61.3</cell><cell>61.4</cell><cell></cell><cell>63.1</cell><cell>65.4</cell><cell>68.0</cell><cell>66.9</cell></row><row><cell cols="9">CCOT ECO UPDT ATOM SiamBAN DiMP PrDiMP KYS TREG</cell></row><row><cell cols="2">[10] [8]</cell><cell>[4]</cell><cell>[9]</cell><cell>[5]</cell><cell>[2]</cell><cell>[11]</cell><cell>[3]</cell></row><row><cell cols="4">AUC(%) 48.8 46.6 53.7 58.4</cell><cell>59.4</cell><cell>62.0</cell><cell cols="3">63.5 63.5 66.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Comparison with state-of-the-art trackers on NFS.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gang Hua and Herv? J?gou</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>ECCV Workshops</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Know your surroundings: Exploiting scene information for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Siamese box adaptive network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zedu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fully convolutional online tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07109</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ECO: efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ATOM: accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Correlation-guided attention for corner detection based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05884</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Siamese attentional keypoint network for high performance visual tracking. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyue</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamido</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page">105448</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Siamcar: Siamese fully convolutional classification and regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matas, et. The seventh visual object tracking vot2019 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshop (IC-CVW)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking VOT2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><forename type="middle">P</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luka Cehovin Zajc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hand posture recognition using finger geometric feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A benchmark and simulator for UAV tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Al-Subaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Siam r-cnn: Visual tracking by re-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning attentions: Residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiple human tracking based on multi-view upper-body detection and discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><surname>Haizhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
