<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combined Scaling for Open-Vocabulary Image Classification Mingxing Tan</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-29">29 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<email>zihangd@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
							<email>golnazg@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
							<email>kawaguch@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
							<email>jiahuiyu@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
							<email>yitingchen@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmingxing@google</forename><surname>Com Quoc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">HANXIAOL@GOOGLE.COM</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ADAMSYUWEI@GOOGLE.COM</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">YONGHUI@GOOGLE.COM</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">QVL@GOOGLE.COM</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Combined Scaling for Open-Vocabulary Image Classification Mingxing Tan</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-29">29 Apr 2022</date>
						</imprint>
					</monogr>
					<note>*: Equal contributions. Corresponding authors: {HYHIEU,ZIHANGD}@GOOGLE.COM. Editor: To be assigned. 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a combined scaling method -named BASIC -that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best-published similar models -CLIP and ALIGN -by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy.</p><p>To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN.</p><p>We encountered two main challenges with the scaling rules of BASIC. First, the main challenge with implementing the combined scaling rules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To overcome the memory limit, we propose two simple methods which make use of gradient checkpointing and model parallelism. Second, while increasing the dataset size and the model size has been the defacto method to improve the performance of deep learning models like BASIC, the effect of a large contrastive batch size on such contrastive-trained image-text models is not well-understood. To shed light on the benefits of large contrastive batch sizes, we develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent advances in multimodal training approaches such as CLIP <ref type="bibr">(Radford et al., 2021)</ref> and ALIGN <ref type="bibr">(Jia et al., 2021)</ref> have the potential to eliminate the need for collecting labeled training data for every new application. Using natural language as a weak supervision signal, CLIP and ALIGN achieve the impressive top-1 accuracy of 76.2% and 76.4% on ImageNet ILSVRC-2012 without learning from any labeled ImageNet data. In addition to the promising accuracy on ImageNet, the so-called "zero-shot" models in CLIP and ALIGN demonstrate two important properties. First, these models are versatile, as they can be directly deployed on many downstream tasks without task-specific data for finetuning. Second, CLIP and ALIGN models are more robust than traditional classifiers. Robustness evaluations on benchmarks with natural distribution shifts <ref type="bibr">(Hendrycks et al., 2021b,a;</ref><ref type="bibr" target="#b67">Recht et al., 2019;</ref><ref type="bibr" target="#b3">Barbu et al., 2019;</ref> show that the accuracy of models like CLIP and ALIGN typically drops less than 10%, while the accuracy of supervised and semi-supervised models might drop as much as 40% <ref type="bibr">(Taori et al., 2020;</ref><ref type="bibr" target="#b80">Szegedy et al., 2013)</ref>.</p><p>Despite their versatility and robustness, the best models from CLIP and ALIGN are still not as competitive as supervised and semi-supervised models when enough labeled data is available, which can limit their potential applications. For example, the best CLIP and ALIGN models have an accuracy around 76% on ImageNet, which is only comparable with a supervised ResNet-50 <ref type="bibr" target="#b28">(He et al., 2015)</ref>, and significantly worse than the state-of-the-art supervised training on ImageNet (without extra data: 87.1% <ref type="bibr" target="#b98">(Yuan et al., 2021)</ref>, and with extra data: 90.88% <ref type="bibr">(Dai et al., 2021)</ref>). Therefore, narrowing the gap from these models to supervised and semi-supervised models would make the image-text contrastive learning approach in CLIP and ALIGN a viable alternative for image classification.</p><p>In this paper, we develop significantly better image classifiers that leverage the image-text contrastive learning approaches like CLIP and ALIGN at a much larger scale. In particular, we scale up the contrastive learning framework of CLIP <ref type="bibr">(Radford et al., 2021)</ref> and ALIGN <ref type="bibr">(Jia et al., 2021)</ref> in 3 dimensions: dataset size, model size, and batch size. For the data, we expand the ALIGN dataset <ref type="bibr">(Jia et al., 2021</ref>) from 1.7B noisy image-text pairs to 6.6B pairs, i.e., almost 4x larger. For the models, we choose CoAtNet, an architecture with higher learning capacity <ref type="bibr">(Dai et al., 2021)</ref>, and scale it to 3B parameters, i.e., 3.75x more weights and 8x more FLOPs than the largest models in CLIP and ALIGN. For the batch size, we use 65536 contrastive learning examples per minibatch, i.e., 2x more than CLIP and 4x more than ALIGN.</p><p>Overview of our implementation. The fundamental bottleneck of training large models at larger batch sizes is the limited memory of deep learning accelerators such as GPUs and TPUs. We propose two approaches that allow practitioners to overcome such memory limits.</p><p>Our first approach (Section 4) makes use of micro-batch pipelining <ref type="bibr" target="#b37">(Huang et al., 2019)</ref> and gradient accumulation (GradAccum) <ref type="bibr" target="#b63">(Ott et al., 2018;</ref><ref type="bibr" target="#b99">Zhai et al., 2021)</ref>. Our second approach (Section 5) utilizes the model parallelism scheme of Single-Program Multi-Data (SPMD) <ref type="bibr" target="#b50">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b97">Xu et al., 2021)</ref> to distribute the weights of certain layers in our networks onto different devices. While our SPMD approach is faster than our pipelining approach, and can deliver exact computations, the SPMD approach requires more manual designs to scale to arbitrarily large contrastive batch sizes, and hence, is less general than the pipelining approach.</p><p>Both our pipelining approach and our SPMD approach make use of gradient checkpointing , which is also called rematerialization in certain literature <ref type="bibr" target="#b48">(Kumar et al., 2019;</ref><ref type="bibr" target="#b39">Jain et al., 2020)</ref>. The idea behind rematerialization is to discard certain intermediate values in the forward pass of a neural network to save memory, and then recompute -i.e., rematerialize -these values only when they are needed for gradient computation in the network's backward pass.</p><p>Overview of our theoretical insights. While the benefits of large datasets and large models for deep learning models have become established knowledge, the benefits of large batch size are less well-understood in the context of relatively new image-text contrastive models. To understand such benefits, we develop a theoretical analysis of the image-text contrastive learning framework of CLIP and ALIGN. Our analysis establishes that using a larger contrastive batch size in CLIP and ALIGN's framework leads to a smaller generalization gap of the resulting models. <ref type="bibr">ALIGN (Jia et al., 2021)</ref> CLIP <ref type="bibr">(Radford et al., 2021)</ref>  Overview of our empirical results. Our proposed method, called BASIC, for Batch, Data and Model SIze Combined Scaling, achieves drastic improvements over CLIP and ALIGN models. For instance, on ImageNet, the largest BASIC model achieves 85.7% top-1 accuracy, without learning from any labeled example in the ImageNet training set. This result surpasses similar models in CLIP and ALIGN 9.3%. This BASIC model also shows significant improvements on robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, the model achieves an average of 83.7% top-1 accuracy, only a small drop from its original ImageNet accuracy (see <ref type="table" target="#tab_1">Table 1</ref>). When tested against CLIP on the other 17 image classification benchmarks, e.g., CIFAR, Caltech101, Flowers, etc. BASIC outperforms CLIP on 13 out of these 17 benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Large-scale pretraining and the contrastive loss. As computer vision models grow in their size and capacity, many weakly-supervised and self-supervised pretraining methods have been proposed to learn good visual representations. On one hand, pretraining with a classification loss on large weakly-labeled datasets such as Instagram hashtags or JFT can produce significant gains on downstream tasks such as ImageNet <ref type="bibr" target="#b41">(Joulin et al., 2016;</ref><ref type="bibr" target="#b56">Mahajan et al., 2018;</ref><ref type="bibr" target="#b45">Kolesnikov et al., 2020;</ref><ref type="bibr">Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b99">Zhai et al., 2021)</ref>. On the other hand, self-supervised methods which leverage existing structures in unlabeled data to train models have been developed. A promising development in self-supervised learning is the contrastive loss, with representative works like CPC (van den Oord et al., 2018), SimCLR <ref type="bibr">(Chen et al., 2020a,b)</ref> and MoCo <ref type="bibr" target="#b29">(He et al., 2020;</ref><ref type="bibr" target="#b13">Chen et al., 2020c)</ref>. In this paper, we scale up the contrastive learning framework, which we will revisit in detail in Section 3.</p><p>Contrastive-learned image-text models. Unlike the single-modal contrastive approaches mentioned in the previous paragraph, our work leverages data from two modalities: image and text. Using images with accompanying text is related to the literature on image-captioning models, such as <ref type="bibr" target="#b89">(Vinyals et al., 2015;</ref><ref type="bibr" target="#b43">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b96">Xu et al., 2015;</ref><ref type="bibr" target="#b41">Joulin et al., 2016;</ref><ref type="bibr" target="#b52">Li et al., 2017;</ref><ref type="bibr" target="#b69">Sariyildiz et al., 2020;</ref><ref type="bibr" target="#b100">Zhang et al., 2020;</ref><ref type="bibr">Desai and Johnson, 2021)</ref>. While learning to generate captions from images can induce good visual representations, it is not the goal of this paper. Instead, this paper focuses on establishing the ability of models to classify images based on textual descriptions. This focus makes our work closely related to the recent work of image-text models such as CLIP <ref type="bibr">(Radford et al., 2021)</ref> and ALIGN <ref type="bibr">(Jia et al., 2021)</ref>. Similar to CLIP and ALIGN, our work also learns the mapping between images and texts, which is related to many previous works, such as <ref type="bibr" target="#b33">(Hironobu et al., 1999;</ref><ref type="bibr" target="#b92">Weston et al., 2010;</ref><ref type="bibr" target="#b74">Socher and Fei-Fei, 2010;</ref><ref type="bibr" target="#b75">Socher et al., 2013;</ref><ref type="bibr" target="#b34">Hodosh et al., 2013;</ref><ref type="bibr" target="#b62">Norouzi et al., 2013;</ref><ref type="bibr">Kiros et al., 2014;</ref><ref type="bibr" target="#b76">Socher et al., 2014;</ref><ref type="bibr">Akata et al., 2015b,a;</ref><ref type="bibr">Nam et al., 2017;</ref><ref type="bibr" target="#b22">Faghri et al., 2017;</ref><ref type="bibr" target="#b53">Li et al., 2019;</ref><ref type="bibr" target="#b54">Liu et al., 2019;</ref><ref type="bibr">Lu et al., 2019;</ref><ref type="bibr" target="#b58">Messina et al., 2020;</ref><ref type="bibr" target="#b14">Chen et al., 2020d;</ref><ref type="bibr" target="#b97">Chen et al., 2021)</ref>.</p><p>Standardizing the term "Zero-shot transfer learning". In contrast to the term "zero-shot learning", the term "zero-shot transfer learning" emerges much more recently from CLIP <ref type="bibr">(Radford et al., 2021)</ref> and ALIGN <ref type="bibr">(Jia et al., 2021)</ref>. While neither of these two papers defines the term "zero-shot transfer learning", it is suggested from their experiments that this term refers to a model's ability to classify images based on textual descriptions. Contextualizing our work with CLIP, ALIGN, and contrasting this line of work to the rich literature on zero-shot learning, we propose to use the term "open-vocabulary image classification" to refer to the ability of BASIC, CLIP, and ALIGN models, i.e., to classify images by associating them to textual descriptions. Throughout this paper, we will use our proposed term "open-vocabulary image classification", and we hope that the term will become standardized in subsequent literature.</p><p>Data, model and batch scaling. Scaling has proven to be a powerful tool to boost the efficacy of vision model pretraining. There are three dimensions one can scale on. The simplest dimension is data. Indeed, recent efforts have shown that the more data we train on, the better the models become <ref type="bibr" target="#b41">(Joulin et al., 2016;</ref><ref type="bibr" target="#b56">Mahajan et al., 2018;</ref><ref type="bibr" target="#b45">Kolesnikov et al., 2020;</ref><ref type="bibr">Dosovitskiy et al., 2021;</ref>. The second dimension is the model size, with representative works such as EfficientNet, VITs and related works <ref type="bibr">Le, 2019, 2021;</ref><ref type="bibr" target="#b82">Tan et al., 2020;</ref><ref type="bibr">Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b99">Zhai et al., 2021;</ref><ref type="bibr">Bello et al., 2021)</ref>. Lastly, scaling up batch sizes is also the key for improving the model effectiveness <ref type="bibr" target="#b26">(Goyal et al., 2017)</ref>, especially for the contrastive loss <ref type="bibr" target="#b11">(Chen et al., 2020a;</ref><ref type="bibr" target="#b83">Tian et al., 2020;</ref><ref type="bibr">Jia et al., 2021;</ref><ref type="bibr">Radford et al., 2021)</ref>. Our work is inspired by the power of scaling, and pushes the limits in all the dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background on Image-text Contrastive Learning and Open-Vocabulary Image Classification</head><p>In this section, we revisit the contrastive training framework for parallel image-text data, as introduced by CLIP <ref type="bibr">(Radford et al., 2021)</ref> and ALIGN <ref type="bibr">(Jia et al., 2021)</ref>. In doing so, we define the notations that will be used throughout the remaining of this paper. Let x ? X be an arbitrary image and y ? Y be an arbitrary text sequence. The image-text contrastive training framework <ref type="bibr">(Radford et al., 2021;</ref><ref type="bibr">Jia et al., 2021)</ref> trains an image encoder F and a text encoder G to map x and y into a D-dimensional unit sphere, i.e., F (x), G(y) ? S D . The desiderata of these encoders is that images and text sequences of similar semantics should be mapped to nearby points in the latent space, while those with different semantics should be mapped to distant points in the space. To train F and G to achieve such desiderata, a minibatch gradient training procedure is used. At each step in this training procedure, F and G receives B image-text pairs, i.e. (x i , y i ) for i = 1, 2, ..., B. Based on the embeddings computed by F and G, a similarity matrix A is computed, where A i,j = F (x i ) G(y i ) /? . Here, ? is called the softmax temperature which serves to steepen or dampen the softmax distributions in the rows and columns of A. From this similarity matrix A, two softmax-like losses are computed based on the rows and the columns of A:</p><formula xml:id="formula_0">RowLoss B = ? 1 B B i=1 log A i,j B k=1 A i,k<label>(1)</label></formula><formula xml:id="formula_1">ColumnLoss B = ? 1 B B j=1 log A i,j B k=1 A k,j<label>(2)</label></formula><formula xml:id="formula_2">ContrastiveLoss B = RowLoss B + ColumnLoss B 2<label>(3)</label></formula><p>Minimizing ContrastiveLoss B encourages the entries on the diagonal of A to be large while the entries elsewhere to be small. Equivalently, images and text sequences from the same pair in the minibatch, i.e. x i and y i , will be embedded into nearby points, while those from different pairs, i.e. x i and y j =i , will be embedded into distant points. The resulting encoders F and G thus achieve the desiderata of the contrastive learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Batch Size Scaling with Pipelining and Gradient Accumulation</head><p>We start this section by discussing the memory bottleneck in the contrastive training framework as described in Section 3. We focus on memory because it is the most crucial bottleneck which hinders two out of three dimensions that we want to scale, i.e., model size and batch size. We further show that the vanilla pipelining algorithm <ref type="bibr" target="#b37">(Huang et al., 2019)</ref> with gradient accumulation (GradAccum) <ref type="bibr" target="#b63">(Ott et al., 2018;</ref><ref type="bibr" target="#b99">Zhai et al., 2021)</ref> is not directly applicable to contrastive learning. We then describe our modifications to make GradAccum work for constrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Memory Bottleneck for Image-Text Contrastive Learning</head><p>The memory bottleneck. The consensus among representative work in contrastive learning <ref type="bibr">(Chen et al., 2020b,c;</ref><ref type="bibr" target="#b29">He et al., 2020;</ref><ref type="bibr" target="#b11">Chen et al., 2020a)</ref> is that the larger the networks trained with a larger contrastive batch size performs better. This observation is further explained by our theoretical analysis in Section 6, and is confirmed by empirical results in Section 10. Therefore, we want to enlarge the networks F , G, and the batch size B. However, this will create a memory bottleneck. Three well-known techniques to relieve memory burden are gradient accumulation (GradAccum) <ref type="bibr" target="#b63">(Ott et al., 2018;</ref><ref type="bibr" target="#b99">Zhai et al., 2021)</ref>, re-materialization (or gradient checkpointing) <ref type="bibr" target="#b27">(Griewank and Walther, 2000;</ref> and model parallelism <ref type="bibr" target="#b37">Huang et al., 2019;</ref><ref type="bibr" target="#b50">Lepikhin et al., 2020)</ref>. Note that all three techniques are orthogonal and complementary to each other. Next in section 4.2, we present an approach based on pipelining model parallelism and gradient accumulation.</p><p>Vanilla GradAccum. Consider training a model weight vector ? to minimize a loss function L. For a batch of B examples {e 1 , e 2 , ..., e B }, let g i be the gradient of L with respect to ? computed on example e i , i.e., g i = ? ? L(?; e i ). In the standard minibatch setting, we update ? with the average batch gradient We now analyze the steps of GradAccum. For simplicity, assume that M evenly divides B, and that microbatch i-th consists of examples e j 's with (i ? 1)M + 1 ? j ? iM . With this assumption, the GradAccum procedure first initializes a zero vector? of the same size with ?. Then, sequentially for each microbatch i-th, the microbatch gradient c i = iM j=(i?1)M +1 g j /M is added to?. In the end,? holds the correct minibatch gradient, up to a normalization constant K = B/M .</p><formula xml:id="formula_3">g = B i=1 g i /B.</formula><p>GradAccum cannot be naively applied to contrastive learning. There are two properties that make GradAccum not applicable to contrastive learning. First, in order to evaluate the loss ContrastiveLoss B in Equation 3, we need all entries of the similarity matrix A. Hence, we cannot rely only on examples in every microbatch i-th to compute the microbatch gradients c i 's. Second, GradAccum must allocate memory for the cumulative gradient?. 1 As? has as many elements as ?, its memory grows as we scale up the networks F and G. This growth becomes a more pronounced issue as we scale up our models. For reference, our largest model has 3B weights, occupying roughly 11GB of accelerator memory. Spending another 11GB on g, while possible, defeats the purpose of saving memory in GradAccum. In the remaining of this subsection, we discuss how to modify GradAccum so that we can use it to scale up contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modifying Pipelining and GradAccum for the Contrastive Loss</head><p>Microbatching the contrastive loss. To enable proper GradAccum, a key observation is that while we need the entire similarity matrix A to compute ContrastiveLoss B in Equation 3, we do not need to store all the intermediate results leading to the matrix in memory. This observation immediately connects to rematerialization, which trades computation for memory by dropping some intermediate hidden states during the forward pass and re-computing them during back-propagation. Following this insight, we propose to combine re-materialization with gradient accumulation by microbatching the contrastive loss and re-materializing each microbatch.</p><p>Specifically, we first run a forward pass on the networks F , G to compute the entire similarity matrix A while discarding all intermediate hidden states. Then, we use A to compute ContrastiveLoss B and the gradient ? A ContrastiveLoss B and microbatch this gradient along the batch axis. Finally, for each microbatch, we re-materialize the hidden states, i.e., rerun the forward computation, and back-prop and accumulate the corresponding gradient microbatch of ? A L c into the weights of the networks F , G.</p><p>Algorithm 1 presents this procedure in detail and provides the memory analysis for each step. As shown, our algorithm can compute the exact microbatch gradients from an entire batch of B examples, with the peak</p><formula xml:id="formula_4">Inputs ? Networks F , G with a weight vector ? = [? F , ? G ], Memory Analysis ? A minibatch of B (image, text) pairs {(x i , y i )} B i=1 , B is the contrastive batch size. ? Microbatch size M . Assuming M evenly devices B.</formula><p>M is the largest in-memory batch size.</p><formula xml:id="formula_5">Yields ? Gradients ? ? ContrastiveLoss for B/M</formula><p>The loss is computed as in Equation 3 ? microbatches of the minibatch.</p><p>1 Allocate embedding matrices X, Y ? R D?B D is the embedding size ?(BD) 2 For i = 1 to B/M do:</p><p>Sequentially compute the embeddings for 3</p><p>Let J ? {j : (i ? 1)M + 1 ? j ? iM } microbatches of images and text sequences, 4 X :,J ? F (x J ) not saving the activations of F and G.</p><formula xml:id="formula_6">?(M ? Mem(F )) 5 Y :,J ? G(y J ) ?(M ? Mem(G)) 6 A ? X ? Y /? A ? R B?B is the similarity matrix 7 RowLoss B ? ? 1 B B i=1 log Ai,j B k=1 A i,k</formula><p>The contrastive loss in Equation 3</p><p>?</p><formula xml:id="formula_7">(B 2 ) 8 ColumnLoss B ? ? 1 B B j=1 log Ai,j B k=1 A k,j 9 ContrastiveLoss B ? RowLoss B +ColumnLoss B 2 10 dA ? BackProp(ContrastiveLoss, A) Back-prop to compute ? A 11 dX ? Y ? dA Because A = X Y 12 dY ? X ? dA 13 For i = 1 to B/M do:</formula><p>Repeat a forward pass on F , G to 14</p><p>Let</p><formula xml:id="formula_8">J ? {j : (i ? 1)M + 1 ? j ? iM } back-prop the gradients from dX, dY to 15 d? F ? ForwardAndBackProp(dX :,J , ? F ) the weights ? F , ? G . ?(M ? Mem(F )) 16 d? G ? ForwardAndBackProp(dY :,J , ? G ) ?(M ? Mem(G)) 17 Yield d? = [d? F , d? G ]</formula><p>Algorithm 1: Pseudo code of our gradient accumulation process for the contrastive loss. Here Mem(F ), Mem(G) denote the memory required for a pass for the networks F , G. As shown in our memory analysis, at the cost of repeating one forward pass for F , G (lines 13-16), our procedure's peak memory footprint is dominated by</p><formula xml:id="formula_9">?(M ? max {Mem(F ), Mem(G)}).</formula><p>memory usage of ?(M ? max {Mem(F ), Mem(G)}), instead of ?(B ? (Mem(F ) + Mem(G))). We note that our algorithm can be flexibly modified to work different microbatch-sizes, i.e., M , for the image network F and the text network G. This flexibility allows for more efficient computations, e.g., when one network is smaller than another and thus, can operate with larger microbatches.</p><p>Accumulating the microbatch gradients. Algorithm 1 yields a stream of microbatch gradients c 1 , ..., c B/M , which need to be accumulated, i.e., averaged, into? to perform the batch weight update. As discussed, we want to avoid allocating extra memory for?. To do this, we need two assumptions about our training implementation. Our first assumption is that we use an optimizer which involves gradient moments <ref type="bibr" target="#b60">(Nesterov, 1983;</ref><ref type="bibr" target="#b84">Tieleman and Hinton, 2012;</ref><ref type="bibr">Kingma and Ba, 2015;</ref><ref type="bibr" target="#b55">Loshchilov and Hutter, 2019;</ref><ref type="bibr" target="#b72">Shazeer and Stern, 2018)</ref>. This assumption motivates our idea to avoid allocating?: since the optimizer already allocates the memory for gradient moments, typically called slots, we will directly accumulate the microbatch gradients c i 's into these slots. We illustrate this idea with Adam (Kingma and Ba, 2015), a popular optimizer that involves two gradient moments. At training step t, Adam receives the averaged minibatch gradient? and makes the following updates to its gradient moments v 1 and v 2 :</p><formula xml:id="formula_10">g = 1/B ? B i=1 g i = 1/ (B/M ) K ? B/M i=1 c i v (t) 1 = ? 1 v (t?1) 1 + (1 ? ? 1 )? v (t) 2 = ? 2 v (t?1) 2 + (1 ? ? 2 )? 2</formula><p>Accumulating the microbatch gradients c i 's to v 1 is straightforward. We can simply modify v 1 's single update with? into K = B/M updates as follows:</p><formula xml:id="formula_11">v 1 ? k i v 1 + (1 ? ? 1 )c i , where k i = ? 1 if i = 1 1/K otherwise</formula><p>Unfortunately, the same approach is not applicable for v 2 , as the square of the sum is generally different from the sum of the squares, i.e. ( c i ) 2 = c 2 i . However, the difference between these two quantities turns out to be:</p><formula xml:id="formula_12">1 K K i=1 c 2 i sum of squares ? 1 K K i=1 c i 2 square of sum = E c 2 i ? E[c i ] 2 = Var[c i ] ,</formula><p>which we can estimate. Indeed, since each c i 's is the mean of M per-example gradients g j 's in the i-th microbatch, we can treat c i 's as the population mean of M observed examples drawn from a random variable g ? Uniform{g 1 , ..., g B }. This treatment allows us to use the familiar identity: </p><formula xml:id="formula_13">Var[c i ] = Var 1 M iM j=(i?1)M +1 g j = Var[g] M<label>(4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Batch Size Scaling with the Single-Program Multiple-Data (SPMD) Scheme</head><p>In Section 4, we have seen that one circumvent the memory bottleneck of large models and large batch sizes for image-text contrastive learning by: (1) "chunk" a large batch of B image-text pairs into arbitrarily smaller microbatches, (2) compute the gradient for each microbatch, and (3) accumulate them. While such an approach is generic and can work for any global contrastive batch size B and any microbatch size M , there are two steps in the approach that makes the resulting gradient inexact. The first inexact computation comes from the approximations when accumulating the microbatch gradients. This is a necessary tradeoff to avoid allocating the memory to accumulate the microbatch gradients. The second inexact computation is more subtle, and is specific to our modeling choice, and has also been noted by <ref type="bibr" target="#b37">Huang et al. (2019)</ref>. Specifically, when our networks F and G depend on the batch, e.g., via the batch normalization layers in our image encoder F , then the outputs of the networks for multiple microbatches are generally different from those for one entire global batch. When the microbatch size M is too small compared to B, the discrepancies become very large and can cause the covariate shift in the image encoder F which was the original motivation for batch normalization. More recent image encoders can overcome such inexact computations because they avoid batch normalization by replacing it with layer normalization like Vision Transformer <ref type="bibr">(Dosovitskiy et al., 2021)</ref>, or just do not use any normalization at all like NFNet <ref type="bibr" target="#b8">(Brock et al., 2021)</ref>. However, the inexact gradient accumulation remains even for such models.</p><p>To overcome these inexact computations, in this section, we discuss an alternate approach to circumvent the memory bottleneck. This approach is based on the SPMD programming scheme. We find that not only does our SPMD method provide exact computations which lead to better results than pipelining and GradAccum, but our SPMD method also has a better latency per training step. However, as we shall see in Section 5.1 and Section 5.2, our SPMD method requires several manual designs, which make it less generic than pipelining and GradAccum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Weight Sharding</head><p>As model sizes grow, model weights occupy a significant part of accelerator memory. In modern optimizers for deep learning models, such as Adam (Kingma and Ba, 2015), RMSprop <ref type="bibr" target="#b84">(Tieleman and Hinton, 2012)</ref>, and AdamW <ref type="bibr" target="#b55">(Loshchilov and Hutter, 2019)</ref>, every weight tensor is additionally accompanied by the first and second gradient moments, hence tripling its memory footprint. Furthermore, in the vanilla data parallelism training, all these weights are replicated to all accelerators. In our experiments with a relatively large model size, roughly 4GB of accelerator memory is occupied by these weights and their gradient moments, which is significant for the typical 16GB memory in an accelerator in 2022, such as a Google TPU core or an Nvidia RTX 3080 GPU.</p><p>Here, we split the weight tensors in our encoder networks, i.e. F and G in Section 3, into multiple accelerator cores, and only combine these tensors together when the whole tensor is needed to perform certain computations. Note that upon splitting a weight tensor to multiple cores, we also split its first and second gradient moments in the similar way. <ref type="figure">Figure 1</ref> illustrates our weight sharding strategy on the 2D convolution operation which is prevalent in image encoder models.  <ref type="figure">Figure 1</ref>: An illustrative example for our model parallelism design. Shown is 2D convolution operation with a 3x3 kernel sharded to 4 cores. Gray cells represent the tensor values that are mirrored across all cores, while cells of other colors represent per-core independent tensor values. The convolution's input is a tensor of shape [N, H, W, i] which is sharded along its first dimension so that each core has processes a tensor of shape [N/4, H, W, i]. The convolution's kernel is a tensor of shape [3, 3, i, o], but each core only stores one shard of the kernel which has size [3, 3, i/4, o]. Before convolving, every core receives the kernel shards from all other cores and concatenate the shares, forming the complete kernel of size [3, 3, i, o]. After convolving, the complete kernel is discarded from all cores' memory.</p><formula xml:id="formula_14">w[1] [3, 3, i/4, o] w[2] [3, 3, i/4, o] w[3] [3, 3, i/4, o] w[4] [3, 3, i/4, o] w [3, 3, i, o] w [3, 3, i, o] w [3, 3, i, o] w [3, 3, i, o] input[1] [N/4, H, W, i] input[2] [N/4, H, W, i] input[3] [N/4,</formula><p>Our approach is based on the Single-Program Multiple-Data (SPMD) technique, which has been successfully applied to train large language models in previous works such as in <ref type="bibr" target="#b97">Xu et al. (2021)</ref>; <ref type="bibr" target="#b50">Lepikhin et al. (2020)</ref>. In the SPMD technique, we define a computational graph which represents our entire training program. This computational graph is compiled once, and then is replicated identically to all computational cores to run the training program. While all of our computational cores run an identical program, they are allowed to receive different inputs and hence can produce different outputs. These inputs and outputs can be organized in certain ways to define arbitrarily complex model parallelism strategies. Next, we describe how we apply the SPMD technique on our model weights only.  Our training program runs typically on a cluster of 2048 TPUv3 cores. We partition these 2048 cores into R replicas, each of which uses 2048/R cores. The value of R governs how the weights of our image encoder F and our text encoder G are stored in the memory of our 2048 cores. In particular, all weight tensors in the networks F and G are split into R equal parts, each lives in one of the R cores in a replica. Note that since we have 2048/R replicas, the weights of our image and text encoders are still replicated for 2048/R times. For instance, our cores 1 st , 2 nd , ... R th can each store 1/R of the weight tensors, and then the cores R + 1 st , R + 2 nd , ..., 2R th store an identical copy of these tensors. Thus, using fewer replicas and more cores per replica leads to a better memory utilization, at a higher overhead for cross-cores communications. We empirically find that using 512 replicas and 4 cores per replica offers a good balance.</p><formula xml:id="formula_15">conv_1x1 [N, H, W, Cx4] input [N, H, W, C] batch_norm [N, H, W, Cx4] gelu [N, H, W, Cx4] depthwise_3x3 [N, H, W, Cx4] batch_norm [N, H, W, Cx4] gelu [N, H, W, Cx4] GlobalAveragePool [N, 1, 1, C] swish [N, H, W, C] gelu [N, H, W, C] conv_1x1 [N, H, W, C] conv_1x1 [N, H, W, Cx4] conv_1x1 [N, H, W, C] batch_norm [N, H, W, C] gelu [N, H, W, C] + x sigmoid [N, H, W, Cx4] input [N, H, W, C] qkv_attention [N, T, C] layer_norm [N, T, C] + layer_norm [N, T, C] linear [N, T, Cx4] gelu [N, T,</formula><p>It is important to note that we only apply SPMD on our model weights, and not on any other steps of our computations. This means that if our training program receives an input batch of B examples, then these B examples are distributed equally to all our 2048 cores. In other words, each of our 2048 cores processes B/2048 examples, regardless of the value of R. We find that this design choice disentangles our weight sharding strategy from the rematerialization strategy, as described next in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Rematerialization</head><p>The technique of rematerialization, also widely known as gradient checkpointing , preserves the accelerator memory while training neural networks. It works by not saving certain values from a network's forward pass, and recompute them in the backward pass only when their values are needed for a particular calculation. For instance, if our image encoder F , as discussed in Section 3 has 100 layers, a rematerialization program can decide that after the forward pass, only the values of layers 10 th , 20 th , ..., 90 th are kept in an accelerator's memory, while the values of other layers are removed. If all layers of F consumes similar memory, this rematerialization program has reduced the memory cost by 10 times, at the trade off that the values of the unsaved layers in the forward pass, such as layer 21 st or layer 72 nd , have to be recomputed in the backward pass.</p><p>We select which layer to rematerialize in our image encoder F and our text encoder G based on a simple heuristic. Ideally, we want to rematerialize the layers that are fast to recompute but consumes the more memory. Since we utilize weight sharding, as described in Section 5.1, the computations that involve weights are slower than normal because of their overhead time for cross-core communications. As such, we keep almost all layers that involve weights, such as convolution, attention, and dense feed-forwards, in our accelerator's memory. In contrast, layers that do not involve weights, such as activation functions, batch normalization, and layer normalization, are all rematerialized. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates our rematerialization strategy for all three block types in our image and text encoders: the mobile-inverse convolutional block <ref type="bibr" target="#b81">(Tan and Le, 2019;</ref><ref type="bibr">Dai et al., 2021)</ref>, the attention block, and the feed-forward blocks <ref type="bibr" target="#b87">(Vaswani et al., 2017)</ref>.</p><p>We find this design choice beneficial, because in modern encoder architectures, every layer that involves weights is typically followed by a normalization layer or an activation layer. As such, our design allows more than half of the encoder's activation values to be removed from the accelerator's memory after each forward pass, while leaving only the light computational steps to be repeated in each backward pass. We empirically find that weight sharding and rematerialization, each of our forward-backward pass is 1.4 times slower than the vanilla implementation of the same batch size.</p><p>Exceptions to our heuristics. Certain parts of our encoders do not follow these general heuristics, as we find that doing so saves a certain amount of time at the cost of using a little extra memory. Here, we describe these exceptions:</p><p>1. All weights in batch normalization and layer normalization in our models, including the ?'s and ?'s and the moving average statistics of batch normalization, are not sharded. Instead, these weights are replicated to all computational cores to avoid cross-cores communications, because they are one dimensional vectors which do not occupy much memory.</p><p>2. All computations the Squeeze-and-Excitation blocks (SE; <ref type="bibr" target="#b35">(Hu et al., 2018)</ref>) of our models are rematerialized, including the convolutions. This is because these SE blocks only involve 1x1 convolution with reduced internal channels, making them less costly to recompute. In addition, all the weights of these 1x1 convolutions are replicated to all of our cores, because they have a small memory footprint but are reused in the backward pass for rematerialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Pipelining and Gradient Accumulation</head><p>We measure and compare the step time and peak memory usage of our SPMD approach and our Pipelining and GradAccum approach from Section 4. We measure these pieces of information as the contrastive batch size B becomes larger. In particular, we start with B = 2 16 and double B until we reach B = 2 20 . When B increases, but our model can still fit into the our memory only with vanilla data parallelism, we measure the step time and the peak memory using this setting. These measures serve as the reference to quantify how much overhead is introduced by pipelining, SPMD, or rematerialization as a whole. When B grows and our models can no longer train with merely data parallelism, we experiment with the Pipelining and GradAccum, and compare the resulting programs with the SPMD programs of the same model and batch size. Note that for the pipelining approach, we set the microbatch size to the largest size that vanilla data parallelism can fit in our accelerator's memory. For all settings, we profile a model in 15 seconds. Our profiling tool tracks the time and memory usage of our models during their forward and backward passes in each step. Note that our backward pass time includes the time our models spend on their rematerialized computations (as discussed in Section 5.2), as these computations happen while the models compute their gradients. Note that other than these forward and backward passes, every step of our models has some extra overheads for miscellaneous computations that are not recorded by our profiling tool, e.g., gradient clippings and updating model parameters.</p><p>All of our measurements are reported in <ref type="table" target="#tab_7">Table 2</ref>. From the table, it can be seen that our model parallelism strategy leads to a faster overall step time, compared to the pipelining approach in the same setting. Breaking down these step times, it can be seen that the run time of our strategy's model forward pass is very close to  the run time of pipelining's forward pass, but our backward time is often a lot faster. For instance, in our largest setting, with the medium-sized model and the contrastive batch size B = 2 20 , our backward time is more than 1.2 seconds faster than that of the pipelining approach, amounting to about 10% of the total step time. Additionally, our strategy also has a faster total step time, perhaps because do not need to spend extra time to accumulate the microbatch gradients like the pipelining approach.</p><p>Finally, we note that as B grows larger, the SPMD approach typically occupies more accelerator memory than does the pipelining approach. This is because in the pipelining approach, increasing the contrastive batch size B only leads to more microbatches, but does not change the micro batch size, and so the accelerator's memory remains constant. As such, the pipelining approach is still applicable if B grows larger than 2 20 , but the SPMD strategy has to be redesigned, e.g. by deciding to rematerialize a larger portion of our image and text encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Theoretical Insight on the Role of Contrastive Batch Size</head><p>In this section, we provide a theoretical insight that increasing the contrastive batch size tends to improve the performance of the final model, which motivates and partially justifies the design of our new algorithm in the previous section. Let? 1 , . . . ,? B be the sequence of the text sentence inputs used in training. Similarly, let? 1 , . . . ,? M be the sequence of the text sentence inputs used in testing. We then define the normalized training loss by?</p><formula xml:id="formula_16">B (x, y) = ? B exp(F (x) G(y)) B k=1 exp(F (x) G(? k )) = ? exp(F (x) G(y)) 1 B B k=1 exp(F (x) G(? k ))</formula><p>, where we multiply B to scale the loss correctly in the regime of B ? ?. That is, since the unnormalized loss goes to zero (? B (x, y)/B ? 0) as the batch size approach infinity (B ? ?), analyzing the unnormalized version of the loss? B (x, y)/B can mistakenly predict benefits of the large contrastive batch size. We avoid this with the normalization. Similarly, we define the normalized testing loss b?</p><formula xml:id="formula_17">M (x, y) = ? exp(F (x) G(y)) E?[exp(F (x) G(?))]</formula><p>.</p><p>Then, the prediction at testing time for a new input x is given by</p><formula xml:id="formula_18">pred(x) = argmax j?{1,2,...,M } F (x) G(? j ) = argmin j?{1,2,...,M }? M (x,? j ). Therefore,? B (x i ,? i ) is minimized during training for training points (x i ,? i ) while we want to minimiz? M (x, y)</formula><p>to make a prediction at a new point x. This leads to the question of the generalization from training to unseen-data, which can be studied by analyzing the upper bound on the following quantity:</p><formula xml:id="formula_19">E x,y [? M (x, y)] ?? S [? B (x, y)], where? S [? B (x,?)] is the empirical training loss with a dataset, S = ((x i ,? i )) m i=1 , of size m. To analyze this in a statistical setting, we define a vector v ? R D by v i = F (x) i ? B k=1 exp(F (x) G(? k ))G(? k ) i B k=1 exp(F (x) G(? k )) for all i ? {1, . . . , D}, and assume that? 1 ,? 2 , . . . ,? B iid ? p y ,? 1 ,? 2 , . . . ,? M iid ? p y , exp(F (x) G(y)) ? c 1 , B (x, y) ? c 2 , F (x) 2 ? c 3 , exp(F (x) G(y)) 1 B B k=1 exp(F (x) G(? k )) ? c 4 , F (x i ) 2 ? c 5 , v 2 ? c 6 , y 2 ? c 7 , and x 2 ? c 8 , with probability one. Moreover, by defining ?(x) = E?[exp(F (x) G(y))]? 1 B B k=1 exp(F (x) G(? k )), we assume that ? is c 9 -Lipschitz; i.e., |?(x) ? ?(x )| ? c 9 x ? x 2 for all x ? X ? R ? .</formula><p>To provide a concrete insight, we consider standard deep neural networks, of the form</p><formula xml:id="formula_20">G(y) = (? L ? ? L?1 ? ? L?1 ? ? L?2 ? ? ? ? 1 ? ? 1 )(y), F (x) = (? L ? ? L ?1 ? ? L ?1 ? ? L ?2 ? ? ? ? 1 ? ? 1 )(x),</formula><p>where ? l (q) = W l q represents the linear transformation and ? l is an element-wise nonlinear activation function. Similarly, ? l (q) = W l q and ? l is an element-wise activation function.</p><p>The following theorem provides an insight on the role of the contrastive batch size to close the accuracy gap from contrastive models to their supervised counterparts:</p><p>Theorem 1 Suppose that the activation functions ? and ? l are 1-Lipschitz and positive homogeneous for</p><formula xml:id="formula_21">all l ? [L ? 1]. Let G = {y ? G(y) : (?l ? [L ? 1])[ W l F ? M l ] ? (W L ) k F ? M L,k } and F = {x ? F (x) : (?l ? [L ? 1])[ W l F ? M l ] ? (W L ) k F ? M L ,k } where (W l ) k is the k-th row of W l .</formula><p>Then, for any ? &gt; 0, with probability at least 1 ? ?, the following holds for all F ? F and G ? G:</p><formula xml:id="formula_22">E x,y [? M (x, y)] ?? S [? B (x, y)] ? Q 1 ? m + Q 2 ? 2B + c 2 ln(2/?) 2m , where Q 1 = 2 ? 2c 4 c 2 5 + c 2 6 (Q 1,1 +Q 1,2 ), Q 2 = c 1 E x,y [A(x, y)](Q 2,1 +Q 2,2 ), Q 1,1 = c 7 ( 2 log(2)L + 1) L?1 l=1 M l D k=1 M L,k , Q 1,2 = c 8 ( 2 log(2)L + 1) L ?1 l=1 M l D k=1 M L ,k , Q 2,1 = 2 ? 2c 8 c 9 + c 1 ? ln( ? ?B/?), Q 2,2 = 2 ? 2c 3 c 7 ( 2 log(2)L + 1) L?1 l=1 M l D k=1 M 2 L,k , and A(x, y) = exp(F (x) G(y)) ( 1 B B k=1 exp(F (x) G(? k )))E?[exp(F (x) G(?))]</formula><p>.</p><p>Proof The proof is presented in Appendix H.</p><p>Theorem 1 shows that the generalization gap E </p><formula xml:id="formula_23">x,y [? M (x, y)] ?? S [? B (x,</formula><formula xml:id="formula_24">? m + 1 ? B ),</formula><p>with high probability. This shows the importance of the contrastive batch size B with the week supervision setting without explicit labels: i.e., if B is small, the generalization gap can be large, even with a lot of training samples with large m. This is different from a classification task with a standard supervision, where a large m is sufficient to reduce the generalization gap. In sum, Theorem 1 provides the insight that we should increase both the number of samples m and the contrastive batch size B to improve the performance of the final model via the weekly supervised contrastive learning.</p><p>For general models beyond the standard deep neural networks, the following theorem provides a similar insight on the importance of the contrastive batch size:</p><p>Theorem 2 Let F be a set of maps x ? F (x) and G be a set of maps y ? G(y). Then, for any ? &gt; 0, with probability at least 1 ? ?, the following holds for all F ? F and G ? G:</p><formula xml:id="formula_25">E x,y [? M (x, y)] ?? S [? B (x, y)] ? C 1 ? 2B + c 2 ln(2/?) 2m + C 2 D k=1 (R m (F k ) + R m (G k )) + C 3RB (G), where R m (H) := E S,? [sup h?H 1 m m i=1 ? i h(x i , y i )], F k = {x ? F (x) k : F ? F}, G k = {y ? G(y) k : G ? G},R B (G) = E y,? sup G?G 1 B B i=1 ? i G(y i ) 2 , C 1 = c 1 E x,y [A(x, y)]Q 2,1 , C 2 = 2 ? 2c 4 c 2 5 + c 2 6 , and C 3 = 2c 1 c 3 E x,y [A(x, y)].</formula><p>Here, ? 1 , . . . , ? m are independent uniform random variables taking values in {?1, 1}. , with high probability, for general models F and G. Because we are not specifying the types of the models F and G, we incur the additional terms that capture the model complexity of F and G: i.e., R m (F k ) + R m (G k ) andR B (G). The values of these model complexity terms differ for different models and typically scale as</p><formula xml:id="formula_26">R m (F k ) + R m (G k ) = O( 1 ? m ) andR B (G) = O( 1 ? B</formula><p>) in terms of m and B as illustrated in the proof of Theorem 1 for standard deep neural networks. Therefore, it is desirable to use a large contrastive batch size, which motivates our new algorithm in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Data and Model Scaling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Larger image-text dataset</head><p>Starting from the ALIGN dataset, which contains 1.7B weakly-aligned image-text pairs <ref type="bibr">(Jia et al., 2021)</ref>, we collect 5B more image-text pairs, hence expanding the dataset size by roughly 4 times. We acquire these 5B image-text pairs from the JFT dataset. In the JFT dataset, each image is associated with one or multiple classes. We convert these classes into a text sequence: "{class_1} and {class_2} and ... and {class_k}". We combine the instances from JFT into ALIGN, forming our extended dataset, which we denote by ALIGN+JFT.</p><p>To tokenize the texts from ALIGN+JFT, we randomly sample 200M sentences and use them to train a sentence piece model <ref type="bibr" target="#b47">(Kudo and Richardson, 2018)</ref> with a vocabulary size of 32K pieces. Using this tokenizer, we filter and discard the text sequences which are longer than 64 tokens. In our preliminary experiments, we find that using a tokenizer directly learned from ALIGN+JFT and adapting this filtering step can boost our top-1 accuracy on ImageNet ILSVRC-2012 by more than 1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Larger Model Architectures</head><p>We find that for the same computational budget, it is more beneficial to invest in scaling up the image encoder, rather than the text encoder. Thus, for our image encoder, we use the largest CoatNet architecture <ref type="bibr">(Dai et al., 2021)</ref> due to its proven large learning capacity. This network has convolution layers followed by attention layers. For our text encoder, we use a simple transformer <ref type="bibr" target="#b87">(Vaswani et al., 2017)</ref>. Unlike ALIGN <ref type="bibr">(Jia et al., 2021)</ref> which extracts the final text representations using a [CLS] token similar to <ref type="bibr">BERT (Devlin et al., 2018)</ref>, we average the representations across all steps at the top layer of our transformer.</p><p>By experimenting with the scaling benefits for small models and generalizing these findings to larger models, we choose three model sizes, termed BASIC-{S,M,L} for Small, Medium, and Large. In Appendix A, we report our architectures and their computational costs and provide a small-scale study on the effects of scaling model sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Pretraining and Finetuning</head><p>To further speed up the training of our networks, we make use of pretraining. In our experiments, we first pretrain the image encoder on a large labeled dataset using the standard softmax classification loss. After pretraining the image encoder, we fix all of its weights and just train the text encoder using contrastive learning. Compared to contrastive learning with GradAccum, the pretraining-finetuning procedure is much more efficient in terms of peak memory usage. This is because we never have to compute the gradients of both the image encoder and the text encoder, which allows automated compiler optimizations to free up unused memory on-the-fly.</p><p>Despite its reduced memory usage, we find that this pretraining-finetuning scheme has a weakness: it never exposes the image encoder to noisy image-text data, which makes the image encoder fail on certain tasks. For instance, while some pretrained-and-finetuned models achieve similar accuracy to their contrastive counterparts on ImageNet or CIFAR, they completely fail on an easier task -MNIST. This is because our pre-training labeled dataset, which mostly consists of natural images, has very few digit images. Meanwhile, our noisy image-text dataset has plenty instances that can teach a model certain optical character recognition skills.</p><p>As will be shown in Section 9, our best experimental results are achieved using a hybrid procedure. First, we pretrain the image encoder on a large labeled dataset, then fix its weights and train the text encoder using the contrastive loss on our image-text dataset. Finally, we finetune both image and text encoders, using our GradAccum technique when needed. In Section 10, we present ablation studies to analyze the effects of pretraining, finetuning, and other alternative training procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Training details</head><p>Labeled data for pretraining. For pretraining (Section 8), we use the JFT dataset. This dataset has been used in previous publications <ref type="bibr" target="#b99">(Zhai et al., 2021;</ref><ref type="bibr">Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b45">Kolesnikov et al., 2020)</ref>, but it has been constantly expanded. The JFT version used in our experiments has 5B images, each of which can be associated to one or multiple labels out of 29K possible classes.</p><p>Data filtering. A problem with training on large auto-curated datasets like ALIGN and JFT is that these datasets might unintentionally contain examples from our test sets. To avoid such contaminations, we filter all instances in our training data that has a structural similarity index (SSIM <ref type="bibr" target="#b91">(Wang et al., 2004)</ref>) of at least 0.5 with any image from our evaluation benchmarks.</p><p>Optimizer. We train our models with our own optimizer called AdaFactorW, adapted from two existing ones: AdaFactor <ref type="bibr" target="#b72">(Shazeer and Stern, 2018)</ref> and AdamW <ref type="bibr" target="#b55">(Loshchilov and Hutter, 2019)</ref>. Specifically, we factorize our second gradient moments like AdaFactor, and decouple the weight decay from all moments like AdamW. To further save memory, we follow <ref type="bibr" target="#b99">Zhai et al. (2021)</ref> and store the first gradient moments in bfloat16. We observe, however, that while we can store these moments in bfloat16, we need to convert them into float32 prior to computing our weight updates to avoid numerical instability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Birdsnap <ref type="formula" target="#formula_0">Caltech101</ref>   Other hyperparameters. For all experiments, we train and evaluate with the image resolution of 224x224. While we can increase this resolution to gain performance <ref type="bibr">Le, 2019, 2021;</ref><ref type="bibr" target="#b85">Touvron et al., 2019;</ref><ref type="bibr">Radford et al., 2021;</ref><ref type="bibr">Jia et al., 2021)</ref>, we choose not to do this and instead, reserve our computational resources for scaling up our model and our batch size. All of our other hyper-parameters can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Results on Image Classification Benchmarks</head><p>We first present the open-vocabulary image classification performance of our BASIC models. We compare our models BASIC-{S,M,L} to CLIP models with similar computational budgets <ref type="bibr">(Radford et al., 2021)</ref> on 17 natural image classification datasets. Details about these datasets can be found in Appendix C.</p><p>Open-vocabulary image classification models require textual prompts, which we take from CLIP <ref type="bibr">(Radford et al., 2021)</ref> for consistent comparison. We suspect that using prompts which are tuned for our models can further improve our results as shown in <ref type="bibr">(Lester et al., 2021)</ref>, because the text sequences in our training data have a different distribution from the text sequences in CLIP. <ref type="table" target="#tab_10">Table 3</ref> shows the comparison. From the table, it can be seen that BASIC models conclusively outperform CLIP models of the same computational budgets. Specifically, BASIC models demonstrate higher accuracy than CLIP models on 13 out of 17 datasets. On the Oxford IIIT Pets dataset, BASIC-L achieves 97.9% mean per-class recall which sets a new state-of-the-art, despite having never seen any training images from the dataset. On ther other hand, BASIC models have low accuracy on EuroSAT, MNIST, and PCam. MNIST is where BASIC models perform worst, where the highest accuracy is only 40.3%. We discuss these failure cases further in Section 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Results on Robustness Benchmarks</head><p>Despite the convincing accuracy of modern deep learning models on ImageNet, concerns have been raised about their robustness <ref type="bibr" target="#b80">(Szegedy et al., 2013)</ref>. These concerns arise from a common failure mode of ImageNettrained models: subtle changes to their input images, which are imperceptible to humans, can wildly alter their predictions with high confidence, e.g., from "golden retriever" into "goldfish".</p><p>In CLIP, <ref type="bibr">Radford et al. (2021)</ref> have studied certain aspects of this failure mode. They have not drawn a definitive conclusion whether to attribute such failures to deep learning, ImageNet, or a combination of them. Instead, they cautioned against generalizing "too far from [their] initial findings".</p><p>Here we advance CLIP's study on the robustness of zero-shot models in two aspects. First, we analyze our BASIC models presented previously in Section 9.2 and reaffirm that zero-shot models are indeed more robust than their ImageNet-trained counterparts. Second, we perform an experiment which suggests that ImageNet's labeled training examples might be responsible for making ImageNet-trained models less robust. Similar to CLIP's authors, we caution readers that our experiment presents a correlation, not a causal analysis. In other words, we do not attribute the lack of robustness in ImageNet-trained models to the dataset.</p><p>More accurate open-vocabulary image classification models are also more robust. We evaluate BASIC-{S,M,L} models from Section 9.2 on 5 robustness benchmarks derived from ImageNet: ImageNet-A <ref type="bibr" target="#b32">(Hendrycks et al., 2021b)</ref>, ImageNet-R <ref type="bibr" target="#b31">(Hendrycks et al., 2021a)</ref>, ImageNet-V2 <ref type="bibr" target="#b67">(Recht et al., 2019)</ref>, ImageNet-Sketch , and ObjectNet <ref type="bibr" target="#b3">(Barbu et al., 2019)</ref>. These benchmarks have images in all or a subset of the 1000 ImageNet classes, but their inputs are selected from certain natural distribution shifts, which can cause ImageNet-trained models to make many more mistakes. Our numerical results are highlighted in <ref type="table" target="#tab_1">Table 1</ref> from Section 1. To visualize the data trend, in <ref type="figure" target="#fig_3">Figure 3</ref> The data points from our BASIC models extend the prediction from CLIP: open-vocabulary image classification models have a higher effective robustness <ref type="bibr">(Radford et al., 2021;</ref><ref type="bibr">Taori et al., 2020)</ref>, i.e. they have higher robustness than ImageNet-trained models with the same ImageNet accuracy. To extrapolate from this trend, we fit a logistic curve (red dashes) to the zero-shot accuracy and robustness of open-vocabulary image classification models. The plot shows that this line meets the ideal robustness line at about 91% on the   <ref type="figure">Figure 4</ref>: Top-1 accuracy of BASIC models on ImageNet and on 5 robustness benchmarks. In all cases, as the BASIC models are trained on more ImageNet labeled data (1%, 10%, 20%, and 50%), their ImageNet accuracy significantly increase, but their accuracy on the robustness benchmarks increase much less, or decrease.</p><p>x-coordinate. In other words, our plot predicts that a model which achieves about 91% zero-shot accuracy on ImageNet, i.e., just slightly better than the state-of-the-art ImageNet-trained model <ref type="bibr">(Dai et al., 2021)</ref>, will also achieve the ideal robustness.</p><p>ImageNet-finetuned models are less robust. We now study the effect of ImageNet's labeled data on our models. We take the converged BASIC-{S,M,L} checkpoints from Section 9.2 and continue to train them on 1%, 10%, 20%, and 50% of ImageNet's labeled examples. Note that we continue training these checkpoints using the contrastive loss, where the names of ImageNet classes are utilized as text sequences accompanying their images. This is different from CLIP's linear probing approach, which we do not perform to avoid potential confounding factors from our study, e.g. linear classifiers might behave differently from our open-vocabulary image classifiers. We then compare the accuracy of these finetuned models on ImageNet and on the 5 robustness benchmarks. The results are visualized in <ref type="figure">Figure 4</ref>. The figure shows a clear trend: as our model learns from more labeled ImageNet data, they become more accurate on ImageNet, but these gains do not carry over to the robustness benchmarks. Specifically, with the exception of ImageNet-V2, for which the accuracy of finetuned models stay the same (for BASIC-L) or slightly increase (for BASIC-M), for all other robustness benchmarks, the finetuned models suffer from significant performance drops. In the extreme case, 3% accuracy gain on ImageNet leads to 8.3% accuracy drop for ImageNet-R.</p><p>What makes our finetuned models less robust? A quick glance at our results might lead to the superficial conclusion that our models have overfit, as our finetuning sets are a lot smaller than ALIGN and JFT. However, this overfitting theory does not explain the trend observed in <ref type="figure">Figure 4</ref>: training on more labeled ImageNet data makes our models less robust. We hope our observation invites further causal analysis on the effects of ImageNet's labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">The Importance of Batch Size Scaling</head><p>To demonstrate the role of large batch sizes, we conduct several controlled experiments for BASIC-S and BASIC-M on ALIGN. For both BASIC-S and BASIC-M, we fix all hyperparameters as shown in <ref type="table" target="#tab_16">Table 6</ref>, but vary the batch size and the number of training steps. Models that are trained with larger batch sizes are trained with fewer steps to guarantee that they "see" the same number of examples. <ref type="table" target="#tab_12">Table 4 presents</ref>     <ref type="table" target="#tab_12">Table 4</ref> and <ref type="figure">Figure 5</ref> both suggest that training for more steps cannot equalize the benefit of large batch sizes. This phenomenon is consistent with the observation from SimCLR <ref type="bibr">(Chen et al., 2020a,b)</ref>: large batch sizes help contrastive learning. SimCLR observes that the benefit of large batch sizes saturate at 8192. In contrast, our results in <ref type="table" target="#tab_12">Table 4</ref> and <ref type="figure">Figure 5</ref> show that lager batch sizes continue to benefit our models until 32768, and even until 65536 as in Section 9.2. We suspect that the benefits for large batch sizes do not saturate because our dataset size and model size are both larger than those of SimCLR, e.g.  We now study the benefits of other scaling dimensions, data and model scaling, on the quality of our models. We also study pretraining as an alternate training procedure to contrastive learning. We train BASIC-{S,M} models in 6 different settings and plot their final top-1 ImageNet accuracy in <ref type="figure" target="#fig_7">Figure 6</ref>. Below, we compare and analyze the settings.</p><p>First, BASIC-S and BASIC-M respectively gain 5.3% and 5.8% accuracy when we expand the contrastive training dataset from ALIGN to ALIGN+JFT. These gains, albeit large, are smaller than the gain by enlarging the model size, e.g., 11.7% when going from BASIC-S to BASIC-M.</p><p>Next, we study the effects of pretraining image encoders on JFT. As can be seen from <ref type="figure" target="#fig_7">Figure 6</ref>, models whose image encoders are pretrained on JFT and whose text encoders are subsequently trained on ALIGN, i.e., the red bars, have similar performances with models trained from scratch on ALIGN+JFT, i.e., the blue bars. Their similar accuracy suggest that the training losses -softmax cross-entropy or contrastive -have a much smaller effect than the datasets. In other words, when given the same dataset, the image encoders in BASIC models learn to become equally good, regardless of their loss functions.</p><p>To our surprise, training the text encoders for JFT-pretrained image encoders on ALIGN+JFT gains 1% for BASIC-S and 1.8% for BASIC-L, compared to training these text encoders on ALIGN. We suspect that these gains come from better representations for the textual prompts, since the models trained on ALIGN+JFT also sees the textual prompts which consist of clean JFT class names. However, this speculation needs a more thorough study to understand.</p><p>Finally, we find that if we take a converged model whose image encoder is pretrained on JFT and whose text encoder is trained on ALIGN+JFT, then we continue to train both its image encoders and text encoders at a small learning rate. This extra training phase gains us 1.4% ImageNet accuracy for BASIC-S, 0.6% for BASIC-M, and 0.4% for BASIC-L (not shown in this section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Limitations</head><p>Despite the strong results of our open-vocabulary image classifier, especially on natural image classification tasks, they inevitably have their shortcomings. In this section, we discuss the problems that we find with our BASIC models.</p><p>Open-vocabulary image classification models do not perform well on test sets that are underrepresented in the training datasets. We emphasize the failures of BASIC on two test sets where BASIC models are much worse than CLIP models: EuroSAT, MNIST, PatchCamelyon (PCam) (see <ref type="table" target="#tab_10">Table 3</ref> from Section 9.2). Here, we summarize that BASIC models fail on MNIST and PCam because our training datasets ALIGN and JFT have relatively few images of handwritten digits and of lymph nodes, which are the domain of these datasets. Compared to MNIST and PCam, BASIC models do better on EuroSAT which consist of satellite land images, but their accuracy is lower than that of CLIP models. This is because the class names for these satellite images are not very descriptive to BASIC models. More analysis for these failures are in Appendix G.</p><p>Open-vocabulary image classification requires prompt engineering. In this paper, we use the prompts from CLIP <ref type="bibr">(Radford et al., 2021)</ref> to make our results comparable to previous works. In Appendix G, we present examples which show that prompts that are badly chosen or adversarially chosen can hurt the accuracy of open-vocabulary image classification models by flipping their predictions. These examples suggest that prompt engineering is an important research topic to make open-vocabulary image classification models robust and reliable, but the topic is of out of the scope of this paper.</p><p>Combined scaling is expensive. As reported in Appendix E, the hardware and training time for our models are not small. Despite the training cost, we can use the models in this paper without any finetuning, and hence avoid the finetuning cost. We hope that future research can reduce our models' training expense, e.g., larger accelerator memory can save the extra re-materialization steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Conclusion</head><p>Open-vocabulary image classification represents a new paradigm where pretrained models can be used directly for downstream applications without collecting any application-specific data. However, in order to become practical for real-world applications, open-vocabulary image classification models need to bridge the accuracy gap to supervised and semi-supervised models.</p><p>In this paper, we presented combined scaling techniques that significantly boost the performance of open-vocabulary image classification models. We show that scaling in the data size, the model size, and the batch size all improves the final model's accuracy and robustness. To overcome the memory limit arising from combined scaling, we devise a simple gradient accumulation method based on re-materialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model sizes</head><p>In our preliminary experiments, we experimented with different model sizes. <ref type="table" target="#tab_11">Table 5</ref> presents the final, most compute-to-performance efficient model sizes, which we use throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image model</head><p>Text model</p><p>Model <ref type="bibr">(Dai et al., 2021)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyperparameters and other implementation details</head><p>Our training and evaluation code will eventually be released. Here, we summarize a few important details. All of our hyper-parameters are in <ref type="table" target="#tab_16">Table 6</ref>.</p><p>No regularization. Other than the decoupled weight decay in AdaFactorW, we do not use any other regularization technique. In fact, we find that with BASIC-S and BASIC-M, if we add other forms of regularization such as stochastic depth <ref type="bibr" target="#b36">(Huang et al., 2017)</ref> or dropout <ref type="bibr" target="#b78">(Srivastava et al., 2014)</ref>, our ImageNet top-1 accuracy drops substantially. This suggests that our datasets are very large and perhaps in such situation, regularization techniques do more harm than good by causing optimization difficulty to our models.</p><p>Another important effect of not using regularization in our training framework is to make the rematerialization steps in Section 4.2 consistent. If we apply random perturbations to our forward passes, e.g. by skipping layers like in stochastic depth or by setting random values to zeros, then two forward passes for re-materialization (see Lines 2-5 and 11-14 in Algorithm 1) will compute two different passes. While we could treat such difference as a form of regularization noise, our early experiment show that with dropout-like regularizations, our training loss stays relatively large throughout the course of training. This observation suggests that the noise causes some optimization difficulty to our models, so we opt not to use any dropout-like regularization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Datasets Details</head><p>Here, we present the details of the datasets which we use to evaluate our BASIC models in Section 9.2. It is worth noting that not all these datasets use the accuracy as the performance metric. This is because these datasets have a certain level of imbalance between their classes, as well as other properties that make them accuracy not the best suitable metric for them. For instance, the dataset Caltech-101 has a class called "Background" which refers to any image that does not belong to its predefined 101 classes. One certainly cannot come up with a textual description that describes this "class". As such, Caltech-101 is evaluated using mean per-class recall. Details about other datasets are in <ref type="table">Table 7</ref>.</p><p>Dataset Reference Abbreviation in <ref type="table" target="#tab_10">Table 3</ref>   <ref type="table">Table 7</ref>: Details of the datasets used in this paper to evaluate BASIC models. The evaluation results are presented in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_10">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Further Discussion on Robustness</head><p>In Section 9.3, we present a surprising result: finetuning converged BASIC checkpoints on more ImageNet labeled data leads to worse robustness results. The metric for robustness in Section 9.3 is the average top-1 accuracy of the finetuned models on 5 robustness benchmarks derived from ImageNet <ref type="bibr">(Hendrycks et al., 2021b,a;</ref><ref type="bibr" target="#b67">Recht et al., 2019;</ref><ref type="bibr" target="#b3">Barbu et al., 2019;</ref>. It turns out that each of these benchmarks can demonstrate slightly different results for the finetuned models. Here, we discuss such benchmarks.</p><p>ImageNet-V2 <ref type="bibr" target="#b67">(Recht et al., 2019)</ref>. This dataset is collected in a process that closely follows the process to collect and annotate the images in the standard ILSVRC-2012 validation set, which is typically referred to as "ImageNet" in the literature (and our paper as well). As such, gains observed on ImageNet often transfer to ImageNet-V2. Recent works such as EfficientNets <ref type="bibr">Le, 2019, 2021)</ref> or ViT <ref type="bibr">(Dosovitskiy et al., 2021)</ref> also demonstrate the similar trend. For our experiment in Section 9.3, BASIC-M's robustness accuracy improves along with its ImageNet accuracy, following this trend. However, BASIC-L's robustness does not.</p><p>We suspect this trend is because BASIC-L's learning capacity is larger than that of BASIC-M, so BASIC-L picks up more "spurious" patterns from ImageNet, making it less robust than BASIC-M.</p><p>ImageNet-R . ImageNet-R is a special robustness dataset in our study. Not only of our BASIC models but also other zero-shot models -CLIP and ALIGN -are more accurate on ImageNet-R than they are on ImageNet (see <ref type="table" target="#tab_1">Table 1</ref>). These data points alone would suggest that ImageNet-R is somewhat easier than ImageNet, until we look at the significant accuracy drops for other methods on ImageNet-R. For instance, Noisy Student  and Meta Pseudo Labels <ref type="bibr">(Pham et al., 2021)</ref> respectively achieve only 74.9% and 72.7% accuracy on ImageNet-R, despite their accuracy of 88.4% and 90.2% on ImageNet ILSVRC-2012. The real reason for such discrepancy in ImageNet-R is that ImageNet-R is collected by selecting the ImageNet classes from visual art pieces, such as paintings, cartoons, graffiti, origami, and sculptures. These art pieces are often displayed in a clean environment, free of noises such as multiple classes per image, making the images easier to recognize. As such, BASIC, CLIP, and ALIGN, all perform better on ImageNet-R. However, ImageNet-R images have a drastically different distribution compared to ImageNet labeled training images, as they are respectively art images and natural images. This is why ImageNet-trained models display a much lower accuracy on ImageNet, compared to zero-shot models.</p><p>The case of ObjectNet <ref type="bibr" target="#b3">(Barbu et al., 2019)</ref>. From <ref type="table" target="#tab_1">Table 1</ref>, it can be seen that BASIC model's improvement over ALIGN and CLIP on Object is significantly lower than others on other benchmarks, i.e., 6.6% compared to more than 8% (except for ImageNet-R, for which the accuracy of all models are saturated at over 90%). We find out the reason is that, even though ObjectNet has images from the same classes with ImageNet, these objects turn out to have their own more descriptive names, e.g. the class name "chairs" in ImageNet could be "chairs by <ref type="bibr">[viewpoint]</ref>" or "chairs with [background]". As we later show in Section G, using different class names and prompts can affect our results. This effect has also been observed in CLIP <ref type="bibr">(Radford et al., 2021)</ref>.</p><p>Here, we take the same class names and prompts for ImageNet and use them for ObjectNet. We suspect that using ObjectNet-specific class names and prompts can improve our result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Computational Cost</head><p>All of our models are implemented in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and trained on Tensor Processing Units (TPUs <ref type="bibr" target="#b42">(Jouppi et al., 2017)</ref>). Our BASIC-S and BASIC-M models are all trained on TPUv3 chips, while our BASIC-L models are trained on TPUv4 chips. These TPUv4 chips in their MegaCore mode can offer 32GB of memory, out of which our BASIC-L models use 30.1GB, which means that our model essentially saturates the TPU's memory. We note that oftentimes, a small portion of TPU memory needs to be reserved for their low-level infra systems. Therefore, our BASIC-L models essentially saturate the accelerators with the largest memory currently available. Given this memory usage, we use Algorithm 1 with the microbatch size M = 8192 and the batch size N = 65536 to train this model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Analysis: Successful Classification Examples</head><p>Open-vocabulary image classification models open the door to versatile applications. This section is dedicated to demonstrating their versatility. In <ref type="figure" target="#fig_8">Figure 7</ref>, we visualize some predictions of our best model, BASIC-L, on instances that are less expected on traditional image classification benchmarks. We come up with the text sequences and demonstrate that the model can indeed align images to the most appropriate sequence.  &lt;1e-3 A shiba inu dog in cold weather.</p><p>&lt;1e-3 A corgi dog in warm weather.</p><p>&lt;1e-3 A shiba inu dog in warm weather.</p><p>&lt;1e-3</p><p>An alarm clock that reads 7:00 pm. ()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.288</head><p>An alarm clock that reads 10:00 am.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.183</head><p>An alarm clock that reads 12:00 pm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.167</head><p>An alarm clock that reads 4:00 pm. 0.162 An alarm clock that reads 7:00 am. 0.133 An alarm clock that reads 2:00 pm. 0.067 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Failure Analysis</head><p>Most machine learning models fail in certain tests. It is important to identify such failure cases, to understand the failing causes, and if possible, to come up with fixes. Here, we first look at the test benchmarks in <ref type="table" target="#tab_10">Table 3</ref> from Section 9.2 where BASIC models perform worse than CLIP models. We identify the cause of failures for BASIC models and recommend certain fixes that can improve their performance. Then, in Section G.2, we present some erroneous behaviors of BASIC models via selected examples. These examples reveal some weaknesses of open-vocabulary image classification models, and invite future research to improve them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 The benchmarks where BASIC fails</head><p>From Section 9.2, we see that BASIC models have particularly low performance on EuroSat <ref type="bibr" target="#b30">(Helber et al., 2018)</ref>, <ref type="bibr">MNIST (LeCun et al., 2010), and</ref><ref type="bibr">Patch Camelyon (Veeling et al., 2018)</ref>. The accuracy of BASIC-L on these datasets are 51.0%, 40.3%, and 59.6% respectively. For what it's worth, BASIC-L's accuracy are better than those of our smaller models, i.e., BASIC-S and BASIC-M, so our central message in this paperscaling helps -is not altered. Here, we focus on analyzing the failures of BASIC-L.</p><p>Patch Camelyon (PCam). PCam is perhaps the most sensitive dataset among the three benchmarks where BASIC-L performs poorly. This dataset consists of images extracted from histopathologic scans of lymph node sections, and models are asked to make the binary prediction -whether an input image has a cancerous lymph node or note. For such an important task, the top-1 accuracy of both BASIC-L (59.6%) and CLIP (63.0%) are far below the bars for practical deployments. We remark that PCam is a binary classification task, so the accuracy of BASIC-L and CLIP are just slightly above random guessing. Their poor performance, however, are quite understandable: classifying lymph nodes requires much more specific training, compared to classifying common natural images. As our training data are weakly crawled and automatically curated from the internet, without any emphasis on medical images, our BASIC-L model cannot learn enough to perform well on PCam. We suspect the same speculation also holds for CLIP, as their data collection and curation process is comparable to ours. Finally, the low accuracy of CLIP and BASIC models on PCam is an assertion that despite the benefits of open-vocabulary image classification models, they are not ready to be deployed to tasks that require in-domain expertise, e.g. medical knowledge.</p><p>EuroSAT. This dataset consists of satellite images taken for certain types of lands. Models are asked to classify input images into one out of 10 given types of lands. The land types can be seen in <ref type="figure" target="#fig_14">Figure 8</ref>. The failure of BASIC-L on EuroSAT is an example for the importance of prompt engineering in open-vocabulary image classification learning for image-text models. In <ref type="figure" target="#fig_14">Figure 8</ref>, we show that by changing the dataset's class names and the model's set of prompts, into words and phrases that essentially have the same meaning to humans, we can improve the accuracy of BASIC-L from 51.0% to 55.7%. We do not further explore the changes in class names and prompts to improve BASIC-L's performance on EuroSAT, as they belong to a different topic from the focus of this paper -combined scaling. However, our findings on this EuroSAT dataset suggests that contrastive image-text models do not really "understand" texts. This is perhaps because of the low quality of the texts in our training data, unlike the millions of words from books and articles like the training data of NLP models such as <ref type="bibr">BERT (Devlin et al., 2018)</ref>.</p><p>MNIST. MNIST is a classical dataset in computer vision for handwritten digit classification. Simple models can achieve more than 99.5% accuracy, and yet BASIC-L achieves the humble 40.3% accuracy. Unlike the case of PCam, i.e. there is not enough training data in our training dataset, for MNIST, we find that the ALIGN dataset has a fair amount of images that contain digits, either handwritten or printed. This means that the image encoder of BASIC-L has seen digit figures, and suggests that the failures might be more attributable to the text encoder, similar to the case of EuroSAT. In <ref type="figure">Figure 9</ref>, we show the confusion matrices of BASIC-L models with three sets of class names: using the digits such as {'0', '1', ...}, using the the texts such as {'one', 'two'', ...}, and using both such as {'0 or zero', '1 or one', ...}. Unfortunately, we cannot improve BASIC-L's accuracy on MNIST, like we did for EuroSAT: BASIC-L's accuracy is low in all three cases, but the confusion matrices are visibly different: BASIC-L models 'thinks' that many digits look like '3' for the digit-only class names, but many digits look like '1 or one' in the digit-and-text class names. Again, humans who understand languages will not make these mistakes. We think these mistakes constitute a new type or robustness failures, which we hope will invite further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Example failure cases</head><p>From the confusion matrices of BASIC-L on two benchmarks, EuroSAT <ref type="bibr" target="#b30">(Helber et al., 2018)</ref> and MNIST <ref type="bibr">(Le-Cun et al., 2010)</ref>, we observe that the prompts and class names are crucial for the performance of zero-shot transfor models. Here, we select and present a few examples to demonstrate the failures of BASIC-L. <ref type="figure">Figure 10</ref> visualizes these examples. Digit only: we use the class names {"0", "1", ..., "9"}; Text only: {"one", "two", ..., "nine"}; Digit and Text: {"0 or zero", "1 or one", ..., "9 or nine"}. The model has vastly different confusion matrices for different class name, suggesting that it does not understand the meaning of these strings, but instead, simply learns to match their embeddings.</p><p>More than 6 kittens in total. () 0.472 More than 4 kittens in total. 0.342 More than 2 kittens in total.</p><p>0.186 More than 6 puppies in total.</p><p>&lt;1e-3 More than 2 puppies in total.</p><p>&lt;1e-3 More than 4 puppies in total.</p><p>&lt;1e-3</p><p>No strawberries found in the photo.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Proofs</head><p>In this appendix, we complete the proof of Theorem 1 and Theorem 2 by gradually analyzing the gap from the general case to the special case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 General case</head><p>Lemma 3 Let F be a set of maps x ? F (x) and G be a set of maps y ? G(y). Then, for any ? &gt; 0, with probability at least 1 ? ?, the following holds for all F ? F and G ? G:</p><formula xml:id="formula_27">E x,y [? M (x, y)] ?? S [? B (x, y)] (5) ? c 1 E x,y [A(x, y)] 1 ? 2B 2 ? 2c 8 c 9 + c 1 ? ln( ? ?B/?) + 2 c 1 sup x?X R B (H x F ,G,e ) + 2R m (H F ,G,? B ) + c 2 ln(2/?) 2m . where H x F ,G,e = {y ? exp(F (x) G(y)) : F ? F, G ? G}, H F ,G,? B = {(x, y) ?? B (x, y) : F ? F, G ? G}, and A(x, y) = exp(F (x) G(y)) 1 B B k=1 exp(F (x) G(? k )) E?[exp(F (x) G(?))]</formula><p>.</p><p>Here,</p><formula xml:id="formula_28">R m (H) := E S,? [sup h?H 1 m m i=1 ? i h(x i , y i )]</formula><p>where ? 1 , . . . , ? m are independent uniform random variables taking values in {?1, 1}.</p><p>Proof We first decompose the difference as follows:</p><formula xml:id="formula_29">E x,y [? M (x, y)] ?? S [? B (x, y)] = E x,y [? M (x, y)] ? E x,y [? B (x, y)] + E x,y [? B (x, y)] ?? S [? B (x, y)] (6) = E x,y [? M (x, y) ?? B (x, y)] + (E x,y [? B (x, y)] ?? S [? B (x, y)]).<label>(7)</label></formula><p>For the inside of the expectation in the first term, we can write it as</p><formula xml:id="formula_30">M (x, y) ?? B (x, y) (8) = exp(F (x) G(y)) 1 B B k=1 exp(F (x) G(? k )) ? exp(F (x) G(y)) E?[exp(F (x) G(?))] (9) = exp(F (x) G(y))E?[exp(F (x) G(?))] ? exp(F (x) G(y))( 1 B B k=1 exp(F (x) G(? k ))) ( 1 B B k=1 exp(F (x) G(? k )))E?[exp(F (x) G(?))] (10) = exp(F (x) G(y)) E?[exp(F (x) G(?))] ? 1 B B k=1 exp(F (x) G(? k )) 1 B B k=1 exp(F (x) G(? k )) E?[exp(F (x) G(?))]<label>(11)</label></formula><p>Using Lemma 4 (see below) with the assumption that? 1 ,? 2 , . . . ,? B iid ? p y and exp(F (x) G(y)) ? c 1 with probability one, we have that for any ? &gt; 0 and x ? X , with probability at least 1 ? ?, the following holds for all F ? F and G ? G:</p><formula xml:id="formula_31">E?[exp(F (x) G(y))] ? 1 B B k=1 exp(F (x) G(? k )) ? 2 sup x?X R B (H x F ,G,e ) + c 1 ln(1/?) 2B ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_32">sup x?X R B (H x F ,G,e ) = sup x?X E y,? sup hx?H x F ,G,e 1 B B i=1 ? i h x (y i ) ,</formula><p>and Let us choose the metric space (M, d) to be the Euclidian space R ? with the Euclidian metric. That is, we have the -covering of X with the Euclidean balls of radius r, with the r-converging number of</p><formula xml:id="formula_33">H x F ,G,e = {y ? exp(F (x) G(y)) : F ? F, G ? G}. Recall that ?(x) = E?[exp(F (x) G(y))] ? 1 B B k=1 exp(F (x) G(? k )) is c 9 -Lipschitz; i.e., |?(x) ? ?(x )| ? c 9 x ? x 2 for all x ? X where x 2 ? c 8 for all X .</formula><formula xml:id="formula_34">N C (r, X ) ? (2c 8 ? ?/r) ? .</formula><p>Thus, by setting r = 2c 8</p><formula xml:id="formula_35">? B , N C (r, X ) ? ( ? ?B) ? .</formula><p>Using these,</p><formula xml:id="formula_36">sup x?X ?(x) = inf c?C sup x?X ?(x) ? ?(c) + ?(c) ? inf c?C sup x?X |?(x) ? ?(c)| + sup c?C ?(c) ? rc 9 + sup c?C ?(c) = 2c 8 c 9 ? B + sup c?C ?(c).</formula><p>Here, using equation 12 with union bounds, we have that for any ? &gt; 0, with probability at least 1 ? ?, the following holds for all F ? F and G ? G:</p><formula xml:id="formula_37">sup c?C ?(c) ? 2 sup x?X R B (H x F ,G,e ) + c 1 ln( ? ?B) ? /?) 2B ? 2 sup x?X R B (H x F ,G,e ) + c 1 ? ln( ? ?B/?) 2B</formula><p>.</p><p>Therefore, for any ? &gt; 0, with probability at least 1 ? ?, the following holds for all F ? F and G ? G:</p><formula xml:id="formula_38">sup x?X ?(x) ? 2 sup x?X R B (H x F ,G,e ) + 2c 8 ? B + c 1 ? ln( ? ?B/?) 2B (13) = 2 sup x?X R B (H x F ,G,e ) + 1 ? 2B 2 ? 2c 8 c 9 + c 1 ? ln( ? ?B/?) .</formula><p>Combining equations equation 11 and equation 13 with union bound, we have that for any ? &gt; 0, with probability at least 1 ? ?,</p><formula xml:id="formula_39">M (x, y) ?? B (x, y) (14) = exp(F (x) G(y)) E?[exp(F (x) G(y))] ? 1 B B k=1 exp(F (x) G(? k )) 1 B B k=1 exp(F (x) G(? k )) E?[exp(F (x) G(?))]<label>(15)</label></formula><formula xml:id="formula_40">? exp(F (x) G(y)) 1 ? 2B 2 ? 2c 8 + c 1 ? ln(c 9 ? ?B/?) + 2 sup x?X R B (H x F ,G,e ) 1 B B k=1 exp(F (x) G(? k )) E?[exp(F (x) G(?))] .<label>(16)</label></formula><p>By defining</p><formula xml:id="formula_41">A(x, y) = exp(F (x) G(y)) 1 B B k=1 exp(F (x) G(? k )) E?[exp(F (x) G(?))]</formula><p>, we have that for any ? &gt; 0, with probability at least 1 ? ?,</p><formula xml:id="formula_42">E x,y [? M (x, y) ?? B (x, y)] (17) ? c 1 E x,y [A(x, y)] 1 ? 2B 2 ? 2c 8 c 9 + c 1 ? ln( ? ?B/?) + 2 c 1 sup x?X R B (H x F ,G,e ) .</formula><p>For the second term, using Lemma 4 with the assumption that? B (x, y) ? c 2 for (x, y) ? p (x,y) , we have that for any ? &gt; 0, with probability at least 1 ? ?, the following holds for all F ? F and G ? G:</p><formula xml:id="formula_43">E x,y [? B (x, y)] ?? S [? B (x, y)] (18) = E x,y [? B (x, y)] ? 1 m m i=1 ? B exp(F (x i ) G(? i )) B k=1 exp(F (x i ) G(? k )) ? 2R m (H F ,G,? B ) + c 2 ln(1/?) 2m ,<label>(19)</label></formula><p>where</p><formula xml:id="formula_44">H F ,G,? B = {(x, y) ?? B (x, y) : F ? F, G ? G}.</formula><p>Combining equations equation 7, equation 17, and equation 18 with union bound, we have that for any ? &gt; 0, with probability at least 1 ? ?, the following holds all F ? F and G ? G:</p><formula xml:id="formula_45">E x,y [? M (x, y)] ?? S [? B (x, y)] ? c 1 E x,y [A(x, y)] 1 ? 2B 2 ? 2c 8 c 9 + c 1 ? ln( ? ?B/?) + 2 c 1 sup x?X R B (H x F ,G,e ) + 2R m (H F ,G,? B ) + c 2 ln(2/?) 2m .</formula><p>The proof of Lemma 3 partially builds up on Lemma 4 below. Lemma 4 is a direct application of previous results <ref type="bibr" target="#b4">(Bartlett and Mendelson, 2002;</ref><ref type="bibr" target="#b59">Mohri et al., 2012;</ref><ref type="bibr" target="#b71">Shalev-Shwartz and Ben-David, 2014)</ref> to our problem. We provide a proof of Lemma 4 by slightly modifying the proof of a previous work <ref type="bibr" target="#b59">(Mohri et al., 2012</ref>, Theorem 3.1) for the completeness (the proof utilizes the nonnegativity of h to have a slightly tighter bound than Theorem 26.5 of <ref type="bibr" target="#b71">Shalev-Shwartz and Ben-David, 2014)</ref>:</p><p>Lemma 4 Let H be a set of maps z ? h(z) such that h(z) ? [0, ?] for all z in its domain. Then, for any ? &gt; 0, with probability at least 1 ? ? over an i.i.d. draw of m i.i.d. samples (z i ) m i=1 , the following holds for all maps h ? H:</p><formula xml:id="formula_46">E z [h(z)] ? 1 m m i=1 h(z i ) + 2R m (H) + ? ln(1/?) 2m ,<label>(20)</label></formula><formula xml:id="formula_47">where R m (H) := E (z 1 ,...,zm),? [sup h?H 1 m m i=1 ? i h(z i )]</formula><p>where ? 1 , . . . , ? m are independent uniform random variables taking values in {?1, 1}.</p><formula xml:id="formula_48">Proof Let S = (z i ) m i=1 and S = (z i ) m i=1 . Define ?(S) = sup h?H E x,y [h(z)] ? 1 m m i=1 h(z i ).<label>(21)</label></formula><p>To apply McDiarmid's inequality to ?(S), we compute an upper bound on |?(S) ? ?(S )| where S and S be two test datasets differing by exactly one point of an arbitrary index i 0 ; i.e., S i = S i for all i = i 0 and S i 0 = S i 0 . Then,</p><formula xml:id="formula_49">?(S ) ? ?(S) ? sup h?H h(z i 0 ) ? h(z i 0 ) m ? ? m .<label>(22)</label></formula><p>Thus, by McDiarmid's inequality, for any ? &gt; 0, with probability at least 1 ? ?,</p><formula xml:id="formula_50">?(S) ? E S [?(S)] + ? ln(1/?) 2m 1 m m i=1 ? i h(z i )) = 2R m (H)<label>(28)</label></formula><p>where the fist line follows the definitions of each term, the second line uses the Jensen's inequality and the convexity of the supremum, and the third line follows that for each ? i ? {?1, +1}, the distribution of each term ? i (h(z i ) ? h(z i )) is the distribution of (h(z i ) ? h(z i )) since S and S are drawn iid with the same distribution. The forth line uses the subadditivity of supremum.</p><formula xml:id="formula_51">H.1.1 ANALYZING sup x?X R B (H x F ,G,e ) AND R m (H F ,G,? B )</formula><p>The bound in Lemma 3 contains two complex terms, sup x?X R B (H x F ,G,e ) and R m (H F ,G,? B ), that are challenging to interpret and to be further analyzed. The following two lemmas bound those two terms by more interpretable quantities:</p><p>Lemma 5 Let F be a set of maps x ? F (x) and G be a set of maps y ? G(y). Then,</p><formula xml:id="formula_52">sup x?X R B (H x F ,G,e ) ? c 1 c 3 E y,? sup G?G 1 B B i=1 ? i G(y i ) 2 .</formula><p>Proof Since the derivative of exponential function exp(q) is exp(q) and we assume exp(F (x) G(y)) ? c 1 , the exponential function in the bounded domain of exp(F (x) G(y)) ? c 1 has Lipschitz constant of c 1 .</p><p>Therefore,</p><formula xml:id="formula_53">E y,? sup hx?H x F ,G,e 1 B B i=1 ? i h x (y i ) = E y,? sup F ?F ,G?G 1 B B i=1 ? i exp(F (x) G(y i )) ? c 1 E y,? sup F ?F ,G?G 1 B B i=1 ? i F (x) G(y i ) = c 1 B E y,? sup F ?F ,G?G F (x) B i=1 ? i G(y i ) . ? c 1 B E y,? sup F ?F ,G?G F (x) 2 B i=1 ? i G(y i ) 2 ? c 1 B sup F ?F F (x) 2 E y,? sup G?G B i=1 ? i G(y i ) 2 = c 1 sup F ?F F (x) 2 E y,? sup G?G 1 B B i=1 ? i G(y i ) 2 .</formula><p>Therefore, We now combine the above lemmas to complete the proof of Theorem 2:</p><p>Proof [Proof of Theorem 2] From Lemma 3, we have that for any for any ? &gt; 0, with probability at least 1 ? ?, the following holds for all F ? F and G ? G: </p><formula xml:id="formula_54">E x,</formula><formula xml:id="formula_55">1 B B i=1 ? i G(y i ) 2 + 2 ? 2c 4 c 2 5 + c 2 6 D k=1 (R m (F k ) + R m (G k )) + c 2 ln(2/?) 2m = C 1 ? 2B + C 2 R B (G) + C 3 D k=1 (R m (F k ) + R m (G k )) + c 2 ln(2/?) 2m . H.2 Bounding E y,? sup G?G 1 B B i=1 ? i G(y i ) 2</formula><p>and D k=1 (R m (F k ) + R m (G k )) for the special case with deep neural networks</p><p>We now want to bound E y,? sup G?G m i=1 ? i G(y i ) 2 and D k=1 (R m (F k ) + R m (G k )) in the case where F and G represent deep neural networks. We consider standard deep neural networks, of the form</p><formula xml:id="formula_56">G(y) = (? L ? ? L?1 ? ? L?1 ? ? L?2 ? ? ? ? 1 ? ? 1 )(y) F (x) = (? L ? ? L ?1 ? ? L ?1 ? ? L ?2 ? ? ? ? 1 ? ? 1 )(x)</formula><p>where ? l (q) = W l q and ? l is an element-wise activation function. Similarly, ? l (q) = W l q and ? l is an element-wise activation function. </p><formula xml:id="formula_57">( L l=1 M l ) ? B . Proof Since B i=1 ? i G(y i ) 2 = B i=1 ? i W L (? L?1 ? ? L?1 ? ? L?2 ? ? ? ? 1 ? ? 1 )(y) 2 ? W L F B i=1 ? i (? L?1 ? ? L?1 ? ? L?2 ? ? ? ? 1 ? ? 1 )(y) 2 ,</formula><p>the proof steps of Theorem 1 of <ref type="bibr" target="#b25">(Golowich et al., 2018)</ref> work to bound E ? sup G?G B i=1 ? i G(y i ) 2 . Therefore, using the proof of Theorem 1 of <ref type="bibr" target="#b25">(Golowich et al., 2018)</ref>, This proves the first statement. For the second statement, since (W l ) k F ? M L,k , we have that</p><formula xml:id="formula_58">E ? sup G?G 1 B B i=1 ? i G(y i ) 2 ?<label>c</label></formula><formula xml:id="formula_59">W L 2 F = D k=1 (W L ) k 2 F ? D k=1 M 2 L,k .</formula><p>This implies that W l F ? D k=1 M 2 L,k . Thus, using Lemma 7,</p><formula xml:id="formula_60">E y,? sup G?G 1 B B i=1 ? i G(y i ) 2 ? c 7 ( 2 log(2)L + 1)( L?1 l=1 M l ) D k=1 M 2 L,k ? B .</formula><p>H.3 Combining all together for the special case with deep neural networks</p><p>We now combine the above lemmas to complete the proof of Theorem 1 for the special case with deep neural networks:</p><p>Proof [Proof of Theorem 1] From Lemma 3, for any F and G, and for any ? &gt; 0, with probability at least 1 ? ?, the following holds for all F ? F and G ? G: (R m (F k ) + R m (G k )) .</p><p>Finally, using Lemma 8 for the particular F and G with deep neural networks, we have that Combining those, we have that for any ? &gt; 0, with probability at least 1 ? ?, the following holds for all F ? F and G ? G: </p><formula xml:id="formula_61">E x,y [? M (x, y)] ?? S [? B (x, y)] ? c 1 E x,y [A(x, y)] 1 ? 2B 2 ? 2c 8 c 9 + c 1 ? ln( ? ?B/?) + 2c 3 E y,? sup G?G 1 B B i=1 ? i G(y i ) 2 + 2 ?<label>2c</label></formula><formula xml:id="formula_62">( L ?1 l=1 M l ) D k=1 M L ,k ? m + c 2 ln(2/?) 2m ? Q 1 ? m + Q 2 ? 2B + c 2 ln(2/?) 2m</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Generic rematerialization map for the blocks in our CoAtNet models. Left: in the Mobile Inverse Convolution blocks (MBConv), all batch normalization layers and activation layers, as well as all layers in the squeeze and excitation steps, are rematerialized. Right: in Transformer blocks, only the layer normalization layers and activation layers are rematerialized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Proof</head><label></label><figDesc>The proof is completed in Appendix H. Similarly to Theorem 1, Theorem 2 shows that the gap E x,y [? M (x, y)] ?? S [? B (x, y)] decreases as the number of samples m and the contrastive batch size B increase, at the rate of O(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, we plot the accuracy of zero-shot models -BASIC, CLIP (Radford et al., 2021), and ALIGN (Jia et al., 2021) -and of 200 ImageNet-trained models collected by Taori et al. (2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Top-1 accuracy on ImageNet vs. average top-1 accuracy on 5 robustness benchmarks. Zero-shot models (red stars and yellow rhombuses) have significantly higher effective robustness (Taori et al., 2020) compared to ImageNet-trained models (blue dots).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the ImageNet top-1 zero-shot accuracy of all models at the end of their training, and Figure 5 visualizes their entire validation accuracy curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>ALIGN with 1.7B examples compared to ImageNet with 1M examples, and BASIC-{S, M} compared to ResNet-{50,101,152}. This comparison suggests the benefits of our method -combined scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Break-down contributions of data scaling and model scaling for BASIC-S and BASIC-M. Shown are the ImageNet top-1 accuracy of our BASIC-{S,M} models under different training settings. Models trained from scratch on ALIGN+JFT has almost the same performance with models pretrained on JFT and then finetuned on ALIGN or on ALIGN+JFT. Models that are pretrained and then have both their image and text encoders finetuned reach the highest accuracy. Figure best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Selected classification examples from BASIC-L over unseen images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>(Radford et al., 2021)  Prompts we find 'a centered satellite photo of {}.', 'a centered satellite photo of {}' 'a centered satellite photo of a {}.', 'a satellite photo of {}' 'a centered satellite photo of the {}.', 'a photo of {} taken from a satellite' 'a photo of {} taken from the sky' 'a picture of {} taken from the sky by a satellite' 'a picture of {} taken by a satellite' 'a picture of {} taken by a satellite in space' 'a picture of {} taken by a satellite in its orbit' Confusion matrices of BASIC-L on the EuroSAT classification dataset<ref type="bibr" target="#b30">(Helber et al., 2018)</ref>. Shown are the confusion matrices obtained from open-vocabulary image classification from BASIC-L, using prompts and class names and CLIP, compared to the same model using prompts and class names that we tuned. The zero-shot top-1 accuracy with our prompts and class names are 4.7% higher, and the confusion matrix illustrates this by showing more concentration on the diagonal. Confusion matrices of BASIC-L's predictions on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>0342 Figure 10 :</head><label>034210</label><figDesc>saber on the left and a red light saber on the right. () 0.297 a blue light saber on the right and a red light saber on the left. 0.235 a red light saber on the left and a blue light saber on the right. 0.205 a red light saber on the right and a blue light saber on the left. 0.173 a red light saber to the right of a blue light saber. 0.054 a red light saber to the left of a blue light saber. 0.Selected failure cases for BASIC-L over unseen images. (1) The first block indicates that the model is not precise in object counting and does not well handle negation in the prompts, possibly due the nature of our training data. (2) The middle block shows two examples to indicate that prompt engineering can play a critical role in providing the model with sufficient context to produce the desired output. (3) The last block shows that the model does not have the sense of left and right, which is a relic of random left-right flips of images which we apply during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>For any metric space (M, d) and subset M ? M, the (closed) ball of radius r at centered at c is denoted by B (M,d) [c, r] = {x ? M : d(x, c) ? r}, and the r-converging number of M with the covering C is defined by N C (r, M ) = min |C| : C ? M, M ? ? c?C B (M,d) [c, r] }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Let F be a set of maps x ? F (x) and G be a set of maps y ? G(y). Then,R m (H F ,G,? B ) (F k ) + R m (G k )) . where F k = {x ? F (x) k : F ? F} and G k = {y ? G(y) k : G ? G}.Proof Recall thatH F ,G,? B = {(x, y) ?? B (x, y) : F ? F, G ? G}Using the definitions, R m (H F ,G,? B ) = E (x,y),? , q) 2 ? c 4 c 2 5 + c 2 6 . 2 COMBINING ALL TOGETHER FOR THE GENERAL CASE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Lemma 7</head><label>7</label><figDesc>Suppose that the function ? l is 1-Lipschitz and positive homogeneous for all l ? [L ? 1] and y 2 ? c 7 for all y ? Y. Let G = {y ? G(y) : (?l ? [L])[ W l F ? M l ]}. Then,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Lemma 8 Proof</head><label>8</label><figDesc>7 ( 2 log(2)L + 1)( L l=1 M l ) ? B Suppose that the function ? l is 1-Lipschitz and positive homogeneous for all l ? [L ? 1] andx ? c 8 for all x ? X . Let F = {x ? F (x) : (?l ? [L ?1])[ W l F ? M l ]? (W l ) k F ? M L ,k } where (W l ) k is the k-th row of W l . Suppose that the function ? l is 1-Lipschitz and positive homogeneous for all l ? [L ? 1] and y ? c 7 for all y ? Y. Let G = {y ? G(y) : (?l ? [L ? 1])[ W l F ? M l ] ? (W L ) k F ? M L,k } where (W l ) k is the k-th row of W l . Then, D k=1 (R m (F k ) + R m (G k )) From Theorem 1 of<ref type="bibr" target="#b25">(Golowich et al., 2018)</ref>, we have thatR m (F k ) ? c 8 ( 2 log(2)L + 1)( L ?1 l=1 M l )M L ,k ? m and R m (G k ) ? c 7 ( 2 log(2)L + 1)( (F k ) + R m (G k )) ? D k=1 c 7 ( 2 log(2)L + 1)( L?1 l=1 M l )M L,k ? m + c 8 ( 2 log(2)L + 1)( L ?1 l=1 M l )M L ,k ? m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>E</head><label></label><figDesc>x,y [? M (x, y)] ?? S [? B (x, y)] ? c 1 E x,y [A(x, y)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(F k ) + R m (G k )) + c 2 ln(2/?) 2m ? c 1 E x,y [A(x, y)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The Memory Bottleneck for Image-Text Contrastive Learning . . . . . . . . . . . . . . . . 7 4.2 Modifying Pipelining and GradAccum for the Contrastive Loss . . . . . . . . . . . . . . . . 8 5 Batch Size Scaling with the Single-Program Multiple-Data (SPMD) Scheme 10 5.1 Weight Sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.2 Rematerialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.3 Comparison with Pipelining and Gradient Accumulation . . . . . . . . . . . . . . . . . . . 13 6 Theoretical Insight on the Role of Contrastive Batch Size 14 7 Data and Model Scaling 17 7.1 Larger image-text dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 7.2 Larger Model Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Training details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 9.2 Results on Image Classification Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . 19 9.3 Results on Robustness Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 10 Ablation Study 21 10.1 The Importance of Batch Size Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 10.2 Data Scaling, Model Scaling, and Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . 22 The benchmarks where BASIC fails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 G.2 Example failure cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 H Proofs 36 H.1 General case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 H.1.1 Analyzing sup x?X R B (H x F ,G,e ) and R m (H F ,G,? B ) . . . . . . . . . . . . . . . . . . 40 H.1.2 Combining all together for the general case . . . . . . . . . . . . . . . . . . . . . . 43 H.2 Bounding E y,? sup G?G (F k ) + R m (G k )) for the special case with deep neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 H.3 Combining all together for the special case with deep neural networks . . . . . . . . . . . . 46</figDesc><table><row><cell>Contents G Failure Analysis</cell><cell>32</cell></row><row><cell>1 Introduction 2 Related Work 3 Background on Image-text Contrastive Learning and Open-Vocabulary Image Classification 4 Batch Size Scaling with Pipelining and Gradient Accumulation 4.1 8 Pretraining and Finetuning G.1 1 B B i=1 ? i G(y i ) 2 and D k=1 (R m</cell><cell>4 5 7 7 17</cell></row><row><cell>9 Experiments</cell><cell>18</cell></row><row><cell>9.1 11 Limitations</cell><cell>23</cell></row><row><cell>12 Conclusion</cell><cell>23</cell></row><row><cell>A Model sizes</cell><cell>29</cell></row><row><cell>B Hyperparameters and other implementation details</cell><cell>29</cell></row><row><cell>C Evaluation Datasets Details</cell><cell>30</cell></row><row><cell>D Further Discussion on Robustness</cell><cell>30</cell></row><row><cell>E Computational Cost</cell><cell>31</cell></row><row><cell>F Qualitative Analysis: Successful Classification Examples</cell><cell>31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Highlights of our key results. Shown are the top-1 accuracy of our method, BASIC, and similar baselines -CLIP and ALIGN -on ImageNet and other robustness test sets. None of these models have learned from any labeled training example in ImageNet. On average, BASIC surpasses the baselines by the significant 10.1 percentage points.</figDesc><table><row><cell>BASIC (ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>When our accelerator memory can only hold M B examples, GradAccum splits the batch of B examples into smaller batches with at most M examples, called microbatches, then computes the gradients of the microbatches, and averages them.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Comparison between our SPMD programs and our Pipelining &amp; GradAccum programs. Shown are the step times and memory footprints of our small-sized and medium-sized models at different batch sizes. With the same model size and batch size, our SPMD design results in a larger device memory footprint and Pipelining &amp; GradAccum, but SPMD is faster.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>y)] approaches zero as the number of samples m and the contrastive batch size B increase, at the rate of O( 1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Performances of BASIC and CLIP models(Radford et al., 2021)  on 17 image classification benchmarks. The first two blocks compare models of similar numbers of weights and FLOPs. The last block compares the largest CLIP and BASIC models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Figure 5 :</head><label>5</label><figDesc>ImageNet held-out validation accuracy curves with different batch sizes. Models with smaller batch sizes are trained for more steps to ensure a fair comparison. The comparison shows that despite seeing the same number of training examples, models with larger batch sizes reach higher performances than models with more training steps. Image best viewed in color.</figDesc><table><row><cell cols="4">Batch size Steps BASIC-S BASIC-M</cell></row><row><cell>4096</cell><cell>800K</cell><cell>55.6</cell><cell>64.8</cell></row><row><cell>8192</cell><cell>400K</cell><cell>57.6</cell><cell>67.7</cell></row><row><cell>16384</cell><cell>200K</cell><cell>58.8</cell><cell>69.4</cell></row><row><cell>32768</cell><cell>100K</cell><cell>59.3</cell><cell>70.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4 :</head><label>4</label><figDesc>Top</figDesc><table><row><cell>-1 ImageNet accuracy at the end</cell></row><row><cell>of the training for our BASIC-{S,M} models</cell></row><row><cell>trained with different batch sizes and num-</cell></row><row><cell>bers of training steps. All models are trained</cell></row><row><cell>for the same number of epochs, but models</cell></row><row><cell>trained with larger batch sizes has a higher</cell></row><row><cell>accuracy.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 5 :</head><label>5</label><figDesc>Model sizes. For the image models, all specifications can be found from the model names inDai et al. (2021).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters all of our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 8</head><label>8</label><figDesc>summarizes the training cost for each phase of our models BASIC-{S,M,L} as in Section 9.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 8 :</head><label>8</label><figDesc>Computational usages to train our models. Core?Days is the product of the number of training days and the number of cores used to train the models. For instance, using 2048 TPUs in 1 day equals to 2.048 Cores?Days. We use this metric because sometimes, our jobs are run on different numbers of TPUs due to limited availability.</figDesc><table><row><cell>Eggs with mixed expressions. ()</cell><cell>0.944</cell><cell>One plus one equals three. ()</cell><cell>0.468</cell></row><row><cell>Emojis with mixed expressions.</cell><cell>0.032</cell><cell>One plus one equals one.</cell><cell>0.264</cell></row><row><cell>Happy eggs.</cell><cell>0.022</cell><cell>One plus one equals two.</cell><cell>0.240</cell></row><row><cell>Sad eggs.</cell><cell>0.002</cell><cell>One minus one equals three.</cell><cell>0.014</cell></row><row><cell>Happy emojis.</cell><cell>&lt;1e-3</cell><cell>One minus one equals two.</cell><cell>&lt;1e-3</cell></row><row><cell>Sad emojis.</cell><cell>&lt;1e-3</cell><cell>One minus one equals one.</cell><cell>&lt;1e-3</cell></row><row><cell>Cosplayed pikachu. () Cosplayed charmander. Real pikachu. Cosplayed eevee. Real charmander. Real eevee.</cell><cell>0.764 0.219 0.012 0.006 &lt;1e-3 &lt;1e-3</cell><cell cols="2">Chemistry equation on a white-board. () Math equation on a whiteboard. Physics equation on a whiteboard. 0.012 0.958 0.024 Chemistry equation on a paper. 0.005 Math equation on a paper. &lt;1e-3 Physics equation on a paper. &lt;1e-3</cell></row><row><cell cols="2">A shirari dog in cold weather. () 0.961</cell><cell></cell><cell></cell></row><row><cell>A shirari dog in warm weather.</cell><cell>0.038</cell><cell></cell><cell></cell></row><row><cell>A corgi dog in cold weather.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>() 0.393 No blueberries found in the photo. 0.304 No bananas found in the photo. 0.297 No coconuts found in the photo. 0.003 No pineapples found in the photo. 0.002 No oranges found in the photo.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.001</cell></row><row><cell>Closed road. ()</cell><cell>0.716</cell><cell></cell><cell cols="2">Traffic sign indicating intersection. () 0.927</cell></row><row><cell>Slippery road.</cell><cell>0.170</cell><cell></cell><cell cols="2">Traffic sign indicating closed road.</cell><cell>0.027</cell></row><row><cell>Intersection.</cell><cell>0.076</cell><cell></cell><cell cols="2">Traffic sign indicating sharp left.</cell><cell>0.027</cell></row><row><cell>Stop.</cell><cell>0.034</cell><cell></cell><cell cols="2">Traffic sign indicating sharp right.</cell><cell>0.016</cell></row><row><cell>Sharp left.</cell><cell>0.003</cell><cell></cell><cell cols="2">Traffic sign indicating slippery road.</cell><cell>0.003</cell></row><row><cell>Sharp right.</cell><cell>0.002</cell><cell></cell><cell>Traffic sign indicating stop.</cell><cell>&lt;1e-3</cell></row><row><cell cols="2">red light saber. ()</cell><cell>0.992</cell><cell cols="2">blue light saber. () 0.990</cell></row><row><cell cols="2">blue light saber.</cell><cell>0.005</cell><cell>red light saber.</cell><cell>0.004</cell></row><row><cell>red led light.</cell><cell></cell><cell>0.002</cell><cell>blue led light.</cell><cell>0.003</cell></row><row><cell>red neon light.</cell><cell></cell><cell>&lt;1e-3</cell><cell>blue neon light.</cell><cell>0.003</cell></row><row><cell>blue led light.</cell><cell></cell><cell>&lt;1e-3</cell><cell>red led light.</cell><cell>&lt;1e-3</cell></row><row><cell cols="2">blue neon light.</cell><cell>&lt;1e-3</cell><cell>red neon light.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>y [? M (x, y)] ?? S [? B (x, y)]Then by using Lemma 5 and 6,E x,y [? M (x, y)] ?? S [? B (x, y)] ? c 1 E x,y [A(x, y)] 1 ? 2B 2 ? 2c 8 c 9 + c 1 ? ln( ? ?B/?) + 2c 3 E y,? sup</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>G?G</cell></row><row><cell>? c 1 E x,y [A(x, y)]</cell><cell>1 ? 2B</cell><cell>2</cell><cell>?</cell><cell cols="2">2c 8 c 9 + c 1 ? ln( ?</cell><cell>?B/?) +</cell><cell>2 c 1</cell><cell>sup x?X</cell><cell>R B (H x F ,G,e )</cell></row><row><cell cols="2">+ 2R m (H F ,G,? B ) + c 2</cell><cell cols="3">ln(2/?) 2m</cell><cell>.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Revisiting resnets: Improved training and scaling strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><surname>Woo Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1059" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning the best pooling strategy for visual semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedald. Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Analysis of Single Layer Networks in Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<editor>CVPR, 2021. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Virtex: Learning visual representations from textual annotations</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>VOC2007) Results</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename><surname>Rob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Size-independent sample complexity of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference On Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="297" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Griewank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Walther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="45" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<title level="m">Natural adversarial examples. CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-to-word transformation based on dividing and vector quantizing images with words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Mori Hironobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hironobu</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Oka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Citeseer</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Checkmate: Breaking the memory wall with optimal tensor rematerialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alek</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshit</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diemthu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maire</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Nix ; Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doe</forename><forename type="middle">Hyun</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle</pubPlace>
		</imprint>
	</monogr>
	<note>In-datacenter performance analysis of a tensor processing unit. Arxiv 1704.04760</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>Arxiv 1411.2539</idno>
		<editor>ICLR, 2015. Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient rematerialization for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoya</forename><surname>Svitkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Vee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist,2" />
	</analytic>
	<monogr>
		<title level="m">AAAI, 2008. Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning visual n-grams from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Aligning visual regions and textual concepts for semantic-grounded image representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<editor>ICLR, 2019. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Decoupled weight decay regularization</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A vector-contraction inequality for rademacher complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fine-grained visual textual alignment for cross-modal retrieval using transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Marchand-Maillet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Foundations of machine learning</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate o(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>Arxiv 1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop in Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Meta pseudo labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning visual representations with caption annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Bulent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Sch?nfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Understanding machine learning: From theory to algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno>Arxiv 1804.04235</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<idno>1811.02084</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamsa</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osbert</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>Arxiv 1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>Arxiv 1312.6199</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Measuring robustness to natural distribution shifts in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021. Mingxing Tan, Ruoming Pang</title>
		<editor>CVPR, 2020. Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Rotation equivariant CNNs for digital pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quynh</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2017. Jianxiong Xiao</title>
		<meeting><address><addrLine>James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Maggioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04663</idno>
		<title level="m">General and scalable parallelization for ml computation graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Volo: Vision outlooker for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno>2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong ; Yasuhide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contrastive learning of medical visual representations from paired images and text. Arxiv</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">747</biblScope>
		</imprint>
	</monogr>
	<note>CVPR, 2017. Yuhao Zhang, Hang Jiang,</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Using a vector-contraction inequality, i.e., Corollary 4 of (Maurer, 2016) with the additional expectation of both sides of the inequality</title>
		<imprint/>
	</monogr>
	<note>we have that</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
