<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BDANet: Multiscale Convolutional Neural Network with Cross-directional Attention for Building Damage Assessment from Satellite Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chen</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Pan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jianyu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Liang</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Qian</forename><surname>Du</surname></persName>
						</author>
						<title level="a" type="main">BDANet: Multiscale Convolutional Neural Network with Cross-directional Attention for Building Damage Assessment from Satellite Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Building damage assessment</term>
					<term>convolutional neu- ral network (CNN)</term>
					<term>satellite image</term>
					<term>multi-scale feature fusion</term>
					<term>cross-directional attention</term>
					<term>CutMix</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fast and effective responses are required when a natural disaster (e.g., earthquake, hurricane, etc.) strikes. Building damage assessment from satellite imagery is critical before relief effort is deployed. With a pair of pre-and postdisaster satellite images, building damage assessment aims at predicting the extent of damage to buildings. With the powerful ability of feature representation, deep neural networks have been successfully applied to building damage assessment. Most existing works simply concatenate pre-and post-disaster images as input of a deep neural network without considering their correlations. In this paper, we propose a novel two-stage convolutional neural network for Building Damage Assessment, called BDANet. In the first stage, a U-Net is used to extract the locations of buildings. Then the network weights from the first stage are shared in the second stage for building damage assessment. In the second stage, a two-branch multi-scale U-Net is employed as backbone, where pre-and post-disaster images are fed into the network separately. A cross-directional attention module is proposed to explore the correlations between pre-and post-disaster images. Moreover, CutMix data augmentation is exploited to tackle the challenge of difficult classes. The proposed method achieves stateof-the-art performance on a large-scale dataset -xBD. The code is available at https://github.com/ShaneShen/BDANet-Building-Damage-Assessment.</p><p>Index Terms-Building damage assessment, convolutional neural network (CNN), satellite image, multi-scale feature fusion, cross-directional attention, CutMix</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>N ATURAL disasters, such as earthquakes, floods and tsunami, can cause serious social and economic devastation. When a natural disaster strikes, accurate and immediate responses are required in Humanitarian Assistance and Disaster Response (HADR) for saving thousands of lives <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Before these responses, rescue planning and preparations are conducted based on damage analysis. With the rapid development of remote sensing technology, high resolution satellite images are now available for damage assessment. Traditionally, these images of disaster areas are analyzed by experts, which may be time-consuming if the areas are large. Therefore, automatic information extraction from satellite images, such as building segmentation and damage assessment, is imperative under time-critical situations.</p><p>Building damage assessment plays a pivotal role in HADR, which aims at predicting the damage level for each pixel based on building segmentation. It can be divided into two categories according to the use of pre-disaster images. For the methods only using post-disaster images, the damage assessment is considered as a segmentation task <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. However, without pre-disaster images, building assessment errors may be inevitable since post-disaster images cannot provide precise contours of intact building objects. With pre-disaster images, well-shaped buildings can be utilized for damage assessment. Note the problem of damage assessment with paired images shares some similarities with change detection, as both tasks aim to find the changed regions according to two images acquired at different time <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Compared with change detection, building damage assessment is a more challenging task because it is required to classify different damage levels. Moreover, the unchanged objects (undamaged buildings) are required in building damage assessment, which is not considered in change detection.</p><p>Recently, deep learning-based methods, such as convolutional neural networks (CNN), have shown their effectiveness in various tasks. With a set of labeled data, deep learning-based methods can automatically learn image feature representations from low level to high level, without selecting hand-crafted image features. Many researchers have leveraged deep neural networks for building damage assessment and achieved significant progress <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Xu et al. <ref type="bibr" target="#b10">[11]</ref> investigated the capability of CNN for building damage detection by identifying damaged and undamaged buildings. Weber et al. <ref type="bibr" target="#b11">[12]</ref> considered building damage assessment as a semantic segmentation task, in which damage levels are assigned to different class labels.</p><p>With a pair of pre-and post-disaster images for building damage assessment, a key question is how to effectively model the correlations between these images. Standard practice is to use a two-branch CNN-like architecture with feature fusion schemes. For instance, Hao et al. <ref type="bibr" target="#b12">[13]</ref> concatenated the features from pre-and post-disaster images and fed them into a CNN-based framework. Gupta et al. <ref type="bibr" target="#b13">[14]</ref> developed a framework that uses the difference between pre-and postdisaster features as input of the deep neural network, which is denoted as RescueNet. More recently, attention mechanism has been used with deep neural networks for remote sensing images processing <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, which is a strategy of allocating larger weights to informative parts of an image/feature. For instance, the average pooling is applied to extract the channel attention information for remote sensing image segmentation <ref type="bibr" target="#b16">[17]</ref>, and a non-local-based attention module is utilized in the arXiv:2105.07364v1 [cs.CV] 16 May 2021  network to explore the spatial relations of satellite images, in <ref type="bibr" target="#b12">[13]</ref>. Another challenge of building damage assessment from satellite imagery lies in the visual similarity between certain classes (e.g., no damage and minor damage). These classes are considered as difficult classes. To illustrate this problem, we use the xBD <ref type="bibr" target="#b17">[18]</ref> dataset, which is the largest dataset for building damage assessment to date, as an example. <ref type="figure" target="#fig_0">Fig. 1</ref> shows a pair of images from this dataset. Based on visual observation, it is difficult to distinguish between classes such as no damage and minor damage due to similar appearance. <ref type="table" target="#tab_0">Table I</ref> reports the classification results of ResNet-50 on xBD. From the classification confusion matrix, about 24.2% of minor damage are mis-classified as no damage. One effective strategy to cope with difficult classes and improve the model performance is data augmentation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Data augmentation has been widely used as a pre-processing technique to artificially increase the size of dataset in segmentation tasks <ref type="bibr" target="#b20">[21]</ref>. Recently, CutMix <ref type="bibr" target="#b21">[22]</ref> is proposed as a new data augmentation technique, which generates a new image by combining two image samples, to enhance the generalization ability of neural networks. CutMix directly cuts and pastes image patches from one image to another, which can be easily used in many tasks.</p><p>Motivated by the above observations, we introduce a twostage CNN-based framework, named BDANet, for building damage assessment. First, a single U-Net <ref type="bibr" target="#b22">[23]</ref> is used for building segmentation (Stage 1). U-Net is an encoder-decoder network architecture that has been widely used in segmentation tasks, e.g., in <ref type="bibr" target="#b23">[24]</ref>. The results of building segmentation are used to guide the locations of building damages in the second stage. Then a two-branch multi-scale CNN (U-Net structure) is applied for damage assessment (Stage 2). By using the network weights from building segmentation, the network in Stage 2 can be trained more efficiently. Due to the scale variance of building objects, we introduce a multi-scale feature fusion (MFF) module in the encoder layer, which enables the network to aggregate features from multiple scales. In addition, a cross-directional attention (CDA) module is proposed to explore the correlations between features from pre-and postdisaster images. By leveraging channel-wise and spatial-wise correlations, the network can fuse and enhance the features from pre-and post-disaster images. Moreover, to tackle difficult classes, CutMix is employed for data augmentation. Specifically, we only apply CutMix to difficult classes to make the network pay more attention to those classes, thereby learning more robust representations for their separation. In the experiments, we show that this strategy yields superior classification performance over simply adopting CutMix for all classes.</p><p>The contributions of this paper are summarized as follows.</p><p>? We present a two-stage CNN-based learning framework for building segmentation (Stage 1) and damage assessment (Stage 2). The building segmentation helps the network to locate the building objects. The proposed framework achieves state-of-the-art performance on a large-scale building damage assessment benchmark -xBD <ref type="bibr" target="#b17">[18]</ref>. ? To reduce the influence of scale variance of buildings, a multi-scale feature fusion (MFF) module is introduced to learn features from multiple scales. ? A cross-directional attention (CDA) module is proposed to effectively aggregate the features from pre-and postdisaster images. With the attention module, informative channel and spatial information can be extracted to enhance the feature representation. ? We also unveil the challenge of difficult classes in building damage assessment and explore an effective data augmentation strategy, CutMix <ref type="bibr" target="#b21">[22]</ref>, to cope with this challenge. The remainder of this paper is organized as follows. Section II gives an overview of related works. Section III presents the details of the proposed BDANet for building damage assessment. Section IV provides comprehensive experimental results and parameter analysis. Finally, conclusion is drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Building Damage Assessment</head><p>Building damage-based analysis, including detection, segmentation and assessment, are vital topics in HADR. Recently, many algorithms have been developed for building damage analysis <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. For instance, in <ref type="bibr" target="#b25">[26]</ref>, to detect the damaged building objects after an earthquake, change features, including texture, color, statistical similarity and correlation descriptors, are all analyzed before applying a support vector machine (SVM) classifier. In <ref type="bibr" target="#b27">[28]</ref>, Zhu et. al. proposed an instance segmentation network, named MSNet, for building damage detection. In <ref type="bibr" target="#b28">[29]</ref>, a multiple kernel learning framework is proposed. To improve performance of building damage detection, CNN feature and 3D cloud point feature are integrated in the framework. In <ref type="bibr" target="#b29">[30]</ref>, an unsupervised anomaly detection method is developed to automatically detect the location of damaged buildings.</p><p>Different from building damage detection and segmentation, building damage assessment aims at identifying the damage level of building objects after natural disasters. Depending on whether to use pre-disaster images, building damage assessment can be divided into two categories. One category only uses post-disaster images and the other uses paired pre-and post-disaster images for damage assessment. Many previous works focus on using only post-disaster images. For example, in <ref type="bibr" target="#b30">[31]</ref>, SVM is used to assess the building damage after earthquake with QuickBird satellite images. In <ref type="bibr" target="#b31">[32]</ref>, objectbased analysis is integrated with machine learning methods to preserve the boundary and detailed information of remote sensing satellite images in damaged buildings. However, without pre-disaster images, building assessment errors can be easily made because post-disaster images cannot provide precise boundaries/contours of complete building objects. Therefore, researchers resort to use a pair of satellite images (pre-and post-disaster) for building damage assessment. Cooner et al. <ref type="bibr" target="#b32">[33]</ref> extracted textural and structural features from both preand post-disaster images, which improved the performance of earthquake damage assessment. Due to the limited amount of labeled data, some works simplify the damage assessment task as binary classification, which assigns damage and no damage labels to building objects. For example, Xu et al. <ref type="bibr" target="#b10">[11]</ref> compared the performance of different CNN models in detecting damage buildings in the Haiti earthquake.</p><p>Recently, researchers start to differentiate more building damage levels for better disaster analysis. Different from the binary classification, the extent of damage is considered, such as minor damage and major damage. In <ref type="bibr" target="#b5">[6]</ref>, the level of building damage after earthquake is evaluated by a framework integrating CNN and ordinal regression. In <ref type="bibr" target="#b33">[34]</ref>, to reduce the uncertainty in predicting building damage levels, building objects in the center of an image patch are considered while the surrounding buildings are occluded by negating the pixels. Hence, the model can focus on the buildings in the center of an image patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. U-Net</head><p>U-Net is a CNN-based network that was first proposed for biomedical image segmentation <ref type="bibr" target="#b22">[23]</ref>. Compared with traditional CNN-based architectures, U-Net introduced downsampling and up-sampling to aggregate feature from multiple scales. By adopting the skip connection, features from low levels and high levels are integrated to enhance the learning ability of network <ref type="bibr" target="#b34">[35]</ref>. Owning to these advantages, U-Nets have been widely used in image segmentation <ref type="bibr" target="#b35">[36]</ref> and classification <ref type="bibr" target="#b36">[37]</ref>. U-Net-based architectures have also been applied in remote sensing image processing. For instance, in <ref type="bibr" target="#b37">[38]</ref>, U-Net is introduced for very-high resolution remote sensing image segmentation. In <ref type="bibr" target="#b38">[39]</ref>, U-Net is integrated with multi-scale strategy for building instance extraction. In <ref type="bibr" target="#b39">[40]</ref>, U-Net is used as a feature extraction method for building segmentation, which provides promising results. Moreover, U-Net-based architectures have been applied in a variety of tasks, such as road extraction <ref type="bibr" target="#b40">[41]</ref> and change detection <ref type="bibr" target="#b41">[42]</ref>. However, the U-Net architecture is simple and cannot be directly used in building damage assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention-based Models</head><p>Attention-based models <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> have been widely used in computer vision tasks, which learn to put more emphasis on important information of images/features. Recently, many attention-based models have been developed and achieved great success in remote sensing image processing tasks, such as image classification and segmentation <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In <ref type="bibr" target="#b16">[17]</ref>, a channel-attention mechanism is integrated with a fully convolutional network (FCN) for high-resolution aerial image segmentation. In <ref type="bibr" target="#b45">[46]</ref>, a foreground-aware network is developed, which incorporates both spatial and channel attention models to improve foreground modeling, for geospatial object segmentation. In <ref type="bibr" target="#b46">[47]</ref>, an attention embedding module is designed to embed attention from high-level layers into lowlevel ones to enrich the semantic information for satellite image segmentation. In <ref type="bibr" target="#b47">[48]</ref>, a self-attention mechanism that considers both spatial-dipartite context aggregation information and the relative position encoding information is proposed for remote sensing scene classification.</p><p>Attention-based models are barely applied in building damage assessment. In <ref type="bibr" target="#b12">[13]</ref>, a non-local attention model <ref type="bibr" target="#b43">[44]</ref> is adopted to capture long-range spatial information of pre-and post-disaster images. However, the computational cost of the non-local mechanism is very high because the attention map is calculated based on high-resolution features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Augmentation Strategies</head><p>Data augmentation has been widely used as a pre-processing technique to artificially increase the size of dataset in computer vision tasks <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Data augmentation methods usually employ image transformations, such as image flip, rotation and crop, to improve the model generalization performance. Manually annotating remote sensing images is difficult and time-consuming. As a result, labeled remote sensing data are usually limited. To facilitate the training of deep neural networks, data augmentation is critical in deep learning-based remote sensing image processing.</p><p>Recently, various data augmentation techniques have emerged and shown their efficiency. Cutout <ref type="bibr" target="#b50">[51]</ref> randomly masks out square regions of input to force the network pay attention to diverse image regions, thereby improving the feature representation capability of CNNs. Some works focus on synthesizing image samples to expand the size of training data. Cubuk et al. <ref type="bibr" target="#b19">[20]</ref> proposed a search algorithm for image augmentation, which enables the network to find the best validation accuracy. In <ref type="bibr" target="#b51">[52]</ref>, Mixup is introduced to regularize the neural network by using linear combination of pairs of samples and labels. Different from the Mixup strategy that merges two images with linear summation, CutMix <ref type="bibr" target="#b21">[22]</ref> directly cuts and pastes image patches from one image to another, which can be used in segmentation tasks.</p><p>At present, most deep learning based methods still follow the architectures of change detection and seldom consider the completeness of building objects. Furthermore, due to the similarities between certain classes, it is difficult for the network to distinguish their differences. To deal with this problems, a two-stage framework is proposed for building damage assessment. The MMF and CDA models are proposed to enhance the feature representation. Moreover, the CutMix strategy is employed to alleviate the difficulty of classifying hard classes. <ref type="figure" target="#fig_1">Fig. 2</ref> presents an overview of the proposed BDANet, which consists of two stages: building segmentation (Stage 1) and damage assessment (Stage 2). In Stage 1, a single U-Net branch (i.e., the upper one) is used for building segmentation. This U-Net branch uses only pre-disaster images as input and produces segmentation masks of building objects. In Stage 2, the pre-and post-disaster images are fed into two network branches separately. To further enhance the feature representations, the multi-scale feature fusion (MFF) module and the cross-directional attention (CDA) module are introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>We describe the details of the proposed damage assessment framework and its components in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Two-stage Framework</head><p>Although the building damage assessment task shares some similarities with change detection task, there are several key differences. First, building damage assessment not only requires to detect the changes but also needs to classify their damage levels. Second, the undamaged buildings also need to be detected, which are usually unnecessary in change detection. Change detection-based networks mainly follow a threebranch architecture, which contains a dual-branch for image pairs learning and a fusion branch for change detection <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. These networks are designed for learning the differences of image pairs while unchanged objects (e.g. undamaged buildings) are ignored. Due to these problems, directly using networks for change detection cannot satisfy the needs of building damage assessment. Therefore, in this paper, a twostage learning framework is proposed for building damage assessment.</p><p>Stage 1: building segmentation As shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>, to locate the building objects, only pre-disaster images are used as input to the network in this stage. The network is separately trained for building objects segmentation in this stage. Compared with post-disaster images, building objects in pre-disaster images are intact and well-shaped. The network is designed based on the architecture of U-Net <ref type="bibr" target="#b22">[23]</ref>. In the encoder part, a ResNet <ref type="bibr" target="#b54">[55]</ref> is used as backbone to extract deep features. By using the strided convolution, the input image is first down-sampled into half of the original resolution. With the network becoming deeper, feature maps are projected into a higher dimension for high-level feature extraction. In the decoder part, the resolution of features are gradually recovered to the original input size by upsampling. To capture the details of target objects, skip connections are utilized in the network (see <ref type="figure" target="#fig_1">Fig. 2(a)</ref>). Before each upsampling operation, the lowlevel feature maps from the encoder layer are concatenated with the high-level feature maps. The output of the network is a binary segmentation map indicating the building distribution in a satellite image. Similar to some segmentation tasks, the cross-entropy loss is used in this stage, which is formulated as</p><formula xml:id="formula_0">L = ? N i y i log? i + (1 ? y i ) log(1 ?? i ),<label>(1)</label></formula><p>where L quantifies the overall loss by comparing the target label y i with predicted label? i , N represents the number of training pixels. The objective of Stage 1 is locating the building objects. As many buildings are damaged in post-disaster images, it is difficult for the network in Stage 2 to perform building segmentation and damage assessment at the same time. Therefore, to guide the assessment process of Stage 2, a single U-Net using only pre-disaster images is first applied to identify the locations of buildings.</p><p>Stage 2: damage assessment In Stage 2, both pre-and post-disaster images are fed into a two-branch network. The backbone network for damage assessment is the same as that for building segmentation. The convolutional kernels are shared in both branches. Therefore, the parameter weights obtained from Stage 1 are used here for weights initialization.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, the feature maps from two branches are concatenated in dconv4 layer. Then the output of damage assessment is generated after a convolutional layer. The crossenropy loss is also applied in Stage 2. Let the output of building segmentation be P b ? R 2?H?W , where H and W denote the height and width of the output, respectively. Then the segmentation results can be expressed as</p><formula xml:id="formula_1">P B = arg max(P b ),<label>(2)</label></formula><p>where P B ? {0, 1} 1?H?W with 1 representing for buildings and 0 for background.</p><p>Let the output of damage assessment be P d ? R C?H?W , where C denotes the number of damage levels. To guide the building locations, the final output P is represented as</p><formula xml:id="formula_2">P = arg max(P B ? P d ),<label>(3)</label></formula><p>where ? denotes the element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-scale Feature Fusion Module</head><p>As the scale of buildings varies in images captured from various disasters and regions, learning a robust representation of building objects under various scales is important. To this end, a multi-scale feature fusion (MFF) module is introduced in the encoder of the network. The input image is processed with three different scales: 1? input resolution (original size), 0.5? input resolution, 0.25? input resolution. Features from different scales are extracted by a convolutional block and then embedded in the encoder of the network, as shown in <ref type="figure" target="#fig_1">Fig.  2(b)</ref>. Specifically, the MFF contains three streams. The first stream uses images with the original resolution. In this stream, a convolutional layer with stride 2 is used to generate feature maps with half of the input size. This stream is the same as the first layer of ResNet, which is used as the backbone of the proposed framework. The 0.5? resolution images are used as input of the second stream. The size of feature maps of this stream keeps the same with 0.5? resolution images. Then feature maps from 1? and 0.5? streams are concatenated for further processing. To keep a similar setting of the number of channels as the ResNet, a dimension reduction is conducted on the concatenated feature map. In the third stream, the feature map of 0.25? resolution is concatenated with the second convolutional block of ResNet. In this way, features from three different image scales are embedded in the network, which enables the network to learn a robust features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-directional Attention Module</head><p>To further explore the correlations between pre-and postdisaster features, a cross-directional attention (CDA) module is developed. Inspired by the squeeze and excitation (SE) block <ref type="bibr" target="#b55">[56]</ref>, the proposed CDA module focuses on recalibrating features from channel and spatial dimensions. Moreover, the channel and spatial information from pre-and post-disaster features is aggregated together in a cross manner and then embedded in the network respectively. The details are depicted  <ref type="figure" target="#fig_2">Fig. 3</ref>. Let U pre ? R E?h?w and U post ? R E?h?w be the feature maps obtained from the two branches of U-Net respectively, where E denotes the number of channels of feature maps, h and w are the height and width of feature maps, respectively. Then the channel information can be extracted by</p><formula xml:id="formula_3">? 3 dconv1-1 32 ? 32 ? 512 3 ? 3 dconv21-1 32 ? 32 ? 1024 dconv22-1 32 ? 32 ? 1024 3 ? 3 dconv1-2 64 ? 64 ? 256 3 ? 3 dconv21-2 64 ? 64 ? 512 dconv22-2 64 ? 64 ? 512 3 ? 3 dconv1-3 128 ? 128 ? 96 3 ? 3 dconv21-3 128 ? 128 ? 192 dconv22-3 128 ? 128 ? 192 3 ? 3 dconv1-4 256 ? 256 ? 32 3 ? 3 dconv21-4 256 ? 256 ? 32 dconv22-4 256 ? 256 ? 64 3 ? 3 Output 512 ? 512 ? 1 - - - Output 512 ? 512 ? 5 - in</formula><formula xml:id="formula_4">I cha = ?(P g ([U pre , U post ])),<label>(4)</label></formula><p>where [U pre , U post ] denotes the concatenation of feature maps, P g (?) represents the global average pooling, ?(?) is the sigmoid function, and I cha is a feature vector of E channels after dimension reduction from 2E. Then the new features from two branches can be formulated as</p><formula xml:id="formula_5">U cha pre = I cha * U post + U pre , U cha post = I cha * U pre + U post ,<label>(5)</label></formula><p>where * denotes the channel-wise multiplication between the input feature maps and vector I cha . The output of channel feature fusion is then used in the spatial feature fusion. We concatenate U cha pre and U cha post , and feed them into a 1 ? 1 convolution P conv as</p><formula xml:id="formula_6">I spa = ?(P conv ([U cha pre , U cha post ])),<label>(6)</label></formula><p>where P conv ([U cha pre , U cha post ]) ? R 1?H?W . Then the output features are</p><formula xml:id="formula_7">U spa pre = I spa ? U cha post + U pre , U spa post = I spa ? U cha pre + U post ,<label>(7)</label></formula><p>where I spa ? U cha post and I spa ? U cha pre denote the spatial elementwise multiplication. As a result, channel and spatial information from pre-and post-disaster branches are effectively aggregated in the CDA module. It is worth noting that the proposed CDA module consists of only simple convolution and matrix operations, which is easy to implement and integrate with existing CNN architectures.</p><p>Therefore, the overall architecture of BDANet is established. The detailed parameters are listed in <ref type="table" target="#tab_0">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Augmentation with CutMix</head><p>As discussed in Section I, several damage levels are difficult to distinguish from each other due to their similarities in satellite images. To address this challenge, we leverage the CutMix <ref type="bibr" target="#b21">[22]</ref> data augmentation scheme to increase the sample sizes of difficult classes, for better feature representations. Specifically, image patches are cut from samples that contain difficult classes (i.e., minor and major damage classes for the xBD dataset based on the results in <ref type="table" target="#tab_0">Table I)</ref>, and then pasted into any random sample images. The CutMix procedure is illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. Let {X pre A , X post A } ? R c?H?W and Y A ? R H?W be a randomly selected training sample (an image pair) and label, (X pre B , X post B , Y B ) be a randomly selected sample from difficult classes, where c is the channel number of images. Then the CutMix operation in this task can be defined asX</p><formula xml:id="formula_8">pre = M ? X pre A + (1 ? M ) ? X pre B , X post = M ? X post A + (1 ? M ) ? X post B , Y = M ? Y A + (1 ? M ) ? Y B ,<label>(8)</label></formula><p>where M ? R 1?W ?H denotes a binary mask indicating where to cut out and fill in from two image samples, and (X pre ,X post ,? ) represents the generated new sample. With the increased sample size using CutMix, the network is forced to pay more attention to these difficult classes and learn more robust representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Dataset and Evaluation</head><p>To evaluate the effectiveness of our proposed method, the xBD dataset <ref type="bibr" target="#b17">[18]</ref> is used in the experiments. The xBD dataset is the only public available large-scale dataset of satellite images for building segmentation and damage assessment at present. It was sourced from the DigitalGlobe Open Program 1 ,   <ref type="table" target="#tab_0">Table III</ref>. It is worth mentioning that the data is imbalanced and the damage level is highly skewed toward no damage. The number of each damage level's polygons is reported in <ref type="table" target="#tab_0">Table IV</ref>. The experimental results are evaluated by using the F1 score (F 1 b ) for building segmentation and the harmonic mean of class-wise damage classification F1 (F 1 d ) for building damage assessment, which are defined as:</p><formula xml:id="formula_9">F 1 b = 2T P 2T P + F P + F N , F 1 d = n n i=1 1/F 1 Ci ,<label>(9)</label></formula><p>respectively, where T P , F P and F N are the number of truepositive, false-positive and false-negative pixels of segmentation results, respectively, 1/F 1 Ci denotes the F1 score of each damage level for damage assessment, and C i denotes the damage level. F1 score is a challenging evaluation index as it heavily penalizes the overfitting to over-represented classes. The overall score F 1 s <ref type="bibr" target="#b17">[18]</ref> comprehensively evaluates the performance of building segmentation and damage assessment, which is formulated by a weighted average of F 1 b and F 1 d :</p><formula xml:id="formula_10">F 1 s = 0.3 ? F 1 b + 0.7 ? F 1 d .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We implement the proposed framework using Pytorch. Experiments are conducted on a computer with an Intel i9-9920X CPU and two NVIDIA TITAN-V GPUs. Images are cropped to a size of 512 ? 512 pixels for training. The prediction for the testing set is performed on the original size of images (1024 ? 1024). Apart from CutMix, basic data augmentation is conducted in the training phase, such as flip and rotation. Due to GPU memory limitation, we use ResNet-50 as the backbone for all compared networks with pre-trained weights loaded from the PyTorch library. The number of channels in each convolutional block is <ref type="bibr">[64,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048]</ref>. In the decoder, the number of channel in each convolutional block is <ref type="bibr">[512,</ref><ref type="bibr">256,</ref><ref type="bibr">96,</ref><ref type="bibr" target="#b31">32]</ref>. The number of convolutional channels are the same as Stage 1 and Stage 2. The cross-entropy loss is used for both building segmentation and damage assessment. The optimization method is Adam. In the building segmentation stage, the learning rate is 0.00015 and the number of epoch is 120. In the damage assessment stage, the learning rate is 0.0002 and the number of epoch is 25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results Analysis</head><p>To validate the effectiveness of the proposed framework, we employ several state-of-the-art methods for comparison on xBD.</p><p>1) Two change detection-based networks, including WNet <ref type="bibr" target="#b52">[53]</ref> and U-Net++ <ref type="bibr" target="#b56">[57]</ref> are evaluated for building damage assessment.</p><p>2) The RescueNet <ref type="bibr" target="#b13">[14]</ref> uses a dilated ResNet-50 as the backbone network. The difference of features from pre-and post-disaster images is utilized for damage assessment.</p><p>3) In Weber's method <ref type="bibr" target="#b11">[12]</ref>, after feature extraction with ResNet-50, features from pre-and post-disaster images are concatenated for feature fusion. Then the segmentation is performed for damage assessment. 4) Several state-of-the-art semantic segmentation networks, including FCN <ref type="bibr" target="#b57">[58]</ref>, SegNet <ref type="bibr" target="#b58">[59]</ref> and DeepLabv3 <ref type="bibr" target="#b59">[60]</ref>, are also employed for comparison. We set the two-stage framework unchanged and only replace the U-Net-based architecture in Stage 2 with these mentioned networks. According to <ref type="table" target="#tab_4">Table  V</ref>, both methods provided low scores compared with other networks. It is reasonable Apart from our proposed two-stage framework, a network without the multi-scale feature fusion (MFF) module, crossdirectional attention (CDA) module and CutMix is used as a clean baseline for comparison, which is denoted as the vanilla network. The damage assessment results of different approaches are reported in <ref type="table" target="#tab_4">Table V</ref>. The proposed framework using the vanilla network alone outperforms the existing methods, e.g., <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b11">[12]</ref>, in terms of all three metrics (F 1 s , F 1 b and F 1 d ). Unlike the RescueNet and Weber's method, the decoder and skip connections are used in our framework. Therefore, our method provides a better performance in the building segmentation stage (i.e., an improvement of over 2% in F 1 b ). Moreover, our method achieves a significant improvement (about 12%, 0.493?0.616) in minor damage level compared with RescueNet <ref type="bibr" target="#b13">[14]</ref>.</p><p>Different from our proposed framework, the change detection-based networks (WNet <ref type="bibr" target="#b52">[53]</ref> and U-Net++ <ref type="bibr" target="#b56">[57]</ref>) perform buidling segmentation and damage assessment in one network. Accodring to Table V, low scores of F 1 s are provided by both methods. It is reasonable because is difficult for networks to locate buildings when post-damage images are also fed into the networks. From <ref type="table" target="#tab_4">Table V</ref>, WNet provided the lowest score on F 1 b and the no damage level. As FCN uses simple upsampling to predict the damage levels and hardly considers features from shadow layers, the score in F 1 s is not satisfactory compared with other methods under the same   framework. In contrast, SegNet and DeepLabv3 have achieved promising results using our proposed framework. Compared with the vanilla network, our overall approach incorporating the proposed components further improves the accuracy for damage assessment (0.782 vs 0.757 in F 1 d ), and the improvement is consistent for all the damage levels. By considering both the building segmentation and damage assessment performance (i.e., the overall score F 1 s ), our proposed twostage framework outperforms the existing methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b11">[12]</ref> by a considerable margin. The classification confusion matrix of proposed framework is reported in <ref type="table" target="#tab_0">Table VI</ref>. Compared with classification accuracy in <ref type="table" target="#tab_0">Table I</ref>, much improvements are achieved in the minor damage and major damage level, indicating the effectiveness of our proposed method. Some visual results of building segmentation are presented in <ref type="figure" target="#fig_4">Fig. 5</ref>. By decomposing the framework into two stages, the network can focus on building segmentation in Stage 1, achieving good segmentation results as compared with the ground truth segmentation maps. Therefore, the results can be used to effectively guide the building locations for damage assessment in Stage 2. <ref type="figure" target="#fig_5">Fig. 6</ref> shows a visual comparison of damage assessment results of different networks. For U-Net++, it is obvious that this method can not provide precise outlines of building objects and thus many segmentation mistakes can be found. For FCN, more mis-classification can be found from its classification map compared with other methods. We can also observe that much mis-classification come from the levels of minor and major damage because both two levels are difficult classes. Compared with FCN and SegNet, DeepLabv3 provides a better visual result. Overall, our proposed BDANet achieves the best performance with less damage assessment errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>To investigate the contribution of MFF module, CDA module and CutMix data augmentation, we carry out ablation studies on these components. <ref type="table" target="#tab_0">Table VII</ref> reports the results of vanilla network with these individual components. It can be seen that CDA and MFF bring about 0.6% and 0.5% improvement in F 1 s , respectively. Moreover, the CutMix strategy on difficult classes boosts the overall performance (F 1 s ) by 1.3%, which is a considerable gain. In particular, the accuracy of minor damage class is improved by 2.7% (0.578 vs 0.605) with CutMix.</p><p>1) Analysis of MFF. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, three feature streams are used to extract the features from different image resolutions. Therefore, this ablation study investigates the effectiveness of the three streams. Note that when only the stream with the original image resolution (1?) is used, the framework is consistent with the vanilla network. <ref type="table" target="#tab_0">Table VIII</ref> reports the results of different streams applied in the network, where the 0.5? and 0.25? scales yield about 0.4% and 0.2% performance gain in F 1 s , respectively. When three streams are used together in the network, an improvement of 0.5% is obtained, indicating that learning features from different image scales is useful for improving the overall performance of damage assessment.</p><p>The feature maps from different streams are also depicted in <ref type="figure" target="#fig_6">Fig. 7</ref>. Specifically, we visualize several feature maps extracted from the first convolutional layer of the three streams. We can observe that local and detailed information can be extracted from the 1? stream, while more global information is captured from streams with lower resolution (0.5? and 0.25?).        2) Analysis of CDA. To explore the correlations between  pre-and post-disaster features, three CDA modules are used in the decoder of the network, which are represented as dconv1, dconv2 and dconv3 in <ref type="figure" target="#fig_1">Fig. 2</ref>. This experiment investigates the influence of the number of CDA modules as well as the embedding position of CDA in the network. <ref type="table" target="#tab_0">Table IX</ref> shows that only slight performance gain is achieved when CDA is added in the convolutional layer of a small scale (dconv1). In contrast, a better improvement is obtained when CDA is added in the layer with a larger scale (dconv3). This is reasonable because more detailed information can be extracted when the feature maps are in a larger scale. When all three CDA modules are used in the network, around 0.6% improvement is achieved compared with the vanilla network. As discussed in Section III-C, the CDA module is developed from the SE module <ref type="bibr" target="#b55">[56]</ref>. Therefore, in this experiment, we further evaluate the effectiveness of CDA when compared with the SE module. In the experiment, CDA is replaced with the channel (cha) and spatial (spa) attention modules of SE. As reported in <ref type="table" target="#tab_9">Table X</ref>, only 0.1% improvement is achieved when SE channel attention module is applied in the network. With both channel and spatial attention of SE, more gains are obtained in F 1 s . Our proposed CDA provides a better result in comparison with all SE modules, which indicates that exploring the features between pre-and post-disaster images is effective in improving the damage assessment performance. 3) Analysis of CutMix. For the xBD dataset, minor damage and major damage are the most difficult classes based on the results in <ref type="table" target="#tab_0">Table I</ref>. Therefore, in our proposed method, CutMix data augmentation is mainly focused on these two damage levels to generate more training samples for them. We further conduct experiments to investigate the influence of CutMix on different damage levels. <ref type="table" target="#tab_0">Table XI</ref> presents the overall scores when CutMix is applied on different damage levels. It is obvious that there is barely any improvement when CutMix is used on the entire dataset (i.e., considering all classes). In contrast, a significant improvement is achieved when only minor and major damage levels are considered in the data augmentation. This class-specific data augmentation strategy facilitates the network to learn better representations for those classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Computational Cost and Training Efficiency</head><p>We also compare the parameter size, computational cost (FLOP) and inference speed of the proposed overall framework, the vanilla network with MFF and CDA modules. The CutMix strategy is used in data pre-processing stage and thus it does not consume additional computational resources in the network. As shown in <ref type="table" target="#tab_0">Table XII</ref>, the MFF module adds a little more computational cost since more convolutional operations are applied. Although the CDA module has a complicated architecture, it uses few convolutional operations. Therefore, it includes less than 1M additional parameters. This results in only a slight increase in computational cost (107.6 vs 92.9 GFLOPs). Moreover, our proposed framework only increases less than 2M parameters and about 60G FLOPs as compared with the vanilla network. We also calculate the average in-ference time (s) for each image (1024 ? 1024 pixels) in the testing phase. The additional runtime (0.016s) is negligible.</p><p>In addition, we evaluate the training efficiency of using weights from Stage 1. As discussed in Section III, the network weights from Stage 1 are used as initial weights in the network of Stage 2 for fine tuning. <ref type="figure" target="#fig_7">Fig. 8</ref> plots the validation accuracy and loss versus the number of epochs. It is obvious that a higher validation accuracy and a smaller loss value are achieved at the first epoch when adopting the weights from Stage 1 as initialization. With the number of epochs increasing, the curve of using weights from Stage 1 converges faster. <ref type="figure" target="#fig_7">Fig.  8</ref> shows that at least 5 epochs are saved when using the weights from Stage 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Evaluation on Building Change Detection Dataset</head><p>As more datasets on building damage assessment are difficltut to acquire, a building change detection dataset LEVIR-CD <ref type="bibr" target="#b60">[61]</ref> is used for evaluation of our proposed framework. These images are captured in several cities of Texas, US from 2002 to 2018. The dataset collected 637 high resolution images (0.5m per pixel) with a size of 1024 ? 1024 via Google Earth. Among these images, 509 images pairs are used for training and 128 image pairs are used for testing.</p><p>Some change detection-based methods, including WNet <ref type="bibr" target="#b52">[53]</ref> and U-Net++ <ref type="bibr" target="#b56">[57]</ref>, are uesd in comparison with BDANet. For the change detection task, only the network in Stage 2 is used for training. The results of different methods on LEVIR-CD are reported in <ref type="table" target="#tab_0">Table XIII</ref>. From the <ref type="table">Table,</ref> we can observe that the proposed method achieved best score on three indexes (precision, recall and F1), indicating that BDANet is also effective on other dataset. <ref type="figure" target="#fig_8">Fig. 9</ref> provides some visual results of different change detection methods. We can also observe that the proposed BDANet provides better performance compared with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, a two-stage CNN-based learning framework, named BDANet, is proposed for building damage assessment from satellite images. In Stage 1, a U-Net architecture is used for building segmentation, and the output is used for guiding the building locations in damage assessment. In Stage 2, the MFF and CDA modules are used for further improving the assessment performance. MFF is used to learn features from various scales of images, which enables the network to build a robust feature representation. The CDA module is used to explore the correlations between pre-and post-disaster features and enhance the feature representation capability. Moreover, a data augmentation strategy CutMix is employed to mitigate the challenge of difficult classes. Experimental results show that significant improvements can be achieved when CutMix is applied on difficult damage levels. The proposed method also yields state-of-the-art building damage assessment performance on the xBD dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>A pair of images and their annotations from the xBD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the proposed framework (BDANet). The U-Net structure is used in both stages. It consists of two stages: (a) Stage 1: building segmentation, (b) Stage 2: damage assessment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Framework of the proposed cross-directional attention (CDA) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Data augmentation with CutMix for difficult classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Visual examples of building segmentation in Stage 1. First row to third row are: pre-disaster images, ground truth of building segmentation, and segmentation results of Stage 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Damage assessment results. From left to right: (a) pre-disaster image, (b) post-disaster image, (c) the ground-truth, (d) FCN, (e) SegNet, (f) DeepLab and (g) our proposed network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Visualization of feature maps of MFF module. First column: the input images with 1?, 0.5? and 0.25? resolution. (a) Feature maps of 1? stream. (b) Feature maps of 0.5? stream. (c) Features maps of 0.25? stream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Evaluation of the training efficiency in Stage 2 by applying network weights from Stage 1 as initialization. (a) Validation accuracy (%). (b) Cross-entropy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Building change detection results of different methods on LEVIR-CD dataset. (a) before change, (b) after change, (c) ground truth, (d) WNet, (e) U-Net++ and (f) our proposed BDANet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Classification confusion matrix (%) of ResNet-50 baseline on xBD testing set.</figDesc><table><row><cell>Damage Level</cell><cell>C0</cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell></row><row><cell cols="2">Background (C0) 98.6</cell><cell>0.9</cell><cell>0.2</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell>No damage (C1)</cell><cell>7.1</cell><cell>88.7</cell><cell>3.2</cell><cell>0.8</cell><cell>0.1</cell></row><row><cell>Minor Damage (C2)</cell><cell>6.3</cell><cell>24.2</cell><cell>60.0</cell><cell>9.2</cell><cell>0.4</cell></row><row><cell>Major Damage (C3)</cell><cell>3.0</cell><cell>6.6</cell><cell cols="2">14.9 73.1</cell><cell>2.4</cell></row><row><cell>Destroyed (C4)</cell><cell>5.5</cell><cell>2.4</cell><cell>1.2</cell><cell>8.9</cell><cell>82.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Parameters of the BDANet.</figDesc><table><row><cell></cell><cell>Stage 1</cell><cell></cell><cell></cell><cell></cell><cell>Stage 2</cell><cell></cell><cell></cell></row><row><cell>Layer</cell><cell>Feature Size</cell><cell>Kernel Size</cell><cell>Layer</cell><cell>Feature Size</cell><cell>Layer</cell><cell>Feature Size</cell><cell>Kernel Size</cell></row><row><cell>Input-pre</cell><cell>512 ? 512 ? 3</cell><cell>-</cell><cell>Input-pre</cell><cell>512 ? 512 ? 3</cell><cell>Input-post</cell><cell>512 ? 512 ? 3</cell><cell>-</cell></row><row><cell>conv1-1</cell><cell>256 ? 256 ? 64</cell><cell>5 ? 5</cell><cell>conv21-1</cell><cell>256 ? 256 ? 64</cell><cell>conv22-1</cell><cell>256 ? 256 ? 64</cell><cell>5 ? 5</cell></row><row><cell>conv1-2</cell><cell>128 ? 128 ? 256</cell><cell>3 ? 3</cell><cell>conv21-2</cell><cell>128 ? 128 ? 256</cell><cell>conv22-2</cell><cell>128 ? 128 ? 256</cell><cell>3 ? 3</cell></row><row><cell>conv1-3</cell><cell>64 ? 64 ? 512</cell><cell>3 ? 3</cell><cell>conv21-3</cell><cell>64 ? 64 ? 512</cell><cell>conv22-3</cell><cell>64 ? 64 ? 512</cell><cell>3 ? 3</cell></row><row><cell>conv1-4</cell><cell>32 ? 32 ? 1024</cell><cell>3 ? 3</cell><cell>conv21-4</cell><cell>32 ? 32 ? 1024</cell><cell>conv22-4</cell><cell>32 ? 32 ? 1024</cell><cell>3 ? 3</cell></row><row><cell>conv1-5</cell><cell>16 ? 16 ? 2048</cell><cell>3 ? 3</cell><cell>conv21-4</cell><cell>16 ? 16 ? 2048</cell><cell>conv22-4</cell><cell>16 ? 16 ? 2048</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>The xBD dataset splits and annotation numbers.</figDesc><table><row><cell>Split</cell><cell>Image No.</cell><cell>Polygon No.</cell></row><row><cell>Train</cell><cell>18336</cell><cell>632228</cell></row><row><cell>Test</cell><cell>1866</cell><cell>109724</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Damage level annotations and the distribution.</figDesc><table><row><cell></cell><cell>No damage</cell><cell>Minor</cell><cell>Major</cell><cell>Destroyed</cell></row><row><cell>No.</cell><cell>313003</cell><cell>36860</cell><cell>29904</cell><cell>31560</cell></row><row><cell>%</cell><cell>76.04</cell><cell>8.98</cell><cell>7.29</cell><cell>7.69</cell></row><row><cell cols="5">which was released for sudden major disaster events. Specif-</cell></row><row><cell cols="5">ically, the Open Data Program was chosen for its availability</cell></row><row><cell cols="5">of high-resolution imagery from many disparate regions of</cell></row><row><cell cols="5">the world. In this dataset, 19 different disasters (such as</cell></row><row><cell cols="5">hurricanes, floods, wildfire and earthquakes) are selected at</cell></row><row><cell cols="5">various locations with more than 800,000 building annotations.</cell></row><row><cell cols="5">The dataset consists of image pairs (pre-and post-disaster) of</cell></row><row><cell cols="5">size 1024 ? 1024 pixels. Each image contains three spectral</cell></row><row><cell cols="5">bands (red, green, blue) and has a resolution of 0.8 meter</cell></row><row><cell cols="5">per pixel. The damage assessment contains 4 levels, including</cell></row><row><cell cols="5">no damage, minor damage, major damage and destroyed. The</cell></row><row><cell cols="4">training and testing sets are listed in</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Building damage assessment results on xBD dataset with different methods.</figDesc><table><row><cell>Method</cell><cell>F 1s (overall)</cell><cell>F 1 b</cell><cell>F 1 d</cell><cell>No damage</cell><cell>Minor</cell><cell>Major</cell><cell>Destroyed</cell></row><row><cell>WNet [53]</cell><cell>0.737</cell><cell>0.817</cell><cell>0.703</cell><cell>0.884</cell><cell>0.518</cell><cell>0.684</cell><cell>0.855</cell></row><row><cell>U-Net++ [57]</cell><cell>0.740</cell><cell>0.819</cell><cell>0.707</cell><cell>0.886</cell><cell>0.523</cell><cell>0.689</cell><cell>0.858</cell></row><row><cell>RescueNet [14] (2020)</cell><cell>0.741</cell><cell>0.835</cell><cell>0.697</cell><cell>0.906</cell><cell>0.493</cell><cell>0.722</cell><cell>0.837</cell></row><row><cell>Weber et al. [12] (2020)</cell><cell>0.770</cell><cell>0.840</cell><cell>0.740</cell><cell>0.885</cell><cell>0.563</cell><cell>0.771</cell><cell>0.808</cell></row><row><cell>FCN [58]</cell><cell>0.765</cell><cell>0.864</cell><cell>0.722</cell><cell>0.919</cell><cell>0.532</cell><cell>0.708</cell><cell>0.861</cell></row><row><cell>SegNet [59]</cell><cell>0.782</cell><cell>0.864</cell><cell>0.747</cell><cell>0.921</cell><cell>0.567</cell><cell>0.746</cell><cell>0.859</cell></row><row><cell>DeepLabv3 [60]</cell><cell>0.788</cell><cell>0.864</cell><cell>0.756</cell><cell>0.919</cell><cell>0.580</cell><cell>0.761</cell><cell>0.859</cell></row><row><cell>Ours (vanilla network)</cell><cell>0.789</cell><cell>0.864</cell><cell>0.757</cell><cell>0.923</cell><cell>0.578</cell><cell>0.760</cell><cell>0.869</cell></row><row><cell>Ours (BDANet)</cell><cell>0.806</cell><cell>0.864</cell><cell>0.782</cell><cell>0.925</cell><cell>0.616</cell><cell>0.788</cell><cell>0.876</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="6">: Classification confusion matrix (%) of proposed</cell></row><row><cell cols="2">BDANet on xBD testing set.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Damage Level</cell><cell>C0</cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell></row><row><cell cols="2">Background (C0) 98.7</cell><cell>0.7</cell><cell>0.3</cell><cell>0.2</cell><cell>0.1</cell></row><row><cell>No damage (C1)</cell><cell>6.7</cell><cell>88.8</cell><cell>3.4</cell><cell>1.0</cell><cell>0.2</cell></row><row><cell>Minor Damage (C2)</cell><cell>5.3</cell><cell>18.5</cell><cell>64.5</cell><cell>11.2</cell><cell>0.5</cell></row><row><cell>Major Damage (C3)</cell><cell>2.9</cell><cell>4.8</cell><cell cols="2">12.6 77.8</cell><cell>1.9</cell></row><row><cell>Destroyed (C4)</cell><cell>5.9</cell><cell>1.8</cell><cell>0.7</cell><cell>8.9</cell><cell>82.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>Ablation study of different components (MFF, CDA and CutMix strategy) in the proposed framework.</figDesc><table><row><cell>Method</cell><cell>F 1s</cell><cell>F 1 b</cell><cell>F 1 d</cell><cell cols="3">No damage Minor Major Destroyed</cell></row><row><cell>Ours (vanilla)</cell><cell cols="3">0.789 0.864 0.757</cell><cell>0.923</cell><cell>0.578</cell><cell>0.760</cell><cell>0.869</cell></row><row><cell>vanilla + MFF</cell><cell cols="3">0.794 0.864 0.764</cell><cell>0.924</cell><cell>0.588</cell><cell>0.766</cell><cell>0.870</cell></row><row><cell>vanilla + CDA</cell><cell cols="3">0.795 0.864 0.765</cell><cell>0.923</cell><cell>0.592</cell><cell>0.769</cell><cell>0.871</cell></row><row><cell>vanilla + CutMix</cell><cell cols="3">0.802 0.864 0.775</cell><cell>0.926</cell><cell>0.605</cell><cell>0.779</cell><cell>0.872</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII :</head><label>VIII</label><figDesc>Analysis of the MFF module. The combinations of feature streams with different scales (1?, 0.5? and 0.25?) are used for comparison.</figDesc><table><row><cell>Method</cell><cell cols="4">No damage Minor Major Destroyed F 1s</cell></row><row><cell>Ours 1?</cell><cell>0.923</cell><cell>0.578 0.760</cell><cell>0.869</cell><cell>0.789</cell></row><row><cell>1? + 0.5?</cell><cell>0.924</cell><cell>0.586 0.763</cell><cell>0.870</cell><cell>0.793</cell></row><row><cell>1? + 0.25?</cell><cell>0.924</cell><cell>0.580 0.762</cell><cell>0.870</cell><cell>0.791</cell></row><row><cell>1? + 0.5? + 0.5?</cell><cell>0.924</cell><cell>0.588 0.766</cell><cell>0.870</cell><cell>0.794</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX :</head><label>IX</label><figDesc>Influence of the positions (dconv1, dconv2 and dconv3) of CDA modules in the network.</figDesc><table><row><cell>(a) 1? stream</cell><cell></cell></row><row><cell>(b) 0.5? stream</cell><cell></cell></row><row><cell>(c) 0.25? stream</cell><cell></cell></row><row><cell>Method</cell><cell>dconv1 dconv2 dconv3 F 1s</cell></row><row><cell>Ours (vanilla)</cell><cell>0.789</cell></row><row><cell>vanilla + CDA</cell><cell>0.790</cell></row><row><cell>vanilla + CDA</cell><cell>0.791</cell></row><row><cell>vanilla + CDA</cell><cell>0.793</cell></row><row><cell>vanilla + CDA</cell><cell>0.794</cell></row><row><cell>vanilla + CDA</cell><cell>0.795</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X :</head><label>X</label><figDesc>The damage assessment results of using CDA or SE modules.</figDesc><table><row><cell>module</cell><cell cols="4">No damage Minor Major Destroyed F 1s</cell></row><row><cell>SE cha [56]</cell><cell>0.923</cell><cell>0.579 0.761</cell><cell>0.871</cell><cell>0.790</cell></row><row><cell>SE spa [56]</cell><cell>0.923</cell><cell>0.580 0.762</cell><cell>0.871</cell><cell>0.792</cell></row><row><cell>SE cha + spa [56]</cell><cell>0.923</cell><cell>0.586 0.763</cell><cell>0.871</cell><cell>0.793</cell></row><row><cell>CDA</cell><cell>0.923</cell><cell>0.592 0.769</cell><cell>0.871</cell><cell>0.795</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XI :</head><label>XI</label><figDesc>Analysis of CutMix applied to different damage levels.</figDesc><table><row><cell>Method</cell><cell>No damage Minor Major Destroyed F 1s</cell></row><row><cell>Ours (vanilla)</cell><cell>0.789</cell></row><row><cell>vanilla + CutMix</cell><cell>0.790</cell></row><row><cell>vanilla + CutMix</cell><cell>0.797</cell></row><row><cell>vanilla + CutMix</cell><cell>0.802</cell></row><row><cell>vanilla + CutMix</cell><cell>0.795</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XII :</head><label>XII</label><figDesc>Computational cost and inference speed analysis of the network in Stage 2.</figDesc><table><row><cell>Method</cell><cell cols="3">Params (M) FLOPs (G) Inference time (s/img)</cell></row><row><cell>Ours (vanilla)</cell><cell>32.5</cell><cell>92.9</cell><cell>0.174</cell></row><row><cell>vanilla + MFF</cell><cell>33.8</cell><cell>141.7</cell><cell>0.186</cell></row><row><cell>vanilla + CDA</cell><cell>33.1</cell><cell>107.6</cell><cell>0.181</cell></row><row><cell>Ours (BDANet)</cell><cell>34.4</cell><cell>155.4</cell><cell>0.190</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XIII :</head><label>XIII</label><figDesc>Comparison of different methods on LEVIR-CD dataset.</figDesc><table><row><cell>Method</cell><cell>precision</cell><cell>recall</cell><cell>F1</cell></row><row><cell>WNet [53]</cell><cell>0.879</cell><cell>0.995</cell><cell>0.934</cell></row><row><cell>U-Net++ [57]</cell><cell>0.890</cell><cell>0.996</cell><cell>0.940</cell></row><row><cell>BDANet (Stage 2)</cell><cell>0.895</cell><cell>0.995</cell><cell>0.942</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.digitalglobe.com/ecosystem/open-data</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Triple collocation to assess classification accuracy without a ground truth in case of earthquake damage assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pierdicca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anniballe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Noto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bignami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mannella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="485" to="496" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sensitive damage detection of reinforced concrete bridge slab by time-variant deconvolution of shf-band radar signal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mizutani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tarumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1478" to="1488" />
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Forecasting daily wildfire activity using poisson regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Coffield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Foufoula-Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Randerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4837" to="4851" />
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A seasonal-window ensemblebased thresholding technique used to detect active fires in geostationary remotely sensed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Reinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Assessment of the degree of building damage caused by disaster using convolutional neural networks in combination with ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">2858</biblScope>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structural Building Damage Detection with Deep Learning: Assessment of a State-of-the-Art CNN in Operational Conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Tonolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">2765</biblScope>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Change detection using landsat time series: A review of frequencies, preprocessing, algorithms, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="370" to="384" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A critical synthesis of remotely sensed optical image change detection techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Tewkesbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Comber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intelligent real-time earthquake detection by recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5440" to="5449" />
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building damage detection in satellite imagery using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zaytseva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)-Humanitarian Assistance and Disaster Response Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building disaster damage assessment in satellite imagery with multi-temporal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kan?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)-AI for Earth Sciences Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An attention-based system for damage assessment using satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Bartusiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Konz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Latourette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gribbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Comer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06643</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rescuenet: Joint building segmentation and damage assessment from satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">AFNet: Adaptive fusion network for remote sensing image semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiscale visual attention networks for object detection in vhr remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="310" to="314" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-Resolution Aerial Images Semantic Segmentation Using Deep Fully Convolutional Network With Channel Attention Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3492" to="3507" />
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">xbd: A dataset for assessing building damage from satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hosfelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sajeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gaston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d mri brain tumor segmentation using autoencoder regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention -MICCAI</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiscale Ushaped cnn building instance extraction framework with edge constraint for high-spatial-resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comprehensive review of earthquake-induced building damage detection with remote sensing techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="85" to="99" />
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Earthquake damage mapping: An overall assessment of ground surveys and vhr image change detection after l&apos;aquila 2009 earthquake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anniballe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Noto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bignami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stramondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pierdicca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="page" from="166" to="178" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Assessment of convolutional neural network architectures for earthquake-induced building damage detection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Msnet: A multilevel instance segmentation network for natural disaster damage assessment in aerial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="2023" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disaster damage detection through synergistic use of deep learning and 3d point cloud features derived from very high resolution oblique aerial images, and multiple-kernel-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vetrivel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vosselman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="45" to="59" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Post-disaster building damage detection from earth observation imagery using unsupervised and transferable anomaly detecting generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tilon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vosselman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">4193</biblScope>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Urban building damage detection from very high resolution imagery using one-class svm and spatial relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2009-07" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="112" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Damage Assessment of 2010 Haiti Earthquake with Post-Earthquake Satellite Image by Support Vector Selection and Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Musaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Ersoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1025" to="1035" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detection of urban damage using remote sensing and machine learning algorithms: Revisiting the 2010 haiti earthquake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Cooner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">868</biblScope>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Assessing building damage by learning the deep feature correspondence of before and after aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Presa-Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</title>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey on U-shaped networks in medical image segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">409</biblScope>
			<biblScope unit="page" from="244" to="258" />
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-scale fully convolutional network for gland segmentation using three-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">380</biblScope>
			<biblScope unit="page" from="150" to="161" />
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A geometry-attentional network for als point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="26" to="40" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Symmetrical dense-shortcut deep fully convolutional networks for semantic segmentation of very-high-resolution remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1633" to="1644" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-scale spatial and channel-wise attention for improving object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A multiple-feature reuse network to extract buildings from remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Road extraction from highresolution remote sensing imagery using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1461</biblScope>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1194" to="1206" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bidirectional guided attention network for 3-d semantic detection of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Foreground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">LANet: Local attention embedding to improve the semantic segmentation of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Positional context aggregation network for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="943" to="947" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Data Augmentation Using Random Image Cropping and Patching for Deep CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2917" to="2931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Data augmentation for hyperspectral image classification with deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="593" to="597" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">From W-Net to CDGAN: Bitemporal change detection via deep learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="4063" to="4067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recalibrating Fully Convolutional Networks With Spatial and Channel &quot;Squeeze and Excitation&quot; Blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="540" to="549" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">End-to-end change detection for high resolution satellite images using improved unet++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1382</biblScope>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1662</biblScope>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
