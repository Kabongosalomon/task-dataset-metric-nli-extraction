<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Line Detection Using Mirror Attention and Comparative Ranking and Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Line Detection Using Mirror Attention and Comparative Ranking and Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic lines</term>
					<term>line detection</term>
					<term>attention</term>
					<term>ranking</term>
					<term>matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel algorithm to detect semantic lines is proposed in this paper. We develop three networks: detection network with mirror attention (D-Net) and comparative ranking and matching networks (R-Net and M-Net). D-Net extracts semantic lines by exploiting rich contextual information. To this end, we design the mirror attention module. Then, through pairwise comparisons of extracted semantic lines, we iteratively select the most semantic line and remove redundant ones overlapping with the selected one. For the pairwise comparisons, we develop R-Net and M-Net in the Siamese architecture. Experiments demonstrate that the proposed algorithm outperforms the conventional semantic line detector significantly. Moreover, we apply the proposed algorithm to detect two important kinds of semantic lines successfully: dominant parallel lines and reflection symmetry axes. Our codes are available at https://github.com/dongkwonjin/Semantic-Line-DRM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A semantic line <ref type="bibr" target="#b27">[28]</ref> can be roughly defined as a dominant line, separating different semantic regions in a scene, which is reasonably approximated by an end-to-end straight line, as exemplified in <ref type="figure">Fig. 1</ref>.</p><p>Semantic lines are essential components in high-level image understanding <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57]</ref>. In photography, photographic composition rules, such as horizontal, diagonal, and symmetric ones, are described by semantic lines <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>. Under perspective projection, dominant parallel lines in the 3D world are projected to semantic lines in 2D images, intersecting at vanishing points and conveying depth impressions <ref type="bibr" target="#b56">[57]</ref>. Also, in autonomous driving systems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, the boundaries of road lanes, sidewalks, or crosswalks are important sematic lines. However, it is difficult to detect semantic lines, which are often unobvious and implied by complex boundaries of semantic regions.</p><p>Although many techniques have been developed to detect lines by exploiting low-level cues <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref> or deep features <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56]</ref>, they extract many short (possibly noisy) line segments or rather obvious lines in man-made environments. Also, several attempts <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52]</ref> have been made to detect unobvious horizon lines. However, horizons are just a specific type of semantic lines. Recently, SLNet, which is a general semantic line detector, was proposed in <ref type="bibr" target="#b27">[28]</ref>. Although SLNet provides promising results, it tends to detect many redundant lines near the boundaries of semantic regions.</p><p>In this paper, we propose a novel semantic line detection algorithm, called DRM, which consists of three networks: detection network with mirror attention (D-Net) and comparative ranking and matching networks (R-Net and M-Net). In <ref type="figure">Fig. 2</ref>, D-Net first extracts semantic lines by classifying and regressing candidate lines. For effective detection, we design the mirror attention module and the region pooling layer in D-Net. Then, by comparing semantic lines in pairs, R-Net selects the most semantic line and M-Net removes redundant lines overlapping with the selected one. This comparative ranking and matching process is performed iteratively. In <ref type="figure">Fig. 2</ref>, three iterations are performed to yield three semantic lines. Experimental results demonstrate that the proposed DRM algorithm outperforms the conventional SLNet <ref type="bibr" target="#b27">[28]</ref> significantly.</p><p>This work has the following major contributions:</p><p>-We develop D-Net to detect semantic lines, in which the mirror attention module and the region pooling layer extract discriminate features effectively. -We propose two Siamese networks, R-Net and M-Net, for pairwise ranking and matching of semantic lines. -We construct a challenging dataset (SEL Hard) of semantic lines, which are highly implied in cluttered scenes. 1 -We also apply the proposed algorithm to two important line detection tasks: dominant parallel lines and reflection symmetry axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Line detection</head><p>Lines are geometrically important cues to describe the layouts or structural information of images. In line segment detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>, many short line segments are detected using low-level cues (e.g. image gradients). However, this approach may not discriminate meaningful lines from noisy ones. To utilize higherlevel cues, deep learning methods have been proposed <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56]</ref>. In <ref type="bibr" target="#b22">[23]</ref>, two networks were used to predict, respectively, a line heat map and junctions in man-made environments. Then, the wireframe, summarizing the scene, was obtained by connecting the junctions based on the heat map. In <ref type="bibr" target="#b55">[56]</ref>, a network verified whether a candidate line was salient or not, where the candidate was also generated by connecting two junctions. In <ref type="bibr" target="#b48">[49]</ref>, the line segment detection was posed as the dual problem of region coloring to address local ambiguity and class imbalance. In <ref type="bibr" target="#b39">[40]</ref>, a network was trained to yield the coordinates of a bounding box, whose diagonal was the resultant line segment. However, these methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56]</ref> detect rather obvious lines in man-made environments.</p><p>Meanwhile, several methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52]</ref> have been developed to detect implied lines. In <ref type="bibr" target="#b47">[48]</ref>, horizon lines were directly estimated by CNNs, without requiring geometric constraints. In <ref type="bibr" target="#b51">[52]</ref>, horizons were detected similarly to <ref type="bibr" target="#b47">[48]</ref>, but their locations were refined by exploiting vanishing points. In <ref type="bibr" target="#b11">[12]</ref>, soft labels of horizon line parameters were used to train the regression network. In <ref type="bibr" target="#b27">[28]</ref>, the first semantic line detector was proposed, which can detect general, semantically meaningful lines. Semantic lines, located near the boundaries of semantic regions, represent the layout and composition of an image, even when the boundaries are not straight lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention mechanisms in CNNs</head><p>Human visual system pays more attention to salient parts of a scene for efficiency <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. Similarly, attempts have been made to bias the processing resource of a neural network towards more informative parts of input data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. Also, attention mechanisms have been developed to improve the representation power of convolutional layers in CNN-based vision tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref>. In <ref type="bibr" target="#b45">[46]</ref>, Wang et al. adopted an encoder-decoder structure to obtain a pixel-wise attention mask of a convolutional feature map. In <ref type="bibr" target="#b21">[22]</ref>, to address the interdependencies of filter responses, Hu et al. used the average-pooled feature at each channel to compute the channel-wise attention. In <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47]</ref>, this channel-wise attention module has been modified to obtain both spatial and channel-wise attention. In <ref type="bibr" target="#b24">[25]</ref>, multiple attention maps were obtained from intermediate convolutional layers, and then the ensemble of those maps was applied to the last layer. In <ref type="bibr" target="#b52">[53]</ref>, Zhao and Wu applied spatial attention to lower layers to focus on local details and channel-wise attention to higher layers to capture contextual cues. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Metric learning and order learning</head><p>Metric learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> constructs an appropriate feature embedding space, where similar objects are located tightly while dissimilar objects are far from one another. In contrast, in order learning <ref type="bibr" target="#b29">[30]</ref>, embedded features are ordered according to the ranks or priorities of objects. Both the similarity and order relationships depend on target applications and are implicitly defined by userprovided examples. Accordingly, the learned metric is useful for matching similar objects, e.g., in image retrieval <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>, person re-identification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>, and fewshot learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref>. On the other hand, the learned order can be used to rank or sort objects, as done in image quality assessment <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref>, object detection <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>, and age estimation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Algorithm</head><p>We propose a novel semantic line detection algorithm, called DRM, which is composed of three networks: D-Net, R-Net, and M-Net. <ref type="figure" target="#fig_1">Fig. 3</ref> is an overview of the proposed DRM algorithm. First, we generate candidate lines by connecting two points, uniformly sampled on image boundaries <ref type="bibr" target="#b27">[28]</ref>. Second, D-Net extracts semantic lines by classifying and regressing the candidate lines. For discriminative feature extraction, we design the mirror attention module and the region pooling layer. Third, through pairwise comparisons, we iteratively select the most meaningful semantic line and remove the other semantic lines overlapping with the selected one. For this purpose, we develop R-Net and M-Net in the Siamese architecture. </p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">D-Net: Semantic line detection with mirror attention</head><p>Mirror attention: A semantic line separates a region into two distinct subregions. Those two regions can be semantically different as in <ref type="figure" target="#fig_2">Fig. 4</ref>(a), (b), or symmetric around the line as in <ref type="figure" target="#fig_2">Fig. 4</ref>(c), (d). In the former case, if a region is mirrored along the line, it should contain quite different objects from the other region. In the latter case, it should be almost identical with the other region. To summarize, the line is semantic because of the mirrored dissimilarity (heterogeneity of two regions) or mirrored similarity (symmetry). Based on this observation, we develop the mirror attention module in <ref type="figure" target="#fig_3">Fig. 5</ref>. Given a feature map, the mirror attention module generates an attention mask, which is then used to reweight the feature map to make it more discriminative. We apply the mirror attention module to a convolutional feature map X = [X 1 , X 2 , . . . , X C ] ? R H?W ?C , where H, W , and C denote the height, the width, and the number of channels. Since pixels near a candidate line are more relevant for semantic line detection, we first obtain a weighted feature map</p><formula xml:id="formula_1">Y = [Y 1 , Y 2 , . . . , Y C ] ? R H?W ?C , where X c (k) for pixel k is weighted by Y c (k) = ?(d k ) ? X c (k).<label>(1)</label></formula><p>Here, d k is the distance of pixel k from the candidate line and ?(?) is the Gaussian weighting function.</p><p>To analyze the mirror relationships around the candidate line, we obtain the mirrored feature map? by flipping Y across the line. If a flipped pixel is outside the feature map, it is set to zero. Then, we concatenate Y and? and obtain an initial mask A 0 ? R H?W by</p><formula xml:id="formula_2">A 0 = f 0 ([Y,? ])<label>(2)</label></formula><p>where f 0 is a convolutional layer using a single filter of size n ? n ? 2C. We set n to 3 empirically. To increase the receptive field and capture the semantics from a wider region, we use two more convolution layers to yield</p><formula xml:id="formula_3">A 2 = f 2 (f 1 (A 0 )),</formula><p>where f 1 or f 2 uses a single filter of size (2n + 1) ? (2n + 1) ? 1. Finally, we obtain the attention mask A = ?(A 2 ) where ?(?) is the sigmoid activation function. We apply the mirror attention module to two deep layers of D-Net, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The attention mask may over-suppress the values in the weighted feature map Y . To prevent this, we adopt the residual attention scheme <ref type="bibr" target="#b45">[46]</ref>. More specifically, we obtain the attended feature map</p><formula xml:id="formula_4">Y att = [Y 1 att , Y 2 att , . . . , Y C att ], where Y c att is given by Y c att = (1 + A) ? Y c<label>(3)</label></formula><p>for each 1 ? c ? C. <ref type="figure" target="#fig_2">Fig. 4</ref> shows examples of attention masks. In <ref type="figure" target="#fig_2">Fig. 4</ref>(a)?(d), there are roughly two semantic regions around the candidate line. We see that one region is attended with small weights, while the other with big weights. Thus, the feature difference between the two regions is emphasized, facilitating the semantic line detection. Note that the mirror attention module is trained in an end-to-end manner such that emphasizing masks are generated in both cases of mirrored dissimilarity ( <ref type="figure" target="#fig_2">Fig. 4</ref>(a) or (b)) and mirrored similarity ( <ref type="figure" target="#fig_2">Fig. 4</ref>(c) or (d)). On the other hand, in <ref type="figure" target="#fig_2">Fig. 4</ref>(e) or (f), a less informative mask is generated because the candidate line is not semantic. Region pooling: Whereas the conventional algorithm <ref type="bibr" target="#b27">[28]</ref> uses a line pooling layer, we design a region pooling layer to extract more discriminated features from the mirror-attended feature map Y att . We set two adjacent regions U and V along the candidate line, which contain pixels whose distances from the line are less than a threshold, respectively. Then, we aggregate the regional information of U and V into u and v ? R C ;</p><formula xml:id="formula_5">u = 1 |U| k?U Y att (k) and v = 1 |V| k?V Y att (k).<label>(4)</label></formula><p>Then, u and v are concatenated to form the feature vector of the candidate line. D-Net architecture: We plug the proposed mirror attention module and region pooling layer into the classification-regression framework of <ref type="bibr" target="#b27">[28]</ref>. Hence, D-Net takes an image and a candidate line, parameterized by l = (x s , y s , x e , y e ), and yields classification and regression results. <ref type="figure" target="#fig_1">Fig. 3(a)</ref> shows its architecture. We use the 13 convolution layers of VGG16 <ref type="bibr" target="#b35">[36]</ref> as the backbone, and implement two mirror attention modules after Conv10 and Conv13, respectively. From each mirror-attended feature map, the region pooling layer extracts the feature vector. The two vectors are concatenated and fed into fully connected layers FC1 and FC2. Finally, D-Net branches into two parallel output layers: one for classifying the candidate line (Cls), and the other for computing regression offsets for the line parameters (Reg). Cls computes the softmax vector p = (p, q), where p is the probability that the candidate line l is semantic. Reg outputs a line offset ?l. When p &gt; 0.5, D-Net declares that the regressed line l + ?l is semantic.</p><p>To train D-Net, when a candidate line is annotated byp and ?l, we minimize the loss L(p,p, ?l, ?l) = L cls (p,p) + ?L reg (?l, ?l)</p><p>where L cls (p,p) and L reg (?l, ?l) are the classification loss and the regression loss, respectively, and ? is a balancing parameter. L cls is the cross-entropy loss over the two classes (semantic and non-semantic). L reg (?l, ?l) = ?(?l ? ?l), where ? is the smooth L 1 loss <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">R-Net and M-Net: Comparative ranking and matching</head><p>Note that D-Net detects many semantic lines from densely sampled candidate lines, as illustrated in <ref type="figure">Fig. 2</ref>. Since each candidate is tested independently, semantically identical lines are detected closely. For example, in <ref type="figure">Fig. 2</ref>, there are three groups, each of which contains semantically identical lines. From each group, we select the most reliable line, while removing the other redundant ones. To this end, we develop the comparative ranking and matching networks, referred to as R-Net and M-Net, respectively. Given a pair of semantic lines, R-Net finds which one is more reliable and M-Net determines whether they are semantically identical or not. Thus, R-Net is related to priority in order learning <ref type="bibr" target="#b29">[30]</ref>, while M-Net is to similarity in metric learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. Both R-Net and M-Net are implemented as binary classifiers in the Siamese architecture. <ref type="figure" target="#fig_4">Fig. 6</ref> shows R-Net, which yields a softmax probability vector p r = (p r , q r ). Here, p r or q r is the probability that line i is more reliable or less reliable than line j, respectively. Since D-Net is well-trained for semantic line detection, we truncate it before the FC2 layer and use it as the feature extractor of R-Net. Then, the features of the two lines are concatenated, propagated into two fully connected layers, and categorized into one of the binary classes. The crossentropy loss is used to train R-Net. Also, M-Net is implemented in the same way as R-Net, except that it yields a softmax vector p m = (p m , q m ), where p m or q m is the probability that the two lines are semantically identical or not, respectively.</p><p>Using R-Net and M-Net, we perform the selection and removal of semantic lines alternately. At step t, we measure the reliability of each semantic line. Specifically, the reliability r i of semantic line i is defined as</p><formula xml:id="formula_7">r i = Nt j=1, j =i p r ij (6)</formula><p>where N t is the number of available semantic lines at step t, and p r ij is the probability that line i is more reliable than line j. Then, we select the most reliable line i * by i * = arg max i r i .</p><p>We then remove the lines that are semantically identical with line i * . Specifically, line j is removed if the matching probability p m i * j from M-Net is higher than 0.5. We iteratively perform the alternate selection and removal until N t = 0. In the example of <ref type="figure">Fig. 2</ref>, three iterations are performed to select the three resultant lines. The firstly selected line is called the primary line.</p><p>We configure the training data for R-Net and M-Net as follows. Note that a detected semantic line is declared to be correct if its mean intersection over union (mIoU) ratio with the ground-truth is higher than 0.85 <ref type="bibr" target="#b27">[28]</ref>. After training D-Net on the SEL dataset <ref type="bibr" target="#b27">[28]</ref>, we use the correctly detected semantic lines in the training images to train R-Net and M-Net. For R-Net, a ground-truth semantic line and one of its detection results are used as an input pair. To encode the one-hot vectorp r = (p r ,q r ), the ground-truth line is used as the more reliable one than the detection result. For M-Net, we use two detected lines as input. To encodep m = (p m ,q m ), if the two lines correspond to the same ground-truth, they are regarded as semantically identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEL:</head><p>The semantic line (SEL) dataset <ref type="bibr" target="#b27">[28]</ref> contains 1,750 outdoor images in total, which are split into 1,575 training and 175 testing images. Each semantic line is annotated by the coordinates of the two end-points on an image boundary. If an image has a single dominant line, it is set as the ground truth primary semantic line. If an image has multiple semantic lines, the line with the best rank by human annotators is set as the ground-truth primary line, and the others as additional ground-truth semantic lines. In SEL, 61% of the images contain multiple semantic lines. SEL Hard: In addition to the SEL dataset, we construct a more challenging test dataset, called SEL Hard. Its semantic lines are more implied (or less obvious), are more severely occluded, and are in more cluttered scenes. We collect 300 images from the ADE20K image segmentation dataset <ref type="bibr" target="#b53">[54]</ref>, manually annotate semantic lines, and then also select primary lines. Notice that SEL Hard is constructed for testing semantic line detectors and is not used for training them. The supplemental document describes the annotation process in detail and provides example images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic line detection results</head><p>We assess primary and multiple semantic line detection performances on the SEL and SEL Hard datasets.</p><p>We measure the accuracy for primary semantic line detection and the precision and recall rates for multiple semantic line detection, based on the mIoU metric <ref type="bibr" target="#b27">[28]</ref>. A semantic line is regarded as correctly detected if its mIoU score with the ground-truth is greater than a threshold ? . Then, the accuracy of the primary semantic line detection is defined as</p><formula xml:id="formula_9">Accuracy = N c N (8)</formula><p>where N c is the number of the test images whose primary semantic lines are correctly detected, and N is the number of all test images. For the multiple semantic line detection, the precision and the recall are computed by Precision = N l N l + N e , Recall = N l N l + N m <ref type="bibr" target="#b8">(9)</ref> where N l is the number of correctly detected semantic lines, N e is the number of false positives, and N m is the number of false negatives. <ref type="figure" target="#fig_5">Fig. 7</ref> compares the accuracy, precision, and recall curves of the proposed DRM algorithm and the conventional SLNet algorithm <ref type="bibr" target="#b27">[28]</ref> on the SEL dataset. The proposed DRM outperforms SLNet in all three curves in the entire range of the threshold ? . <ref type="table" target="#tab_0">Table 1</ref> reports the area under curve (AUC) performances of the accuracy, precision, and recall curves in <ref type="figure" target="#fig_5">Fig. 7</ref>, which are denoted by AUC A, AUC P, and AUC R, respectively. DRM provides higher AUC A, AUC P, and AUC R than SLNet by 2.54, 5.00, and 3.66, respectively. <ref type="table" target="#tab_0">Table 1</ref> also compares the performances on SEL Hard. For this comparison as well, we use the same DRM and SLNet, which are trained using the training images in the SEL dataset. Since SEL Hard consists of more challenging images, the performances are lower than those on SEL. Nevertheless, on SEL Hard, DRM outperforms SLNet by significant margins 7.09, 12.97, and 7.01 in terms of AUC A, AUC P, and AUC R, respectively. <ref type="figure" target="#fig_6">Fig. 8</ref> compares detection results qualitatively. Compared to SLNet, the proposed DRM detects implied, as well as obvious, semantic lines more precisely.  Also, DRM suppresses redundant lines more effectively. More detection results are available in the supplemental document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation studies</head><p>We conduct ablation studies to analyze the efficacy of the proposed D-Net, R-Net, and M-Net on the SEL dataset.</p><p>Efficacy of mirror attention model: <ref type="table" target="#tab_1">Table 2</ref> compares the performances of several ablated methods. First, to demonstrate the impacts of the mirror attention module in D-Net, we do not use the comparative ranking and matching (R-Net and M-Net). Instead, we adopt the non-maximum suppression (NMS) scheme in <ref type="bibr" target="#b27">[28]</ref>, which removes overlapped semantic lines based on low-level edge features. Method I uses no attention module. Method II uses the attention module in <ref type="figure" target="#fig_3">Fig. 5</ref> but without concatenating a flipped feature map. Method III replaces the mirror attention module with the spatial-channel attention in <ref type="bibr" target="#b46">[47]</ref>. As compared with no attention in I, the two attention schemes in II and III improve the accuracy and recall scores but lower the precision score. On the contrary, the proposed mirror attention model in IV improves all three scores and also outperforms the two alternative schemes in II and III. Also, by comparing IV with II, we see that the mirroring of feature maps across semantic lines is effective for emphasizing informative regions. More specifically, the mirroring improves AUC A, AUC P, and AUC R by 0.14, 2.65, and 1.37, respectively.  Efficacy of R-Net and M-Net: By comparing methods IV and V, we see that the proposed DRM algorithm provides 1.16, 1.46 and 0.93 higher AUC A, AUC P, and AUC R scores, by employing R-Net and M-Net instead of NMS. This indicates that the proposed comparative ranking and matching is an effective approach to select reliable semantic lines and remove redundant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Applications</head><p>We apply the proposed DRM algorithm to detect two kinds of semantically important lines: dominant parallel lines and reflection symmetry axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dominant parallel lines</head><p>When projected onto a 2D image, dominant parallel lines in the 3D world convey depth impressions, intersecting at a vanishing point (VP) <ref type="bibr" target="#b56">[57]</ref>. Despite researches  on the VP detection <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b56">57]</ref>, a single VP is less informative for conveying the depth information than those projected parallel lines are. Hence, we apply the proposed DRM algorithm to detect dominant parallel lines in a 2D image. We first extract semantic lines using D-Net. To this end, D-Net is trained to detect semantic lines passing through VPs. Next, using R-Net, the primary semantic line is selected. In this application, M-Net is trained to declare a pair of lines, which are parallel in the 3D space, as 'matched.' Then, we repeatedly select the semantic line yielding the highest 'matched' score to the primary semantic line. To avoid overlapping, we remove the semantic lines whose mIoUs with an already selected one are greater than 0.85 after each selection. We assess the proposed algorithm on the AVA landscape dataset <ref type="bibr" target="#b56">[57]</ref>. It contains 2,275 training and 275 test landscape images. For each image, a dominant VP and two dominant parallel lines are annotated. We declare a detected line as correct, when its distance to the ground-truth VP is smaller than a threshold. Then, we compute AUC A scores by varying the threshold. <ref type="table" target="#tab_2">Table 3</ref> compares the AUC A scores of the conventional SLNet <ref type="bibr" target="#b27">[28]</ref>, 'D-Net+NMS,' and the proposed algorithm according to the number K of detected lines in each image. As more lines are selected in an image, the accuracy score is lowered. However, for every K, D-Net+NMS outperforms SLNet, which indicates that D-Net detects dominant parallel lines more precisely. Moreover, by employing R-Net and M-Net, the proposed algorithm further improves the performances. <ref type="figure" target="#fig_7">Fig. 9</ref> shows that the proposed algorithm detects dominant lines that pass through VPs accurately. Next, we detect a VP as the intersecting point of the first two selected lines. <ref type="table" target="#tab_3">Table 4</ref> compares this VP detection scheme with the existing methods <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b56">57]</ref>. Angle accuracies AA1 ? , AA2 ? , AA10 ? are used as the performance metrics, as done in <ref type="bibr" target="#b54">[55]</ref>. Two performances of NeurVPS are reported in <ref type="bibr" target="#b54">[55]</ref>. <ref type="table" target="#tab_3">Table 4</ref> includes their accuracies when the same training data as the proposed algorithm are used. Note that the proposed algorithm focuses on the detection of dominant lines and provides VPs as side results. In contrast, the existing methods are tailored for the VP detection. Therefore, when the tolerance angles are small (1 ? or 2 ? ), the proposed algorithm yields poorer accuracies than the existing methods. However, when the tolerance angle is 10 ? , the proposed algorithm outperforms them. This indicates that the proposed algorithm can detect rough locations of VPs with a high recall rate, although it lacks the precision of the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reflection symmetry axes</head><p>Reflection symmetry is a common, but important visual property in various scenes, such as landscapes and man-made structures <ref type="bibr" target="#b31">[32]</ref>. However, since reflection symmetry axes are often highly implied or even invisible, their detection should exploit semantic regions around the axes. Accordingly, we train the proposed algorithm to detect the reflection symmetry axis of an image as the primary semantic line. More specifically, we train D-Net to extract symmetry axes as semantic lines, and R-Net to prioritize those axes among the detected lines. We empirically find that the Gaussian weighting in (1) and the residual attention in (3) are less effective in this task. Hence, we exclude those operations from the mirror attention module in <ref type="figure" target="#fig_3">Fig. 5</ref>. Then, we extract semantic lines using D-Net, and choose the most reliable one as the symmetry axis via (7) using R-Net.</p><p>We test the proposed algorithm on three datasets: ICCV <ref type="bibr" target="#b15">[16]</ref>, NYU <ref type="bibr" target="#b8">[9]</ref>, and SYM Hard. ICCV provides 100 training and 96 test images, and NYU contains 176 test images. In SYM Hard, we collect 45 images from photo sharing websites <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, each of which includes a reflection symmetry axis. The axis is implied, and the neighboring regions are not exactly symmetric. Thus, its detection is challenging. Since the proposed algorithm detects symmetry axes as primary lines, we compare the proposed algorithm with the existing methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>  <ref type="figure">Fig. 10</ref>: Detection results of symmetry axes. The ground-truth axes are in red, the detection results of Loy and Eklundh <ref type="bibr" target="#b32">[33]</ref> are in green, and those of the proposed algorithm are in yellow. using the AUC A metric. We train the proposed algorithm using the ICCV training images and use it to assess the performances on all three datasets. <ref type="table" target="#tab_4">Table 5</ref> compares the results. On ICCV, NYU, and SYM Hard, the proposed algorithm outperforms the existing methods by at least 0.83, 1.93, and 2.74, respectively. <ref type="figure">Fig. 10</ref> compares detection results of the proposed algorithm with those of Loy and Eklundh <ref type="bibr" target="#b32">[33]</ref>. The proposed algorithm detects symmetry axes more robustly.</p><p>More experimental results are available in the supplemental document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed a novel semantic line detector using D-Net, R-Net, and M-Net. First, D-Net extracts semantic lines using the mirror attention module. Second, R-Net selects the most semantic line through ranking. Third, M-Net removes redundant lines overlapping with the selected one through matching. The second and third steps are alternately performed to yield reliable semantic lines as output. Experimental results demonstrated that the proposed DRM algorithm outperforms the conventional SLNet significantly. Moreover, it was shown that the proposed algorithm can be applied to successfully detect two important kinds of semantic lines: dominant parallel lines and reflection symmetry axes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :Fig. 2 :</head><label>12</label><figDesc>Examples of various semantic lines. arXiv:2203.15285v1 [cs.CV] 29 Mar 2022 Comparative ranking and matching (R-Net and M-Net) Semantic line detection (D-Net) Illustration of the proposed DRM algorithm: First, D-Net extracts semantic lines (orange). Second, R-Net selects the most semantic line (yellow). Third, M-Net removes redundant lines overlapping with the selected one. The second and third steps are iteratively applied. In this example, three semantic lines are selected. The first one is called the primary semantic line (dashed red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>=Fig. 3 :</head><label>3</label><figDesc>( s , s , e , e ) Candidate line Image (b) Comparative ranking and matching (R-Net and M-Net) (a) Semantic line detection (D-Net) An overview of the proposed algorithm: (a) D-Net detects semantic lines, by classifying and regressing candidate lines, based on mirror attention (MA). (b) R-Net selects the most meaningful semantic line and M-Net removes redundant lines alternately through pairwise comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>[Top] A semantic (or candidate) line is shown in red, while two regions producing the line are in cyan and yellow.[Bottom] The attention mask is colorcoded: red and blue depict big and small values. Note that the two regions are semantically different from each other in (a) and (b) and symmetric in (c) and (d). In (e) and (f), the candidate lines are not semantic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Illustration of the mirror attention process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Siamese architecture for R-Net: Given a pair of detected lines from D-Net, R-Net decides whether one line is more reliable or less reliable than the other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Comparison of the accuracy, precision, and recall curves of the proposed DRM and the conventional SLNet in terms of the threshold ? on the SEL dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Comparison of semantic line detection results. The left two images are from SEL, and the other three from SEL Hard. Primary and multiple semantic lines are depicted in dashed red and solid yellow, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Detection results of dominant parallel lines. For reference, the groundtruth vanishing points are depicted by red cross symbols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the AUC scores (%) on the SEL and SEL Hard datasets.</figDesc><table><row><cell></cell><cell></cell><cell>SEL</cell><cell></cell><cell></cell><cell>SEL Hard</cell><cell></cell></row><row><cell></cell><cell cols="6">AUC A AUC P AUC R AUC A AUC P AUC R</cell></row><row><cell>SLNet</cell><cell>92.00</cell><cell>80.44</cell><cell>83.50</cell><cell>73.59</cell><cell>74.22</cell><cell>70.68</cell></row><row><cell>Proposed DRM</cell><cell>94.54</cell><cell>85.44</cell><cell>87.16</cell><cell>80.68</cell><cell>87.19</cell><cell>77.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The ablation studies in terms of the mirror attention module and comparative ranking and matching. AUC scores (%) of primary and multiple semantic line detection are compared on the SEL dataset.</figDesc><table><row><cell>AUC A AUC P AUC R</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>AUC A scores (%) in the dominant parallel line detection, according to the number K of detected lines.</figDesc><table><row><cell>K</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>SLNet [28]</cell><cell>46.83</cell><cell>41.73</cell><cell>38.58</cell><cell>37.22</cell><cell>36.44</cell></row><row><cell>D-Net+NMS</cell><cell>52.62</cell><cell>48.93</cell><cell>46.24</cell><cell>44.71</cell><cell>43.30</cell></row><row><cell>Proposed</cell><cell>56.31</cell><cell>54.36</cell><cell>52.42</cell><cell>51.17</cell><cell>50.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of AA scores (%) for the dominant VP detection.</figDesc><table><row><cell></cell><cell>AA1 ?</cell><cell>AA2 ?</cell><cell>AA10 ?</cell></row><row><cell>Zhou et al. [57]</cell><cell>18.5</cell><cell>33.0</cell><cell>60.0</cell></row><row><cell>NeurVPS [55]</cell><cell>19.6</cell><cell>35.9</cell><cell>65.9</cell></row><row><cell>Proposed</cell><cell>8.6</cell><cell>22.9</cell><cell>68.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of AUC A scores (%) of the symmetry axis detection.</figDesc><table><row><cell></cell><cell>ICCV</cell><cell>NYU</cell><cell>SYM Hard</cell></row><row><cell>Cicconet et al. [8]</cell><cell>80.80</cell><cell>82.85</cell><cell>68.99</cell></row><row><cell>Elawady et al. [14]</cell><cell>87.24</cell><cell>83.83</cell><cell>73.90</cell></row><row><cell>Cicconet et al. [9]</cell><cell>87.38</cell><cell>87.64</cell><cell>81.04</cell></row><row><cell>Loy &amp; Eklundh [33]</cell><cell>89.77</cell><cell>90.85</cell><cell>81.99</cell></row><row><cell>Proposed</cell><cell>90.60</cell><cell>92.78</cell><cell>84.73</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">SEL Hard is available at https://github.com/dongkwonjin/Semantic-Line-DRM.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flickr</surname></persName>
		</author>
		<ptr target="https://www.flickr.com/13" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Googleimage</surname></persName>
		</author>
		<ptr target="https://www.google.com/13" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">EDLines: A real-time line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Topal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog. Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep age estimation: From classification to ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using ranking-CNN for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vehicle re-identification with viewpoint-aware metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A convolutional approach to reflection symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cicconet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog. Lett</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding mirror symmetry via registration and optimal symmetric pairwise assignment of curves: Algorithm and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cicconet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV Workshops</title>
		<meeting>IEEE ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Control of goal-directed and stimulus-driven attention in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Corbetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Shulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meaningful alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desolneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Soft labels for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wavelet-based reflection symmetry detection via textural and color histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elawady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ducottet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Colantoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV Workshops</title>
		<meeting>IEEE ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The photographer&apos;s eye: Composition and design for better digital photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Focal Press</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">ICCV Challenge: Detecting symmetry in the wild</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Proc. IEEE ICCV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SOML: Sparse online metric learning with application to image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust road boundary estimation for intelligent vehicles in challenging scenarios based on a semantic graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recent progress in road and lane detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="745" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised distance metric learning for collaborative image retrieval and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimed. Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learn to pay attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Photo aesthetics ranking network with attributes and content adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Skew estimation of natural images based on a salient line detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13020</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic line detection and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Photographic composition classification and dominant geometric element detection for outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Order learning and its application to age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR (2020)</title>
		<meeting>ICLR (2020)</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RankIQA: Learning from rankings for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Computational symmetry in computer vision and computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hel-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting symmetry and symmetric constellations of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust detection of lines using the progressive probabilistic hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BAM: Bottleneck attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">You reap what you sow: Using videos to generate high precision object proposals for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sem-LSD: A learning-based semantic line segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06591</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to rank proposals for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">LSD: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Horizon lines in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">QANet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ICLR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with relational inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Detecting vanishing points using global image context in a non-manhattan world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE2020K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">NeurVPS: Neural vanishing point scanning via conic convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Detecting dominant vanishing points in natural scenes with application to composition-sensitive image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Farhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
