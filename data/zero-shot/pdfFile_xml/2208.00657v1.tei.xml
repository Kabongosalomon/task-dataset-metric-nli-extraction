<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIAMIXFORMER: A SIAMESE TRANSFORMER NETWORK FOR BUILDING DETECTION AND CHANGE DETECTION FROM BI-TEMPORAL REMOTE SENSING IMAGES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-25">August 25, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Mohammadian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="laboratory">Human-Computer interaction lab</orgName>
								<orgName type="institution">Tarbiat Modares University Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foad</forename><surname>Ghaderi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="laboratory">Human-Computer interaction lab</orgName>
								<orgName type="institution">Tarbiat Modares University Tehran</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SIAMIXFORMER: A SIAMESE TRANSFORMER NETWORK FOR BUILDING DETECTION AND CHANGE DETECTION FROM BI-TEMPORAL REMOTE SENSING IMAGES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-25">August 25, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Building detection and change detection using remote sensing images can help urban and rescue planning. Moreover, they can be used for building damage assessment after natural disasters. Currently, most of the existing models for building detection use only one image (pre-disaster image) to detect buildings. This is based on the idea that post-disaster images reduce the model's performance because of presence of destroyed buildings. In this paper, we propose a siamese model, called SiamixFormer, which uses pre-and post-disaster images as input. Our model has two encoders and has a hierarchical transformer architecture. The output of each stage in both encoders is given to a temporal transformer for feature fusion in a way that query is generated from pre-disaster images and (key, value) is generated from post-disaster images. To this end, temporal features are also considered in feature fusion. Another advantage of using temporal transformers in feature fusion is that they can better maintain large receptive fields generated by transformer encoders compared with CNNs. Finally, the output of the temporal transformer is given to a simple MLP decoder at each stage. The SiamixFormer model is evaluated on xBD, and WHU datasets, for building detection and on LEVIR-CD and CDD datasets for change detection and could outperform the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural disasters such as earthquakes, floods, and tsunamis have always been a human concern that take lives of many people every year and impose many financial losses on different countries. Highly affected by the changes in global climate, natural disasters have become stronger and more frequent in recent years <ref type="bibr" target="#b0">[1]</ref>.One of the most important tasks in response phase of disaster management is assessing damages to buildings and roads from satellite images as fast as possible. To this end, it is essential to detect buildings and identify the level of changes in them. Detecting building and their changes can also be used for urban planning, rescue planning, and preparation before disasters. Traditionally, these analyses are performed by experts, which is a time-consuming task <ref type="bibr" target="#b1">[2]</ref>.</p><p>With recent advances in computer vision, it is possible to analyze satellite images automatically with high accuracy and speed. In traditional methods, hand-crafted features such as color, shadow, edge, and roof texture were extracted from images, and a feature vector was generated for each sample. Then classification or clustering was done using classic machine learning algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. In more recent techniques, like convolutional neural networks (CNNs), features are extracted in a data-driven manner. Much effort has been done on utilizing CNN models and fusing deep and shallow features, in this problem. For example, a dual-stream network (DS-Net) that adaptively captures local and long-range information is proposed in <ref type="bibr" target="#b6">[7]</ref>. The authors of <ref type="bibr" target="#b7">[8]</ref> introduced a multi-scale fusion network to tackle the problem of different scales of buildings in remote sensing images. A Siamese fully connected network is proposed in <ref type="bibr" target="#b8">[9]</ref> and it is shown that using two inputs can help the model obtain better segmentation accuracy. arXiv:2208.00657v1 [cs.CV] 1 Aug 2022</p><p>Using bi-temporal images is a common approach in problems such as change detection or building damage assessment. In this approach, images that are taken at two different times are compared in order to assess the difference between them. In general, combining the features extracted from bi-temporal images is performed at different levels as follows:</p><p>1. Segmentation map level: Two separate encoder-decoder pairs and a fusion module are used. Each encoderdecoder pair generates a segmentation map for each bi-temporal image, and the differentiating module is used to compare them and generate the final change map <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. 2. Feature level: Two separate encoders, a common decoder and a fusion module are used. The extracted features from each encoder are compared and fused by the fusion module, and the decoder generates the final segmentation map <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. 3. Input image level: Bi-temporal images are combined. An encoder-decoder pair extracts the features from combination of the two images and produces the final change map <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Recently, many researchers from different domains used transformer architectures. These models were first introduced for natural language processing <ref type="bibr" target="#b15">[16]</ref>, however, their application expanded quickly to other domains, e.g., computer vision applications <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Due to their robust feature presentation, large and global receptive field, and the capability of modeling long-range dependency between pixels, transformer models can be used in remote sensing problems such as building detection and change detection as well <ref type="bibr" target="#b18">[19]</ref>. Xiao et al. <ref type="bibr" target="#b19">[20]</ref> presented STEB-UNET model, which is a combination of swin transformer and U-Net architectures. Because each building occupies a tiny part of the remote sensing images, Chen et al. represented buildings as a set of sparse feature vectors and introduced the sparse token transformer and could reduce computational complexity <ref type="bibr" target="#b18">[19]</ref>. Bandara et al. used a hierarchical structure for the transformer encoder and used two encoders to extract features of bi-temporal images <ref type="bibr" target="#b20">[21]</ref>. They concatenated each stage's output and fed it to an decoder. Chen et al. introduced the BIT-CD model by combining CNN and transformer architectures <ref type="bibr" target="#b21">[22]</ref>.</p><p>The authors of <ref type="bibr" target="#b17">[18]</ref>, extended the idea of transformers and proposed SegFormer architecture, which is a model for semantic segmentation problems. Although this model has fewer parameters compared with the other transformer-based models, it performs better in benchmark datasets <ref type="bibr" target="#b17">[18]</ref>. Considering the success of Segformer, recently, different models based on this architecture have been introduced, e.g., the Damformer <ref type="bibr" target="#b22">[23]</ref> and the Changeformer <ref type="bibr" target="#b20">[21]</ref> models that have been used for building damage assessment and change detection problems, respectively. In both models, CNNs are used in the feature fusion section.</p><p>In this paper, we propose the SiamixFormer model that uses bi-temporal images for building detection. Experimental results confirm that this approach outperforms the existing methods for building and change detection problems. Our proposed model uses a temporal transformer for feature fusion that maintains a large receptive field and considers the temporal relationship between features, and hence improves the results. The contributions of our work are as follows:</p><p>? We proposed a fully-transformer model named SiamixFormer, which can be used for semantic segmentation problems using bi-temporal input images, such as building detection and change detection from remote sensing images. ? We used bi-temporal images for building detection, contrary to other models that use mono-temporal images, and obtained improved performance. ? Our model is superior to other state-of-the-art methods in terms of F1-score and IoU for building detection and change detection.</p><p>The structure of the paper is as follows. In section 2, we introduce our proposed model and its details. In section 3, we present the experimental results and compare the performance of our model with that of the state-of-the-art models. Finally, we conclude the paper in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we introduce our proposed model, named SiamixFormer, which stands for Siamese mix transformer. First, we explain the data pipeline and the overall architecture of the model. After that, encoder, temporal transformer, and decoder architectures are described. Finally, the loss functions that we used for different datasets are explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall architecture and data pipeline</head><p>As shown in <ref type="figure">Figure 1</ref>, two input images taken from the same place at two different times are given to the model. For building detection task, the output of the model is a segmentation map corresponding to the T 1 image. The output of <ref type="figure">Figure 1</ref>: Overview of the proposed SiamixFormer model that consists of four main modules: two encoders that extract features of T 1 and T 2 input images taken from the same place at two different times; a feature fusion module using a temporal transformer to fuse extracted features considering the temporal feature and their context; and a light-weight decoder.</p><p>the model in change detection task is a change map indicating the changes happened between the two images. The SiamixFormer model consists of four main modules:</p><p>? Encoder-I dedicated to processing the T 1 image.</p><p>? Encoder-II dedicated to processing the T 2 image.</p><p>? Feature fusion module.</p><p>? Decoder module.</p><p>In our model, we use encoders that are consisting of four transformer stages with a hierarchical structure. As depicted in <ref type="figure">Fig. 1</ref>, height and width of the output of subsequent transformer blocks decrease, while the number of channels increase. The output of the transformer blocks are fed to the feature fusion module as well.</p><p>Using bi-temporal images and the two parallel encoders helps us to obtain diverse features from the same place. Taking advantage of the temporal changes between the two input images and their context, the extracted features are fused in the feature fusion module using the temporal transformer. Each temporal transformer's output of the feature fusion module is given directly to the decoder, and a segmentation map is generated using a multilayer perceptron (MLP) network. In the building detection problem, the segmentation map highlights the buildings in T 1 image, and in the change detection problem, it detects the changes between T 1 and T 2 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoders</head><p>We used the architecture of SegFormer encoders in our proposed method. The SegFormer architecture is an encoderdecoder model that has a hierarchical transformer structure. Six types of SegFormers exist according to the number of layers in its encoder. SegFormer-B0 is the smallest model for fast inference, and SegFormer-B5 is the largest model for the best performance <ref type="bibr" target="#b17">[18]</ref>. In a similar approach, our proposed model has six implementation types, i.e., SiamixFormer-0 to SiamixFormer-5.</p><p>The proposed encoder architecture has four transformer blocks and the output of each transformer block has the the dimensionality of H 2 (i+1) ? W 2 (i+1) ? C i where H and W are respectively the height and width of the input image, i ? {1, 2, 3, 4} is the transformer block number, and C i is number of channel in transformer block-i while C i+1 &gt; C i . This provides high-resolution coarse features and low-resolution fine-grained features that help semantic segmentation performance.</p><p>In each transformer block, CNN splits the input data into the desired patches with the specified patch size, stride, and padding, which helps the patches to overlap and maintain local continuity around the patches. Then these patches are flattened, and query, key, and value are generated. We use the method suggested in <ref type="bibr" target="#b23">[24]</ref> to reduce computational complexity and generate the new query, key, and value. The formulas used for reducing the length of the sequences are given below:</p><formula xml:id="formula_0">X new = Linear(C.R, C)(Reshape( N R , C.R)(X))<label>(1)</label></formula><p>where R is reduction ratio and X denotes the sequence to be reduced, i.e., Q, K, and V. Reshape( N R , C.R)(X) denotes reshape X to the one with the shape of N R ? (C.R). Linear(C in , C out ) denotes a linear layer that takes input with C in channel and generates output with C out channel. So we have new Q, K, and V with size ( N R , C). This process reduces the computational complexity from</p><formula xml:id="formula_1">O(N 2 ) to O( N 2 R ), where N = H ? W .</formula><p>In the SiamixFormer, R is set to [8,4,2,1] from transformer block-1 to transformer block-4.</p><p>Each transformer block consists of many layers, and each layer consists of some heads. <ref type="table" target="#tab_0">Table 1</ref> shows the number of layers and heads for each model in different transformer blocks. The new values of Q, K, and V are given in parallel to each layer's heads. Heads use the self-attention module, which can be considered as follow:</p><formula xml:id="formula_2">Attention(Q, K, V ) = Sof tmax( QK T ? d head )V (2)</formula><p>where d head is the head's dimensionality.</p><p>The outputs of the parallel heads are concatenated, and multi-head self-attention(MHA) module's output is generated.</p><p>To consider positional encoding, a 3?3 convolution is used in two layers of MLP, and finally, the output of each layer is generated. Each layer's output is given to the next layer, and this procedure continues until the last layer in transformer block. These steps can be formulated as follows:</p><formula xml:id="formula_3">Z l = M HA(Z l?1 ) + Z l?1 , l ? {1, 2, ..., L} Z l = M LP (GELU (Conv 3?3 (M LP (? l )))) +? l , l ? {1, 2, ..., L}<label>(3)</label></formula><p>where Z l is layer's output, and GELU denotes Gaussian Error Linear Unit activation function <ref type="bibr" target="#b24">[25]</ref>, L is number of layer in transformer block, MHA is multi-head self-attention module and MLP is fully connected layer. SiamixFormer-0</p><formula xml:id="formula_4">H 1 = 1 L 1 = 2 C 1 = 32 H 2 = 2 L 2 = 2 C 2 = 64 H 3 = 5 L 3 = 2 C 3 = 160 H 4 = 8 L 4 = 2 C 4 = 256</formula><p>SiamixFormer-1</p><formula xml:id="formula_5">H 1 = 1 L 1 = 2 C 1 = 64 H 2 = 2 L 2 = 2 C 2 = 128 H 3 = 5 L 3 = 2 C 3 = 320 H 4 = 8 L 4 = 2 C 4 = 512</formula><p>SiamixFormer-2</p><formula xml:id="formula_6">H 1 = 1 L 1 = 3 C 1 = 64 H 2 = 2 L 2 = 3 C 2 = 128 H 3 = 5 L 3 = 6 C 3 = 320 H 4 = 8 L 4 = 3 C 4 = 512</formula><p>SiamixFormer-3</p><formula xml:id="formula_7">H 1 = 1 L 1 = 3 C 1 = 64 H 2 = 2 L 2 = 3 C 2 = 128 H 3 = 5 L 3 = 18 C 3 = 320 H 4 = 8 L 4 = 3 C 4 = 512</formula><p>SiamixFormer-4</p><formula xml:id="formula_8">H 1 = 1 L 1 = 3 C 1 = 64 H 2 = 2 L 2 = 8 C 2 = 128 H 3 = 5 L 3 = 27 C 3 = 320 H 4 = 8 L 4 = 3 C 4 = 512</formula><p>SiamixFormer-5</p><formula xml:id="formula_9">H 1 = 1 L 1 = 3 C 1 = 64 H 2 = 2 L 2 = 6 C 2 = 128 H 3 = 5 L 3 = 40 C 3 = 320 H 4 = 8 L 4 = 3 C 4 = 512</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Temporal Transformer</head><p>Since the SiamixFormer model has been introduced for bi-temporal problems such as building detection and change detection, we had to use a method for feature fusion that in addition to maintaining large receptive field, can consider the temporal relationship between features. Inspired by <ref type="bibr" target="#b25">[26]</ref> we used temporal transformer for this purpose. The difference between a temporal transformer and the other transformers is that instead of creating Query, Key, and Value from a single input, the Query is extracted from the T 1 data stream, and Key, Value are extracted from the T 2 data stream. The process of the temporal transformer can be formulated as:</p><formula xml:id="formula_10">Z i = Attention(Y 1 i , Y 2 i , Y 2 i ) + Y 1 i , i ? {1, 2, 3, 4} Z i = M LP (? i ) +? i<label>(4)</label></formula><p>where Y 1 i , Y 2 i denote output of T 1 and T 2 data stream transformer block-i, respectively. Also Z i denotes output of the i th temporal transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Decoder</head><p>Using a hierarchical transformer allows us to have large receptive fields, and hence we can use a lightweight decoder. In our method, first, the output of each temporal transformer in each stage is entered to one MLP module in order to unify the number of channels. Next, the outputs are upsampled to the same size of H 4 ? W 4 , and concatenated to fuse features from different stages. Finally, the concatenated feature maps are fed to the last MLP to generate the final output with the size of H 4 ? W 4 ? N cls , where N cls denotes number of the classes. The decoder can be formulated as follows:</p><formula xml:id="formula_11">F i = Linear(C i , C)(Z i ), ?? F i = U psample( H 4 ? W 4 )(F i ), ?i F = Linear(4C, C)(Concat(F i )), ?i M = Linear(C, N cls )(F )<label>(5)</label></formula><p>where Linear is fully connected layer, and Upsample, Concat denote upsampling and concatenating operators, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Loss function</head><p>SiamixFormer model has been utilized for building detection and change detection problems. The datasets that we used are imbalanced in different levels, and hence models tend to majority classes in some cases. To tackle this problem, we used different loss functions or weights for different datasets. The details are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Weighted Cross-entropy</head><p>To explain weighted cross-entropy, assume that the vectorized segmentation map is? which can be represented as:</p><formula xml:id="formula_12">Y = {? i , i = 1, 2, ..., H ? W },? i ? {0, 1}<label>(6)</label></formula><p>where? i represents a pixel in the image. Weighted cross-entropy can be formulated as follows.</p><formula xml:id="formula_13">L W CE = 1 H ? W H?W i=1 w cls . log( exp(? i [cls]) 1 l=0 exp(? i [l]) )<label>(7)</label></formula><p>where w cls indicates class weights <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Focal Loss</head><p>Focal loss is used for datasets that are imbalanced. It applies a modulating term to cross-entropy loss, and the loss value will increase for misclassified data <ref type="bibr" target="#b27">[28]</ref>. Focal loss can be formulated as follows:</p><formula xml:id="formula_14">L F ocal (p t ) = ?(1 ? p t ) ? . log(p t ) p t = p if y = 1 1 ? p otherwise<label>(8)</label></formula><p>where y ? {0, 1} specifies the ground-truth class and p ? [0, 1] is the model's estimated probability. ? ? 0 is a hyperparameter and selecting bigger ? values, yields to reduced relative loss for well-classified samples. <ref type="figure">Figure 2</ref>: Labels of the CDD dataset, before and after pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Dice Loss</head><p>Dice coefficient is a metric widely used to calculate the similarity between two images. In segmentation problems, it can be adjusted as a loss function and formulated as follows: <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_15">L Dice = 1 ? 2.Y.sof tmax(? ) Y + sof tmax(? )<label>(9)</label></formula><p>We used sum of dice and focal loss in the LEVIR-CD and the WHU dataset. Also we used weighted cross-entropy for the CDD dataset and cross-entropy for the xBD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation</head><p>In order to investigate the effectiveness of our proposed model on building detection and change detection problems, we conducted the experiments on two different datasets for each problem. The datasets are introduced in the sequel:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">xBD</head><p>xBD is the largest publicly available dataset for building segmentation and disaster damage assessment. In this dataset, satellite images of 19 types of disasters such as earthquakes, floods, wildfires, and hurricanes with the size of 1024?1024 and a resolution of 0.8 m/pixel are collected. The dataset also includes more than 850,000 buildings with an area of more than 450,000 km 2 annotated. There are 18,336/1,866/1,866 images and 632,228/109,724/108,784 buildings in this dataset for train, validation, and test, respectively <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">WHU</head><p>The WHU dataset includes aerial images from 2012 and 2016 of areas where a 6.3-magnitude earthquake occurred in 2011. This dataset is labeled for building detection and change detection. It contains 1260 pairs of images for train and 690 pairs of images for test in the size of 512?512. There are also 12,796 buildings in 2012 and 16,077 buildings in <ref type="figure">Figure 3</ref>: Performance of different SiamixFormer models on the xBD dataset in building detection. In a) red rectangles show some buildings that even though were not labeled in the ground truth (GT) image, the SiamixFormer models succeeded to detect them correctly.</p><p>2016 in an area of 20.5 km 2 in this dataset. We have used this dataset for building detection by predicting the buildings in 2016 images <ref type="bibr" target="#b8">[9]</ref> and Due to the GPU memory capacity limitation, we split the images to 256?256 without overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">LEVIR-CD</head><p>This dataset contains 637 pairs of images in the size of 1024 ? 1024 and a resolution of 0.5 m/pixel, collected from 20 different regions. It focuses on building-related changes, including building growth and building decline. LEVIR-CD covers various buildings such as villa residences, tall apartments, small garages, and large warehouses. This dataset contains 31,333 building changes, with an average of approximately 50 building changes per image and 987 pixels per image change. Due to the GPU memory capacity limitation, we split the images to 256?256 without overlap, as suggested in <ref type="bibr" target="#b21">[22]</ref>. Finally, we obtained 7120 images for train, 1024 images for validation, and 2048 images for test <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">CDD</head><p>This dataset contains 11 pairs of satellite images with a resolution of 0.03 m/pixel to 1 m/pixel, collected in different seasons. Seven images have the size of 4,725?2,200, and four images with the size of 1,900?1,000, which have been clipped with the size of 256?256. Therefore, we obtain 10,000 images for train, 3000 for validation, and 3000 test images <ref type="bibr" target="#b30">[31]</ref>. In this dataset, labels are images in jpg format, and their values are in the range of [0,255] instead of {0,1}.</p><p>To handle this issue, we must consider a threshold and cluster the values into two classes, 0 and 1. By choosing any value for the threshold, some noise is produced in the label images, which makes the learning process challenging. We first considered all values greater than 0 as class 1, which creates much noise; then, we removed these noises by using erosion and dilation. <ref type="figure">Figure 2</ref> shows some examples of these images before and after this pre-processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Evaluation metrics</head><p>The experimental results of building detection are evaluated using the F1-score (F 1 b ) and intersection over union (IoU b ) for only building class, which are defined as:</p><formula xml:id="formula_16">F 1 b = 2T P 2T P + F P + F N IoU b = T P T P + F P + F N<label>(10)</label></formula><p>where TP, FP, and FN are the number of true-positive, false-positive, and false-negative pixel of segmentation result, respectively. Moreover, the experimental results of change detection are assessed using the mean of F1-score and IoU for all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We used MMSegmentation <ref type="bibr" target="#b31">[32]</ref> codebase and trained the models on a single NVIDIA Geforce 1080Ti GPU. For the encoders, we used the pre-trained weights of the SegFormer trained on the ImageNet-1k dataset. The temporal transformer and decoder were randomly initialized. During training, we applied data augmentation through random resize with a ratio of 0.5-2.0 and random horizontal flipping. We also used random cropping with 512?512 for the xBD dataset. The model was trained using an AdamW optimizer for 1M iteration. Due to memory capacity limitations, we used a batch size of 1 for all datasets. Learning rate was set to an initial value of 6 ? 10 ?5 and a poly LR schedule with the default factor of 1.0 was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison and Analysis</head><p>In this section, we compare the performance of our SiamixFormer model in building detection and change detection with other existing deep learning (CNN-based or transformer-based) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Building Detection</head><p>We considered the following models for building detection: ? BDANet [2]: introduced for building damage assessment, uses a UNet-based model and only pre-disaster images for building detection.</p><p>? RAPNet <ref type="bibr" target="#b32">[33]</ref>: uses a combination of atrous convolution (AC), deformable convolution (DC), pyramid pooling module (PPM), FPN, and attention mechanism (AM) for building detection.</p><p>? DamFormer <ref type="bibr" target="#b22">[23]</ref>: is based on the SegFormer architecture. It uses CBAM <ref type="bibr" target="#b33">[34]</ref> for feature fusion and both pre-disaster and post-disaster images for building detection and building damage assessment.</p><p>? DTCDSCN <ref type="bibr" target="#b34">[35]</ref>: proposed for change detection. It consists of three parts: two parts perform semantic segmentation, and one part performs change detection. This model has been used for building detection using two inputs on the WHU dataset.</p><p>Unlike BDANet and RAPNet models, our SiamixFormer model uses two inputs for building detection. As shown in <ref type="table" target="#tab_1">Table 2</ref>, using pre-disaster and post-disaster images can improve the model's performance in terms of F 1 b and IoU b metrics. This is because the number of destroyed and major-damaged buildings in post-disaster images is small compared to no-damaged and minor-damaged buildings, so using post-disaster images can help detect buildings better.  On the other hand, using two inputs can help train the models with large receptive fields better. This way, we can maintain the large receptive field and properly fuse the extracted features. <ref type="table" target="#tab_1">Table 2</ref> shows that the SiamixFormer model using the temporal transformer in the feature fusion section obtained better results compared with the DamFormer model, which uses CBAM for feature fusion. In <ref type="figure">Figure 3</ref>, the qualitative results of different SimixFormer models on the holdout images of the xBD dataset are presented.</p><p>In order to show the robustness of our proposed model in building detection, we also evaluate model on the WHU dataset. This dataset is usually used for change detection, but the DTCDSCN model used this dataset for building detection and change detection. As can be seen in <ref type="table" target="#tab_1">Table 2</ref>, the SiamixFormer model has also achieved outstanding results on this dataset. <ref type="figure" target="#fig_0">Figure 4</ref> shows the qualitative results of different SimixFormer models on some sample images from the test set of the WHU dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Change Detection</head><p>We considered the following models to compare with our proposed model in change detection problem:</p><p>? DSAMNET <ref type="bibr" target="#b35">[36]</ref>: fuses different levels of extracted features using metric module and CBAM.</p><p>? SNUNet <ref type="bibr" target="#b36">[37]</ref>: based on NestedUNet, fuses the extracted features using the ensemble channel attention module.</p><p>? BIT <ref type="bibr" target="#b21">[22]</ref>: uses the CNN backbone and converts the extracted features into semantic tokens. Then, passed the tokens to transformer encoder and generates a change map with transformer decoder. Feature differencing module is the last stage.</p><p>? BIT(IMP-ViTAEv2-S) <ref type="bibr" target="#b37">[38]</ref> : used BIT structure with ViTAEv2-S backbone.</p><p>? ChangeFormer <ref type="bibr" target="#b20">[21]</ref>: is based on SegFormer and concatenates the features extracted from each stage for feature fusion.</p><p>? UVACD <ref type="bibr" target="#b38">[39]</ref>: fuses the features extracted from the CNN backbone using a visual transformer.</p><p>Most of the models introduced for change detection problem use two decoders to generate a segmentation map for each image and afterwards use another module to detect the changes. These modules analyze the difference between the two segmentation maps. However, we used our SiamixFormer model for change detection without any alteration to the model that was designed for building detection. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the SiamixFormer model achieved promising results on both LEVIR-CD and CDD datasets compared with the other methods. <ref type="figure" target="#fig_1">Figures 5 and 6</ref> show the qualitative results of different SimixFormer models on some samples of the test sets of these datasets.</p><p>A notable point about the SiamixFormer model is that the temporal transformer, which is used for feature fusion in the model, fuses the features in building detection tasks in a way that the T 2 stream outputs assist the T 1 stream in correctly detecting buildings. On the other hand, in change detection tasks, this module fuses features of the two streams in such a way that the difference between them is given to the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation study</head><p>In order to analyze the usefulness of different parts of the proposed architecture, we investigated the effects of using bi-temporal images and using the temporal transformers at different stages of the feature fusion module in experiments conducted on the xBD dataset. As shown in <ref type="table" target="#tab_3">Table 4</ref>, the SiamixFormer-5 can improve 2.24% the F 1 b metric compared with the SegFormer-B5. This confirms the effect of using pre-disaster and post-disaster images in improved building detection. In another attempt, we used a CNN model to extract features, a temporal transformer (TT) for feature fusion and SegFormer-B5 architecture. The SiamixFormer-5 could achieve 5.61% better F 1 b than this model. This shows that using bi-temporal inputs can help the model if the encoder module can produce features with large receptive fields and the feature fusion module can maintain large receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we proposed a fully-transformer model with a hierarchical architecture named SiamixFormer. The model was used for building detection and change detection on four different datasets. The SiamixFormer model uses two SegFormer-based encoders with four stages, each receiving one of the bi-temporal images and extracting features with large receptive fields. In each stage, the output of the encoders, in addition to being given to the next stage, is also given to the temporal transformer of the same stage. The temporal transformer is used for feature fusion and achieves this target by exploiting temporal features. Moreover, the temporal transformer can maintain large receptive fields. Experimental results show that using bi-temporal images and temporal transformers in feature fusion can improve the model's performance in both building detection and change detection. Future work will focus on fitting the proposed model on building classification in building damage assessment problem. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Performance of different SiamixFormer models on the WHU dataset in building detection problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Performance of different SiamixFormer models on the Levir-CD dataset in change detection problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Performance of different SiamixFormer models on the CCD dataset in change detection problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Detailed settings of SiamixFormer's encoder. H, L, and C denote the number of heads, the number of layers, and the number of channels in different transformer blocks, respectively.</figDesc><table><row><cell>model</cell><cell>block-1</cell><cell>block-2</cell><cell>block-3</cell><cell>block-4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of different method in building detection problem on xBD and WHU datasets.</figDesc><table><row><cell></cell><cell cols="2">xBD</cell><cell cols="2">WHU</cell></row><row><cell>Method</cell><cell>F 1 b</cell><cell>IoU b</cell><cell>F 1 b</cell><cell>IoU b</cell></row><row><cell>RAPNet [33]</cell><cell>-</cell><cell>73.26</cell><cell>-</cell><cell>-</cell></row><row><cell>BDANet [2]</cell><cell>86.40</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DamFormer [23] 86.86</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DTCDSCN [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.86</cell></row><row><cell cols="5">SiamixFormer-0 86.46 76.43 95.53 91.43</cell></row><row><cell cols="5">SiamixFormer-1 87.23 77.35 96.01 92.31</cell></row><row><cell cols="5">SiamixFormer-2 88.05 78.66 96.22 92.70</cell></row><row><cell cols="5">SiamixFormer-3 88.30 79.06 96.32 92.90</cell></row><row><cell cols="5">SiamixFormer-4 88.35 79.14 96.59 93.40</cell></row><row><cell cols="5">SiamixFormer-5 88.43 79.26 96.69 93.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison in change detection task on LEVIR-CD and CDD datasets.</figDesc><table><row><cell></cell><cell cols="2">LEVIR-CD</cell><cell cols="2">CDD</cell></row><row><cell>Method</cell><cell>F 1</cell><cell>IoU</cell><cell>F 1</cell><cell>IoU</cell></row><row><cell>DSAMNET [36]</cell><cell>-</cell><cell>-</cell><cell cols="2">93.69 88.13</cell></row><row><cell>SNUNet [37]</cell><cell cols="3">88.16 78.83 96.2</cell><cell>-</cell></row><row><cell>BIT [22]</cell><cell cols="2">89.31 80.68</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">BIT(IMP-ViTAEv2-S) [38] 91.26</cell><cell>-</cell><cell>97.02</cell><cell>-</cell></row><row><cell>ChangeFormer [21]</cell><cell cols="2">90.40 82.48</cell><cell>-</cell><cell>-</cell></row><row><cell>UVACD [39]</cell><cell cols="2">91.30 83.98</cell><cell>-</cell><cell>-</cell></row><row><cell>SiamixFormer-0</cell><cell cols="4">89.47 82.29 92.05 85.81</cell></row><row><cell>SiamixFormer-1</cell><cell cols="4">89.29 82.03 92.78 87.00</cell></row><row><cell>SiamixFormer-2</cell><cell cols="4">90.57 83.88 95.52 91.62</cell></row><row><cell>SiamixFormer-3</cell><cell cols="4">90.54 83.84 96.48 93.33</cell></row><row><cell>SiamixFormer-4</cell><cell cols="4">90.70 84.05 96.85 94.00</cell></row><row><cell>SiamixFormer-5</cell><cell cols="4">91.58 85.38 97.13 94.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the effects of using pre-disaster and post-disaster images and using the temporal transformers (TT) at stages of the feature fusion module. Experiments were conducted on the xBD dataset.</figDesc><table><row><cell>Method</cell><cell>pre-disaster post-disaster</cell><cell>F 1 b</cell></row><row><cell>SegFormer-B5</cell><cell>?</cell><cell>86.19</cell></row><row><cell>CNN + TT + SegFormer-B5</cell><cell></cell><cell>82.82</cell></row><row><cell>SiamixFormer-5</cell><cell></cell><cell>88.43</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tackling climate change with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Priya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Kaack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Milojevic-Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waldman-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="96" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BDANet: Multiscale convolutional neural network with cross-directional attention for building damage assessment from satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building detection from aerial images using invariant color features and shadow information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beril</forename><surname>Sirmacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Unsalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 23rd international symposium on computer and information sciences</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multichannel insar building edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giampaolo</forename><surname>Ferraioli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1224" to="1231" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved building detection using texture information. International Archives of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Awrangjeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunsun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clive</forename><forename type="middle">S</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="143" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fuzzy clustering with a modified MRF energy function for change detection in synthetic aperture radar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoguo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linzhi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="109" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A local-global dual-stream network for building extraction from very-high-resolution remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiscale U-shaped CNN building instance extraction framework with edge constraint for high-spatial-resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="6106" to="6120" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunping</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building instance change detection from large-scale aerial images using convolutional neural networks and simulated samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunping</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1343</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The temporal dynamics of slums employing a CNN-based change detection approach. Remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monika</forename><surname>Kuffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Persello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2844</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1662</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optical remote sensing image change detection based on attention mechanism and image difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueli</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="7296" to="7307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Change detection of deforestation in the brazilian amazon using landsat data and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo Pozzobon De</forename><surname>Bem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osmar</forename><surname>Ab?lio De Carvalho Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">Fontes</forename><surname>Guimar?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto Arnaldo Trancoso</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">901</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using adversarial network for multiple change detection in bitemporal remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiage</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building extraction from remote sensing images with sparse token transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxia</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">4441</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A swin transformer-based encoding booster integrated in u-shaped network for building extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilong</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2611</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A transformer-based siamese network for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaminda</forename><surname>Wele Gedara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01293</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Remote sensing image change detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zipeng</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dual-tasks siamese transformer framework for building damage assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongruixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Nemni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofia</forename><surname>Vallecorsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Bromley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10953</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Siamtrans: Zero-shot multi-frame image restoration with pre-trained siamese transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1747" to="1755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weighted rank aggregation of cluster validation measures: a monte carlo cross-entropy approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasyl</forename><surname>Pihur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susmita</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somnath</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1607" to="1615" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Jorge</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardoso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Creating xbd: A dataset for assessing building damage from satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryce</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirav</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Hosfelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Sajeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jigar</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keane</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howie</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gaston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Change detection in remote sensing images using conditional adversarial networks. International Archives of the Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Va Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu Rubis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiscale building extraction with refined attention pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinglin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building change detection for remote sensing images using a dual-task constrained deep siamese convolutional network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqian</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="811" to="815" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Snunet-cd: A densely connected siamese network for change detection of vhr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An empirical study of remote sensing pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A network combining a transformer and a convolutional neural network for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2228</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
