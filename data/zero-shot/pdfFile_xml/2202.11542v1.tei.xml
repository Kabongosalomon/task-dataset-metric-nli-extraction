<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Amodal Panoptic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
							<email>mohan@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
							<email>valada@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Amodal Panoptic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans have the remarkable ability to perceive objects as a whole, even when parts of them are occluded. This ability of amodal perception forms the basis of our perceptual and cognitive understanding of our world. To enable robots to reason with this capability, we formulate and propose a novel task that we name amodal panoptic segmentation. The goal of this task is to simultaneously predict the pixelwise semantic segmentation labels of the visible regions of stuff classes and the instance segmentation labels of both the visible and occluded regions of thing classes. To facilitate research on this new task, we extend two established benchmark datasets with pixel-level amodal panoptic segmentation labels that we make publicly available as KITTI-360-APS and BDD100K-APS. We present several strong baselines, along with the amodal panoptic quality (APQ) and amodal parsing coverage (APC) metrics to quantify the performance in an interpretable manner. Furthermore, we propose the novel amodal panoptic segmentation network (APSNet), as a first step towards addressing this task by explicitly modeling the complex relationships between the occluders and occludes. Extensive experimental evaluations demonstrate that APSNet achieves state-of-the-art performance on both benchmarks and more importantly exemplifies the utility of amodal recognition. The benchmarks are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans rely on their ability to perceive complete physical structures of objects even when they are only partially visible, to navigate through their daily lives <ref type="bibr" target="#b20">[21]</ref>. This ability, known as amodal perception, serves as the link that connects our perception of the world to its cognitive understanding. However, unlike humans, robots are limited to modal perception <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref>, which restricts their ability to emulate the visual experience that humans have. In this work, we bridge this gap by proposing the amodal panoptic segmentation task.</p><p>Any given scene can broadly be categorized into two components: stuff and thing. Regions that are amorphous or uncountable belong to stuff classes (e.g., sky, road, sidewalk, etc.), and the countable objects of the scene belong to thing classes (e.g., cars, trucks, pedestrians, etc.). The amodal panoptic segmentation task illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> (b) aims to concurrently predict the pixel-wise semantic segmentation labels of visible regions of stuff classes, and instance segmentation labels of both the visible and occluded regions of thing classes. We believe this task is the ultimate frontier of visual recognition and will immensely benefit the robotics community. For example, in automated driving, perceiving the whole structure of traffic participants at all times, irrespective of partial occlusions <ref type="bibr" target="#b31">[32]</ref>, will minimize the risk of accidents. Moreover, by inferring the relative depth ordering of objects in a scene, robots can make complex decisions such as in which direction to move relative to the object of interest <ref type="bibr" target="#b10">[11]</ref> to obtain a clearer view without additional sensor feedback.</p><p>Amodal panoptic segmentation is substantially more challenging as it entails all the challenges of its modal counterpart (scale variations, illumination changes, cluttered back-ground, etc.) while simultaneously requiring more complex occlusion reasoning. This becomes even more complex for non-rigid classes such as pedestrians. These aspects also reflect on the groundtruth annotation effort that it necessitates. In essence, this task requires an approach to fully grasp the structure of objects and how they interact with other objects in the scene to be able to segment occluded regions even for cases that seem ambiguous.</p><p>Our contributions in this paper are twofold. First, we propose the novel task of amodal panoptic segmentation, a comprehensive scene recognition problem. To fully establish the task as well as to encourage future research, we extend two challenging urban driving datasets with amodal panoptic segmentation labels to create the KITTI-360-APS and BDD100K-APS benchmarks. We present several baselines for this task by combining state-of-the-art amodal instance segmentation methods with top-down panoptic segmentation networks. Further, we introduce two evaluation metrics referred to as amodal panoptic quality (APQ) and amodal parsing coverage (APC), to coherently quantify the performance of segmentation of stuff classes in visible regions and thing classes in both visible and occluded object regions. The APQ metric measures the performance independent of the size of instances and the APC metric considers the size of instances while giving more importance to the segmentation quality of larger objects than smaller objects. We introduce the size-dependent metric since a variety of applications seek high-quality segmentation of objects closer to the camera than far away objects, such as in autonomous driving.</p><p>Second, we propose the novel APSNet architecture that consists of a shared backbone and task-specific semantic and amodal instance segmentation heads followed by a parameter-free fusion module that yields the amodal panoptic segmentation output. In our approach, we split the amodal bounding box contents into the visible region mask of the target object, the occluded region mask of the target object referred to as the occlusion mask, and the object masks that occludes the target object referred to as the occluder. The occluder and occlusion features enable the amodal mask head to identify occlusion regions, while the visual and occlusion features enable the network to predict the amodal mask of the object. Furthermore, we refine the visible mask with amodal features in conjunction with visible features to impart occlusion awareness. To prevent the loss of localization of features in favor of semantic features, we increase the receptive field for context aggregation with dilated convolutions instead of downsampling in the semantic head. We make our code and models publicly available at http://amodalpanoptic.cs.uni-freiburg.de.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Panoptic segmentation approaches can be categorized into proposal-free and proposal-based methods. Proposal-free methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34]</ref> first perform semantic segmentation, followed by applying various techniques to group thing pixels such as instance center regression <ref type="bibr" target="#b29">[30]</ref>, Hough-voting <ref type="bibr" target="#b15">[16]</ref>, or pixel affinity <ref type="bibr" target="#b13">[14]</ref> to obtain instance segmentation. On the other hand, in proposal-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>, typically a network head generates the object bounding boxes along with their masks and a parallel head yields the semantic segmentation output. In this work, we propose a top-down amodal panoptic segmentation architecture. We choose the top-down over the bottom-up approach due to its ability to handle large-scale variation in instances which plays a vital role in segmenting thing class objects.</p><p>Li et al. <ref type="bibr" target="#b16">[17]</ref> introduce the amodal instance segmentation task for which their approach relies on the directions of high heatmap values computed for each object to iteratively enlarge the corresponding object modal bounding box. Follmann et. al <ref type="bibr" target="#b5">[6]</ref> propose a class-specific amodal instance segmentation approach called ORCNN which replaces the single instance mask head of Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> with amodal and inmodal instance mask heads. Further, they employ an occlusion mask prediction head on top of the modal-specific heads. Subsequently, Qi et al. <ref type="bibr" target="#b24">[25]</ref> introduce the multi-level coding module to explicitly impart global information for better segmentation of the occluded area. VQ-VAE <ref type="bibr" target="#b11">[12]</ref> replaces the fully convolutional instance mask heads with variational autoencoders. Their method first classifies the input features into intermediate shape codes and then recovers complete object shapes from the intermediate shape codes. To learn the aforementioned discrete shape codes, they pretrain a vector quantized variational autoencoder model on the amodal groundtruth masks. Xiao et al. <ref type="bibr" target="#b35">[36]</ref> use a shapeprior memory codebook with an autoencoder to refine the initial amodal mask prediction from Mask R-CNN. Similar to <ref type="bibr" target="#b11">[12]</ref>, they pretrain the autoencoder on amodal groundtruth masks. More recently, BCNet <ref type="bibr" target="#b12">[13]</ref> employs two overlapping GCN layers that detect the occluding objects and partially occluded object instances to decouple the boundaries of both the occluding and occluded object instances.</p><p>Lastly, Zhu et al. <ref type="bibr" target="#b39">[40]</ref> propose amodal semantic segmentation with the COCO amodal dataset. Their task requires the prediction of visible and invisible regions of thing classes in a class-agnostic manner while allowing multiple detections of the same objects. The COCO amodal dataset does not provide any labels for amorphous regions (wall, floor, etc.). In contrast, we introduce two benchmark datasets that treat all prominent amorphous regions (road, sidewalk, etc.) and non-traffic participants (pole, fence, etc.) in an urban scene as stuff, similar to the standard convention followed in panoptic segmentation <ref type="bibr" target="#b14">[15]</ref>. Consequently, our datasets consider all the traffic participants (cars, pedestrians, etc.) as part of thing classes. Furthermore, our amodal panoptic segmentation task allows at most one semantic label and instance-ID assignment to the pixel of visible regions. This discourages overlaps and requires predictions to be class-specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Amodal Panoptic Segmentation Task</head><p>For a given set of C semantic classes, the goal of the amodal panoptic segmentation task is to map each pixel i of a given input image to a set A i comprising pairs of (c, ?, v ) ? C ? N ? V , where c represents the semantic class for the pixel, ? represents the instance ID, and v ? V represents the visibility of the prediction pair where V is encoded as V ? {1, 2}. Here, ? of each pair in set A i associates a group of pixels that have the same semantic class but belong to a different segment, and are unique for each segment for the given image. v determines whether the corresponding ? is the visible part (v = 1) of its segment or the occluded part (v = 2). Moreover, in the set A i , at most one pair with v = 1 is feasible. Additionally, for c ? C s the corresponding ? is irrelevant, where C s is the subset of C that consists of stuff semantic classes.</p><p>For simplicity, we can define the amodal panoptic segmentation task at the object segment level. Given an input image, the task aims to predict all the visible stuff class segments where each stuff class can have at most one segment associated with it. In contrast, for thing classes, each class can have more than one visible segment associated with it. Further, the segmentation of each thing class segment can comprise both visible and occluded region segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Metrics</head><p>In this section, we present the metrics that we use to evaluate the performance of amodal panoptic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Amodal Panoptic Quality</head><p>In order to facilitate quantitative evaluation, we adapt the standard panoptic quality (PQ) <ref type="bibr" target="#b14">[15]</ref> metric used for quantifying the performance of panoptic segmentation by accounting for the invisible or occluding regions in our new metric that we name amodal panoptic quality (APQ). Consider a set of groundtruth segments consisting of subset S and subset T . S and T consist of segments corresponding to stuff and thing classes respectively. Similarly, we have predictions with subsets S and T . For a given stuff class c, we obtain the corresponding matching stuff segments M S c = {(s , s) ? S c ? S c : IoU(s , s) &gt; 0} as each image can have at most one predicted segment and at most one groundtruth segment. Thus, APQ sc corresponding to the stuff class c is then computed as</p><formula xml:id="formula_0">APQ sc = 1 |S c | (s ,s)?M Sc IoU(s , s),<label>(1)</label></formula><p>where |S c | is the total number of stuff groundtruth segments corresponding to class c. The computed APQ sc follows the scheme suggested in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Next, for a thing class c we obtain the matching segments by solving a maximum weighted bipartite matching problem <ref type="bibr" target="#b34">[35]</ref> for each pair (V, V ) and (O, O ). Here, V and O are the subsets of T corresponding to the visible and occluded region segments. V and O are similar subsets of T . This unique matching of segments splits the groundtruth and predicted thing class segments (T and T ) into three sets: matched pairs of segments (TP), unmatched groundtruth segments (FN), and unmatched predicted segments (FP). Hence APQ tc corresponding to the thing class c is then defined as</p><formula xml:id="formula_1">APQ tc = (t ,t)?T Pc IoU(t , t) |T P c | + |F P c | + |F N c | .<label>(2)</label></formula><p>Then, the overall APQ metric is the average over all the classes and is given by</p><formula xml:id="formula_2">APQ = c?Cs AP Q sc + c?Ct APQ tc |C s | + |C t | ,<label>(3)</label></formula><p>where C s is the set of stuff semantic classes and C t is the set of thing semantic classes. Further, to explicitly analyze the performance of the model for visible and invisible or occluded regions, the APQ tc is comprised of APQ vtc and APQ otc which are computed with respect to the visible regions and occluded regions, respectively as</p><formula xml:id="formula_3">APQ vtc = (v ,v)?T Pc IoU(v , v) |T P cv | + |F P cv | + |F N cv | ,<label>(4)</label></formula><formula xml:id="formula_4">APQ otc = (o ,o)?T Pc IoU(o , o) |T P co | + |F P co | + |F N co | ,<label>(5)</label></formula><p>where v and o are the visible and occluded regions of the predicted instance segments, v and o are the visible and occluded parts of the groundtruth instance segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Amodal Parsing Coverage</head><p>The amodal panoptic quality metric is based on matching segments and as a consequence, it treats all the instances equally irrespective of their sizes. However, in some applications, a relatively higher segmentation quality of larger objects is more desirable than smaller objects such as in portrait segmentation and autonomous driving. This factor motivated Yang et al. <ref type="bibr" target="#b37">[38]</ref> to formulate the parsing covering (PC) metric for panoptic segmentation which accounts for the size of instances. We adapt the PC metric for amodal panoptic segmentation and propose the amodal parsing coverage (APC) metric. Let P c and P c be the groundtruth and prediction for a c semantic class respectively. If c is a stuff class, the coverage of stuff class c (Cov sc ) is computed similar to the coverage computation in PC, defined as</p><formula xml:id="formula_5">Cov sc = 1 N c X?Pc |X| ? max X ?P c IoU (X , X),<label>(6)</label></formula><p>where N c is the total number of pixels corresponding to class c in the groundtruth. For a thing class c, the groundtruth segmentation P c and the predicted segmentation P c are divided into visible segmentation P vc and invisible or occluded segmentation P oc . Then the coverage (Cov tc ) for the thing class c is defined as</p><formula xml:id="formula_6">Cov tc = N vc ? Cov vtc + N oc ? Cov otc N vc + N oc ,<label>(7)</label></formula><p>where N vc and N oc are the total numbers of pixels corresponding to class c in the groundtruth for visible and occluded regions respectively, and</p><formula xml:id="formula_7">Cov vtc = 1 N vc X?Pvc |X| ? max X ?P vc</formula><p>IoU (X , X), (8)</p><formula xml:id="formula_8">Cov otc = 1 N oc X?Poc |X| ? max X ?P oc IoU (X , X). (9)</formula><p>Finally, APC is computed as the average over combined stuff and thing class coverage over all semantic classes as</p><formula xml:id="formula_9">APC = c?Cs Cov sc + c?Ct Cov tc |C s | + |C t | ,<label>(10)</label></formula><p>where C s and C t is the set of stuff and thing semantic classes respectively. In summary, the proposed APC is devoid of any segment matching and incorporates the area weighted IoU to emphasize on the segmentation quality of large objects. Consequently, this metric accentuates segmentation quality of large occluded regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets</head><p>In this section, we first give an overview of the annotation protocol that we employ for curating the amodal panoptic segmentation benchmark datasets followed by a brief description of each of the datasets. We choose the aforementioned datasets as they provide large-scale instance annotations that are consistent in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Anotation Protocol</head><p>We annotate two large-scale urban scene understanding datasets, KITTI-360 and BDD100K. We follow a semiautomatic annotation pipeline similar to <ref type="bibr" target="#b32">[33]</ref>. Specifically, we use the state-of-the-art EfficientPS <ref type="bibr" target="#b19">[20]</ref> model pretrained on the Mapillary Vistas <ref type="bibr" target="#b21">[22]</ref> and Cityscapes <ref type="bibr" target="#b4">[5]</ref> datasets. We annotate images with pixel-level labels for amodal instance segmentation of thing classes and semantic segmentation of stuff classes. For amodal instance annotations, we fine-tune the pretrained EfficientPS model on the KINS dataset <ref type="bibr" target="#b24">[25]</ref> which consists of amodal instance segmentation labels for urban road scenes. We generate pseudo amodal instance masks for a subset of the target dataset (BDD100K and KITTI-360).</p><p>Subsequently, a human annotator manually corrects and refines these resulting pseudo labels. We then again fine-tune the EfficientPS model on the refined annotations and generate a new set of pseudo amodal instance masks for the next subset of the target dataset. We reiterate the aforementioned process until the entire dataset is fully annotated. Similarly, for semantic segmentation annotations, we fine-tune the pretrained EfficientPS model on the semantic segmentation labels of BDD100K. We then use this fine-tuned model to generate pseudo semantic segmentation labels of stuff classes and follow the iterative semi-automatic annotation procedure. We adapt the publicly available labeling tool from <ref type="bibr" target="#b4">[5]</ref> for our manual annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">KITTI-360-APS</head><p>We extend the KITTI-360 <ref type="bibr" target="#b17">[18]</ref> dataset which has semantic and instance labels with amodal panoptic annotations and name it the KITTI-360-APS dataset. It consists of nine sequences of urban street scenes with annotations for 61,168 images of resolution 1408 ? 376 pixels. Our dataset comprises 10 stuff classes. We define a class as stuff if the class has amorphous regions or is incapable of movement at any point in time. Road, sidewalk, building, wall, fence, pole, traffic sign, vegetation, terrain, and sky are the stuff classes. Further, the dataset consists of 7 thing classes, namely car, pedestrians, cyclists, two-wheeler, van, truck, and other vehicles. Please note that we merge the bicycle and motorcycle class into a single class called two-wheelers. We use the sequence 10 of the KITTI-360 dataset as the validation set and the rest of the sequences as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">BDD100K-APS</head><p>The Berkeley Deep Drive (BDD100K) <ref type="bibr" target="#b38">[39]</ref> instance segmentation dataset comprises of 157 training sequence and 39 validation sequences. Each sequence contains 202 images of resolution 1280 ? 720 pixels with instance segmentation groundtruth labels. For our BDD100K-APS dataset, we select 12 sequences from the training set and 3 sequences from the validation set. We provide amodal panoptic annotations for 10 stuff classes and 6 thing classes. Road, sidewalk, building, fence, pole, traffic sign, fence, terrain, vegetation, and sky are the stuff classes. Whereas, pedestrian, car, truck, rider, bicycle, and bus are the thing classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Baselines</head><p>We introduce a total of six baselines for our proposed amodal panoptic segmentation task. We create the baselines by building upon the EfficientPS <ref type="bibr" target="#b19">[20]</ref> model which is a state-of-the-art top-down panoptic segmentation network and replace its instance segmentation head with different existing amodal instance segmentation approaches. We choose the baseline's amodal head based on two aspects: the relevance of existing architectures to our task and the complexity involved in adapting the approach for our purpose. Hence, we adopt the following five state-of-the-art amodal instance segmentation methods for the instance head of our baselines: ORCNN <ref type="bibr" target="#b5">[6]</ref>, VQ-VAE <ref type="bibr" target="#b11">[12]</ref>, Shape Prior <ref type="bibr" target="#b35">[36]</ref>, ASN <ref type="bibr" target="#b24">[25]</ref>, and BCNet <ref type="bibr" target="#b12">[13]</ref>. We introduce an additional baseline called Amodal-EfficientPS in which we add an extra amodal mask prediction layer to the instance head of the EfficientPS architecture. We use the post-processing step described in <ref type="bibr" target="#b19">[20]</ref> to compute the panoptic segmentation output. We first obtain the amodal mask for each instance in the panoptic segmentation output using the amodal mask logits channels associated with the corresponding instance ID. We then employ the sigmoid function on the selected amodal mask logits and threshold it at 0.5 to obtain the final amodal binary mask. The set of the amodal binary mask along with its class prediction and instance ID is concatenated with the panoptic segmentation output to yield the final amodal panoptic prediction. We describe each of the architectures of the baselines and the post-processing step in detail in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">APSNet Architecture</head><p>In this section, we present a brief overview of our proposed APSNet architecture and then detail each of its constituting components. <ref type="figure" target="#fig_1">Fig. 2</ref> (a) depicts the topology of APSNet that follows the top-down approach. It consists of a shared backbone that comprises of an encoder and the 2-way Feature Pyramid Network (FPN) <ref type="bibr" target="#b19">[20]</ref>, followed by the semantic segmentation head and amodal instance segmentation head. We employ the RegNet <ref type="bibr" target="#b25">[26]</ref> architecture as the encoder (depicted in red). It consists of a standard residual bottleneck block with group convolutions. The overall architecture of this encoder consists of repeating units of the same block at a given stage and comprises a total of five stages. At the same time, it has fewer parameters in comparison to other encoders but with higher representational capacity. Subsequently, after the 2-way FPN, our network splits into two parallel branches. One of the branches consists of the Region Proposal Network (RPN) and ROI align layers that take the 2-way FPN output as input. The extracted ROI features after the ROI layers are propagated to the amodal instance segmentation head. The second parallel branch consists of the semantic segmentation head that is connected from the fourth stage of the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Amodal Instance Segmentation Head</head><p>Our proposed amodal instance segmentation comprises three parts, each focusing on one of the critical requirements for amodal reasoning. <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> shows the architecture of our amodal instance segmentation head. First, the visible mask head learns to predict the visible region of the target object in a class-specific manner. Simultaneously, an occluder head, class-agnostically predicts the regions that occlude the target object. Specifically, the visible mask head learns to segment background objects for a given proposal and the occluder head learns to segment foreground objects. The occluder head provides a global initial guess estimate of where the occluded region of the target object exists.</p><p>With the features from both visible and occluder mask heads, the amodal instance segmentation head can reason about the presence of the occluded region as well as its shape. This is achieved by employing an occlusion mask head that predicts the occluded region of the target object given the visible and occluder features. Specifically, the occlusion mask head takes the concatenated visible and occluder features along with Mask ROI features as input. We use the Mask ROI features as part of the input to the occlusion mask head to enable reasoning about the given proposal as a whole and not individual visible and occluder regions. Additionally, the occlusion mask head learns to predict the occluded region of the target object in a spatially independent manner. This allows the head to focus only on learning the underlying general shape relationship for a given visible and occluder region that completes the visible region to attain amodal perception. By focusing on the what aspect of the occluded region (what should be the segmentation mask of the occluded region), we ease the learning of the occlusion mask head. Our method allows this ease in training due to denser feedback in contrast to sparser feedback in partial occlusion cases and hence enables capturing of the underlying shape of the occluded region effectively. We present the spatially dependent and independent groundtruth examples in the supplementary material.</p><p>Subsequently, the concatenated visible, occluder, and occlusion mask head features are further processed by a series of convolutions followed by a spatio-channel attention block. The spatio-channel attention block consists of two parallel branches. In one of the parallel branches, global pooling is applied spatially, we refer to this as the channel attention branch. The channel attention branch further consists of two 1 ? 1 convolutions with 64 and 256 output channels respectively. The first 1 ? 1 convolution has a ReLU activation and the second convolution has a sigmoid activation. The output of the channel attention branch is then multiplied with the output of the other parallel branch called the spatial branch. The spatial branch consists of a channel-wise global pooling layer, followed by reshaping the tensor from 1 ? 14 ? 14 to 196 ? 1 ? 1. Subsequently, two 1 ? 1 convolutions are employed with 49 and 196 output channels respectively. The output is then reshaped to a 1 ? 14 ? 14 tensor. Lastly, the output of the two branches is multiplied to compute the final output of the spatio-channel attention block. The aforementioned network layers aim to model the inherent relationship between the visible, occluder and occlusion features. Subsequently, these features are concatenated with the Mask ROI features to act as input to the amodal mask head. This amodal mask head then predicts the final amodal mask for the target object. Additionally, the visible mask is further refined using a second visible mask head that takes the concatenated amodal features and visible features to predict the final inmodal mask.</p><p>Lastly, our amodal instance segmentation head employs the Mask R-CNN bounding box head with two output heads: object classification and amodal bounding box. We use the binary cross-entropy loss for training each of the mask heads in our amodal instance segmentation head. The loss functions are described in detail in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Semantic Segmentation Head</head><p>The architecture of our semantic segmentation head is illustrated in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>. The semantic head takes the ?16 downsampled feature maps from the stage 4 of the RegNet encoder as input. We employ an identical stage 5 RegNet block with the dilation factor of the 3 ? 3 convolutions set to 2. We refer to this block as the dilated RegNet block. Subsequently, we employ a DPC <ref type="bibr" target="#b2">[3]</ref> module to process the output of the dilated block. We then upsample the output to ?8 and ?4 downsampled factor using bilinear interpolation. After each upsampling stage, we concatenate the output with the corresponding features from the 2-way FPN having the same resolution and employ two 3 ? 3 depth-wise separable convolutions to fuse the concatenated features. Finally, we use a 1 ? 1 convolution to reduce the number of output channels to the number of semantic classes followed by a bilinear interpolation to upsample the output to the input image resolution. We employ the weighted per-pixel logloss <ref type="bibr" target="#b0">[1]</ref> for training similar to <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experimental Evaluation</head><p>In this section, we describe the training protocol that we use for the baselines and our proposed APSNet architecture. We then present extensive benchmarking results on KITTI-360-APS and BDD100K-APS in Sec. 8.1. Subsequently, we present a detailed ablation study on the proposed amodal instance head in Sec. 8.2, followed by results for amodal instance segmentation on the KINS <ref type="bibr" target="#b24">[25]</ref> dataset in Sec. 8.3. Finally, we present qualitative comparisons in Sec. 8.4.</p><p>We use PyTorch <ref type="bibr" target="#b22">[23]</ref> for implementing all our architectures and we trained our models on a system with an Intel Xenon (2.20GHz) processor and NVIDIA TITAN RTX GPUs. We train our network on two crop resolutions of the input image according to the dataset. We use crops of 376 ? 1408 pixels and 448 ? 1280 pixels for the KITTI-360-APS and BDD100K-APS dataset respectively. We use a multi-step learning rate schedule with a drop factor of 10. We use a base learning rate of 0.04 and 0.01 for KITTI-360-APS and BDD100k-APS respectively. We train our model on the KITTI-360-APS dataset for 40 epochs and 200 epochs on the BDD100K-APS dataset. We set the milestones as 65% and 90% of the total epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Benchmarking Results</head><p>In this section, we report results comparing the performance of our proposed APSNet architecture against the introduced baselines. For comparisons on KITTI-360-APS and BDD100K-APS, we report results of the models that we trained using the official implementations that have been publicly released by the authors and performed extensive tuning of hyperparameters to the best of our ability. We report results on the validation sets for all the datasets. Tab. 1 presents the benchmarking results.</p><p>In the baselines, all the other components of the amodal panoptic segmentation network remain the same except for the amodal instance head. Therefore, all the baselines achieve the same APQ S and APC S scores. In contrast, our APSNet model that incorporates our proposed semantic  head achieves higher APQ S and APC S scores. This gain of 0.3%-0.5% in the aforementioned metrics demonstrate the better stuff segmentation performance of our architecture. The improvement can be attributed to the ability of our semantic head to increase the receptive field for effective context aggregation by increasing the dilation factor of the subsequent encoding block that outputs features corresponding to ?16 downsampling factor instead of further downsampling. As a consequence, our network does not lose the ability to localize features, providing the decoder with better semantic features to use during the upsampling stage. Among the baselines, the ASN model achieves the highest APQ and APC scores. This method focuses on incorporating the global occlusion context in the model-specific mask prediction heads. The other baselines either capture occlusion features implicitly or learn the occlusion map but do not use the information in the mask prediction heads. The performance of the ASN model demonstrates the importance of incorporating explicitly modeled occlusion features for improved amodal reasoning. Nevertheless, our APSNet outperforms ASN in all the metrics, namely APQ and APC along with the sub-components of the metrics on both datasets. Moreover, it also achieves the highest AP and mIoU scores. These improvements can be partially attributed to the semantic head but the majority of the contribution is due to the proposed amodal instance head. The explicit coarse modeling of occlusion regions with occluder features and the spatially independent modeling of the occluded region given the visible and occluder features provides our amodal mask prediction head with additional cues that positively supplement its amodal reasoning abilities. Hence, our proposed APSNet architecture achieves state-of-the-art performance for the task of amodal panoptic segmentation.</p><p>We further analyze the relationship between the different metrics reported. Although the metrics assess different aspects of amodal scene parsing, due to the close relationship of these aspects, the metrics are positively correlated. This is evident from the reported results. With the increase in the APQ score, the APC score is likely to increase and viceversa. This relationship also extends to the AP and mIoU metrics. Additionally, computing both metrics can be beneficial as the gain or loss proportion in each of the metrics provides more insights. APQ evaluates the amodal parsing quality independent of instance sizes whereas APC emphasizes segmentation quality of larger area instances. Thus, a higher gain in APQ compared to APC can indicate that the amodal segmentation quality of smaller object instances improves greatly compared to larger objects and vice-versa. We further explain this observation with the visible and occluded components of the metrics in the supplementary material.</p><formula xml:id="formula_10">Model APQ T APQ V T APQ O T APC T APQ V T APC O T<label>M1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Ablation Study on Amodal Instance Head</head><p>In this section, we quantitatively demonstrate the importance of each component of our proposed amodal instance head. Tab. 2 presents results from this experiment. We report the metric's thing component and its sub-components. We begin with the model M1 that employs visible or inmodal, and amodal mask prediction heads in the amodal instance head. In the M2 model, we then employ occlusion and visible mask prediction heads on top of which we add an amodal mask prediction head. The improvement in performance shows that modeling visible and occlusion features explicitly improves the amodal reasoning ability. Subsequently, in model M3, we add an occluder mask prediction head in parallel to the occlusion and visible mask prediction head of M2. The amodal mask prediction head is now built on top of these three mask prediction heads. The larger increase in the APC O T score demonstrates that the occlusion region segmentation of nearby objects greatly improves the performance compared to faraway objects. The occluder features that are incorporated enable the amodal mask head to discern the boundaries of the occluded regions. In the M4 model, we <ref type="bibr">ASN [25]</ref> APSNet (Ours) Improvement\Error Map We also show Improvement\Error Map which denotes the pixels that are misclassified by APSNet in red and the pixels that are misclassified by the baseline but correctly predicted by APSNet in green. add another visible mask prediction head that builds upon the visible and amodal mask heads. M4 achieves an improvement in APQ V T by 0.9% and in APC V T by 0.6%. Building upon M4, in the M5 model, we predict spatially independent occlusion masks in addition to a processing block before the amodal mask head. Lastly, in the M6 model, following the processing block, we add the spatio-channel attention block. The improvement in results demonstrates that the processing block generates salient features for the amodal masks which are further enhanced by explicitly modeling the interdependencies between the channels and the spatial correlations of its features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Performance on KINS dataset</head><p>KINS <ref type="bibr" target="#b24">[25]</ref> is a benchmark for amodal instance segmentation. We evaluate the performance of APSNet on this sub-task of the proposed amodal panoptic segmentation by discarding its semantic segmentation head. This benchmark uses the AP metric for evaluating both amodal and inmodal segmentation. Tab. 3 presents results in which we observe that APSNet outperforms the state-of-the-art by 3.4% and 2.9% for amodal and inmodal AP respectively. This demonstrates that our proposed amodal instance head in APSNet also improves the inmodal segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Qualitative Evaluations</head><p>In this section, we qualitatively compare the amodal panoptic segmentation performance of our proposed APSNet with the best performing baseline ASN. <ref type="figure" target="#fig_3">Fig. 3</ref> presents the qualitative results. We observe that both approaches are capable of segmenting partial occlusion cases. However, our APSNet outperforms ASN under partial to moderate occlusion cases such as cluttered cars and pedestrians. Moreover, APSNet achieves better boundary segmentation of visible regions due to the refinement stage of the inmodal mask. The results of our proposed architecture are highly motivating, however the segmentation quality near the boundaries of moderately to heavily occluded regions of non-rigid classes such as pedestrians tends to be poor. These cases are extremely hard to predict for humans as well. However, humans can predict the occluded region with a high degree of consistency <ref type="bibr" target="#b39">[40]</ref>. We hope that this work encourages innovative solutions in the future to address this problem as well as other challenges of amodal panoptic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>In this work, we introduced and addressed the task of amodal panoptic segmentation. We formulated two easily interpretable evaluation metrics for measuring the performance of our proposed task. We introduced several strong baselines for amodal panoptic segmentation by combining state-of-theart individual models of the sub-tasks. Further, we proposed the novel APSNet architecture that achieves state-of-the-art performance for amodal panoptic segmentation and amodal instance segmentation. We believe that these results demonstrate the feasibility of this ultimate scene parsing task and encourage new research avenues in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amodal Panoptic Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rohit Mohan</head><p>Abhinav Valada University of Freiburg {mohan, valada}@cs.uni-freiburg.de</p><p>In this supplementary material, we provide additional details on various aspects of our work. We present dataset statistics for our proposed amodal panoptic segmentation datasets in Sec. 1. We then discuss the baseline architectures and the inference in-depth in Sec. 2 and Sec. 3, respectively. Subsequently, we provide details on the loss functions that we employ to train the amodal instance segmentation head of our APSNet in Sec. 4. Finally, we discuss the benchmarking results on the KITTI-360-APS dataset in detail to reinforce the utility of our proposed evaluation metrics in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Datasets</head><p>In this section, we present statistics and examples for each of the datasets that we introduce. To evaluate the shape complexity of the amodal segments, we compute the shape convexity and simplicity <ref type="bibr" target="#b39">[40]</ref> for each dataset as follows:</p><formula xml:id="formula_11">convexity(S) = Area(S)) Area(ConvexHull(S)) ,<label>(1)</label></formula><p>simplicity(S) = 4? * Area(S) P erimeter(S) .</p><p>(2)</p><p>Tab. 1 presents the shape complexity metric scores for KITTI-360-APS and BDD-APS datasets. Additionally, we compare the convexity and simplicity of our dataset with existing amodal instance segmentation datasets namely COCO-A <ref type="bibr" target="#b39">[40]</ref> and KINS <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">KITTI-360-APS</head><p>The KITTI-360-APS dataset consists of 11 stuff classes namely road, sidewalk, building, wall, fence, pole, traffic sign, vegetation, terrain, and sky. The dataset further comprises 7 thing classes, namely car, pedestrians, cyclists, twowheeler, van, truck, and other vehicles. Tab. 2 presents the thing class distribution for the dataset. We observe that the instances of the car are predominant in thing classes followed by pedestrian and truck classes. The contribution of the Other-Vehicle class to the number of instances is the least with 0.2%. <ref type="figure" target="#fig_0">Fig. 1 (a)</ref> illustrates the histogram of occlusion level which is defined as the fraction of occluded region area. We notice about 60% of the instances are either slightly occluded or not occluded at all in the dataset and the rest of the instances have different degrees of occlusions. The second peak in the graph is observed for near moderate occlusion levels while heavily occluded regions are relatively small in comparison. In terms of shape complexity (Tab. 1), KITTI-360-APS consists of relatively simpler amodal segments indicated by the higher the convexity-simplicity average value which is in line with the intuition <ref type="bibr" target="#b39">[40]</ref> that independent of scene geometry and occlusion patterns, amodal segments tend to be relatively simpler. <ref type="figure" target="#fig_1">Fig. 2</ref> presents examples from our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">BDD100K-APS</head><p>The BDD100K-APS dataset provides amodal panoptic annotations for 10 stuff classes and 6 thing classes. Road, sidewalk, building, fence, pole, traffic sign, fence, terrain, vegetation, and sky are the stuff classes. Whereas, pedestrian, car, truck, rider, bicycle, and bus are the thing classes. In the BDD100K-APS dataset, the number of instances of car and pedestrian classes is relatively close and are the predominant classes followed by the truck class. Bicycle and bus classes have similar instance distributions whereas instances of rider are the least with 1.1% of the total instances. <ref type="figure" target="#fig_0">Fig. 1 (b)</ref> presents the occlusion level distribution of instances of this dataset. About 54% of the instances in the dataset are not occluded or are slightly occluded. The number of instances having a higher degree of occlusion level approximately decreases with an increase in the occlusion level. In Tab. 2, the convexity-simplicity average value for the amodal segments is lower for this dataset implying BDD100K-APS is a more complex dataset due to the presence of a large number of non-rigid objects such as pedestrians.  <ref type="table" target="#tab_2">Table 3</ref>. Thing class distribution of BDD100K-APS dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Baseline Architectures</head><p>We introduce a total of six baselines for our proposed amodal panoptic segmentation task. We create the baselines by building upon the EfficientPS <ref type="bibr" target="#b19">[20]</ref> model which is a state-of-the-art top-down panoptic segmentation network. The EfficientPS architecture consists of four parts. The first part is the shared backbone which is a combination of an encoder and a feature pyramid network (FPN) variant. We employ the EfficientNet-B5 <ref type="bibr" target="#b28">[29]</ref> model as the encoder and remove its squeeze and excitation <ref type="bibr" target="#b9">[10]</ref> connections. We also replace the batch normalization and activation layers with synchronized Inplace Activated Batch Normalization (iABN sync) <ref type="bibr" target="#b1">[2]</ref> and Leaky ReLU activations respectively. The backbone uses the 2-way FPN <ref type="bibr" target="#b19">[20]</ref> on top of the encoder to bidirectionally aggregate multi-scale features. The encoded multi-scale features from the backbone are then propagated to an instance and semantic head. The instance head is a variant of Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> where the convolution operation in the mask prediction heads is replaced by depth-wise separable convolutions. The semantic segmentation head incorporates various modules to focus on modeling of different feature representations: DPC <ref type="bibr" target="#b2">[3]</ref> for capturing long-range contextual information, LSFE <ref type="bibr" target="#b19">[20]</ref> for capturing characteristic features, and MC <ref type="bibr" target="#b19">[20]</ref> for aligning mismatched correction modules. The final component of EfficientPS is an adaptive fusion module that fuses the output of instance and semantic head based on their logits.</p><p>In the baseline architectures, we keep all the components of EfficientPS intact except for the instance segmentation head which is replaced by different state-of-the-art amodal instance segmentation heads namely, Amodal EfficientPS, ORCNN <ref type="bibr" target="#b5">[6]</ref>, VQ-VAE <ref type="bibr" target="#b11">[12]</ref>, Shape Prior <ref type="bibr" target="#b35">[36]</ref>, ASN <ref type="bibr" target="#b24">[25]</ref>, and BCNet <ref type="bibr" target="#b12">[13]</ref>. In the following, we provide a brief overview of the amodal instance segmentation heads of the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Amodal-EfficientPS is an extension of its inmodal</head><p>variant and relies implicitly on the network to learn the relationship between the occluder and occludee along with modeling the appropriate class-specific structures. <ref type="figure" target="#fig_3">Fig. 3 (a)</ref> presents the amodal instance head of Amodal-EfficientPS. 2. ORCNN <ref type="bibr" target="#b5">[6]</ref> employs an invisible mask prediction head in addition to the inmodal and amodal mask prediction heads, to explicitly learn the propagation from visible mask to amodal mask. To do so, the approach designs the invisible mask prediction by abstracting the amodal mask from the visible mask. <ref type="figure" target="#fig_3">Fig. 3 (b)</ref> shows the amodal instance head of ORCNN. 3. ASN <ref type="bibr" target="#b24">[25]</ref> head emphasizes the importance of global information in addition to visible cues for amodal mask prediction. <ref type="figure" target="#fig_3">Fig. 3</ref> (c) presents the ASN amodal instance head. It consists of an additional occlusion classification branch and uses the features from this branch through a multi-level coding (MLC) block to impart the learned global information to the individual inmodal and amodal mask prediction head. The MLC block essentially takes the concatenation of bounding box features and occlusion features from their respective classification branches, performs a series of transpose convolution-convolution operations to process the collective features, and then concatenates it with the modelspecific mask features. This is followed by another series of convolution operations to generate the final modal-specific mask predictions. 4. Shape Prior <ref type="bibr" target="#b35">[36]</ref> approach strongly supports the idea of using the visible region segmentation in conjunction with shape priors as the key to better amodal mask segmentation. <ref type="figure" target="#fig_3">Fig. 3 (d)</ref> depicts the amodal instance head of this approach. The aforementioned head employs two modal-specific fully convolutional network heads with parameter sharing. The first modal-specific heads give the initial mask predictions that are further used as attention for refining the final mask predictions with a feature matching loss and pre-trained shape prior autoencoder. Additionally, the approach also incorporates the shape-prior autoencoder in the non-maximum suppression step of the amodal bounding boxes <ref type="bibr" target="#b26">[27]</ref>. 5. VQ-VAE <ref type="bibr" target="#b11">[12]</ref> seeks to incorporate shape prior information through discrete shape codes while using Vector Quantized Variational Autoencoder for mask segmentation. <ref type="figure" target="#fig_3">Fig. 3 (e)</ref> shows the amodal instance head of VQ-VAE. 6. BCNet <ref type="bibr" target="#b12">[13]</ref> models occluder and occludee with a bilayer GCN layer. To be precise, the approach first predicts the occluder mask and contour segmentation and uses these occluder features in conjunction with the ROI features to segment the occludee or the target object in a class agnostic manner. <ref type="figure" target="#fig_3">Fig. 3</ref> (f) presents the amodal instance head of this approach. In contrast, our APSNet employs FCN based class agnostic occluder mask segmentation head to coarsely model the occlusion regions of the target object as a strong prior and is further refined in a spatially independent manner with an occlusion mask segmentation head. Moreover, we use additional processing blocks with spatio-channel attention to explicitly model the underlying relationship among occluder (general location and shape of the occluded region), occludee (visible region), and the occlusion (precise shape of the occluded region) features before finally computing the amodal mask segmentation. <ref type="figure" target="#fig_7">Fig. 4</ref> illustrates our fragmentation of the amodal bounding box of a target object.</p><formula xml:id="formula_12">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)</formula><p>To summarize, for a better amodal perception performance, an amodal instance head should have the ability to decipher the existence of occlusion regions and be able to reason about the shape given the visible region features. We build our APSNet on these two core ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Inference</head><p>At inference time, to obtain the amodal panoptic segmentation output, we fuse the amodal instance segmentation and the semantic segmentation predictions. There are several fusion heuristics <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref> that have been proposed for panoptic segmentation. We adapt the panoptic fusion proposed in <ref type="bibr" target="#b19">[20]</ref> due to its superior performance over other fusion approaches. This heuristic allows adaptive fusion of the task-specific head outputs, which can alleviate the inherent overlap problem between the outputs of the different heads. The semantic head generates semantic logits of |C s | + |C t | channels where C s and C t are the set of stuff and thing semantic classes. While the amodal instance head outputs a set of object instances consisting of a class prediction score, confidence score, amodal bounding box prediction, inmodal, and amodal mask logits. To apply the panoptic fusion, we need to compute two logits M L A and M L B . We begin with the computation of logit M L A where we apply confidence thresholding to reduce the number of instances followed by the ROI sampling operation for the amodal bounding box on the two model-specific logits to increase their resolution from 28 ? 28 to the input image resolution H ? W . Here, H and W are the height and width of the input image. Subsequently, we compute the inmodal bounding box from the inmodal mask derived from the inmodal mask logits. We then sort the class prediction, the modal-specific logits, and the inmodal bounding box according to the class confidence score. We then employ overlap thresholding using the inmodal mask logits to finally yield the mask logit M L A .</p><p>We compute the second mask logit M L B for the corresponding instances of objects from the semantic head logits by selecting the channel based on the class of the instance and zero-out the logits for that channel outside the inmodel bounding box. Lastly, we fuse the two logits M L A and M L B as</p><formula xml:id="formula_13">F L = (?(M L A ) + ?(M L B )) (M L A + M L B ),<label>(3)</label></formula><p>where ?(?) is the sigmoid function and is the Hadamard product.</p><p>We then concatenate the stuff logits from the semantic head logits with F L. Subsequently, we apply softmax and the argmax operation along the channel dimension to obtain the so-called intermediate prediction (IP ). In the final step, we zero out the stuff classes class labels and copy the semantic head prediction stuff labels to the zero places in IP . We obtain the amodal mask for each instance in IP by accessing the amodal mask logits channels according to the instance ID. We then compute the sigmoid of the selected amodal mask logits and threshold it at 0.5 to obtain the final amodal binary mask. Following, we set the pixels in the amodal binary mask to 2 that does not overlap with the corresponding instance ID mask in IP to represent its occluded regions. The set of this tensor along with its class prediction and instance ID is concatenated with IP to yield the final amodal panoptic prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Amodal Instance Head</head><p>To recapitulate, our proposed amodal segmentation head aims to impart the awareness of the presence of occlusion regions with coarse localization (occluder head) and learn to perceive the occlusion shape given the visible and occluder regions. It also models the necessary interconnecting features of occlusion, occluder, and visible regions (processing block with spatio-channel attention) to be able to predict the amodal mask. Additionally, it uses the computed amodal features to further refine the inmodal mask prediction. Further, to efficiently train the occlusion mask head with dense feedback, our APSNet opts to learn spatially independent occlusion masks. <ref type="figure">Fig. 5</ref> presents examples of the spatially dependent and independent occlusion groundtruth masks.</p><p>The amodal instance segmentation head of APSNet consists of object classification, bounding box regression, and various mask heads. The training loss for bounding box object classification head L cls and the bounding box regression head L bbx is the same as defined in <ref type="bibr" target="#b19">[20]</ref>. Similarly, the visible mask head loss L v mask , occluder mask head loss L od mask , occlusion mask head loss L ol mask , amodal mask head loss L am mask and inmodal mask head loss L inm mask are akin to L mask in <ref type="bibr" target="#b19">[20]</ref> given as where L p (P, P ) is the binary cross-entropy loss, P is the ground truth binary mask, P is the predicted binary mask and K p is the set of positive matches. Thus the overall loss for our proposed amodal instance segmentation head is as</p><formula xml:id="formula_14">L mask (?) = ? 1 |K p | (P,P )?Kp L p (P, P ),<label>(4)</label></formula><formula xml:id="formula_15">L ainst = L cls + L bbxam + L bbx + L v mask + L od mask + L ol mask + L am mask + L inm mask .<label>(5)</label></formula><p>Note that the gradient from the loss L ainst does not flow through the RPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Extended Benchmarking Results</head><p>In this section, we discuss the benchmarking results on the KITTI-360-APS validation set in detail to understand the mutual relationship between the two proposed metrics clearly. Tab. 4 presents the quantitative results using the APQ and APC metrics and all their components. The APS baseline with the trivial implementation of amodal instance head, Amodal-EfficientPS achieves the lowest APQ and APC scores. Similarly, ORCNN that employs a derivative head for occlusion mask prediction over the trivial amodal instance head attains similar performance to Amodal-EfficientPS. However, this similar overall performance of the two networks stems from varying effectiveness of segmenting the visible and invisible regions rather than being the same. APS-EfficientPS has higher APQ V T and APC V T scores implying better visible thing region parsing, whereas ORCNN has higher APQ O T and APC O T values, indicating better occluded thing region parsing. Following, BCNet performs better than Amodal-EfficientPS and ORCNN, lagging behind VQ-VAE by 0.1% in both APQ and APC scores. However, BCNet achieves an improvement of 1.9% in APQ O T and 1.2% in APC O T scores. This difference in the proportional improvement in the two metrics where the increase in performance is higher for APQ O T signifies that BCNet primarily improves the segmentation of partially occluded objects with smaller occlusion regions. When paired with the improvement in APQ V T of 0.7% and APC V T of 1.2% indicates that the approach improves the segmentation of nearby larger objects that are partially occluded. We hypothesize that this is primarily due to the bilayer modeling of occluder and occludee which enables more refined target mask segmentation.</p><p>Subsequently, VQ-VAE adds an occlusion detection branch and mask refinement with shape priors to incorporate amodal reasoning capabilities. Compared to the trivial Amodal-EfficientPS, this approach achieves an improvement of 0.7% in APQ and 0.4% in APC, where the improvement in APQ T and APC T component of the metrics is 1.5% and 1.2% respectively. Next, the Shape Prior model refines the coarsely predicted mask with shape priors in addition but uses a combination of a pre-trained autoencoder with K-Means based codebook. It further incorporates a visible mask refinement step with amodal features. This model has an APQ score of 41.8% and an APC score of 58.2%. Its APQ O T and APC O T are higher than that of VQ-VAE by 0.6% and 0.4% respectively. This improvement suggests that incorporating shape priors with an additional codebook yields better performance. A similar trend is also observed for visible thing region metrics indicating that refining visible mask with amodal features helps improve the performance further.</p><p>Nevertheless, our proposed approach performs the best in all the metrics, namely APQ and APC, and their components. Here, the proportional improvement of APSNet can be observed in visible and occlusion components of APQ T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inmodal Mask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amodal Mask Spatially Dependent Spatially Independent Occlusion Mask</head><p>Occlusion Mask (a) (b) (c) (d) <ref type="figure">Figure 5</ref>. Illustration of spatially dependent and independent occlusion masks. The spatially dependent occlusion mask consists of few pixels compared to the inmodal mask for partial occlusion. On the other hand, the spatially independent occlusion masks that effectively capture the underlying shape of the occluded regions are denser. Thus, enabling stronger feedback during training and consequently resulting in capturing the underlying shape of the occlusion mask effectively. (0.9% and 2.9%) and APC T (0.9% and 1.6%) compared to the best baselines ASN. This demonstrates that our approach improves the segmentation of partial-to-mid occluded objects, however, the performance is limited when it comes to heavily occluded objects, as observed in the qualitative evaluations in the manuscript. To conclude, computing both the metrics for the amodal panoptic segmentation task gives more insights into the performance of an approach, which can be extremely valuable while developing an effective solution for this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APQ APC APQ</head><formula xml:id="formula_16">S APQ T APQ V T APQ O T APC S APC T APC V T APC O T AP</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of (a) panoptic segmentation and (b) amodal panoptic segmentation that encompasses visible regions of stuff classes, and both visible and occluded regions of thing classes as amodal masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Illustration of our proposed APSNet architecture consisting of a shared backbone and parallel semantic and amodal instance segmentation heads followed by a fusion module that fuses the outputs of both heads to yield the amodal panoptic segmentation output. (c) and (b) present the topologies of architectural components of our proposed semantic segmentation head and amodal instance segmentation head respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>S APQ T APC S APC T AP mIOU APQ APC APQ S APQ T APC S APC T AP mIOU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative amodal panoptic segmentation results of our proposed APSNet network in comparison to the state-of-the-art baseline ASN [25] on KITTI-360-APS (a, b) and BDD100K-APS (c, d) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )</head><label>a</label><figDesc>Occlusion level of KITTI-360-APS dataset. (b) Occlusion level of BDD100K-APS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of occlusion level (defined as the fraction of region area that is occluded) in KITTI-360-APS (a) and BDD100K-APS (b) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Visualization of amodal panoptic segmentation groundtruth from our proposed KITTI-360-APS (a-f) and BDD100K-APS (g-l) datasets. In (a) and (f) the second car on the left, (e) the far away cars on the left are heavily occluded by other car instances and vegetation, respectively. Similarly, in (h) and (l) the center cars occlude the car and the truck in front of them to a high degree, respectively. Moreover, we also observe a varying degree of occlusion from partial to mid in all of the visualization examples. The variations in occlusion of instances, cluttered urban road scenes with several thing class instances, and complex stuff classes makes both the proposed datasets extremely challenging for amodal panoptic segmentation. Topologies of various amodal instance segmentation head of the amodal panoptic segmentation baselines. Please note that the boxes enclosed in color dashes in each of the architecture corresponds to the expanded version of the same colored boxes depicted on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of our fragmentation of bounding box of the target object into class-agnostic occluder, class-wise occlusion, and visible masks. Our amodal instance head employs individual mask heads to predict each mask. The features from these mask heads are further processed with a series of convolution operations along with spatio-channel attention to predict the amodal mask of the target object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Amodal-EfficientPS 41.1 57.6 46.2 33.1 58.1 56.6 29.1 44.7 44.9 46.2 54.9 29.9 64.7 41.4 25.6 50.4 ORCNN [6] 41.1 57.5 46.2 33.1 58.1 56.6 29.0 44.5 44.9 46.2 54.9 29.9 64.7 41.5 25.6 50.4 BCNet [13] 41.6 57.9 46.2 34.4 58.1 57.6 30.3 45.8 45.2 46.4 55.0 30.7 64.7 42.1 26.3 51.0 VQ-VAE [12] 41.7 58.0 46.2 34.6 58.1 57.8 30.4 45.9 45.3 46.5 54.9 30.8 64.7 42.2 27.3 51.1 Performance comparison of amodal panoptic segmentation on the KITTI-360-APS and BDD100K-APS validation set. Subscripts S and T refer to stuff and thing classes respectively. All scores are in [%].</figDesc><table><row><cell>Shape Prior [36]</cell><cell>41.8 58.2 46.2 35.0 58.1 58.2 31.0 46.3</cell><cell>45.4 46.6 55.0 31.0 64.8 42.6 27.6 52.4</cell></row><row><cell>ASN [25]</cell><cell>41.9 58.2 46.2 35.2 58.1 58.3 31.1 46.3</cell><cell>45.5 46.6 55.0 31.2 64.8 42.7 27.9 52.5</cell></row><row><cell>APSNet (Ours)</cell><cell>42.9 59.0 46.7 36.9 58.5 59.9 33.4 48.0</cell><cell>46.3 47.3 55.4 32.8 65.1 44.5 29.2 53.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>33.3 41.3 15.1 56.9 59.3 23.4</cell></row><row><cell>M2</cell><cell>33.7 41.4 15.4 57.5 59.4 23.9</cell></row><row><cell>M3</cell><cell>34.6 41.7 15.7 58.2 59.6 24.4</cell></row><row><cell>M4</cell><cell>35.0 42.6 15.7 58.8 60.2 24.5</cell></row><row><cell>M5</cell><cell>35.9 43.6 17.7 59.4 61.6 25.1</cell></row><row><cell>M6 (Ours)</cell><cell>36.9 44.1 18.6 59.9 62.2 25.8</cell></row></table><note>. Evaluation of various architectural components of our proposed amodal instance segmentation head. The performance is shown for the models trained on the KITTI-360 APS dataset and evaluated on the valida- tion set. Subscript T refers to thing classes. Superscripts V and O refer to visible and occluded regions respectively. All scores are in [%].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Amodal AP Inmodal AP</cell></row><row><cell>ORCNN [6]</cell><cell>29.0</cell><cell>26.4</cell></row><row><cell>VQ-VAE [12]</cell><cell>31.5</cell><cell>?</cell></row><row><cell>Shape Prior [36]</cell><cell>32.1</cell><cell>29.8</cell></row><row><cell>ASN [25]</cell><cell>32.2</cell><cell>29.7</cell></row><row><cell>APSNet (Ours)</cell><cell>35.6</cell><cell>32.7</cell></row></table><note>. Amodal instance segmentation results on the KINS dataset. All scores are in [%].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Thing class distribution of KITTI-360-APS dataset.</figDesc><table><row><cell>Class</cell><cell>Car</cell><cell cols="5">Pedestrian Cyclist Two-Wheelers Truck</cell><cell>Van</cell><cell>Other-Vehicles</cell></row><row><cell>Number</cell><cell>192624</cell><cell>6240</cell><cell>3096</cell><cell>2805</cell><cell></cell><cell>6561</cell><cell cols="2">3573</cell><cell>443</cell></row><row><cell>Ratio</cell><cell>89.4%</cell><cell>2.8%</cell><cell>1.4%</cell><cell>1.3%</cell><cell></cell><cell>3.0%</cell><cell cols="2">1.6%</cell><cell>0.2%</cell></row><row><cell></cell><cell>Class</cell><cell>Pedestrian</cell><cell>Car</cell><cell>Truck</cell><cell>Rider</cell><cell cols="2">Bicycle</cell><cell>Bus</cell></row><row><cell></cell><cell>Number</cell><cell>19671</cell><cell>23775</cell><cell>2653</cell><cell>561</cell><cell>1110</cell><cell cols="2">1288</cell></row><row><cell></cell><cell>Ratio</cell><cell>40.1%</cell><cell>48.4%</cell><cell>5.4%</cell><cell>1.1%</cell><cell>2.3%</cell><cell cols="2">2.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>mIOU Amodal-EfficientPS 41.1 57.6 46.2 33.1 41.3 12.7 58.1 56.6 58.5 22.7 29.1 44.7 ORCNN [6] 41.1 57.5 46.2 33.1 41.1 12.8 58.1 56.6 58.1 22.9 29.0 44.5 BCNet [13] 41.6 57.9 46.2 34.4 42.0 14.5 58.1 57.6 59.7 23.9 30.3 45.8 VQ-VAE [12] 41.7 58.0 46.2 34.6 42.2 14.7 58.1 57.8 59.8 23.9 30.4 45.9 Shape Prior [36] 41.8 58.2 46.2 35.0 42.5 15.3 58.1 58.2 60.3 24.3 31.0 46.3 ASN [25] 41.9 58.2 46.2 35.2 42.7 15.4 58.1 58.3 60.4 24.2 31.1 46.3 APSNet (Ours) 42.9 59.0 46.7 36.9 43.6 18.3 58.5 59.9 61.5 25.8 33.4 48.0 Performance comparison of amodal panoptic segmentation on the KITTI-360-APS validation set. Subscripts S and T refer to stuff and thing classes respectively. Subscripts S and T refer to stuff and thing classes respectively. Superscripts V and O refer to visible and occluded regions respectively. All scores are in [%].</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Loss max-pooling for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Samuel Rota Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7082" to="7091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to see the invisible: End-to-end trainable amodal instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Follmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>K?nig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>H?rtinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Klostermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>B?ttger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ssap: Single-shot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bird&apos;s-eye-view panoptic segmentation using monocular frontal view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Gosala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From learning to relearning: A framework for diminishing bias in social robot navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juana</forename><forename type="middle">Valeria</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Londo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning vector quantized shape code for amodal blastomere instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won-Dong</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalit</forename><surname>Ben-Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Needleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00985</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep occlusionaware instance segmentation with overlapping bilayers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06-02" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient decomposition of image and mesh graphs by lifted multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lavou?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Andres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1751" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combined object categorization and segmentation with an implicit shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Bastian Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on statistical learning in computer vision</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Amodal instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="677" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">KITTI-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13410</idno>
		<ptr target="https://creativecommons.org/licenses/by-nc-sa/3.0/.4" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">KITTI-360 dataset license</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vision-based autonomous landing in catastrophe-struck environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05700</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficientps: Efficient panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The importance of amodal completion in everyday perception. i-Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bence</forename><surname>Nanay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2041669518788887</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seamless scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Colovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8277" to="8286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Amodal instance segmentation with kins dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficientlps: Efficient lidar panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Sirohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>B?scher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single-shot instance segmentation by assigning pixels to object boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eike</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Fr?hlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convoluted mixture of deep experts for robust semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International conference on intelligent robots and systems (IROS) workshop, state estimation and terrain perception for all terrain mobile robots</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">There is more than meets the eye: Self-supervised multi-object detection and tracking with sound by distilling multimodal knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juana</forename><forename type="middle">Valeria</forename><surname>Francisco Rivera Valverde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11612" to="11621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7942" to="7951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">West</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Introduction to graph theory</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001" />
			<publisher>Prentice hall Upper Saddle River</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Amodal segmentation based on visible region segmentation and shape prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05093</idno>
		<title level="m">Deeperlab: Single-shot image parser</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<ptr target="https://doc.bdd100k.com/license.html.4" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>BDD100K dataset license available at</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic amodal segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selfsupervised visual terrain classification from unsupervised acoustic feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannik</forename><surname>Z?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
