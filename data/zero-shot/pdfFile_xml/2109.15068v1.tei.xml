<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iShape: A First Step Towards Irregular Shape Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
							<email>yanglei@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
							<email>sunwei@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhang</forename><surname>Huang</surname></persName>
							<email>zhhuang@buct.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
							<email>jackiehuanghaibin@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
						</author>
						<title level="a" type="main">iShape: A First Step Towards Irregular Shape Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a brand new dataset to promote the study of instance segmentation for objects with irregular shapes. Our key observation is that though irregularly shaped objects widely exist in daily life and industrial scenarios, they received little attention in the instance segmentation field due to the lack of corresponding datasets. To fill this gap, we propose iShape, an irregular shape dataset for instance segmentation. iShape contains six sub-datasets with one real and five synthetics, each represents a scene of a typical irregular shape. Unlike most existing instance segmentation datasets of regular objects, iShape has many characteristics that challenge existing instance segmentation algorithms, such as large overlaps between bounding boxes of instances, extreme aspect ratios, and large numbers of connected components per instance. We benchmark popular instance segmentation methods on iShape and find their performance drop dramatically. Hence, we propose an affinity-based instance segmentation algorithm, called ASIS, as a stronger baseline. ASIS explicitly combines perception and reasoning to solve Arbitrary Shape Instance Segmentation including irregular objects. Experimental results show that ASIS outperforms the state-of-the-art on iShape. Dataset and code are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head> <ref type="figure">Figure 1</ref><p>: A typical scene of objects with irregular shape and similar appearance. It has many characteristics that challenge instance segmentation algorithms, including the large overlaps between bounding boxes of objects, extreme aspect ratios (bounding box of the grey mask), and large numbers of connected components in one instance (green and blue masks).</p><p>Instance segmentation aims to predict the semantic and instance labels of each image pixel. Compared to object detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and semantic segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, instance segmentation provides more fine-grained information but is more challenging and attracts more and more research interests of the community. Many methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> and datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> continue to emerge in this field. However, most of them focus on regularly shaped objects and only a few <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref> study irregular ones, which are thin, curved, or having complex boundary and can not be well-represented by regularly rectangular boxes. A more clear definition of irregular shape is "the area of the bounding box is much larger than the area of instance mask or the aspect ratio of the bounding box is large". We think the insufficient exploration of this direction is caused by the lack of corresponding datasets.</p><p>In this work, we present iShape, a new dataset designed for irregular Shape instance segmentation. Our dataset consists of six sub-datasets, namely iShape-Antenna, iShape-Branch, iShape-Fence, iShape-Log, iShape-Hanger, and iShape-Wire. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, each sub-dataset represents scenes of a typical irregular shape, for example, strip shape, hollow shape, and mesh shape. iShape has many characteristics that reflect the difficulty of instance segmentation for irregularly shaped objects. The most prominent one is the large overlaps between bounding boxes of objects, which is hard for proposal-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> due to feature ambiguity and non-maximum suppression (NMS <ref type="bibr" target="#b20">[21]</ref>). Meanwhile, overlapped objects that share the same center point challenge center-based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. Another characteristic of iShape is a large number of objects with similar appearances, which makes embedding-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> hard to learn discriminative embedding. Besides, each sub-dataset has some unique characteristics. For example, iShape-Fence has about 53 connected components per instance, and iShape-Log has a large object scale variation due to various camera locations and perspective transformations. We hope that iShape can serve as a complement of existing datasets to promote the study of instance segmentation for irregular shape as well as arbitrary shape objects.</p><p>We also benchmark popular instance segmentation methods on iShape and find their performance degrades significantly. To this end, we introduce a stronger baseline considering irregular shape in this paper, which explicitly combines perception and reasoning. Our key insight is to simulate how a person identifies an irregular object. Taking the wire shown in <ref type="figure">Figure 1</ref> for example, one natural way is to start from a local point and gradually expand by following the wire contour and figure out the entire object. The behavior of such "following the contour" procedure is a process of continuous iterative reasoning based on local clues, which is similar to the recent affinity-based approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Under such observation, we propose a novel affinity-based instance segmentation baseline, called ASIS, which includes principles of generating effective and efficient affinity kernel based on dataset property to solve Arbitrary Shape Instance Segmentation. Experimental results show that the proposed baseline outperforms popular methods by a large margin on iShape.</p><p>Our contribution is summarized as follows:</p><p>? We propose a brand new dataset, named iShape, which focuses on irregular shape instance segmentation and has many characteristics that challenge existing methods. In particular, we analyzed the advantages of iShape over other instance segmentation datasets.</p><p>? We benchmark popular instance segmentation methods on iShape to reveal the drawbacks of popular methods on irregularly shaped objects.</p><p>? Inspired by human's behavior on instance segmentation, we propose ASIS as a stronger baseline on iShape, which explicitly combines perception and reasoning to solve Arbitrary Shape Instance Segmentation. 2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Existing Datasets</head><p>There are several benchmark datasets collected to promote the exploration of instance segmentation. The COCO <ref type="bibr" target="#b15">[16]</ref> and the Cityscapes <ref type="bibr" target="#b16">[17]</ref> are the most popular ones among them. However, the shapes of target objects in these datasets are too regular. The connected components per instance (CCPI) and average MaxIoU are low in the datasets and state-of-the-art algorithms selected from them can not generalize to more challenging scenarios. Instead, in the scenario of human detection and segmentation, the OC human <ref type="bibr" target="#b19">[20]</ref> and the Crowd Human <ref type="bibr" target="#b28">[29]</ref> introduce datasets with larger MaxIoU. Nevertheless, the OC human dataset only provides a small number of images for testing, and the number of instances per image is too small to challenge instance segmentation algorithms. While the crowd human dataset only provides annotations of object bounding boxes, limiting their application to the instance segmentation field. In the area of photogrammetry, the iSAID <ref type="bibr" target="#b17">[18]</ref> dataset is proposed to lead algorithms to tackle objects with multi scales. However, only a few instances in this dataset are of irregular shape, most of which are rectangular, and the lack of instance overlapping reduces its challenge to instance segmentation algorithms as well. Under the observation that these existing regular datasets are not enough to challenge algorithms for more general scenarios, we propose iShape, which contains irregularly shaped objects with large overlaps between bounding boxes of objects, extreme aspect ratios, and large numbers of CCPI to promote the capabilities of instance segmentation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instance Segmentation Algorithms</head><p>Existing instance segmentation algorithms can be divided into two classes, proposal-based and proposal-free.</p><p>Proposal-based approaches One line of these approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref> solve instance segmentation within a two-stage manner, by first propose regions of interests (RoIs) and then regress the semantic labels of pixels within them. The drawback of these approaches comes from the loss of objects by NMS due to large IoU. Instead, works like <ref type="bibr" target="#b14">[15]</ref> tackle the problem within a single-stage manner. For example, PolarMask <ref type="bibr" target="#b14">[15]</ref> models the contours based on the polar coordinate system and then obtain instance segmentation by center classification and dense distance regression. But the convex hull setting limits its accuracy.</p><p>Proposal-free approaches To shake off the rely on proposals and avoid the drawback caused by them, many bottom-up approaches like <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> are introduced. These works are in various frameworks. The recent affinity-based methods obtain instance segmentation via affinity derivation <ref type="bibr" target="#b26">[27]</ref> and graph partition <ref type="bibr" target="#b30">[31]</ref>. This formulation is more similar to the perception and reasoning procedure of human beings and can handle more challenging scenarios. GMIS <ref type="bibr" target="#b26">[27]</ref> utilizes both region proposals and pixel affinities to segment images and SSAP <ref type="bibr" target="#b27">[28]</ref> outputs the affinity pyramid and then performs cascaded graph partition. However, the affinity kernels of GMIS and SSAP are sparse in angle and distance, leading to missing components of some instances due to loss of affinity connection. To this end, we propose ASIS which includes principles of generating effective and efficient affinity kernel based on dataset property to solve Arbitrary Shape Instance Segmentation and achieve great improvement on iShape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">iShape Dataset</head><p>iShape consists of six sub-datasets. One of them, iShape-Antenna, is collected from real scenes, which are used for antenna counting and grasping in automatic production lines. The other five sub-datasets are synthetic datasets that try to simulate five typical irregular shape instance segmentation scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Creation</head><p>iShape-Antenna Creation. For the creation of iShape-Antenna, we first prepare a carton with a white cushion at the bottom, then randomly and elaborately place antennas in it to generate various scenes. Above the box, there is a camera with a light that points to the inside of the box to capture the scene images. We collect 370 pictures and annotate 3,036 instance masks then split them equally for training and testing. The labeling is done by our supplier. We have checked all the annotations ourselves, and corrected the wrong annotations. Although iShape-Antenna only contains 370 images, the number of instances reaches 3,036 which is more than most categories in Cityscapes <ref type="bibr" target="#b16">[17]</ref> and PASCAL VOC <ref type="bibr" target="#b31">[32]</ref>.</p><p>Synthetic Sub-datasets Creation There are lots of typical irregular shape instance segmentation scenes. Consequently, it is impractical to collect a natural dataset for each typical scene. Since it is traditional to study computer vision problems using synthetic data <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, we synthesize five sub-datasets of iShape which include iShape-Branch, iShape-Fence, iShape-Log, iShape-Hanger, and iShape-Wire, by using CG software Blender. Besides, our dataset focuses on the evaluation of instance segmentation methods on irregularly shaped objects, for which the synthetic images can serve well. In particular, we build corresponding 3D models and place them appropriately in Blender with optional random background and lighting environment, optional physic engine, and random camera position. The creation configs of synthesis sub-datasets are listed below. After setting up the scene, we use a ray tracing render engine to render the RGB image. Besides, We build and open source a blender module, bpycv <ref type="bibr" target="#b35">[36]</ref>, to generate instance annotation. We generate 2500 images for each sub-dataset, 2000 for training, 500 for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Characteristics</head><p>In this sub-section, we analyze the characteristics of iShape and compare it with other instance segmentation datasets. Since each sub-dataset represents irregularly shaped objects in different scenes, we present the statistical results of each sub-dataset separately.</p><p>Dataset basic information. As summarized in <ref type="table" target="#tab_0">Table 1</ref>, iShape contains 12,870 images with 175,840 instances. All images are 1024?1024 pixels and annotated with pixel-level ground truth instance masks. Since iShape focus on evaluating the performance of algorithms on the irregular shape, each scene consists of multiple instances of one class, which is also common cases in industrial scenarios.</p><p>Instance count per image. A larger instance count is more challenging. Despite iSAID getting the highest instance count per image, it is unfair for extremely high-resolution images and normalresolution images to be compared on the indicator. Among iShape, the instance count per image of iShape-Log reaches 28.86 that significantly higher than other normal-resolution datasets.</p><p>The large overlap between objects. We introduce a new indicator, Overlap of Sum (OoS), which aims to measure the degree of occlusion and crowding in a scene, defined as follows:</p><formula xml:id="formula_0">Overlap of Sum = 1 ? | n i=1 Ci| n i=1 |Ci| , n &gt; 0 0, n = 0<label>(1)</label></formula><p>where C means bounding boxes(bbox) or convex hulls(convex) of all instances in the image, n means number of instances, means union operation, and |C i | means to get the area of C i . The statistics of average OoS for bounding box and convex hull are presented in <ref type="table">Table.</ref> 1. For bounding box OoS, All iShape sub-datasets are higher than other datasets, which reflects the large overlap characteristic of iShape. Thanks to the large-area hollow structure, iShape-Fence gets the highest average convex hull OoS 0.63. Moreover, The Average MaxIoU <ref type="bibr" target="#b19">[20]</ref> of all images also reflects the large overlap characteristic of iShape.</p><p>The similar appearance between object instances. Instances from the same object class in iShape share similar appearance, which is challenging to embedding-based algorithms. In particular, any two object instance in iShape-Antenna, iShape-Fence and iShape-Hanger are indistinguishable according to their appearance. They are generated from either industrial standard antennas or copies of the same mesh models. Meanwhile, the appearance of objects in iShape-Branch, iShape-Log, and iShape-Wire are slightly changeable to add some variances, but appearances of different instances are still much more similar than those from other existing datasets in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Aspect ratio. <ref type="table" target="#tab_0">Table 1</ref> presents statistics on the average aspect ratio of the object's minimum bounding rectangle for each dataset. Among them, iShape-Log's aspect ratio reaches 34.14, which is more than 10 times of other regularly shaped datasets. Such a gap is caused by two following reasons: Firstly, the shape of logs has a large aspect ratio. Secondly, partially occluding logs leads to a higher aspect ratio. iShape-Antenna also has a high aspect ratio, 9.86, which exceeds other regularly shaped datasets.</p><p>Connected Components Per Instance (CCPI). Larger CCPI poses a larger challenge to instance segmentation algorithms. Due to the characteristics of irregular shaped objects and the occlusion of scenes, the instance appearance under the mesh shape tends to be divided into many pieces, leading to large CCPI of iShape-Fence. As is shown in <ref type="table" target="#tab_0">Table 1</ref>, the result on CCPI of iShape-Fence is 53.65, about 5 times higher than the second place. iShape-Branch, iShape-Hanger, and iShape-Wire also have a large CCPI that exceeds other regularly shaped datasets.  In the training stage, the network learns to predict the semantic segmentation as well as the affinity map. In the inference stage, first, build graph operation transforms the predicted affinity map into a sparse undirected graph by setting pixels as nodes and the affinity between pixels as edges. Then the graph merge algorithm is applied to the graph. The algorithm will cluster the pixels to yield class-agnostic instance segmentation. Finally, the class assign module will add a category with confidence to each instance using the result of semantic segmentation. Inspired by how a person identifies a wire shown in <ref type="figure">Figure 1</ref>, We propose an affinity-based instance segmentation baseline, called ASIS, to solve Arbitrary Shape Instance Segmentation by explicitly combining perception and reasoning. Besides, ASIS includes principles of generating effective and efficient affinity kernel based on dataset property. In this section, an overview of the pipeline is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baseline Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of ASIS</head><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, we firstly employ the PSPNet <ref type="bibr" target="#b10">[11]</ref> as the backbone and remove its last softmax activation function to extract features. The semantic head, which combines a single convolution layer and a softmax activation function, will input those features and output a C ? H ? W semantic segmentation probability map where C means the total categories number. The affinity head that consists of a single convolution layer and a sigmoid activation function will output a N ? H ? W affinity map, where N is the neighbor number of affinity kernel. Affinity kernel <ref type="bibr" target="#b26">[27]</ref> defines a set of neighboring pixels that needs to generate affinity information. Examples of affinity kernels can be found in <ref type="figure" target="#fig_4">Figure 4</ref>. Each channel of the affinity map represents a probability of whether the neighbor pixel and the current one belong to the same instance.</p><p>During the training stage, we apply the affinity kernel on the instance segmentation ground truth to generate the affinity ground truth. Since affinity ground truth is imbalanced, an OHEM <ref type="bibr" target="#b36">[37]</ref> loss is calculated between the predicted affinity map and the affinity ground truth to effectively alleviate the problem. Affinities that connect segments of fragmented instances are important but hard to learn. Thanks to the difficulty of learning these affinities, the OHEM loss pays more attention to these important affinities.</p><p>For the inference stage, we firstly take pixels as nodes and affinity map as edges to build an undirected sparse graph. The undirected sparse graph in <ref type="figure" target="#fig_3">Figure 3</ref> shows an example of how a pixel node on the spoon should connect the other pixel nodes. Then, we apply the graph merge algorithm <ref type="bibr" target="#b26">[27]</ref> on the undirected sparse graph. The algorithm will merge nodes that have a positive affinity to each other into one supernode, by contrast, keep nodes independent if their affinity is negative. Pixels that merged to the same supernodes are regarded as belonging to the same instance. In this way, we obtain a class-agnostic instance map. A class assign module <ref type="bibr" target="#b26">[27]</ref> will take the class-agnostic instance map and the semantic segmentation result as input, then assign a class label with a confidence value to each instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ASIS Affinity Kernel</head><p>Since instances could be divided into many segments, it is important to design an appropriate affinity kernel to connect those segments that belong to the same instance. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>(b) and <ref type="figure" target="#fig_4">Figure 4</ref>(c), The yellow point is the current pixel. Red points belong to different instances and blue points belong to the same instance of the current pixel. The antenna that the current pixel (yellow point) belongs to has two segments that need to be connected by affinity neighbor. The previous affinity-based approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> don't take into account such problems and cause some failures. Hence, we propose principles of generating effective and efficient affinity kernel based on dataset property to solve Arbitrary Shape Instance Segmentation. Our affinity kernel is shown in 4(a).</p><p>Affinity kernels of GMIS and SSAP are centered symmetric, unfortunately, that will cause redundant outputs. For example, the affinity of pixel (1, 1) with its right side pixel and the affinity of pixel (1, 2) with its left side pixel both mean the probability of these two pixels belonging to one instance. A detailed description of redundant affinity can be found in the appendix. To reduce the network's outputs, redundant affinity neighbors are discarded in the ASIS affinity kernel. As shown in 4(a), affinity neighbors of ASIS are distributed in an asymmetric semicircle structure. Besides, the area covered by asymmetric semicircle affinity kernel is reduced by half, in other words, the demand for receptive fields is reduced, which further reduces the difficulty of CNN learning affinity.</p><p>Two main parameters determine the shape of the ASIS affinity kernel. Kernel radius r k controls the radius of the kernel and determines how far the farthest of two segments can be reached. Affinity neighbor gap g represents the distance between any two nearly affinity neighbors, thus, g controls the sparseness of the affinity neighbor. Since each dataset has its optimal affinity kernel, we propose another algorithm that could adaptively generate appropriate r k and g based on the dataset property. Detailed descriptions of these two algorithms can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We choose representative instance segmentation methods in various paradigms and benchmark them on iShape to reveal the drawbacks of those methods on irregularly shaped objects. All the existing methods are trained and tested on six iShape sub-datasets with their recommended setting. And we further study the effect of our baseline method, ASIS.</p><p>Implementation of GMIS*. Since there is no open-source implementation of GMIS <ref type="bibr" target="#b26">[27]</ref> available, we reproduced GMIS as GMIS*. There are only two differences between ASIS and GMIS*. One is the different affinity kernel, and another is GMIS* does not apply OHEM loss.</p><p>Implementation Details. The input image resolution of our framework is 512 ? 512. The image data augmentation is flipped horizontally or vertically with a probability of 0.5. We use the ResNet-50 <ref type="bibr" target="#b37">[38]</ref> as our backbone network and the weight is initialized with ImageNet [39] pretrained model. All experiments are trained in 4 2080Ti GPUs and batch size is set to 8. The stochastic gradient descent (SGD) solver is adopted in 50K iterations. The momentum is set to 0.9 and weight decay is set to 0.0005. The learning rate is initially set to 0.01 and decreases linearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Results</head><p>We evaluate the proposed ASIS and other popular approaches on iShape. The quantitative results are shown in <ref type="table" target="#tab_1">Table 2</ref> and some qualitative results are reported in <ref type="figure" target="#fig_7">Figure 7</ref>.</p><p>As is shown in <ref type="table" target="#tab_1">Table 2</ref>, the performance of Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> is far from satisfactory on iShape. We think the drop in performance mainly comes from three drawbacks of the design. Firstly, the feature maps suffer from ambiguity when the IoU is large, which is a common characteristic of crowded scenes of irregular shape objects. Also, Mask R-CNN depends on the proposals of RoI, which may be abandoned by the NMS algorithm due to large IoU and lead to missing of some target objects. Moreover, many thin objects can not be segmented by Mask R-CNN because of its RoI pooling, which resizes the feature maps and lost the view of thin objects. The recent proposed end-to-end object detection approach, DETR <ref type="bibr" target="#b39">[40]</ref>, shake of the reliance of NMS and can better deal with objects   <ref type="figure">Figure 6</ref>: Activation maps of different affinity kernels. We calculate the loss for affinities that connects to the yellow point and perform backpropagate, then visualize the gradient on the input image to reveal the activated maps.</p><p>with large IoU and achieve better performance, as shown in the table. However, DETR still suffers from the RoI pooling problems and performs badly on thin objects, as shown in <ref type="figure" target="#fig_7">Figure 7</ref>.</p><p>We also report some qualitative results of SE <ref type="bibr" target="#b22">[23]</ref> in <ref type="figure" target="#fig_7">Figure 7</ref>. As is shown in the figure, one common failure case of SE is that when the length of irregular objects is longer than a threshold, the object will be split into multi instances, for example, the wire in <ref type="figure" target="#fig_7">Figure 7</ref>. We think that's because SE will regress a circle of the target instance and then calculate its IoU with the mask for supervision. However, for long and thin irregular objects, the radius of the center circle can not reach the length of the target object, leading to a multi-split of a long instance. Also, instances that share the same center may cause ambiguity to SE, such as hanger and fence in <ref type="figure" target="#fig_7">Figure 7</ref>. Moreover, many centers of irregular objects lie outside the mask, making it hard to match them to the objects themselves.</p><p>We evaluate SOLO v2 <ref type="bibr" target="#b21">[22]</ref> on the proposed iShape and find that it failed to segment instances that share the same center, for example, fences in <ref type="figure" target="#fig_7">Figure 7</ref>. Also, since SOLO V2 depends on the center point as SE, it also suffers from performance drop caused by object centers that lie outside the mask.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, we report the performance of PolarMask <ref type="bibr" target="#b14">[15]</ref> on our dataset. As is shown in the table, PolarMask can not solve the instance segmentation of irregular objects. That is because PolarMask can only represent a thirty-six-side mask due to its limited number of rays. Hence, it can not handle objects with hollow, for example, the fences. Also, they distinguish different instances according to center regression, which, however, can not handle instances that share the same center. We also find that PolarMask can only tackle some cases of logs in iShape, which looks like circles on the side and fit its convex hull mask setting.</p><p>Thanks to the perception and reasoning mechanism as well as the well-designed affinity kernels of our ASIS, it obtained the best performance on iShape. In <ref type="table" target="#tab_1">Table 2</ref>, ASIS advances GMIS* [27] by 21.7%, advances other popular approaches by 44.2%. However, there are still some drawbacks to the design of ASIS and some failure cases caused by them. For example in <ref type="figure" target="#fig_5">Figure 5</ref>(a), two instances are merged into one. We think that's because the graph merge algorithm is a kind of greedy algorithm, while the greedy algorithm makes optimal decisions locally instead of looking for a global optimum. Hence, ASIS is not robust to false-positive (FP) with high confidence. Also, ASIS fails to connect the two parts of an object if they are far away from each other, for example, the antenna on <ref type="figure" target="#fig_5">Figure 5</ref>(b). We think that's because CNN is not good at learning long-range affinity. Another failure case is the branch in <ref type="figure" target="#fig_7">Figure 7</ref>, ASIS results on iShape-Branch include many parts that have not been merged.</p><p>Those parts lead to a large number of small FPs, which causes a serious performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>Effect of ASIS kernel. In <ref type="table" target="#tab_1">Table 2</ref>, ASIS kernel advances GMIS kernel by 12.3% in iShape. We think that is because our well-designed affinity kernels based on dataset property can better discover the connectivity of different parts of an object. While the GMIS kernel suffers from its sparsity in distance and angle, examples are shown in <ref type="figure">Figure 8</ref>(b) and <ref type="figure">Figure 9</ref> shows the reason for its failure through the affinity visualization. In <ref type="table" target="#tab_2">Table 3</ref>, we also use ground truth affinity map to explore the upper bound of ASIS, where a 98.5% mAP is achieved, showing its great potential. Moreover, we find our non-centrosymmetric design of affinity kernel outperform centrosymmetric one by 10% in the table. We think such a design cut off the output and calculation redundancy and reduce the requirement of large receptive field from CNN, simplifying representation learning. Inspired by Grad-CAM <ref type="bibr" target="#b40">[41]</ref>, we use the gradient of the input image to visualize the model's activation map. As shown in <ref type="figure">Figure 6</ref>, ASIS pays more attention to the discriminative information such as the instance outline, while GMIS focuses more on the area around the center point.  <ref type="figure">Figure 8</ref>: Results compared with GMIS kernel. As shown in (b), GMIS kernel fail to connect segments that belong to one instance.</p><p>(a) ASIS (b) GMIS <ref type="figure">Figure 9</ref>: Affinity visualization. The yellow point is the kernel center. The ASIS kernel has a more reasonable affinity distribution and can connect the two separated parts, while GMIS fails to reach the right segment.</p><p>Effect of OHEM. <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> show that OHEM boosts the performance of ASIS and GMIS by a large margin. We think firstly that is because OHEM can ease problems caused by the imbalance distribution of positive and negative affinity. Besides, affinities that connect segments of fragmented instances are important but hard to learn, which means the OHEM loss pays more attention to these important affinities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduce a new irregular shape instance segmentation dataset (iShape). iShape has many characteristics that challenge existing instance segmentation methods, such as large overlaps, extreme aspect ratios, and similar appearance between objects. We evaluate popular algorithms on iShape to establish the benchmark and analyze their drawbacks to reveal possible improving directions. Meanwhile, we propose a stronger baseline, ASIS, to better solve Arbitrary Shape Instance Segmentation. Thanks to the combination of perception and reasoning as well as the well-designed affinity kernels, ASIS outperforms the state-of-the-art methods on iShape. We believe that iShape and ASIS can serve as a complement to existing datasets and methods to promote the study of instance segmentation for irregular shape as well as arbitrary shape objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The six sub-datasets in iShape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Overview of ASIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Illustration for affinity kernels. (a) ASIS affinity kernel could connect these two segments with two neighbors (blue points). (b) GMIS affinity kernel cannot reach the right segment. (c) Examples of failure case for SSAP affinity kernel. For higher resolutions (top), 5 ? 5 affinity window cannot reach the segment on the right. For lower resolutions (bottom), the view of thin antennas are lost in the resized feature maps. firstly described in Subsection 4.1, then design principles of the ASIS affinity kernel are explained in Subsection 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Two example false cases of ASIS on iShape-Antenna. (a) Two antennas merged into one (blue and orange). (b) ASIS fails to connect the right parts of an object (red and sky blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of different instance segmentation approaches on iShape. More results are shown in https://ishape.github.io or the appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of statistics with different datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Images Instances</cell><cell>Instances /image</cell><cell cols="5">OoS bbox convex MaxIoU Average Aspect CCPI ratio</cell></row><row><cell>Cityscapes</cell><cell>2,975</cell><cell>52,139</cell><cell>17.52</cell><cell>0.14</cell><cell>0.07</cell><cell>0.394</cell><cell>2.29</cell><cell>1.34</cell></row><row><cell>COCO</cell><cell cols="2">123,287 895,795</cell><cell>7.26</cell><cell>0.15</cell><cell>0.09</cell><cell>0.210</cell><cell>2.59</cell><cell>1.41</cell></row><row><cell cols="2">CrowdHuman 15,000</cell><cell>339,565</cell><cell>22.64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>OC Human</cell><cell>4,731</cell><cell>8,110</cell><cell>1.71</cell><cell>0.25</cell><cell>0.20</cell><cell>0.424</cell><cell>2.28</cell><cell>3.11</cell></row><row><cell>iSAID</cell><cell>2,806</cell><cell>655,451</cell><cell>233.58</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2.40</cell><cell>-</cell></row><row><cell>Antenna</cell><cell>370</cell><cell>3,036</cell><cell>8.20</cell><cell>0.62</cell><cell>0.23</cell><cell>0.655</cell><cell>9.86</cell><cell>2.45</cell></row><row><cell>Branch</cell><cell>2,500</cell><cell>26,046</cell><cell>10.14</cell><cell>0.62</cell><cell>0.52</cell><cell>0.750</cell><cell>2.47</cell><cell>10.88</cell></row><row><cell>Fence</cell><cell>2,500</cell><cell>7,870</cell><cell>3.15</cell><cell>0.65</cell><cell>0.63</cell><cell>0.983</cell><cell>1.05</cell><cell>53.65</cell></row><row><cell>Hanger</cell><cell>2,500</cell><cell>49,275</cell><cell>19,71</cell><cell>0.53</cell><cell>0.34</cell><cell>0.685</cell><cell>3.28</cell><cell>4.94</cell></row><row><cell>Log</cell><cell>2,500</cell><cell>72,144</cell><cell>28.86</cell><cell>0.73</cell><cell>0.06</cell><cell>0.843</cell><cell>34.14</cell><cell>2.64</cell></row><row><cell>Wire</cell><cell>2,500</cell><cell>17,469</cell><cell>6.99</cell><cell>0.74</cell><cell>0.60</cell><cell>0.795</cell><cell>3.32</cell><cell>4.76</cell></row><row><cell>iShape</cell><cell>12,870</cell><cell>175,840</cell><cell>13.66</cell><cell>0.65</cell><cell>0.42</cell><cell>0.806</cell><cell>15.84</cell><cell>6.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Qualitative results on iShape. We report the mask mmAP of six sub-datasets and the average of mmAP. All methods use ResNet-50 as the backbone. "w/o" denotes "without". For "62.93/0.05", the small number on the right is the standard deviation of four experiments.</figDesc><table><row><cell>Method</cell><cell cols="4">Antenna Branch Fence Hanger</cell><cell>Log</cell><cell>Wire</cell><cell>Avg</cell></row><row><cell>SOLOv2 [22]</cell><cell>6.6</cell><cell>27.5</cell><cell>0.0</cell><cell>28.8</cell><cell>22.2</cell><cell>0.0</cell><cell>14.07</cell></row><row><cell>PolarMask [15]</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>18.6</cell><cell>0.0</cell><cell>3.10</cell></row><row><cell>SE [23]</cell><cell>38.3</cell><cell>0.0</cell><cell>0.0</cell><cell>49.8</cell><cell>20.9</cell><cell>0.0</cell><cell>18.17</cell></row><row><cell>Mask RCNN [12]</cell><cell>16.9</cell><cell>4.2</cell><cell>0.0</cell><cell>22.1</cell><cell>32.6</cell><cell>0.0</cell><cell>12.63</cell></row><row><cell>DETR [40]</cell><cell>2.1</cell><cell>2.6</cell><cell>0.0</cell><cell>32.2</cell><cell>46.2</cell><cell>0.0</cell><cell>13.85</cell></row><row><cell>GMIS* [27]</cell><cell cols="7">67.6 /0.4 14.9 /0.2 30.6 /0.1 24.8 /0.3 63.2 /0.9 46.1 /1.1 41.21 /0.23</cell></row><row><cell>ASIS w/o OHEM</cell><cell cols="7">82.1 /0.9 17.6 /0.4 48.0 /0.3 40.5 /0.4 66.4 /0.4 66.5 /0.5 53.51 /0.12</cell></row><row><cell>ASIS(ours)</cell><cell cols="7">88.5 /0.3 24.6 /0.4 60.4 /0.4 57.4 /0.2 69.4 /0.2 77.3 /0.4 62.93 /0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study of GMIS and ASIS on iShape-Antenna. "SY" and "ASY" indicate a centrosymmetric or asymmetric affinity kernel respectively. ? denotes equipped with and ? not.</figDesc><table><row><cell></cell><cell cols="5">Affinity Kernel Neighbors Affinity GT OHEM mAP</cell></row><row><cell></cell><cell>GMIS [27]</cell><cell>56 (SY)</cell><cell>? ? ?</cell><cell>? ? -</cell><cell>67.6 81.5 90.2</cell></row><row><cell></cell><cell></cell><cell>28 (ASY)</cell><cell>?</cell><cell>?</cell><cell>77.7</cell></row><row><cell></cell><cell>ASIS(ours)</cell><cell>53 (ASY)</cell><cell>? ? ?</cell><cell>? ? -</cell><cell>82.1 88.5 98.5</cell></row><row><cell>(a) ASIS</cell><cell cols="2">(b) GMIS</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="21" to="37" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gui-Song Xia, and Xiang Bai. isaid: A large-scale dataset for instance segmentation in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshita</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Fahad Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Occluded video instance segmentation: Dataset and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<editor>Xiang Bai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Zhi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Pose2seg: Detection free human instance segmentation</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8837" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9018" to="9028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Affinity derivation and graph merge for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssap: Singleshot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Synthetic data for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nikolenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11512</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Playing for benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2232" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sail-vos: Semantic amodal instance level video object segmentation -a synthetic dataset and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3100" to="3110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">bpycv: Computer vision utils for blender</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://github.com/DIYer22/bpycv" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
