<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Vision Transformer</term>
					<term>Remote Sensing</term>
					<term>Object Detection</term>
					<term>Scene Classification</term>
					<term>Semantic Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale vision foundation models have made significant progress in visual tasks on natural images, where the vision transformers are the primary choice for their good scalability and representation ability. However, the utilization of large models in the remote sensing (RS) community remains under-explored where existing models are still at small-scale, which limits the performance. In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models customized for RS tasks and explore how such large models perform. Specifically, to handle the large image size and objects of various orientations in RS images, we propose a new rotated variedsize window attention to substitute the original full attention in transformers, which could significantly reduce the computational cost and memory footprint while learn better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks demonstrate the superiority of our model over all state-of-the-art models, achieving 81.16% mAP on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also demonstrate competitive performance compared with the existing advanced methods. Further experiments show the advantages of our models on computational complexity and few-shot learning. The code and models will be released at https://github.com/ViTAE-Transformer/Remote-Sensing-RVSA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. The comparison between (a) natural images and (b) RSIs. Here, we show some common categories including bridge and airplane in both types of images. Compared with the natural images from horizontal observation, RSIs tend to be in an overhead view. The natural images are from the ImageNet-1K dataset, while the RSIs are chosen from the UCM, AID, or NWPU datasets. generating the DOM since it is easy to access and can be obtained in real-time. The RSIs are being employed in many valuable applications such as scene recognition for land use and land cover classification for precision agriculture <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and object detection for maritime monitoring <ref type="bibr" target="#b2">[3]</ref>.</p><p>It is necessary to effectively represent the RSI contents and attributes to perform the above applications. In the current remote sensing (RS) field, the convolutional neural networks (CNNs) are the most commonly used models for extracting hierarchical multiscale visual features. However, existing studies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> reveal that the limited receptive field of convolution in each layer makes it difficult for CNNs to pay attention to longrange pixels and extract global context. To address this issue, the self-attention (SA) mechanism <ref type="bibr" target="#b5">[6]</ref> is proposed to obtain flexible global dependency by enabling the interaction between arbitrary pixels in images, delivering promising results in the computer vision (CV) field. Further, vision transformer <ref type="bibr" target="#b6">[7]</ref> adopts the design of multi-head SA (MHSA), which simultaneously implements the above procedure in multiple projected subspaces, which diversifies the extracted contexts and improves the feature representation.</p><p>The plain vision transformer (ViT) <ref type="bibr" target="#b6">[7]</ref> is a very simple architecture that stacks several transformer encoder blocks sequentially after the patch embedding layer, where features after each block are at the same scale. To better adapt the vision transformers to downstream tasks, researchers borrow the idea of hierarchical design in CNNs and devise hierarchical vision transformers accordingly <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. These models are usually pretrained in a supervised way using large-scale datasets and finetuned on the downstream tasks. The recent work <ref type="bibr" target="#b9">[10]</ref> empirically studies the hierarchical vision transformers for RSIs by comparing different pretraining strategies and arXiv:2208.03987v2 [cs.CV] 10 Aug 2022 different models. It confirms the superiority of hierarchical vision transformers over CNNs and reveals the effectiveness of pretraining with RS scene datasets like MillionAID <ref type="bibr" target="#b10">[11]</ref>. Nevertheless, due to the strong presentation ability of the transformers, researchers question the necessity of the hierarchical design in transformers for downstream tasks. Thanks to the development of the unsupervised learning in masked image modeling (MIM) <ref type="bibr" target="#b11">[12]</ref>, recent work reveals that such a pretraining process can give a good initialization for plain ViTs to achieve surprising results on various downstream tasks including object detection and pose estimation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. For example, ViTDet <ref type="bibr" target="#b12">[13]</ref> uses intermediate features at the same scale and upsamples/downsamples them to build the feature pyramid for object detection. The main insight behind their success is that the multiscale prior can be learned from the data during the pretraining, thus making it possible to discard the hierarchical structure.</p><p>Inspired by the above pilot studies, we employ the same MIM pretraining and finetuning routine to investigate the influences of plain ViTs on RS tasks. Usually, obtaining the annotations is expensive and nontrivial since understanding RSIs usually requires some expert experience. By contrast, it is much easier to obtain massive unlabeled RSIs in different resolutions and temporals from different kinds of sensors, e.g., via the abundant satellites for continuous earth observations. How to leverage the abundant unlabeled RSIs for pretraining has become an active research topic in the RS community. A few self-supervised learning (SSL) methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> have been proposed, which however only target CNNs and have not been proven effective for large-scale vision transformer models. Recently, reconstruction-based SSL methods such as MAE <ref type="bibr" target="#b11">[12]</ref> have been proposed and shown effective for pretraining plain ViTs and adapting them for downstream tasks <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In this paper, we also employ MAE to pretrain the plain ViT and the recently proposed ViTAE transformer with about 100M parameters on the MillionAID dataset without using the labels.</p><p>After pretraining, we adapt the vision transformers to downstream tasks via finetuning on the corresponding datasets. To reduce the computational cost and memory footprint, a natural choice is to replace the full self-attention with local window attention <ref type="bibr" target="#b12">[13]</ref>. However, the windows are in fixed sizes and locations, which may restrict the region to extract useful context and limit the model representation ability. A recently proposed method named varied-size attention (VSA) <ref type="bibr" target="#b16">[17]</ref> addresses this issue by learning trainable scaling factors and offset to adapt the size, shape, and location of windows to diverse image content, thus delivering better performance on many tasks. However, RSIs are different from natural images in capturing objects from different view directions because of the gravity-free environment caused by the overhead view as shown in <ref type="figure">Figure 1</ref>. To handle this difference, we propose to extend VSA to rotated varied-size attention (RVSA). It introduces an extra learnable rotation mechanism in determining the window configuration, where oriented windows at different angles, sizes, shapes, and locations are obtained to extract richer context. We evaluate the proposed method on both plain ViT <ref type="bibr" target="#b6">[7]</ref> and ViTAE <ref type="bibr" target="#b8">[9]</ref> models for three kinds of RS tasks including scene classification, semantic segmentation, and object detection. We hope this study can fill the gap and provide useful insights about developing large plain vision transformers for the RS community.</p><p>The main contribution of this study is three-fold: <ref type="bibr" target="#b0">(1)</ref> We demonstrate the possibility of pretraining plain ViTs with about 100 million parameters on RSIs, adapting them for downstream RS tasks, and delivering competitive performance. To our best knowledge, they are so far the largest models in the remote sensing community, making a step towards the remote sensing foundation model. <ref type="bibr" target="#b1">(2)</ref> We introduce a learnable rotation mechanism into the vision transformer to learn varied-size windows with different orientation angles for attention calculation, which is very suitable to deal with RSIs. It promotes the extraction of rich context from the generated windows and learning better feature representation. (3) Experimental results show that the proposed models set new state-of-the-art (SOTA) on the detection task, and obtain competitive performances on classification and segmentation tasks. Besides, we also show the advantages of the proposed models in terms of computational complexity and few-shot learning ability. The remainder of this paper is organized as follows. Section II briefly reviews related works including vision foundation models, window-based vision transformers, and model pretraining for RS tasks. Section III describes the proposed method, where we separately present the implementation of unsupervised MAE pretraining on MillionAID and the proposed RVSA method. The experiment results on the three tasks and the related comprehensive analyses are presented in section IV. Finally, Section V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Vision Foundation Model</head><p>Foundation models based on transformers have demonstrated strong capabilities and exciting homogeneity in both vision and language tasks <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Vision transformers have also experienced rapid development towards large-scale foundation models thanks to their great potential in scalability and structure flexibility, e.g., the model size can be easily scaled up via stacking the same transformer layers with widening dimension <ref type="bibr" target="#b19">[20]</ref>, introducing a mixture of experts <ref type="bibr" target="#b20">[21]</ref>, and introducing inductive bias <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Among them, plain ViT structures have attracted more attention for their simplicity and flexibility in input formats and superior performance in natural image classification tasks <ref type="bibr" target="#b23">[24]</ref>. However, how to adapt the plain ViTs with isotropic structures to downstream visual tasks remains challenging. Recently, ViTDet <ref type="bibr" target="#b12">[13]</ref> demonstrates that although there is no multi-stage structure in plain ViTs, they can generate multi-scale features via simple up-and down-sampling modules and use the window-attention mechanism to reduce computational cost and memory footprint significantly. Similarly, ViTPose <ref type="bibr" target="#b13">[14]</ref> shows the potential of the plain structure on pose estimation tasks with a simpler decoder. Inspired by their success, we argue it is also of great significance to explore the potential of the plain ViT structures in RS tasks, which has been largely ignored. In this work, we develop the first plain ViT backbone networks for remote sensing tasks and scale them up to 100M parameters, which are the largest models in remote sensing literature. Equipped with the proposed RVSA method, they show superior performance on different RS tasks including classification, detection, and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Window-based Vision Transformer</head><p>Although the plain ViTs demonstrate superior performance with model scaling, the full attention operation employed in them brings quadratic complexities over the image size, limiting their applications in downstream visual tasks where highresolution images are very common. To address this issue, window-based MHSA (WMHSA) <ref type="bibr" target="#b7">[8]</ref> has been employed by partitioning the images into non-overlapping windows and conducting MHSA inside each window. WMHSA has linear computational complexity with respect to the image size, making it possible to process high-resolution images. For example, ViTDet <ref type="bibr" target="#b12">[13]</ref> explores interleaved windows-based and full attention modules to process images with sizes up to 1, 024?1, 024. Although the success of WMHSA in balancing memory consumption and performance, it needs extra mechanisms to bridge the connection between different windows, e.g., shifting operations or full attentions. To alleviate such an issue, some works introduce different window partitions with the connection between adjacent windows or additional tokens for cross-window information exchange <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Despite their efficiency in different vision tasks, they rely on hand-crafted designs of the window shape and size as well as sequentially stack these layers for modeling relationships between distant tokens. Recently, VSA <ref type="bibr" target="#b16">[17]</ref> proposes to learn adaptive window size in a data-driven manner, where the windows could cover different regions and promote cross-window information exchange. In this work, we extend the VSA idea to adapt the plain ViT for remote sensing tasks. Specifically, to better adapt the vision transformer to remote images, we propose the RVSA method by introducing a learnable rotation mechanism. Such a strategy provides network-oriented windows at different angles, sizes, shapes, and locations to learn better object representation. The orientation-aware mechanism improves the performance of vision transformers on different RS tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Pretraining for RS task</head><p>Model pretraining is an essential step to improve the performance of deep neural networks on RSIs. Previous methods mostly utilize ImageNet <ref type="bibr" target="#b26">[27]</ref> dataset for pretraining and obtain performance improvement in classification, detection, and segmentation with task-specific designs <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>. Unfortunately, due to the huge difference between natural images and RSIs, there always exist domain gaps to be mitigated when transferring these models pretrained on natural images to the remote sensing tasks. Recently, <ref type="bibr" target="#b9">[10]</ref> proposes to pretrain deep models on a large-scale RS dataset named MillionAID in a fullysupervised manner. It demonstrates that the pretraining on RS datasets can help improve the performance of both CNNs and vision transformers. However, the requirement of more labeled data is still a burden for pretraining much larger models. To break the labeled data insufficiency restriction, RS representations from other resources are explored. For example, GeoKR <ref type="bibr" target="#b30">[31]</ref> leverages the land cover ratio in the digital geographical information product as a weak label for RS representation learning. However, the shift of label distribution between data from different resources still impedes the learning. Some works resort to self-supervised learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref> with different RS characteristics taken into the design, e.g., exploring seasonal variants for constructing positive pairs <ref type="bibr" target="#b14">[15]</ref> or leveraging spatial and temporal information <ref type="bibr" target="#b15">[16]</ref>. However, they only target CNNs. A very recent method RingMo <ref type="bibr" target="#b34">[35]</ref> employs an advanced MiM method named SimMiM <ref type="bibr" target="#b35">[36]</ref> to pretrain large-scale vision transformer models. Different from these works, we use the representative MAE method <ref type="bibr" target="#b11">[12]</ref> for MIM pretraining and specifically focus on advancing plain ViTs for remote sensing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we will first introduce the details about the unsupervised pretraining of plain ViT and ViTAE with MAE and the design of the proposed RVSA modules. Then, we will briefly discuss how to transfer the pretrained models to different RS tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unsupervised Pretraining by MAE</head><p>To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE <ref type="bibr" target="#b11">[12]</ref> with the large-scale RSI dataset, i.e., MillionAID <ref type="bibr" target="#b10">[11]</ref>. In the following, we will briefly introduce the dataset, the MAE method, the network structure used during pretraining, and corresponding implementation details.</p><p>1) MillionAID: The MillionAID dataset <ref type="bibr" target="#b10">[11]</ref> is a largescale dataset with RS scene images and corresponding labels. It contains 100,0848 non-overlapping RS scenes in RGB formats, which is suitable to serve as inputs for typical deep vision models. There are 51 classes in the dataset, where each class has about 2,000?45,000 images. MillionAID is collected from Google Earth, where images are captured with diverse sensors and thus have different resolutions. Generally, the image sizes in the MillionAID dataset range from 110 ? 110 to 31,672 ? 31,672. It should be noted that although the MillionAID dataset contains both images and labels, we only use the images during pretraining and discard the labels by following the unsupervised pretraining routine.</p><p>2) MAE: MAE <ref type="bibr" target="#b11">[12]</ref> aims to recover the masked image parts given the visible ones with an encoder-decoder structure. Specifically, for an input image, it tokenizes the image by first splitting the image into non-overlapping patches and then projecting each patch into a visual token using a patch embedding layer. After that, several visual tokens are dropped from the inputs and treated as masked regions to be predicted, following a pre-defined mask ratio. The remained tokens are fed into the transformer encoder for feature extraction. Then, several learnable mask tokens and the extracted features of visible tokens are processed by the transformer decoder to recover the masked regions. During training, the model is optimized to minimize the distance between the recovered regions and the ground truth masked regions, either in pixel space or feature space. We follow the original MAE setting and calculate the loss in the normalized pixel space. For more details about MAE, please refer to <ref type="bibr" target="#b11">[12]</ref>.</p><p>3) Pretrained Backbone Networks: One key design in MAE is token dropping, which greatly accelerates the training speed and reduces the memory footprint during pretraining. Besides, the random dropping mechanism can improve the spatial diversity of the remained tokens. However, this operation will break the spatial relationship between tokens. Such a limitation makes it unsuitable in pretraining networks with 2D-specific designs, such as window attention operations and 2D down-sampling layers. To this end, operations that do not take 2D prior into consideration are more beneficial in MAE pretraining tasks, e.g., the vanilla full self-attention layers or convolution layers with 1 ? 1 kernel size.</p><p>Following the above analysis, we adopt two backbone networks for pretraining in the paper, i.e., plain ViT <ref type="bibr" target="#b6">[7]</ref> and ViTAE <ref type="bibr" target="#b36">[37]</ref>. The former one is composed of plain transformer encoders with full self-attention layers. This simple structure enables it to seamlessly work with the MAE pretraining since it discards the 2D structures and directly treats the image as a 1D sequence. The latter one incorporates inductive bias such as locality from convolutions along with the full selfattention layers, i.e., employing parallel convolution branches (PCM) along with the MHSA layers. However, it is difficult to directly adopt the ViTAE structure for MAE pretraining since the random masking strategy breaks the spatial relationship while ViTAE uses 3 ? 3 convolutions in PCM. To address this issue, there are small modifications to PCM, i.e., we adopt convolution with kernel size 1 ? 1 in PCM during pretraining. After that, the kernel size is padded to 3?3 when finetuning on specific downstream tasks. Assuming the weight in pretraining for ith convolutional layer is W</p><formula xml:id="formula_0">(i) P = [?] 1?1 (ignoring the channel space), the padded kernel is implemented as follows W (i) F = ? ? ? ? ? ? ? ? ? ? ? ? ? 3?3 ,<label>(1)</label></formula><p>where ? is the learned value during MAE and ? is initialized to 0 and is learnable during finetuning. Besides, we adopt a shallow design of the PCM in the utilized ViTAE model, i.e., a convolutional layer, a batch normalization layer, a SiLU layer, and a convolutional layer in sequence, to save the memory footprint. <ref type="figure">Figure 3</ref> shows the plain transformer block and the basic ViTAE block used for MAE pretraining. To mitigate the issue of lack of position information in ViTAE, a sin-cos positional encoding is added after the patch embedding layer to emphasize the positional information ( <ref type="figure">Figure 3</ref> has not shown for simplicity). More details can be referred to <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b36">[37]</ref>. We utilize the "base" version of the ViT and ViTAE models both with about 100M parameters. The networks are denoted as "ViT-B" and "ViTAE-B", respectively. The detailed structures of the two networks can be found in <ref type="table" target="#tab_0">Table I</ref>, where "Patch Size" represents the size of split patches and "Embedding Dim" is the dimension of the projected tokens. "Head" denotes the number of heads used in MHSA. "Group" represents the number of groups for the convolutions in PCM, which is not used in the ViT-B model. "Ratio" means the expansion ratio of the FFN. "Depth" represents the number of transformer blocks in the two models. It has the same meaning with N as demonstrated in <ref type="figure">Figure 2</ref>. 4) Implementation Details: We utilize two subsets S 1 and S 2 during pretraining. The two subsets have 949,848 and 51,000 images, respectively. They are randomly sampled from  the MillionAID dataset. Note that a class-balance sampling strategy is employed in constructing S 2 , i.e., we randomly select 1,000 images from each category. During pretraining, we use a batch size of 2048, equally distributed on 8 A100 GPUs, with the AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer. The models are trained for 1,600 epochs if not specified, following the default setting in MAE <ref type="bibr" target="#b11">[12]</ref>. Due to the huge distribution difference between the natural images and remote sensing images, we first investigate the optimal mask ratio in MAE for RSIs. To this end, we search for the optimal mask ratio on the S 1 dataset for pretraining. We utilize a linear probing setting and finetuning setting for evaluation. For the linear probing setting, we randomly initialize a linear classifier after the backbone and only update the classifier's parameters during training. The models are trained for 100 epochs with a batch size of 256. For the finetuning setting, parameters from the backbone network and the classifier are together tuned with a layer-wise learning rate decay mechanism <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Except the batch size is set to 64 and the epoch of 200. The evaluation is conducted on existing RS scene classification tasks, including UCM <ref type="bibr" target="#b39">[40]</ref>, AID <ref type="bibr" target="#b40">[41]</ref>, and NWPU <ref type="bibr" target="#b41">[42]</ref> datasets, by finetuning the pretrained models on the three datasets. The models are pretrained for 400 epochs and finetuned for 200 epochs, respectively. The results are available in <ref type="table" target="#tab_0">Table II</ref>. We report the average accuracy on the three evaluation datasets (denoted as "AVG"). It can be observed that with the mask ratio of 0.75, the pretrained models obtain the best performance in both linear probing and finetuning settings. In the following experiments, we set the mask ratio to 0.75 by default. In addition, it can be seen that the performance could be improved when a longer training schedule is used, e.g., 1600 epochs. In the following experiments, all the pretrained models are obtained by training for 1600 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fintuning with Rotated Varied-Size Attention</head><p>Different from natural images, images from RS tasks usually have larger resolutions. It will cause unaffordable training costs when directly transferring the pretrained models with full self-attention on the downstream tasks, due to the quadratic computational complexity of full self-attention. To this end, we substitute the full self-attention modules with windowbased attention modules during the finetuning stage, which reduces the computational complexity to linear with respect to the image size. This substitution is also seamless since the difference between the two kinds of attention lies in the manner of attention calculation, which is parameter-free. However, the original window-based attention operations always take fixedsize window partitions and at a fixed orientation. Due to the gravity-free nature of objects in the remote sensing images, it is probably not the optimal design to use fixed-size window partitions. To address this issue, we design the rotated varied size window attention mechanism, which will be described in detail as follows.</p><p>1) Window-based attention and Varied-size window attention: Window-based attention We will briefly review the window-based attention <ref type="bibr" target="#b7">[8]</ref> and varied-size window attention (VSA) <ref type="bibr" target="#b16">[17]</ref> in this part. Given an input X ? R C?H?W , window-based attention firstly partitions the input X into several non-overlapping windows, i.e., X w ? R C?s?s , where s represents the size of the window. There are a total of H s ? W s windows in the image. Then, features inside each window is transformed to h triplets representing the query, key and value features:</p><formula xml:id="formula_1">{Q (i) w , K (i) w , V (i) w } h i=1 by three linear layers, respectively, where Q (i) w , K (i) w , V (i) w ? R C ?s?s , C = hC ,</formula><p>and h is the number of heads in the attention layers. We denote the generated features as {Q</p><formula xml:id="formula_2">(i,j) w , K (i,j) w , V (i,j) w |j = 1, ? ? ? , HW s 2 } h i=1 ,</formula><p>where j indexes the window. After that, the attention calculations are conducted inside each window, i.e.,</p><formula xml:id="formula_3">F (i,j) w = SA(Q (i,j) w , K (i,j) w , V (i,j) w ) = sof tmax( Q (i,j) w K (i,j)T w ? C )V (i,j) w ,<label>(2)</label></formula><p>where F</p><formula xml:id="formula_4">(i,j) w ? R s 2 ?C .</formula><p>Then, the features from different heads are concatenated along the channel dimension and features from different windows are concatenated along the spatial dimension to recover the shape of the input feature. Varied-size window attention However, such a fixed-size window partition method may be probably not optimal in tasks with the need to deal with objects of various sizes. Recently, VSA is proposed to mitigate this issue by learning the window size in a data-driven manner. Specifically, it treats the fixedsize windows as initialization and extracts the query features from these windows, i.e., Q w . For key and value features, the input X w are used to estimate an offset and scale of the target window</p><formula xml:id="formula_5">S w , O w = Linear(LeakyReLU (GAP (X w ))),<label>(3)</label></formula><p>where S w and O w represent the scaling factor and the offset of the target window. GAP is short for the global average pooling operation. Then, the initial window is transformed according to the estimated two factors, i.e.,</p><formula xml:id="formula_6">? ? ? ? x l y l x r y r ? ? ? ? = ? ? ? ? x c y c x c y c ? ? ? ? + ? ? ? ? x r l y r l x r r y r r ? ? ? ? ,<label>(4)</label></formula><formula xml:id="formula_7">? ? ? ? x l y l x r y r ? ? ? ? = ? ? ? ? x c y c x c y c ? ? ? ? + ? ? ? ? o x o y o x o y ? ? ? ? + ? ? ? ? x r l ? s x y r l ? s y x r r ? s x y r r ? s y ? ? ? ? ,<label>(5)</label></formula><p>where x l , y l , x r , y r represent the coordinates of the left upper and right down corners of the initial window. x c , y c represent the center coordinate of the window, and x r l , y r l , x r r , y r r are the distance between the corner points and the center in horizontal and vertical directions, respectively. o x , o y and s x , s y denote the predicted offsets and scale factors, i.e.,</p><formula xml:id="formula_8">S w = {s x , s y ? R 1 }, O w = {o x , o y ? R 1 }.</formula><p>x l , y l , x r , y r indicate the corners of the transformed window. Then, the key and value features are sampled from the transformed windows and used for attention calculation. The number of sampled key and value tokens are the same as the query tokens to maintain the same computational complexity between the VSA and window-based attention.</p><p>2) Rotated Varied-Size Attention: VSA successfully demonstrates its effectiveness in computer vision tasks on natural images. However, there are significant differences between natural images and RSIs. For example, due to the gravity-free nature of objects in RSIs, they may present at arbitrary angles in the images, while the default windows and the windows generated by VSA are always in the horizontal and vertical directions, which is probably not optimal for RS tasks. To this end, we introduce one extra dimension to control the orientation of the windows, leading to the rotated varied size attention technique in this paper. Specifically, the rotation angle ? w = {? ? R 1 } is predicted along with S w and O w , given the input feature X w , i.e., S w , O w , ? w = Linear(LeakyReLU (GAP (X w ))), <ref type="bibr" target="#b5">(6)</ref> and the transformed coordinates are calculated as: <ref type="figure" target="#fig_1">Figure 4</ref> shows a diagram of the proposed RVSA method. We also propose a variant of RVSA, which allows that key and values tokens can be samples from different windows, i.e., we use separate prediction layers to predict the scale, shift, and rotation factors for key and values tokens, respectively:</p><formula xml:id="formula_9">x l/r y l/r = x c y c + o x o y + cos ? sin ? ? sin ? cos ? x r l/r ? s x y r l/r ? s y .<label>(7)</label></formula><formula xml:id="formula_10">S K w , O K w , ? K w = Linear K (LeakyReLU (GAP (X w ))), S V w , O V w , ? V w = Linear V (LeakyReLU (GAP (X w ))).<label>(8)</label></formula><p>This more flexible module is denoted as RVSA ? .</p><p>3) Computational Complexity Analysis: We analyze the computational complexity of the proposed RVSA module. Given the input X ? R C?H?W , we first partition the input into non-overlapping windows with shape s ? s. Since the complexity is the same for the calculation of each window, we will demonstrate the computational complexity of one window in this section. The features inside the window are used to predict the scale, shift, and rotation factors of the window, with a subsequent global average pooling (GAP) layer, an activation layer, and a linear layer. The GAP layer brings a computational complexity of O(s 2 C). The activation layers bring about zero computations and we dismiss them for simplicity. The following linear layer projects the pooled features from dimension C to dimension 5h, since it needs to predict two scales and shift factors in horizontal and vertical directions and one rotation factor for each head. Thus, the linear projection layer has O(5hC) computational complexity. After that, bilinear sampling is used to sample the key and value tokens from the transformed windows, which has about O(4s 2 C) computational complexity. Thus, the total computational complexity of RVSA for each window is O(5s 2 C + 5hC). There are a total number of HW s 2 windows after the window partition. To this end, the overall extra computational complexity brought by RVSA is O(5HW C(1 + 5h s 2 )). Since the computational complexity of the original window attention is O(2s 2 HW C), where s is always set to 7 and h is set to 12, RVSA only brings marginally extra (i.e., about 11%) computational cost. 4) Implementation Details: To adapt the MAE pretrained models to downstream RS tasks, we replace the plain MHSA modules with the proposed RVSA modules. Following the strategy in ViTDet <ref type="bibr" target="#b12">[13]</ref>, we use full attention blocks at each 1/4 depth layer and use RVSA in all other layers. Specifically, for models with 12 layers like the ViT-B and ViTAE-B, we use full attention layers at the 3rd, 6th, 9th, and 12th layer and uses RVSA in all other layers. The modified networks are denoted as "ViT-B + RVSA" and "ViTAE-B + RVSA" in the paper, respectively. Similarly, we use the original window attention, VSA, and RVSA ? for comparison. These variants are denoted as "ViT-B-Win", "ViT-B + VSA", "ViT-B + RVSA ? ", "ViTAE-B-Win", "ViTAE-B + VSA", and "ViTAE-B + RVSA ? ", respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we evaluate the performance of the proposed models on multiple RS tasks, including scene classification, object detection, and semantic segmentation. We first conduct a series of ablation studies on the object detection task to analyze and find suitable settings for the proposed models. Then, with the help of existing popular frameworks, we compare our methods with the current SOTA approaches on public benchmarks. We also show the superiority of the proposed models in terms of computational complexity (e.g., training speed and memory footprint) and few-shot learning ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Remote Sensing Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Implementation Details and Experimental Settings:</head><p>Following the de facto standard, all models are trained with the AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer, where the learning rate and weight decay are set to 1e-4 and 5e-2, respectively. We use the 1? training schedule with 12 epochs and a batch size of 2. The learning rate is adjusted by a multi-step scheduler, i.e., the learning rate is reduced by 10? at the 8th and 11th epoch. We also adopt the layer-wise learning rate decay strategy, where the decay rate is set to 0.75.</p><p>For a fair comparison with <ref type="bibr" target="#b9">[10]</ref>, we employ the same Oriented R-CNN <ref type="bibr" target="#b45">[46]</ref> detection framework for OBB detection while only changing the backbone. We adopt the default hyperparameters defined in OBBDetection 1 . Following ViTDet <ref type="bibr" target="#b12">[13]</ref>, we separately upsample and downsample the output feature from the last layer through deconvolution and pooling layers to construct the feature pyramid.</p><p>Following Oriented R-CNN, when conducting the singlescale training and testing, the DOTA dataset is cropped to 1,024 ? 1,024 patches with a stride of 824. We also implement the multiscale training and testing, where the original images are first resized to three scales, i.e., (0.5, 1.0, 1.5), which are then cropped to 1,024 ? 1,024 patches with a stride of 524. During training, we adopt the data augmentations including random horizontal and vertical flipping, while the random rotation is considered for multiscale training and testing.   For DOTA-V1.0, the original training and validation sets are jointly used for training following <ref type="bibr" target="#b9">[10]</ref>. We separately evaluate our models on the original testing sets of DOTA-V1.0 and DIOR-R. The evaluation results of DOTA-V1.0 are obtained from the online server by submitting the predictions of the testing set. The average precision (AP) of each class and the mean average precision (mAP) are reported. All experiments are implemented within 2 NVIDIA A100 GPUs.</p><p>3) Determining the Suitable Window Size: Considering ViTDet <ref type="bibr" target="#b12">[13]</ref> and VSA <ref type="bibr" target="#b16">[17]</ref> use different window sizes, we first investigate the influence of window size s on DOTA-V1.0 and DIOR-R. We search for the optimal value of s in the range of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref> and the results are shown in <ref type="table" target="#tab_0">Table  III</ref>. Interestingly, the performance peaks at s = 7 on both datasets, and either increasing or decreasing the window size will lead to a performance drop. The reason can be attributed to two aspects. (1) Although a larger window size increases the receptive field and makes the tokens within each window encode more context, it limits the window diversity due to the reduced window number HW s 2 and thus leads to less-diverse contexts extracted by the model. (2) When decreasing the window size, the number of the attended tokens will also drop, which may result in inefficient information captured during attention. The experiments indicate s = 7 can be a good balance between the token number and window diversity for these RS datasets. Thus we set s = 7 by default in the following experiments. 4) Comparison of Different Attention Methods: After determining the window size s, we compare different attention methods based on ViT-B and the results are listed in <ref type="table" target="#tab_0">Table  IV</ref>. Surprisingly, ViT-B with full self-attention performs worse than ViT-B-Win despite more tokens involved during attention. By introducing the scale and offset factors, the size and location of the window in VSA can be adaptively learned. Compared with the ViT-B-Win, ViT-B + VSA learns adaptive windows for varied-size objects in different positions, improving the detection performance significantly. Nevertheless, the learned windows are horizontal and vertical, which are insufficient to accurately capture the object context in the   We compare the proposed methods with some recently proposed and so far the most advanced methods, and the results are presented in <ref type="table" target="#tab_0">Table VI</ref>-VII. The top three scores in each metric are marked by bold, red and blue, respectively. On the DOTA-V1.0 dataset, we list the results of single-scale training and multiscale training, separately. As can be seen, when training on a single scale, our models have advantages in most categories. Concretely, the proposed models perform the best in nine classes and surpass the previous best method by about 1% mAP. In the more competitive multi-scale setting, our models still win the first place in a total of six categories. Owing to the greatly improved detection results in some challenging  categories such as the roundabout and helicopter, our model sets a new SOTA on DOTA-V1.0, i.e., an mAP of 81.16%, outperforming all previous methods. On the more challenging DIOR-R dataset, the proposed models perform the best in fourteen categories. Specifically, we find that the proposed models achieve surprisingly good results in some categories such as the airplane, where the performance is even improved over 10% mAP. As a result, our models achieve the best performance and significantly outperform the second place by 5% mAP. In addition, we can see that RVSA performs better than RVSA ? on the DOTA-V1.0 dataset, while RVSA ? seems to prefer the DIOR-R dataset. These results are consistent with previous observations (See <ref type="table" target="#tab_6">Table V</ref>). We also provide some detection results in <ref type="figure" target="#fig_2">Figure 5</ref>, where the first two rows are the results of ViTAE-B + RVSA with multiscale training and testing on DOTA-V1.0, and the remained rows are the results of ViTAE-B + RVSA ? with multiscale training and testing on DIOR-R. As can be seen, based on the ViTAE backbone equipped with the proposed RVSA modules, Oriented R-CNN can accurately recognize and locate various artificial or natural objects with high confidence no matter in city and rural scenes or dense and sparse distributions. The above qualitative and quantitative results demonstrate the superiority of the proposed RVSA and its effectiveness in advancing plain ViTs towards remote sensing foundation models.</p><p>B. Remote Sensing Scene Classification 1) Dataset: We also evaluate the proposed models for the scene classification task on the UC Merced Land Use (UCM) dataset <ref type="bibr" target="#b39">[40]</ref>, the Aerial Image Dataset (AID) <ref type="bibr" target="#b40">[41]</ref>, and the famous scene classification benchmark constructed by Northwestern Polytechnical University, called NWPU dataset <ref type="bibr" target="#b41">[42]</ref>. Their details are shown in <ref type="table" target="#tab_0">Table VIII</ref>.</p><p>2) Implementation Details and Experimental Settings: Following <ref type="bibr" target="#b59">[60]</ref>, we consider five settings, including UCM-55, AID-28, AID-55, NWPU-19, and NWPU-28 to comprehensively evaluate the proposed models. Here, the suffix ?mn means 10 ? m% samples are used for training, while the  others are for testing. We change the neuron number of the last linear layer to match the number of categories in each dataset. Note that we use the same pretrained models as those in the experiments for object detection and follow the hyperparameter settings in Section III-A4 during finetuning on these datasets. We use the top-1 accuracy as the evaluation metric.</p><p>3) Experimental Results: <ref type="table" target="#tab_0">Table IX</ref> summarizes the scene classification results of different models on the above five settings. It can be seen that our models are superior to other methods in three settings including UCM-55, AID-28, and NWPU-28. These comparison methods include specially designed scene classification methods that use different backbone networks such as ResNet, VGG, DenseNet, and MobileNet, as well as the recently proposed pretraining-based methods with advanced vision transformers. While in other settings, our models still perform to be competitive with RSP-ViTAEv2-S, which is also pretrained on MillionAID. We notice that our methods mainly perform worse in the NWPU-19 setting. It is because the proposed RVSA needs a certain amount of training data to learn the optimal window configurations, while NWPU-19 has relatively small-scale training data. When expanding the training set as in NWPU-28, our models surpass RSP-ViTAEv2-S and achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Remote Sensing Semantic Segmentation</head><p>We also transfer the pretrained models to the semantic segmentation task. Besides the Potsdam 2 and iSAID <ref type="bibr" target="#b69">[70]</ref> datasets following <ref type="bibr" target="#b9">[10]</ref>, we also adopt a recently proposed domain adaptation segmentation dataset -LoveDA <ref type="bibr" target="#b70">[71]</ref>. Here, we do not distinguish urban and rural areas, and we directly utilize the official division for general segmentation.</p><p>1) Dataset: The categories in iSAID are completely the same as DIOR-R since the two datasets share the same set of scenes while targeting different tasks. The details of these datasets for semantic segmentation are shown in <ref type="table" target="#tab_11">Table X</ref>. 2) Implementation Details and Experimental Settings: Most of the experimental settings are same with <ref type="bibr" target="#b9">[10]</ref>. Considering the more parameters in larger models, we increase the training iterations to 160k. Besides, the layer decay rate is 0.9. Following <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b36">[37]</ref>, the UperNet <ref type="bibr" target="#b71">[72]</ref> is employed as the segmentation framework. We separately upsample or downsample the outputs from block 4, block 6, block 8, and block12 to form the required feature pyramid as in BeiT <ref type="bibr" target="#b38">[39]</ref>. For training on LoveDA, patches of 512 ? 512 resolution are randomly cropped from the images as input. We combine the training and validation sets of LoveDA to form a trainval set for training, while the testing set is unchanged. Besides the overall accuracy (OA) computed for the Potsdam dataset following the common protocol in the RS segmentation community, we report the mean of the intersection over union (IoU) for all categories in iSAID and LoveDA. We follow the single scale setting to calculate all metrics for a fair comparison. These experiments are implemented with 2?4 NVIDIA A100 GPUs. <ref type="table" target="#tab_0">Table XI</ref> shows the results of different methods. It can be seen that our models obtain comparable performance to SOTA methods. We acknowledge that the performance of the proposed models on the segmentation task is not as impressive as those on the object detection and scene classification tasks. We attribute it to two reasons. First, we only use the classical segmentation framework UperNet, which can not effectively propagate highlevel semantics to the high-resolution features. Therefore, it is unable to carry out an accurate pixel-level understanding as the latest proposed frameworks such as UNetFormer <ref type="bibr" target="#b78">[79]</ref> and FactSeg <ref type="bibr" target="#b77">[78]</ref>. Another reason may be that the vision transformer backbones we adopted have the plain structure whose tokens are directly embedded from 16 ? 16 patches and the feature map resolution keeps 1/16 of the input size. Compared with the hierarchical structure like Swin and ViTAEv2, which could generate high-resolution feature maps at early stages, the plain ViTs may lose details, which may be harmful to the pixel-level semantic segmentation task. Nevertheless, the proposed RVSA can still advance the plain ViTs and achieve similar performance to RSP-ViTAEv2-S, demonstrating its strong ability in capturing useful context from the learned rotated varied-size windows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Experimental Results:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with Plain ViT on All Tasks</head><p>In this section, we compare the proposed ViT models equipped with RVSA/RVSA ? and the plain ViT on all tasks. The plain ViT performs well in the scene classification task but has poor performance on the detection and segmentation tasks. This may be because all tokens probably contribute to the description of the scene category and thus taking all tokens into consideration for attention calculation is beneficial for learning image-level feature representation for scene classification. In contrast, object detection and semantic segmentation prefer object-level and pixel-level feature representation, where the semantics highly relate to the nearby tokens while the tokens far away may contribute less. Thus RVSA and RVSA ? could better balance the contribution of nearby and far away tokens and perform much better than the plain ViT on the two tasks. Here, taking ViT-B as an example, we present the average results of different models on all the datasets of different tasks in <ref type="table" target="#tab_0">Table XII</ref>. The scores are computed by averaging the metrics across datasets in each task, e.g., the average detection score of ViT-B + RVSA is (78.75 + 70.67)/2 ? 74.71. As can be seen, ViT-B with the proposed RVSA modules performs much better than the baseline on detection and segmentation tasks since they promote the extraction of rich contexts from diverse windows. Besides, RVSA and RVSA ? perform comparably on the average, although they prefer different datasets at different difficulty levels as shown in <ref type="table" target="#tab_0">Table IX</ref> and XI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Few-shot Learning</head><p>Few-shot learning is an important ability of foundation models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b79">[80]</ref>. To investigate the few-shot learning ability of the proposed models, we conduct the experiments on the DIOR-R datasets using different amounts of training data for object detection. For comparison, we include some smallscale models such as RSP-ResNet-50, RSP-Swin-T, and RSP-ViTAEv2-S, which are trained with all training samples. Their pretrained weights are obtained from the codebase 3 . <ref type="figure">Figure  6</ref> shows the results. As can be seen, the proposed models outperform the corresponding ViT-B and ViTAE-B baseline models regardless of the number of training samples. In addition, they achieve comparable performance to Swin-T by using only 40% training samples, outperform ResNet-50 and Swin-T when 60% training samples are used, and surpass the strong backbone ViTAEv2-S by using 80% samples. These findings validate the good few-shot learning ability of our models. <ref type="figure">Figure 6</ref> also shows that RVSA ? requires more training samples to perform better than RVSA, as demonstrated in Section IV-A4. <ref type="bibr" target="#b2">3</ref> https://github.com/ViTAE-Transformer/ViTAE-Transformer-Remote-Sensing <ref type="figure">Fig. 6</ref>. Results of different models trained with different amounts of training samples on the DIOR-R dataset. The Oriented R-CNN detection framework is adopted for all models.  <ref type="figure" target="#fig_4">Figure 7</ref> shows the visualization results of generated windows from the attention layer in the penultimate block of different models. These models are all based on ViT-B but use different attention methods, including window attention, VSA, RVSA, and RVSA ? . The gray areas denote zero paddings such that the feature size can be divisible by the window size. For example, the ViT-B-Win has ( 1024 16 + 6) 2 = 100 windows in one head when processing DOTA-V1.0 images (see the first row in <ref type="figure" target="#fig_4">Figure 7</ref> (a)) since zero padding length is 6. As can be seen, the windows generated by VSA can be scaled and shifted to match different objects. Nevertheless, VSA is unable to effectively deal with the arbitrary-oriented objects in RSIs, such as the oriented airplanes in the second row of <ref type="figure" target="#fig_4">Figure 7</ref>. By contrast, our RVSA introduces the rotation factor to address this issue, obtaining more diverse windows and promoting the extraction of more rich contexts. It is also noteworthy that the generated windows in one head can well adapt to some airplanes and each head can produce a different set of windows. Therefore, the airplanes can be covered by the windows in different heads, implying that RVSA can better deal with arbitrary-oriented objects. Compare with RVSA, RVSA ? further improves the flexibility of generated windows. By comparing (d) and (e) with (c), we can find that there are slight changes in the window shape for key and value tokens, which could be useful when dealing with challenging samples and there are a large amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualization of RVSA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we make the first attempt to investigate the potential of the plain vision transformer towards remote sensing foundation model. Specifically, we propose a novel rotated varied-size window attention method to advance the performance of plain vision transformers. It generates diverse windows at different angles, sizes, shapes, and locations to cover the arbitrary-oriented objects in remote sensing images and enables to extract rich contexts from the generated windows, thereby promoting the learning of better object representation. We validate the proposed method based on the representative unsupervised pretraining method MAE on typical remote sensing tasks including scene classification, object detection, and semantic segmentation. The results demonstrate the superiority of the proposed method and its effectiveness in advancing the plain vision transformer for different tasks. We hope this study could provide useful insights to the community and inspire more future research on developing remote sensing foundation models, especially based on plain vision transformers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Overall structure of the pretrained vision transformer network. "P" means the positional encoding. The final output tokens are averaged using global average pooling (GAP) for classification. The structures of the adopted blocks in the MAE encoder. (a) ViT block. (b) Modified ViTAE normal cell. Here, BN, LN, and FFN are batch normalization layer, layer normalization layer, and feed-forward network, G-Conv means the group convolutional layer. Img2Seq and Seq2Img are the reshape operation for conducting transformation between 1-D and 2-D features, these functions are not necessary for pretraining since there are only 1-D token sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>The pipeline of the proposed RVSA method in the i-th attention head. w * denotes the predicted windows for sampling key and value tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Some visual detection results. The first two rows are the results of ViTAE-B + RVSA on DOTA-V1.0, while the remained rows are the results of ViTAE-B + RVSA ? on DIOR-R. Best viewed with zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>including the number of parameters (Params), computations (FLOPs), GPU memory footprint, and training time (shown as T trn ). It can be seen that all these models have more than 100M parameters, where ViT-B has the most memory footprint andFLOPs as well as the longest training time (Note that it uses two GPUs) because of the quadratic complexity of full self-attention. ViT-B-Win alleviates these issues by adopting WMHSA, where the parameters reduce slightly because of the use of relative positional encoding instead of absolute positional encoding. Note that the FLOPs of ViT-B + VSA is smaller than ViT-B-Win since the padding operation is implemented after the generation of query, key, and value tokens. ViT-B + VSA brings slightly extra memory footprints than ViT-B-Win due to learnable scale and offset factors. Compared to ViT-B + VSA, ViT-B + RVSA has a similar complexity, while ViT-B + RVSA ? slightly increases the parameters and computational overheads since it adopts individual window prediction layers for key and value tokens. Compared to ViT-B, the proposed ViT-B + RVSA and ViT-B + RVSA ? can save approximately half the memory and accelerate the training speed, while achieving better performance. 5) Compare with State-of-the-art Methods:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of the generated windows of different attention methods based on ViT-B. (a) Window attention. (b) VSA. (c) RVSA. (d) and (e) are the generated windows of RVSA ? for the key and value tokens, separately. The images are from the testing sets of DOTA-V1.0 and DIOR-R, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc>HYPER-PARAMETER SETTINGS OF VIT-B AND VITAE-B.</figDesc><table><row><cell>Network</cell><cell>ViT-B [37]</cell><cell>ViTAE-B [9]</cell></row><row><cell>Patch Size</cell><cell>16</cell><cell>16</cell></row><row><cell>Embedding Dim</cell><cell>768</cell><cell>768</cell></row><row><cell>Head</cell><cell>12</cell><cell>12</cell></row><row><cell>Group</cell><cell>-</cell><cell>192</cell></row><row><cell>Ratio</cell><cell>4</cell><cell>4</cell></row><row><cell>Depth</cell><cell>12</cell><cell>12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc>OF DIFFERENT HYPER-PARAMETER SETTINGS IN MAE FOR PRETRAINING VIT-B ON THREE DATASETS, I.E., UCM, AID, AND NWPU. "AVG" DENOTES THE AVERAGE TOP-1 ACCURACY ON THREE DATASETS.</figDesc><table><row><cell>Top1 Acc (%)</cell><cell>UCM-55</cell><cell>AID-28</cell><cell>NWPU-19</cell><cell>AVG</cell></row><row><cell></cell><cell cols="3">Mask Ratio = 0.6 Epoch = 400</cell><cell></cell></row><row><cell>Linear probe</cell><cell>45.81</cell><cell>57.51</cell><cell>59.03</cell><cell>54.12</cell></row><row><cell>Fine tune</cell><cell>99.62</cell><cell>97.22</cell><cell>94.05</cell><cell>96.96</cell></row><row><cell></cell><cell cols="3">Mask Ratio = 0.75 Epoch = 400</cell><cell></cell></row><row><cell>Linear probe</cell><cell>49.90</cell><cell>61.70</cell><cell>61.28</cell><cell>57.63</cell></row><row><cell>Fine tune</cell><cell>99.71</cell><cell>97.06</cell><cell>94.43</cell><cell>97.07</cell></row><row><cell></cell><cell cols="3">Mask Ratio = 0.9 Epoch = 400</cell><cell></cell></row><row><cell>Linear probe</cell><cell>51.71</cell><cell>58.61</cell><cell>61.22</cell><cell>57.18</cell></row><row><cell>Fine tune</cell><cell>99.43</cell><cell>96.33</cell><cell>93.86</cell><cell>96.54</cell></row><row><cell></cell><cell cols="3">Mask Ratio = 0.75 Epoch = 1600</cell><cell></cell></row><row><cell>Linear probe</cell><cell>51.24</cell><cell>62.20</cell><cell>61.75</cell><cell>58.40</cell></row><row><cell>Fine tune</cell><cell>99.62</cell><cell>97.53</cell><cell>94.56</cell><cell>97.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III THE</head><label>III</label><figDesc>MAP (%) OF VIT-B + RVSA WITH DIFFERENT WINDOW SIZES ON DOTA-V1.0 AND DIOR-R DATASETS.</figDesc><table><row><cell>Window size</cell><cell>4</cell><cell>7</cell><cell>11</cell><cell>14</cell></row><row><cell>DOTA-V1.0</cell><cell>77.84</cell><cell>78.75</cell><cell>77.83</cell><cell>77.44</cell></row><row><cell>DIOR-R</cell><cell>70.55</cell><cell>70.67</cell><cell>70.40</cell><cell>70.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV THE</head><label>IV</label><figDesc>MAP (%) OF DIFFERENT VIT-B VARIANTS ON DOTA-V1.0 AND DIOR-R DATASETS. THE SUFFIX DENOTES DIFFERENT WINDOW ATTENTION METHODS. WA: WINDOW ATTENTION. SF: SCALE FACTOR. OF: OFFSET FACTOR. RF: ROTATION FACTOR.</figDesc><table><row><cell>Method</cell><cell>WA SF &amp; OF RF</cell><cell cols="2">Oriented R-CNN mAP DOTA-V1.0 DIOR-R</cell></row><row><cell>ViT-B</cell><cell></cell><cell>77.05</cell><cell>66.65</cell></row><row><cell>ViT-B-Win</cell><cell></cell><cell>77.19</cell><cell>67.95</cell></row><row><cell>ViT-B + VSA</cell><cell></cell><cell>78.40</cell><cell>70.48</cell></row><row><cell>ViT-B + RVSA</cell><cell></cell><cell>78.75</cell><cell>70.67</cell></row><row><cell>ViT-B + RVSA ?</cell><cell></cell><cell>78.61</cell><cell>70.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V THE</head><label>V</label><figDesc>TRAINING COSTS OF DIFFERENT VIT-B VARIANTS ON DOTA-V1.0 AND DIOR-R DATASETS. At last, we evaluate ViT-B + RVSA ? where the key and value tokens are sampled from different windows to extract contexts more flexibly.Table IVshows that ViT-B + RVSA and ViT-B + RVSA ? perform slightly different on the two datasets. We guess that relaxing the constraint that key and value tokens share the same window can further improve the model representation capability while at the risk of overfitting.</figDesc><table><row><cell>Method</cell><cell cols="5">Oriented R-CNN Params (M) Input Size FLOPs (G) 1 Memory (M) Ttrn (hh:mm:ss)</cell></row><row><cell>DOTA-V1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-B 2</cell><cell>113.73</cell><cell>1,024</cell><cell>717.79</cell><cell>25,757*2</cell><cell>12:30:42</cell></row><row><cell>ViT-B-Win</cell><cell>113.63</cell><cell>1,024</cell><cell>427.43</cell><cell>24,685</cell><cell>11:41:29</cell></row><row><cell>ViT-B + VSA</cell><cell>113.92</cell><cell>1,024</cell><cell>413.26</cell><cell>25,321</cell><cell>12:12:11</cell></row><row><cell>ViT-B + RVSA</cell><cell>114.00</cell><cell>1,024</cell><cell>413.29</cell><cell>25,343</cell><cell>12:31:30</cell></row><row><cell>ViT-B + RVSA ?</cell><cell>114.37</cell><cell>1,024</cell><cell>413.60</cell><cell>25,386</cell><cell>13:08:39</cell></row><row><cell>DIOR-R</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-B</cell><cell>112.47</cell><cell>800</cell><cell>364.56</cell><cell>21,408</cell><cell>09:05:08</cell></row><row><cell>ViT-B-Win</cell><cell>112.39</cell><cell>800</cell><cell>263.63</cell><cell>12,322</cell><cell>06:12:14</cell></row><row><cell>ViT-B + VSA</cell><cell>112.69</cell><cell>800</cell><cell>252.37</cell><cell>12,729</cell><cell>06:56:16</cell></row><row><cell>ViT-B + RVSA</cell><cell>112.76</cell><cell>800</cell><cell>252.40</cell><cell>12,744</cell><cell>07:11:35</cell></row><row><cell>ViT-B + RVSA ?</cell><cell>113.13</cell><cell>800</cell><cell>252.48</cell><cell>12,772</cell><cell>07:43:18</cell></row><row><cell cols="6">RSI that may be displayed in any direction. ViT-B + RVSA</cell></row><row><cell cols="6">addresses this issue by introducing an extra rotation factor,</cell></row><row><cell cols="6">thus generating windows in various directions to better fit the</cell></row><row><cell cols="6">overhead viewing characteristics in remote sensing. It enables</cell></row><row><cell cols="6">the model to effectively extract contexts that are more suitable</cell></row><row><cell cols="6">for describing RS objects. As a result, ViT-B + RVSA performs</cell></row><row><cell cols="6">better than ViT-B + VSA on both DOTA-V1.0 and DIOR-</cell></row><row><cell>R.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Therefore, ViT-B + RVSA ? performs better on the more challenging dataset DIOR-R, which has more training images and categories. Besides accuracy, we also compare the training cost of the above models. Table V summarizes the evaluation metrics</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI RESULTS</head><label>VI</label><figDesc>OF DIFFERENT METHODS ON THE TESTING SET OF THE DOTA-V1.0 DATASET.</figDesc><table><row><cell>Method</cell><cell>Pretrain</cell><cell>Backbone</cell><cell>PL 1</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>Singe-scale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROI Trans. [29]</cell><cell>IMP 2</cell><cell>ResNet-101</cell><cell>88.64</cell><cell>78.52</cell><cell>43.44</cell><cell>75.92</cell><cell>68.81</cell><cell>73.68</cell><cell>83.59</cell><cell>90.74</cell><cell>77.27</cell><cell>81.46</cell><cell>58.39</cell><cell>53.54</cell><cell>62.83</cell><cell>58.93</cell><cell>47.67</cell><cell>69.56</cell></row><row><cell>R 3 Det [47]</cell><cell>IMP</cell><cell>ResNet-101</cell><cell>88.76</cell><cell>83.09</cell><cell>50.91</cell><cell>67.27</cell><cell>76.23</cell><cell>80.39</cell><cell>86.72</cell><cell>90.78</cell><cell>84.68</cell><cell>83.24</cell><cell>61.98</cell><cell>61.35</cell><cell>66.91</cell><cell>70.63</cell><cell>53.94</cell><cell>73.79</cell></row><row><cell>Gilding Vertex [48]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>89.64</cell><cell>85.00</cell><cell>52.26</cell><cell>77.34</cell><cell>73.01</cell><cell>73.14</cell><cell>86.82</cell><cell>90.74</cell><cell>79.02</cell><cell>86.81</cell><cell>59.55</cell><cell>70.91</cell><cell>72.94</cell><cell>70.86</cell><cell>57.32</cell><cell>75.02</cell></row><row><cell>AOPG [44]</cell><cell>IMP</cell><cell>ResNet-101</cell><cell>89.14</cell><cell>82.74</cell><cell>51.87</cell><cell>69.28</cell><cell>77.65</cell><cell>82.42</cell><cell>88.08</cell><cell>90.89</cell><cell>86.26</cell><cell>85.13</cell><cell>60.60</cell><cell>66.30</cell><cell>74.05</cell><cell>67.76</cell><cell>58.77</cell><cell>75.39</cell></row><row><cell>DODet [49]</cell><cell>IMP</cell><cell>ResNet-101</cell><cell>89.61</cell><cell>83.10</cell><cell>51.43</cell><cell>72.02</cell><cell>79.16</cell><cell>81.99</cell><cell>87.71</cell><cell>90.89</cell><cell>86.53</cell><cell>84.56</cell><cell>62.21</cell><cell>65.38</cell><cell>71.98</cell><cell>70.79</cell><cell>61.93</cell><cell>75.89</cell></row><row><cell>S 2 ANet [50]</cell><cell>IMP</cell><cell>ResNet-101</cell><cell>88.70</cell><cell>81.41</cell><cell>54.28</cell><cell>69.75</cell><cell>78.04</cell><cell>80.54</cell><cell>88.04</cell><cell>90.69</cell><cell>84.75</cell><cell>86.22</cell><cell>65.03</cell><cell>65.81</cell><cell>76.16</cell><cell>73.37</cell><cell>58.86</cell><cell>76.11</cell></row><row><cell>ReDet [51]</cell><cell>IMP</cell><cell>ReResNet-50</cell><cell>88.79</cell><cell>82.64</cell><cell>53.97</cell><cell>74.00</cell><cell>78.13</cell><cell>84.06</cell><cell>88.04</cell><cell>90.89</cell><cell>87.78</cell><cell>85.75</cell><cell>61.76</cell><cell>60.39</cell><cell>75.96</cell><cell>68.07</cell><cell>63.59</cell><cell>76.25</cell></row><row><cell>R 3 Det-KLD [52]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>88.90</cell><cell>84.17</cell><cell>55.80</cell><cell>69.35</cell><cell>78.72</cell><cell>84.08</cell><cell>87.00</cell><cell>89.75</cell><cell>84.32</cell><cell>85.73</cell><cell>64.74</cell><cell>61.80</cell><cell>76.62</cell><cell>78.49</cell><cell>70.89</cell><cell>77.36</cell></row><row><cell>Oriented RepPoints [53]</cell><cell>IMP</cell><cell>Swin-T</cell><cell>89.11</cell><cell>82.32</cell><cell>56.71</cell><cell>74.95</cell><cell>80.70</cell><cell>83.73</cell><cell>87.67</cell><cell>90.81</cell><cell>87.11</cell><cell>85.85</cell><cell>63.60</cell><cell>68.60</cell><cell>75.95</cell><cell>73.54</cell><cell>63.76</cell><cell>77.63</cell></row><row><cell>Oriented R-CNN [46]</cell><cell>IMP</cell><cell>ResNet-101</cell><cell>88.86</cell><cell>83.48</cell><cell>55.27</cell><cell>76.92</cell><cell>74.27</cell><cell>82.10</cell><cell>87.52</cell><cell>90.90</cell><cell>85.56</cell><cell>85.33</cell><cell>65.51</cell><cell>66.82</cell><cell>74.36</cell><cell>70.15</cell><cell>57.28</cell><cell>76.28</cell></row><row><cell>Oriented R-CNN [46]</cell><cell>RSP [10]</cell><cell>ViTAEv2-S</cell><cell>89.66</cell><cell>83.04</cell><cell>55.85</cell><cell>75.16</cell><cell>79.95</cell><cell>84.34</cell><cell>88.04</cell><cell>90.90</cell><cell>88.17</cell><cell>85.58</cell><cell>62.64</cell><cell>70.60</cell><cell>76.77</cell><cell>67.15</cell><cell>67.89</cell><cell>77.72</cell></row><row><cell>AO2-DETR [54]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>89.27</cell><cell>84.97</cell><cell>56.67</cell><cell>74.89</cell><cell>78.87</cell><cell>82.73</cell><cell>87.35</cell><cell>90.50</cell><cell>84.68</cell><cell>85.41</cell><cell>61.97</cell><cell>69.96</cell><cell>74.68</cell><cell>72.39</cell><cell>71.62</cell><cell>77.73</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViT-B + RVSA (Ours)</cell><cell>89.07</cell><cell>83.45</cell><cell>59.32</cell><cell>72.15</cell><cell>80.10</cell><cell>85.72</cell><cell>88.41</cell><cell>90.85</cell><cell>88.55</cell><cell>87.14</cell><cell>58.53</cell><cell>69.63</cell><cell>76.71</cell><cell>79.10</cell><cell>72.52</cell><cell>78.75</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViT-B + RVSA ? (Ours)</cell><cell>89.23</cell><cell>81.44</cell><cell>57.91</cell><cell>72.94</cell><cell>79.91</cell><cell>85.08</cell><cell>88.23</cell><cell>90.87</cell><cell>87.37</cell><cell>86.68</cell><cell>59.07</cell><cell>73.62</cell><cell>77.12</cell><cell>78.70</cell><cell>71.06</cell><cell>78.61</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViTAE-B + RVSA (Ours)</cell><cell>89.39</cell><cell>81.38</cell><cell>58.56</cell><cell>73.84</cell><cell>80.29</cell><cell>85.71</cell><cell>88.38</cell><cell>90.86</cell><cell>88.28</cell><cell>86.34</cell><cell>61.25</cell><cell>71.47</cell><cell>77.47</cell><cell>80.20</cell><cell>65.98</cell><cell>78.63</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViTAE-B + RVSA ? (Ours)</cell><cell>89.48</cell><cell>83.44</cell><cell>57.78</cell><cell>71.85</cell><cell>80.33</cell><cell>85.45</cell><cell>88.35</cell><cell>90.88</cell><cell>88.25</cell><cell>86.59</cell><cell>62.17</cell><cell>72.09</cell><cell>77.32</cell><cell>80.44</cell><cell>69.35</cell><cell>78.92</cell></row><row><cell>Multi-scale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R 3 Det [47]</cell><cell>IMP</cell><cell>ResNet-152</cell><cell>89.80</cell><cell>83.77</cell><cell>48.11</cell><cell>66.77</cell><cell>78.76</cell><cell>83.27</cell><cell>87.84</cell><cell>90.82</cell><cell>85.38</cell><cell>85.51</cell><cell>65.67</cell><cell>62.68</cell><cell>67.53</cell><cell>78.56</cell><cell>72.62</cell><cell>76.47</cell></row><row><cell>AO2-DETR [54]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>89.95</cell><cell>84.52</cell><cell>56.90</cell><cell>74.83</cell><cell>80.86</cell><cell>83.47</cell><cell>88.47</cell><cell>90.87</cell><cell>86.12</cell><cell>88.55</cell><cell>63.21</cell><cell>65.09</cell><cell>79.09</cell><cell>82.88</cell><cell>73.46</cell><cell>79.22</cell></row><row><cell>S 2 ANet [50]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>88.89</cell><cell>83.60</cell><cell>57.74</cell><cell>81.95</cell><cell>79.94</cell><cell>83.19</cell><cell>89.11</cell><cell>90.78</cell><cell>84.87</cell><cell>87.81</cell><cell>70.30</cell><cell>68.25</cell><cell>78.30</cell><cell>77.01</cell><cell>69.58</cell><cell>79.42</cell></row><row><cell>ReDet [51]</cell><cell>IMP</cell><cell>ReResNet-50</cell><cell>88.81</cell><cell>82.48</cell><cell>60.83</cell><cell>80.82</cell><cell>78.34</cell><cell>86.06</cell><cell>88.31</cell><cell>90.87</cell><cell>88.77</cell><cell>87.03</cell><cell>68.65</cell><cell>66.90</cell><cell>79.26</cell><cell>79.71</cell><cell>74.67</cell><cell>80.10</cell></row><row><cell>R 3 Det-GWD [55]</cell><cell>IMP</cell><cell>ResNet-152</cell><cell>89.66</cell><cell>84.99</cell><cell>59.26</cell><cell>82.19</cell><cell>78.97</cell><cell>84.83</cell><cell>87.70</cell><cell>90.21</cell><cell>86.54</cell><cell>86.85</cell><cell>73.47</cell><cell>67.77</cell><cell>76.92</cell><cell>79.22</cell><cell>74.92</cell><cell>80.23</cell></row><row><cell>ReDet-DEA [56]</cell><cell>IMP</cell><cell>ReResNet-50</cell><cell>89.92</cell><cell>83.84</cell><cell>59.65</cell><cell>79.88</cell><cell>80.11</cell><cell>87.96</cell><cell>88.17</cell><cell>90.31</cell><cell>88.93</cell><cell>88.46</cell><cell>68.93</cell><cell>65.94</cell><cell>78.04</cell><cell>79.69</cell><cell>75.78</cell><cell>80.37</cell></row><row><cell>DODet [49]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>89.96</cell><cell>85.52</cell><cell>58.01</cell><cell>81.22</cell><cell>78.71</cell><cell>85.46</cell><cell>88.59</cell><cell>90.89</cell><cell>87.12</cell><cell>87.80</cell><cell>70.50</cell><cell>71.54</cell><cell>82.06</cell><cell>77.43</cell><cell>74.47</cell><cell>80.62</cell></row><row><cell>R 3 Det-KLD [52]</cell><cell>IMP</cell><cell>ResNet-152</cell><cell>89.92</cell><cell>85.13</cell><cell>59.19</cell><cell>81.33</cell><cell>78.82</cell><cell>84.38</cell><cell>87.50</cell><cell>89.80</cell><cell>87.33</cell><cell>87.00</cell><cell>72.57</cell><cell>71.35</cell><cell>77.12</cell><cell>79.34</cell><cell>78.68</cell><cell>80.63</cell></row><row><cell>AOPG [44]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>89.88</cell><cell>85.57</cell><cell>60.90</cell><cell>81.51</cell><cell>78.70</cell><cell>85.29</cell><cell>88.85</cell><cell>90.89</cell><cell>87.60</cell><cell>87.65</cell><cell>71.66</cell><cell>68.69</cell><cell>82.31</cell><cell>77.32</cell><cell>73.10</cell><cell>80.66</cell></row><row><cell>Oriented R-CNN [46]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>89.84</cell><cell>85.43</cell><cell>61.09</cell><cell>79.82</cell><cell>79.71</cell><cell>85.35</cell><cell>88.82</cell><cell>90.88</cell><cell>86.68</cell><cell>87.73</cell><cell>72.21</cell><cell>70.80</cell><cell>82.42</cell><cell>78.18</cell><cell>74.11</cell><cell>80.87</cell></row><row><cell>ROI Trans.-KFIoU [57]</cell><cell>IMP</cell><cell>Swin-T</cell><cell>89.44</cell><cell>84.41</cell><cell>62.22</cell><cell>82.51</cell><cell>80.10</cell><cell>86.07</cell><cell>88.68</cell><cell>90.90</cell><cell>87.32</cell><cell>88.38</cell><cell>72.80</cell><cell>71.95</cell><cell>78.96</cell><cell>74.95</cell><cell>75.27</cell><cell>80.93</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViT-B + RVSA (Ours)</cell><cell>87.63</cell><cell>85.23</cell><cell>61.73</cell><cell>81.11</cell><cell>80.68</cell><cell>85.37</cell><cell>88.26</cell><cell>90.80</cell><cell>86.38</cell><cell>87.21</cell><cell>67.93</cell><cell>69.81</cell><cell>84.06</cell><cell>81.25</cell><cell>77.76</cell><cell>81.01</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViT-B + RVSA ? (Ours)</cell><cell>88.81</cell><cell>85.41</cell><cell>60.60</cell><cell>80.73</cell><cell>79.07</cell><cell>84.62</cell><cell>88.46</cell><cell>90.82</cell><cell>84.39</cell><cell>85.81</cell><cell>63.88</cell><cell>70.19</cell><cell>84.30</cell><cell>80.17</cell><cell>80.40</cell><cell>80.51</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViTAE-B + RVSA (Ours)</cell><cell>88.15</cell><cell>85.87</cell><cell>62.25</cell><cell>79.80</cell><cell>80.49</cell><cell>85.26</cell><cell>88.44</cell><cell>90.78</cell><cell>85.44</cell><cell>87.01</cell><cell>64.55</cell><cell>71.83</cell><cell>84.21</cell><cell>81.42</cell><cell>81.88</cell><cell>81.16</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViTAE-B + RVSA ? (Ours)</cell><cell>88.84</cell><cell>85.68</cell><cell>61.21</cell><cell>80.20</cell><cell>81.12</cell><cell>85.14</cell><cell>88.52</cell><cell>90.82</cell><cell>86.42</cell><cell>87.34</cell><cell>65.79</cell><cell>71.97</cell><cell>84.36</cell><cell>81.01</cell><cell>76.09</cell><cell>80.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII RESULTS</head><label>VII</label><figDesc>OF DIFFERENT METHODS ON THE TESTING SET OF THE DIOR-R DATASET. APL: airplane. APO: airport. BF: baseballfield. BC: basketballcourt. BR: bridge. CH: chimney. DAM: dam. ETS: expressway-toll-station. ESA: expressway-service-area. GF: golffield. GTF: groundtrackfield. HA: harbor. OP: overpass. SH: ship. STA: stadium. STO: storagetank. TC: tenniscourt. TS: trainstation. VE: vehicle. WM: windmill.</figDesc><table><row><cell>Method</cell><cell>Pretrain</cell><cell>Backbone</cell><cell>APL 1</cell><cell>APO</cell><cell>BF</cell><cell>BC</cell><cell>BR</cell><cell>CH</cell><cell>DAM</cell><cell>ETS</cell><cell>ESA</cell><cell>GF</cell><cell>GTF</cell><cell>HA</cell><cell>OP</cell><cell>SH</cell><cell>STA</cell><cell>STO</cell><cell>TC</cell><cell>TS</cell><cell>VE</cell><cell>WM</cell><cell>mAP</cell></row><row><cell>RetinaNet-O [58]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>61.49</cell><cell>28.52</cell><cell>73.57</cell><cell>81.17</cell><cell>23.98</cell><cell>72.54</cell><cell>19.94</cell><cell>72.39</cell><cell>58.20</cell><cell>69.25</cell><cell>79.54</cell><cell>32.14</cell><cell>44.87</cell><cell>77.71</cell><cell>67.57</cell><cell>61.09</cell><cell>81.46</cell><cell>47.33</cell><cell>38.01</cell><cell>60.24</cell><cell>57.55</cell></row><row><cell>Faster R-CNN-O [59]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>62.79</cell><cell>26.80</cell><cell>71.72</cell><cell>80.91</cell><cell>34.20</cell><cell>72.57</cell><cell>18.95</cell><cell>66.45</cell><cell>65.75</cell><cell>66.63</cell><cell>79.24</cell><cell>34.95</cell><cell>48.79</cell><cell>81.14</cell><cell>64.34</cell><cell>71.21</cell><cell>81.44</cell><cell>47.31</cell><cell>50.46</cell><cell>65.21</cell><cell>59.54</cell></row><row><cell>ROI Trans. [29]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>63.34</cell><cell>37.88</cell><cell>71.78</cell><cell>87.53</cell><cell>40.68</cell><cell>72.60</cell><cell>26.86</cell><cell>78.71</cell><cell>68.09</cell><cell>68.96</cell><cell>82.74</cell><cell>47.71</cell><cell>55.61</cell><cell>81.21</cell><cell>78.23</cell><cell>70.26</cell><cell>81.61</cell><cell>54.86</cell><cell>43.27</cell><cell>65.52</cell><cell>63.87</cell></row><row><cell>Gilding Vertex [48]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>65.35</cell><cell>28.87</cell><cell>74.96</cell><cell>81.33</cell><cell>33.88</cell><cell>74.31</cell><cell>19.58</cell><cell>70.72</cell><cell>64.70</cell><cell>72.30</cell><cell>78.68</cell><cell>37.22</cell><cell>49.64</cell><cell>80.22</cell><cell>69.26</cell><cell>61.13</cell><cell>81.49</cell><cell>44.76</cell><cell>47.71</cell><cell>65.04</cell><cell>60.06</cell></row><row><cell>AOPG [44]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>62.39</cell><cell>37.79</cell><cell>71.62</cell><cell>87.63</cell><cell>40.90</cell><cell>72.47</cell><cell>31.08</cell><cell>65.42</cell><cell>77.99</cell><cell>73.20</cell><cell>81.94</cell><cell>42.32</cell><cell>54.45</cell><cell>81.17</cell><cell>72.69</cell><cell>71.31</cell><cell>81.49</cell><cell>60.04</cell><cell>52.38</cell><cell>69.99</cell><cell>64.41</cell></row><row><cell>DODet [49]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>63.40</cell><cell>43.35</cell><cell>72.11</cell><cell>81.32</cell><cell>43.12</cell><cell>72.59</cell><cell>33.32</cell><cell>78.77</cell><cell>70.84</cell><cell>74.15</cell><cell>75.47</cell><cell>48.00</cell><cell>59.31</cell><cell>85.41</cell><cell>74.04</cell><cell>71.56</cell><cell>81.52</cell><cell>55.47</cell><cell>51.86</cell><cell>66.40</cell><cell>65.10</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViT-B + RVSA (Ours)</cell><cell>81.10</cell><cell>48.86</cell><cell>81.13</cell><cell>88.27</cell><cell>51.02</cell><cell>79.91</cell><cell>39.12</cell><cell>74.69</cell><cell>88.60</cell><cell>77.83</cell><cell>83.53</cell><cell>46.48</cell><cell>64.14</cell><cell>81.19</cell><cell>84.05</cell><cell>71.47</cell><cell>89.97</cell><cell>65.73</cell><cell>50.58</cell><cell>65.62</cell><cell>70.67</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViT-B + RVSA ? (Ours)</cell><cell>80.92</cell><cell>49.88</cell><cell>81.05</cell><cell>88.52</cell><cell>51.52</cell><cell>80.17</cell><cell>37.87</cell><cell>75.96</cell><cell>88.83</cell><cell>78.46</cell><cell>84.01</cell><cell>46.53</cell><cell>64.18</cell><cell>81.21</cell><cell>84.04</cell><cell>71.34</cell><cell>89.99</cell><cell>65.41</cell><cell>50.53</cell><cell>66.49</cell><cell>70.85</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViTAE-B + RVSA (Ours)</cell><cell>81.05</cell><cell>50.98</cell><cell>81.04</cell><cell>87.90</cell><cell>51.48</cell><cell>80.63</cell><cell>40.96</cell><cell>75.91</cell><cell>88.73</cell><cell>78.84</cell><cell>83.92</cell><cell>46.58</cell><cell>64.57</cell><cell>81.17</cell><cell>82.62</cell><cell>70.98</cell><cell>89.86</cell><cell>65.76</cell><cell>50.36</cell><cell>65.72</cell><cell>70.95</cell></row><row><cell>Oriented R-CNN</cell><cell>MAE</cell><cell>ViTAE-B + RVSA ? (Ours)</cell><cell>81.20</cell><cell>54.71</cell><cell>81.12</cell><cell>88.13</cell><cell>51.83</cell><cell>79.93</cell><cell>36.79</cell><cell>76.06</cell><cell>89.23</cell><cell>78.30</cell><cell>84.46</cell><cell>47.29</cell><cell>65.01</cell><cell>81.19</cell><cell>82.17</cell><cell>70.69</cell><cell>90.03</cell><cell>66.75</cell><cell>50.73</cell><cell>65.40</cell><cell>71.05</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII THE</head><label>VIII</label><figDesc>DETAILS OF DIFFERENT SCENE CLASSIFICATION DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>Number of Sample</cell><cell>Number of Category</cell><cell>Image Size</cell></row><row><cell>UCM</cell><cell>2,100</cell><cell>21</cell><cell>256 ? 256</cell></row><row><cell>AID</cell><cell>10,000</cell><cell>30</cell><cell>600 ? 600</cell></row><row><cell>NWPU</cell><cell>31,500</cell><cell>45</cell><cell>256 ? 256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX RESULTS</head><label>IX</label><figDesc>OF DIFFERENT METHODS FOR SCENE CLASSIFICATION.</figDesc><table><row><cell>Pretrain</cell><cell>Backbone</cell><cell>Method</cell><cell>UCM-55</cell><cell>AID-28</cell><cell>AID-55</cell><cell>NWPU-19</cell><cell>NWPU-28</cell></row><row><cell>IMP</cell><cell>VGG-16</cell><cell>LSENet [28]</cell><cell>98.53</cell><cell>94.41</cell><cell>96.36</cell><cell>92.23</cell><cell>93.34</cell></row><row><cell>IMP</cell><cell>ResNet-50</cell><cell>F 2 BRBM [61]</cell><cell>98.64</cell><cell>96.05</cell><cell>96.97</cell><cell>92.74</cell><cell>94.87</cell></row><row><cell>IMP</cell><cell>ResNet-50</cell><cell>GRMANet [62]</cell><cell>99.29</cell><cell>95.43</cell><cell>97.39</cell><cell>93.19</cell><cell>94.72</cell></row><row><cell>IMP</cell><cell>ResNet-101</cell><cell>EAM [63]</cell><cell>98.81</cell><cell>94.26</cell><cell>97.06</cell><cell>91.91</cell><cell>94.29</cell></row><row><cell>IMP</cell><cell>ResNet-101</cell><cell>MSANet [64]</cell><cell>97.80</cell><cell>93.53</cell><cell>96.01</cell><cell>90.38</cell><cell>93.52</cell></row><row><cell>ASP [65]</cell><cell>ResNet-101</cell><cell>-</cell><cell>-</cell><cell>95.40</cell><cell>-</cell><cell>-</cell><cell>94.20</cell></row><row><cell>IMP</cell><cell>DenseNet-121</cell><cell>MGML-FENet [66]</cell><cell>-</cell><cell>96.45</cell><cell>98.60</cell><cell>92.91</cell><cell>95.39</cell></row><row><cell>IMP</cell><cell>MobileNet-V2 [67]</cell><cell>RBFF [68]</cell><cell>95.83</cell><cell>91.02</cell><cell>93.64</cell><cell>84.59</cell><cell>88.05</cell></row><row><cell>IMP</cell><cell>ViT-B</cell><cell>-</cell><cell>99.15</cell><cell>93.81</cell><cell>96.08</cell><cell>90.96</cell><cell>93.96</cell></row><row><cell>IMP</cell><cell>Swin-T</cell><cell>-</cell><cell>99.43</cell><cell>96.55</cell><cell>98.10</cell><cell>92.73</cell><cell>94.70</cell></row><row><cell>CSPT [69]</cell><cell>ViT-B</cell><cell>-</cell><cell>-</cell><cell>96.75</cell><cell>-</cell><cell>-</cell><cell>95.11</cell></row><row><cell>CSPT</cell><cell>ViT-L</cell><cell>-</cell><cell>-</cell><cell>96.30</cell><cell>-</cell><cell>-</cell><cell>95.62</cell></row><row><cell>RingMo [35]</cell><cell>ViT-B</cell><cell>-</cell><cell>-</cell><cell>96.54</cell><cell>98.38</cell><cell>93.46</cell><cell>95.35</cell></row><row><cell>RingMo [35]</cell><cell>Swin-B</cell><cell>-</cell><cell>-</cell><cell>96.90</cell><cell>98.34</cell><cell>94.25</cell><cell>95.67</cell></row><row><cell>IMP</cell><cell>ViTAEv2-S</cell><cell>-</cell><cell>99.43</cell><cell>96.61</cell><cell>98.08</cell><cell>93.90</cell><cell>95.29</cell></row><row><cell>RSP</cell><cell>ViTAEv2-S</cell><cell>-</cell><cell>99.62</cell><cell>96.91</cell><cell>98.22</cell><cell>94.41</cell><cell>95.60</cell></row><row><cell>MAE</cell><cell>ViT-B + RVSA</cell><cell>-</cell><cell>99.70</cell><cell>96.92</cell><cell>98.33</cell><cell>93.79</cell><cell>95.49</cell></row><row><cell>MAE</cell><cell>ViT-B + RVSA ?</cell><cell>-</cell><cell>99.58</cell><cell>96.86</cell><cell>98.44</cell><cell>93.74</cell><cell>95.45</cell></row><row><cell>MAE</cell><cell>ViTAE-B + RVSA</cell><cell>-</cell><cell>99.56</cell><cell>97.03</cell><cell>98.48</cell><cell>93.93</cell><cell>95.69</cell></row><row><cell>MAE</cell><cell>ViTAE-B + RVSA ?</cell><cell>-</cell><cell>99.50</cell><cell>97.01</cell><cell>98.50</cell><cell>93.92</cell><cell>95.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X THE</head><label>X</label><figDesc>DETAILS OF DIFFERENT SEMANTIC SEGMENTATION DATASETS.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Training Validation Testing</cell><cell>Category</cell><cell>Image Size</cell></row><row><cell>Potsdam</cell><cell>24</cell><cell>-</cell><cell>14</cell><cell>6</cell><cell>6,000 ? 6,000</cell></row><row><cell>iSAID</cell><cell>1411</cell><cell>458</cell><cell>937</cell><cell>15</cell><cell>800 ? 800 ? 4,000 ? 13,000</cell></row><row><cell>LoveDA</cell><cell>2522</cell><cell>1669</cell><cell>1796</cell><cell>7</cell><cell>1,024 ? 1,024</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XI RESULTS</head><label>XI</label><figDesc>OF DIFFERENT METHODS FOR SEMANTIC SEGMENTATION.</figDesc><table><row><cell>Method</cell><cell>Pretrain</cell><cell>Backbone</cell><cell>Potsdam</cell><cell>iSAID</cell><cell>LoveDA</cell></row><row><cell>FCN [73]</cell><cell>IMP</cell><cell>VGG-16</cell><cell>85.59</cell><cell>41.70</cell><cell>46.69</cell></row><row><cell>DANet [74]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>89.72</cell><cell>57.50</cell><cell>-</cell></row><row><cell>PSPNet [75]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>89.45</cell><cell>60.30</cell><cell>48.31</cell></row><row><cell>DeeplabV3+ [76]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>89.74</cell><cell>60.80</cell><cell>48.31</cell></row><row><cell>Semantic FPN [77]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>-</cell><cell>59.30</cell><cell>48.15</cell></row><row><cell>FarSeg [30]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>-</cell><cell>63.70</cell><cell>48.15</cell></row><row><cell>FactSeg [78]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>-</cell><cell>64.80</cell><cell>48.94</cell></row><row><cell>UNetFormer [79]</cell><cell>IMP</cell><cell>ResNet-18</cell><cell>91.30</cell><cell>-</cell><cell>52.40</cell></row><row><cell>UperNet [72]</cell><cell>IMP</cell><cell>ResNet-50</cell><cell>90.64</cell><cell>61.90</cell><cell>51.27</cell></row><row><cell>UperNet</cell><cell>IMP</cell><cell>Swin-T</cell><cell>91.17</cell><cell>64.60</cell><cell>50.00</cell></row><row><cell>UperNet</cell><cell>RingMo [35]</cell><cell>Swin-B</cell><cell>91.74</cell><cell>67.20</cell><cell>-</cell></row><row><cell>UperNet</cell><cell>RSP [10]</cell><cell>ViTAEv2-S</cell><cell>91.21</cell><cell>64.30</cell><cell>53.02</cell></row><row><cell>UperNet</cell><cell>MAE</cell><cell>ViT-B + RVSA</cell><cell>90.60</cell><cell>63.76</cell><cell>51.95</cell></row><row><cell>UperNet</cell><cell>MAE</cell><cell>ViT-B + RVSA ?</cell><cell>90.77</cell><cell>63.85</cell><cell>51.95</cell></row><row><cell>UperNet</cell><cell>MAE</cell><cell>ViTAE-B + RVSA</cell><cell>91.22</cell><cell>63.48</cell><cell>52.26</cell></row><row><cell>UperNet</cell><cell>MAE</cell><cell>ViTAE-B + RVSA ?</cell><cell>91.15</cell><cell>64.49</cell><cell>52.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XII AVERAGE</head><label>XII</label><figDesc>RESULTS ON ALL DATASETS IN EACH TASK.</figDesc><table><row><cell>Model</cell><cell>Detection</cell><cell>Classification</cell><cell>Segmentation</cell></row><row><cell>ViT-B</cell><cell>71.85</cell><cell>97.20</cell><cell>67.58</cell></row><row><cell>ViT-B + RVSA</cell><cell>74.71</cell><cell>96.84</cell><cell>68.77</cell></row><row><cell>ViT-B + RVSA ?</cell><cell>74.73</cell><cell>96.82</cell><cell>68.86</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The FLOPs are calculated for each backbone network.<ref type="bibr" target="#b1">2</ref> This model is trained on 2 GPUs since it encounters the out of memory issue on a single GPU, while other models are trained on a single GPU.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PL: plane. BD: baseball diamond. BR: bridge. GTF: ground track field. SV: small vehicle. LV: large vehicle. SH: ship. TC: tennis court. BC: baseball court. ST: storage tank. SBF: soccer ball field. RA: roundabout. HA: harbor. SP: swimming pool. HC: helicopter. 2 IMP: ImageNet pretraining. RSP: remote sensing supervised pretraining on the MillionAID. MAE: MAE unsupervised pretraining on the MillionAID.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-labelpotsdam.aspx</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crop classification based on feature band set construction and object-oriented approach using hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4117" to="4128" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7789" to="7817" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilayer feature extraction network for military ship detection from high-resolution optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="11" to="058" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10108</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical study of remote sensing pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On creating benchmark dataset for aerial image interpretation: Reviews, guidances and million-aid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="4205" to="4230" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="0" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploring plain vision transformer backbones for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16527</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Vitpose: Simple vision transformer baselines for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ma?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9414" to="9423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geography-aware self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanmay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">190</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vsa: Learning varied-size window attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08446</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="12" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8583" to="8595" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Coca: Contrastive captioners are image-text foundation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01917</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022-06" />
			<biblScope unit="page" from="12" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pale transformer: A general vision transformer backbone with pale-shaped attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2731" to="2739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local semantic enhanced convnet for aerial scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6498" to="6511" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2844" to="2853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Foreground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geographical knowledge-driven representation learning for remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Regioncl: Can simple region swapping contribute to contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12309</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ringmo: A remote sensing foundation model with masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09886</idno>
		<title level="m">SimMIM: A Simple Framework for Masked Image Modeling</title>
		<imprint>
			<date type="published" when="2021-11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">BEiT: BERT Pre-Training of Image Transformers</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GEOProcessing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aid: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Anchor-free oriented proposal generator for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images: A survey and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS-J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="296" to="307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Oriented r-cnn for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="3520" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3163" to="3171" />
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1452" to="1459" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dual-aligned oriented detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Align Deep Features for Oriented Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">3062048</biblScope>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Redet: A rotation-equivariant detector for aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="2786" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning high-precision bounding box for rotated object detection via kullback-leibler divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Oriented reppoints for aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Ao2-detr: Arbitraryoriented object detection transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12785</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rethinking rotated object detection with gaussian wasserstein distance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiaopeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Anchor retouching via model interaction for robust object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The kfiou loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12558</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Remote sensing scene classification via multi-branch local attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y.</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Best Representation Branch Model for Remote Sensing Image Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="9768" to="9780" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Gated Recurrent Multiattention Network for VHR Remote Sensing Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">3093914</biblScope>
			<date type="published" when="2022-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Remote Sensing Image Scene Classification Based on an Enhanced Attention Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1926" to="1930" />
			<date type="published" when="2021-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A Multiscale Attention Network for Remote Sensing Scene Images Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Yk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="9530" to="9545" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Aerial scene parsing: From tile-level scene classification to pixel-wise semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01953</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mgml: Multigranularity multilevel feature ensemble network for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A lightweight relu-based feature fusion for aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Arefeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Nimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y S</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="3857" to="3861" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Consecutive pretraining: A knowledge transfer learning strategy with relevant unlabeled data for remote sensing domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.03860</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">isaid: A large-scale dataset for instance segmentation in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">LoveDA: A remote sensing land-cover dataset for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="418" to="434" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3141" to="3149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="801" to="818" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6392" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Factseg: Foreground activation-driven small object semantic segmentation in large-scale remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unetformer: A unet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS-J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page" from="196" to="214" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
