<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Orbach</surname></persName>
							<email>matano@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orith</forename><surname>Toledo-Ronen</surname></persName>
							<email>oritht@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Spector</surname></persName>
							<email>artems@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranit</forename><surname>Aharonov</surname></persName>
							<email>ranit.aharonov2@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Katz</surname></persName>
							<email>katz@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
							<email>noams@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current TSA evaluation in a cross-domain setup is restricted to the small set of review domains available in existing datasets. Such an evaluation is limited, and may not reflect true performance on sites like Amazon or Yelp that host diverse reviews from many domains. To address this gap, we present YASO -a new TSA evaluation dataset of open-domain user reviews. YASO contains 2215 English sentences from dozens of review domains, annotated with target terms and their sentiment. Our analysis verifies the reliability of these annotations, and explores the characteristics of the collected data. Benchmark results using five contemporary TSA systems show there is ample room for improvement on this challenging new dataset. YASO is available at github.com/IBM/yaso-tsa.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Targeted Sentiment Analysis (TSA) is the task of identifying the sentiment expressed towards single words or phrases in texts. For example, given the sentence "it's a useful dataset with a complex download procedure" the desired output is identifying dataset and download procedure, with a positive and negative sentiments expressed towards them, respectively. Our focus in this work is on TSA of user reviews data in English.</p><p>Till recently, typical TSA evaluation was indomain, for example, by training on labeled restaurant reviews and testing on restaurant reviews. New works (e.g. <ref type="bibr" target="#b5">Rietzler et al. (2020)</ref>) began considering a cross-domain setup, training models on labeled data from one or more domains (e.g., restaurant reviews) and evaluating on others (e.g., laptop reviews). For many domains, such as car or book reviews, TSA data is scarce or non-existent. This suggests that cross-domain experimentation is more realistic, as it aims at training on a small set of labeled domains and producing predictions for reviews from any domain. Naturally, the evaluation in this setup should resemble real-world content from sites like Amazon or Yelp that host reviews from dozens or even hundreds of domains. <ref type="bibr">1</ref> Existing English TSA datasets do not facilitate such a broad evaluation, as they typically include reviews from a small number of domains. For example, the popular SEMEVAL (SE) datasets created by <ref type="bibr" target="#b4">Pontiki et al. (2014</ref><ref type="bibr" target="#b3">Pontiki et al. ( , 2015</ref><ref type="bibr" target="#b2">Pontiki et al. ( , 2016</ref>) (henceforth SE14, SE15, and SE16, respectively), contain English reviews of restaurants, laptops and hotels (see ?2 for a discussion of other existing datasets). To address this gap, we present YASO, 2 a new TSA dataset collected over user reviews taken from four sources: the YELP and AMAZON <ref type="bibr">(Keung et al., 2020)</ref> datasets of reviews from those two sites; the Stanford Sentiment Treebank (SST) movie reviews corpus <ref type="bibr" target="#b10">(Socher et al., 2013)</ref>; and the OPINOSIS dataset of reviews from over 50 topics <ref type="bibr">(Ganesan et al., 2010)</ref>. To the best of our knowledge, while these resources have been previously used for sentiment analysis research, they were not annotated and used for targeted sentiment analysis. The new YASO evaluation dataset contains 2215 annotated sentences, on par with the size of existing test sets (e.g., one of the largest is the SE14 test set, with 1,600 sentences).</p><p>The annotation of open-domain reviews data is different from the annotation of reviews from a small fixed list of domains. Ideally, the labels would include both targets that are explicitly mentioned in the text, as well as aspect categories that are implied from it. For example, in "The restaurant serves good but expensive food" there is a sentiment towards the explicit target food as well as towards the implied category price. This approach of aspect-based sentiment analysis <ref type="bibr">(Liu, 2012)</ref> is implemented in the SE datasets. However, because the categories are domain specific, the annotation of each new domain in this manner first requires defining a list of relevant categories, for example, reliability and safety for cars, or plot and photography for movies. For open-domain reviews, curating these domain-specific categories over many domains, and training annotators to recognize them with per-domain guidelines and examples, is impractical. We therefore restrict our annotation to sentiment-bearing targets that are explicitly present in the review, as in the annotation of open-domain tweets by <ref type="bibr">Mitchell et al. (2013)</ref>.</p><p>While some information is lost by this choice, which may prohibit the use of the collected data in some cases, it offers an important advantage: the annotation guidelines can be significantly simplified. This, in turn, allows for the use of crowd workers who can swiftly annotate a desired corpora with no special training. Furthermore, the produced annotations are consistent across all domains, as the guidelines are domain-independent.</p><p>TSA annotation in a pre-specified domain may also distinguish between targets that are entities (e.g., a specific restaurant), a part of an entity (e.g., the restaurant's balcony), or an aspect of an entity (e.g., the restaurant's location). For example, <ref type="bibr" target="#b4">Pontiki et al. (2014)</ref> use this distinction to exclude targets that represent entities from their annotation. In an open-domain annotation setup, making such a distinction is difficult, since the reviewed entity is not known beforehand.</p><p>Consequently, we take a comprehensive approach and annotate all sentiment-bearing targets, including mentions of reviewed entities or their aspects, named entities, pronouns, and so forth. Notably, pronouns are potentially important for the analysis of multi-sentence reviews. For example, given "I visited the restaurant. It was nice.", identifying the positive sentiment towards It allows linking that sentiment to the restaurant, if the coreference is resolved.</p><p>Technically, we propose a two-phase annotation scheme. First, each sentence is labeled by five annotators that should identify and mark all target candidates -namely, all terms to which sentiment is expressed in the sentence. Next, each target candidate, in the context of its containing sentence, is labeled by several annotators who determine the sentiment expressed towards the candidate -either positive, negative, or mixed (if any). <ref type="bibr">3</ref> The full scheme is exemplified in <ref type="figure">Figure 1</ref>. We note that this scheme is also applicable to general non-review texts (e.g., tweets or news).</p><p>Several analyses are performed on the collected data: (i) its reliability is established through a manual analysis of a sample; (ii) the collected annotations are compared with existing labeled data, when available; (iii) differences from existing datasets are characterized. Lastly, benchmark performance on YASO was established in a cross-domain setup. Five state-of-the-art (SOTA) TSA systems were reproduced, using their available codebases, trained on data from SE14, and applied to predict targets and their sentiments over our annotated texts.</p><p>In summary, our main contributions are (i) a new domain-independent annotation scheme for collecting TSA labeled data; (ii) a new evaluation dataset with target and sentiment annotations of 2215 opendomain review sentences, collected using this new scheme; (iii) a detailed analysis of the produced annotations, validating their reliability; and (iv) reporting cross-domain benchmark results on the new dataset for several SOTA baseline systems. All collected data are available online. 4 2 Related work Review datasets The Darmstadt Review Corpora <ref type="bibr" target="#b18">(Toprak et al., 2010)</ref> contains annotations of user reviews in two domains -online universities and online services. Later on, SE14 annotated laptop and restaurant reviews (henceforth SE14-L and SE14-R). In SE15 a third domain (hotels) was added, and SE16 expanded the English data for the two original domains (restaurants and laptops). Jiang et al. (2019) created a challenge dataset with multiple targets per-sentence, again within the restaurants domain. <ref type="bibr" target="#b8">Saeidi et al. (2016)</ref> annotated opinions from discussions on urban neighbourhoods. Clearly, the diversity of the reviews in these datasets is limited, even when taken together.</p><p>Non-review datasets The Multi-Purpose Question Answering dataset <ref type="bibr" target="#b22">(Wiebe et al., 2005)</ref> was the first opinion mining corpus with a detailed annotation scheme applied to sentences from news documents. <ref type="bibr">Mitchell et al. (2013)</ref> annotated opendomain tweets using an annotation scheme similar to ours, where target candidates were annotated for their sentiment by crowd-workers, yet the annotated terms were limited to automatically detected (a) Target candidates annotation (b) Sentiment annotation <ref type="figure">Figure 1</ref>: The UI of our two-phase annotation scheme (detailed in ?3): Target candidates annotation (top) allows multiple target candidates to be marked in one sentence. In this phase, aggregated sentiments for candidates identified by a few annotators may be incorrect (see ?5 for further analysis). Therefore, marked candidates are passed through a second sentiment annotation (bottom) phase, which separately collects their sentiments. named entities. Other TSA datasets on Twitter data include targets that are either celebrities, products, or companies <ref type="bibr">(Dong et al., 2014)</ref>, and a multi-target corpus on UK elections data <ref type="bibr" target="#b19">(Wang et al., 2017a)</ref>. <ref type="bibr">Lastly, Hamborg et al. (2021)</ref> annotated named entities for their sentiment within the news domain.</p><p>Multilingual Other datasets exist for various languages, such as: Norwegian (?vrelid et al., 2020), Catalan and Basque <ref type="bibr" target="#b1">(Barnes et al., 2018)</ref>, Chinese <ref type="bibr" target="#b27">(Yang et al., 2018)</ref>, Hungarian <ref type="bibr" target="#b14">(Szab? et al., 2016</ref><ref type="bibr">), Hindi (Akhtar et al., 2016</ref>, SE16 with multiple languages <ref type="bibr" target="#b2">(Pontiki et al., 2016)</ref>, Czech <ref type="bibr" target="#b12">(Steinberger et al., 2014)</ref> and German <ref type="bibr">(Klinger and Cimiano, 2014)</ref>.</p><p>Annotation Scheme Our annotation scheme is reminiscent of two-phase data collection efforts in other tasks. These typically include an initial phase where annotation candidates are detected, followed by a verification phase that further labels each candidate by multiple annotators. Some examples include the annotation of claims <ref type="bibr">(Levy et al., 2014)</ref>, evidence <ref type="bibr" target="#b6">(Rinott et al., 2015)</ref> or mentions <ref type="bibr">(Mass et al., 2018)</ref>.</p><p>Modeling TSA can be divided into two subtasks: target extraction (TE), focused on identifying all sentiment targets in a given text; and sentiment classification (SC), of determining the sentiment towards a specific candidate target in a given text. TSA systems are either pipelined systems running a TE model followed by an SC model (e.g., <ref type="bibr">Karimi et al. (2020)</ref>), or end-to-end (sometimes called joint) systems using a single model for the whole task, which is typically regarded as a sequence labeling problem <ref type="bibr">(Li and Lu, 2019;</ref><ref type="bibr">Li et al., 2019a;</ref><ref type="bibr" target="#b23">Hu et al., 2019;</ref><ref type="bibr">He et al., 2019)</ref>. Earlier works <ref type="bibr">(Tang et al., 2016a,b;</ref><ref type="bibr" target="#b7">Ruder et al., 2016;</ref><ref type="bibr">Ma et al., 2018;</ref><ref type="bibr">Huang et al., 2018;</ref><ref type="bibr">He et al., 2018)</ref> have utilized pre-transformer models (see surveys by <ref type="bibr" target="#b9">Schouten and Frasincar (2015)</ref>; <ref type="bibr" target="#b29">Zhang et al. (2018)</ref>). Recently, focus has shifted to using pretrained language models <ref type="bibr" target="#b13">(Sun et al., 2019;</ref><ref type="bibr" target="#b11">Song et al., 2019;</ref><ref type="bibr" target="#b28">Zeng et al., 2019;</ref><ref type="bibr">Phan and Ogunbona, 2020)</ref>. Generalization to unseen domains has also been explored with pre-training that includes domain-specific data <ref type="bibr" target="#b5">Rietzler et al., 2020)</ref>, adds sentiment-related objectives <ref type="bibr" target="#b17">(Tian et al., 2020)</ref>, or combines instance-based domain adaptation <ref type="bibr">(Gong et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Input Data</head><p>The input data for the annotation was sampled from the following datasets: -YELP: 5 A dataset of 8M user reviews discussing more than 200k businesses. The sample included 129 reviews, each containing 3 to 5 sentences with a length of 8 to 50 tokens. The reviews were sentence split, yielding 501 sentences.</p><p>-AMAZON: 6 A dataset in 6 languages with 210k reviews per language <ref type="bibr">(Keung et al., 2020)</ref>. The English test set was sampled in the same manner as YELP, yielding 502 sentences from 151 reviews.</p><p>-SST: 7 A corpus of 11,855 movie review sentences <ref type="bibr" target="#b10">(Socher et al., 2013)</ref> originally extracted from Rotten Tomatoes by <ref type="bibr">Pang and Lee (2005)</ref>. 500 sentences, with a minimum length of 5 tokens, were randomly sampled from its test set.</p><p>-OPINOSIS: 8 A corpus of 7,086 user review sen-tences from Tripadvisor (hotels), Edmunds (cars), and Amazon (electronics) <ref type="bibr">(Ganesan et al., 2010)</ref>. Each sentence discusses a topic comprised of a product name and an aspect of the product (e.g. "performance of Toyota Camry"). At least 10 sentences were randomly sampled from each of the 51 topics in the dataset, yielding 512 sentences.</p><p>Overall, the input data includes reviews from many domains not previously annotated for TSA, such as books, cars, pet products, kitchens, movies or drugstores. Further examples are detailed in Appendix A.</p><p>The annotation input also included 200 randomly sampled sentences from the test sets of SE14-L and SE14-R (100 per domain). Such sentences have an existing annotation of targets and sentiments, which allows a comparison against the results of our proposed annotation scheme (see ?5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">YASO</head><p>Next, we detail the process of creating YASO. An input sentence was first passed through two phases of annotation, followed by several post-processing steps. <ref type="figure">Figure 2</ref> depicts an overview of that process, as context to the details given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Annotation</head><p>Target candidates annotation Each input sentence was tokenized (using spaCy by Honnibal and Montani (2017)) and shown to 5 annotators who were asked to mark target candidates by selecting corresponding token sequences within the sentence. Then, they were instructed to identify the sentiment expressed towards the candidatepositive, negative, or mixed ( <ref type="figure">Figure 1a</ref>).</p><p>This step is recall-oriented, without strict quality control, and some candidates may be detected by only one or two annotators. In such cases, sentiment labels based on annotations from this step alone may be incorrect (see ?5 for further analysis).</p><p>Selecting multiple non-overlapping target candidates in one sentence was allowed, each with its own sentiment. To avoid clutter and maintain a reasonable number of detected candidates, the selection of overlapping spans was prohibited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment annotation</head><p>To verify the correctness of the target candidates and their sentiments, each candidate was highlighted within its containing sentence, and presented to 7 to 10 annotators who were asked to determine its sentiment (without being shown the sentiment chosen in the first phase).</p><p>For cases in which an annotator believes a candidate was wrongly identified and has no sentiment expressed towards it, a "none" option was added to the original labels ( <ref type="figure">Figure 1b</ref>).</p><p>To control the quality of the annotation in this step, test questions with an a priori known answer were interleaved between the regular questions. A per-annotator accuracy was computed on these questions, and under-performers were excluded. Initially, a random sample of targets was labeled by two of the authors, and cases in which they agreed were used as test questions in the first annotation batch. Later batches also included test questions formed from unanimously answered questions in previously completed batches.</p><p>All annotations were done using the Appen platform. <ref type="bibr">9</ref> Overall, 20 annotators took part in the target candidates annotation phase, and 45 annotators worked on the sentiment annotation phase. The guidelines for each phase are given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Post-processing</head><p>The sentiment label of a candidate was determined by majority vote from its sentiment annotation answers, and the percentage of annotators who chose that majority label is the annotation confidence. A threshold t defined on these confidence values (set to 0.7 based on an analysis detailed below) separated the annotations between high-confidence targets (with confidence ? t) and low-confidence targets (with confidence &lt; t).</p><p>A target candidate was considered as valid when annotated with high-confidence with a particular sentiment (i.e., its majority sentiment label was not "none"). The valid targets were clustered by considering overlapping spans as being in the same cluster. Note that non-overlapping targets may be clustered together, for example, if t 1 , t 2 , t 3 are valid targets, t 1 overlaps t 2 and t 2 overlaps t 3 , then all three are in one cluster, regardless of whether t 1 and t 3 overlap. The sentiment of a cluster was set to the majority sentiment of its members.</p><p>The clustering is needed for handling overlapping labels when computing recall. For example, given the input "The food was great", and the annotated (positive) targets The food and food, a system which outputs only one of these targets should be evaluated as achieving full recall. Representing both labels as one cluster allows that (see details in ?6). An alternative to our approach is considering </p><formula xml:id="formula_0">pos/neg/ mixed/none --crowd -- Figure 2:</formula><p>The process for creating YASO, the new TSA evaluation dataset. An input sentence is passed through two phases of annotation (in orange), followed by four post-processing steps (in green).</p><p>any prediction that overlaps a label as correct. In this case, continuing the above example, an output of food or The food alone will have the desired recall of 1. Obviously, this alternative comes with the disadvantage of evaluating outputs with an inaccurate span as correct, e.g., an output of food was great will not be evaluated as an error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Confidence The per-dataset distribution of the confidence in the annotations is depicted in <ref type="figure">Figure</ref> 3a. For each confidence bin, one of the authors manually annotated a random sample of 30 target candidates for their sentiments, and computed a per-bin annotation error rate (see <ref type="table" target="#tab_1">Table 1</ref>). Based on this analysis, the confidence threshold for valid targets was set to 0.7, since under this value the estimated annotation error rate was high. Overall, around 15%-25% of all annotations were considered as low-confidence (light red in <ref type="figure">Figure 3a</ref>).</p><formula xml:id="formula_1">Bin [0.0, 0.7) [0.7, 0.8) [0.8, 0.9) [0.9, 1.0]</formula><p>Error 33.3% 10% 3.3% 3.3% Sentiment labels Observing the distribution of sentiment labels annotated with high-confidence ( <ref type="figure">Figure 3b</ref>), hardly any targets were annotated as mixed, and in all datasets (except AMAZON) there were more positive labels than negative ones. As many as 40% of the target candidates may be labeled as not having a sentiment in this phase (grey in <ref type="figure">Figure 3b</ref>), demonstrating the need for the second annotation phase.</p><p>Clusters While a cluster may include targets of different sentiments, in practice, cluster members were always annotated with the same sentiment, further supporting the quality of the sentiment annotation. Thus, the sentiment of a cluster is simply the sentiment of its targets. The distribution of the number of valid targets in each cluster is depicted in <ref type="figure">Figure 3c</ref>. As can be seen, the majority of clusters contain a single target. Out of the 31% of clusters that contain two targets, 70% follow the pattern "the/this/a/their &lt;T&gt;" for some term T, e.g., color and the color. The larger clusters of 4 or more targets (2% of all clusters), mostly stem from conjunctions or lists of targets (see examples in Appendix C).</p><p>The distribution of the number of clusters identified in each sentence is depicted in <ref type="figure">Figure 3d</ref>. Around 40% of the sentences have one cluster identified within, and as many as 40% have two or more clusters (for OPINOSIS). Between 20% to 35% of the sentences contain no clusters, i.e. no term with a sentiment expressed towards it was detected. Exploring the connection between the number of identified clusters and properties of the annotated sentences (e.g., length) is an interesting direction for future work.</p><p>Summary <ref type="table" target="#tab_3">Table 2</ref> summarizes the statistics of the collected data. It also shows the average pairwise inter-annotator agreement, computed with Cohen's Kappa <ref type="bibr">(Cohen, 1960)</ref>, which was in the range considered as moderate agreement (substantial for SE14-R) by <ref type="bibr">Landis and Koch (1977)</ref>.</p><p>Overall, the YASO dataset contains 2215 sentences and 7415 annotated target candidates. Several annotated sentences are exemplified in Appendix C. To enable further analysis, the dataset includes all candidate targets, not just valid ones, each marked with its confidence, sentiment label (including raw annotation counts), and span. YASO is released along with code for performing the post-processing steps described above, and computing the evaluation metrics presented in ?6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Next, three questions pertaining to the collected data and its annotation scheme are explored.   </p><formula xml:id="formula_2">L R Y A S O 0% 25% 50% 75% 100% % of target candidates [0.9, 1] [0.8, 0.9) [0.7, 0.8) [0.0, 0.7) (a) Annotation confidence distribution L R Y A S O 0% 25% 50% 75% 100% % of HC annotations positive negative mixed none (b) Sentiment labels distribution L R Y A S O 0% 25% 50% 75% 100% % of target clusters 1 2 3 4 (c) Cluster size distribution L R Y A S O 0%</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is the sentiment annotation phase mandatory?</head><p>Recall that each sentence in the target candidates annotation phase was shown to 5 annotators who chose candidates and their sentiments. As a result, each candidate has 1 to 5 "first-phase" sentiment answers that can be aggregated by majority vote to a detection-phase sentiment label. These can be compared with the sentiment labels from the sentiment annotation phase (which are always based on ?7 answers). The distribution of the number of answers arising from the detection-phase labeling is depicted in <ref type="figure" target="#fig_1">Figure 4a</ref>. In most cases, only one or two answers were available (e.g., in ?80% of cases for YELP). <ref type="figure" target="#fig_1">Figure 4b</ref> further details how many of them were correct; for example, those based on one answer for YELP were correct in &lt;50% of cases. In such cases, the sentiment annotation phase is essential for obtaining the correct label. On the other hand, when based on three or more answers, the detectionphase sentiments were correct in ?96% of cases, for all datasets. Such cases may be exempt from the second sentiment annotation phase, thus reducing costs in future annotation efforts.</p><p>What are the differences from SE14? The collected clusters for sentences sampled from SE14 were compared with the SE14 original annotations by pairing each cluster, based solely on its span, with overlapping SE14 annotations (excluding SE14 neutral labels), when available. The sentiments within each pair were compared, and, in most cases, were found to be identical (see <ref type="table" target="#tab_4">Table 3</ref>).  ters revealed only a few were annotation errors (see <ref type="table" target="#tab_7">Table 4</ref>). The others were of one of these categories: (i) Entities, such as company/restaurant names; (ii) Product terms like computer or restaurant; (iii) Other terms that are not product aspect, such as decision in "I think that was a great decision to buy"; (iv) Indirect references, including pronouns, such as It in "It was delicious!". This difference is expected as such terms are by construction excluded from SE14. In contrast, they are included in YASO since by design it includes all spans people consider as having a sentiment. This makes YASO more complete, while enabling those interested to discard terms as needed for downstream applications. The per-domain frequency of each category, along with additional examples, is given in <ref type="table" target="#tab_7">Table 4</ref>. A similar analysis performed on the 20 targets that were exclusively found in SE14 (i.e., not paired with any of the YASO clusters), showed that 8 cases were SE14 annotation errors, some due to complex expressions with an implicit or unclear sentiment. For example, in "They're a bit more expensive then typical, but then again, so is their food.", the sentiment of food is unclear (and labeled as positive in SE14). From the other 12   cases not paired with any cluster, three were YASO annotation errors (i.e. not found through our annotation scheme), and the rest were annotated but with low-confidence.</p><p>What is the recall of the target candidates annotation phase? The last comparison also shows that of the 156 targets 10 annotated in SE14 within the compared sentences, 98% (153) were detected as target candidates, suggesting that our target candidates annotation phase achieved good recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Benchmark Results</head><p>Recall the main purpose of YASO is cross-domain evaluation. The following results were obtained by training on data from SE14 (using its original training sets), and predicting targets over YASO sentences. The results are reported for the full TSA task, and separately for the TE and SC subtasks.</p><p>Baselines The following five recently proposed TSA systems were reproduced using their available codebases, and trained on the training set of each of the SE14 domains, yielding ten models overall.</p><p>-BAT: 11 (Karimi et al., 2020): A pipelined system with domain-specific language models  augmented with adversarial data.</p><p>-LCF: 12 <ref type="bibr">(Yang et al., 2020)</ref>: An end-to-end model based on <ref type="bibr" target="#b11">Song et al. (2019)</ref>, with domain adaptation and a local context focus mechanism.</p><p>-RACL: 13 (Chen and Qian, 2020): An end-to-end multi-task learning and relation propagation system. We used the RACL-GloVe variant, based on pre-trained word embeddings.</p><p>-BERT-E2E: 14 (Li et al., 2019b): A BERT-based end-to-end sequence labeling system. We used the BERT+Linear architecture, which computes pertoken labels using a linear classification layer.</p><p>-HAST+MCRF: A pipeline of (i) HAST, 15 a TE system based on capturing aspect detection history and opinion summary <ref type="bibr">(Li et al., 2018)</ref>; and (ii) MCRF-SA, 16 an SC system utilizing multiple CRF-based structured attention models <ref type="bibr" target="#b24">(Xu et al., 2020a)</ref>.</p><p>Evaluation Metrics As a pre-processing step, any predicted target with a span equal to the span of a target candidate annotated with low-confidence was excluded from the evaluation, since it is unclear what is its true label. The use of clusters within the evaluation requires an adjustment of the computed recall. Specifically, multiple predicted targets contained within one cluster should be counted once, considering the cluster as one true positive. Explicitly, a predicted target and a cluster are span-matched, if the cluster contains a valid target with a span equal to the span of the prediction (an exact span match). Similarly, they are fully-matched if they are span-matched and their sentiments are the same. Predictions that were not span-matched to any cluster were considered as errors for the TE task (since their span was not annotated as a valid target), and those that were not fully-matched to any cluster were considered as errors for the full task. Using span-matches, precision for the TE task is the percentage of spanmatched predictions, and recall is the percentage of span-matched clusters. These metrics are similarly defined for the full task using full-matches.</p><p>For SC, evaluation was restricted to predictions that were span-matched to a cluster. For a sentiment label l, precision is the percentage of fully-matched predictions with sentiment l (out of all span-matched predictions with that sentiment); recall is the percentage of fully-matched clusters with sentiment l (out of all span-matched clusters with that sentiment). Macro-F 1 (mF 1 ) is the average F 1 over the positive and negative sentiment labels (mixed was ignored since it was scarcely in the data, following Chen and Qian (2020)).</p><p>Our data release is accompanied by code for computing all the described evaluation metrics.</p><p>Results <ref type="table" target="#tab_9">Table 5</ref> presents the results of our evaluation. BAT trained on the restaurants data was the best-performing system for TE and the full TSA tasks, on three of the four datasets (YELP, SST and OPINOSIS). For SC, BERT-E2E was the best model on three datasets. Generally, results for SC were relatively high, while TE results by some models may be very low, typically stemming from low recall. The precision and recall results for each task are further detailed in Appendix D.</p><p>Appendix D also details additional results when relaxing the TE evaluation criterion from exact span-matches to overlapping span-matches -where a predicted target and a cluster are span-matched if their spans overlap. While with this relaxed evaluation the TE performance was higher (as expected), the absolute numbers suggest a significant percentage of errors were not simply targets predicted with a misaligned span.</p><p>TSA task performance was lowest for SST, perhaps due to its domain of movie reviews, which is furthest of all datasets from the product reviews training data. Interestingly, it was also the dataset with the lowest level of agreement among humans (see <ref type="figure">Figure 3a</ref>).</p><p>The choice of the training domain is an important factor for most algorithms. This is notable, for example, in the TE performance obtained for YELP: the gap between training on data from the laptops domain or the restaurants domain is ? 20 (in favor of the latter) for all algorithms (except LCF). A likely cause is that the YASO data sampled from YELP has a fair percentage of reviews on food related establishments. Future work may further use YASO to explore the impact of the similarity between the training and test domains, as well as develop new methods that are robust to the choice of the training domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YELP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMAZON</head><p>SST <ref type="table">OPINOSIS   System  Train TE SC TSA TE SC TSA TE  SC TSA TE SC</ref>   . The reported metric is F 1 for target extraction (TE) and the entire task (TSA), and macro-F 1 for sentiment classification (SC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We collected a new open-domain user reviews TSA evaluation dataset named YASO. Unlike existing review datasets, YASO is not limited to any particular reviews domain, thus providing a broader perspective for cross-domain TSA evaluation. Benchmark results established in such a setup with contemporary TSA systems show there is ample headroom for improvement on YASO. YASO was annotated using a new scheme for creating TSA labeled data, that can be also applied to non-review texts. The reliability of the annotations obtained by this scheme has been verified through a manual analysis of a sample and a comparison to existing labeled data.</p><p>One limitation of our scheme is that aspect categories with a sentiment implied from the reviews were excluded, since their annotation requires prespecifying the domain along with its associated categories. While this may limit research for some applications, the dataset is useful in many realworld use cases. For example, given a brand name, one may query a user reviews corpus for sentences containing it, and analyze the sentiment towards that brand in each sentence along with the sentiment expressed to other terms in these sentences.</p><p>Future work may improve upon the presented results by training on multiple domains or datasets, adapting pre-trained models to the target domains in an unsupervised manner (e.g., <ref type="bibr" target="#b5">Rietzler et al. (2020)</ref>), exploring various data augmentation techniques, or utilizing multi-task or weak-supervision algorithms. Another interesting direction for further research is annotating opinion terms within the YASO sentences, facilitating their co-extraction with corresponding targets <ref type="bibr" target="#b20">(Wang et al., 2016</ref><ref type="bibr" target="#b21">(Wang et al., , 2017b</ref>, or as triplets of target term, sentiment, and opinion term <ref type="bibr">(Peng et al., 2020;</ref><ref type="bibr" target="#b25">Xu et al., 2020b)</ref>.</p><p>All benchmark data collected in this work are available online. <ref type="bibr">17</ref> We hope that these data will facilitate further advancements in the field of targeted sentiment analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Annotation Guidelines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Target Candidates Annotation</head><p>Below are the guidelines for the labeling task of detecting potential targets and their sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General instructions</head><p>In this task you will review a set of sentences. Your goal is to identify items in the sentences that have a sentiment expressed towards them.</p><p>Steps 1. Read the sentence carefully.</p><p>2. Identify items that have a sentiment expressed towards them.</p><p>3. Mark each item, and for each selection choose the expressed sentiment:</p><p>(a) Positive: the expressed sentiment is positive.</p><p>(b) Negative: the expressed sentiment is negative.</p><p>(c) Mixed: the expressed sentiment is both positive and negative.</p><p>4. If there are no items with a sentiment expressed towards them, proceed to the next sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rules &amp; Tips</head><p>? Select all items in the sentence that have a sentiment expressed towards them.</p><p>? It could be that there are several correct overlapping selections. In such cases, it is OK to choose only one of these overlapping selections.</p><p>? The sentiment towards a selected item(s) should be expressed from other parts of the sentence, it cannot come from within the selected item (see Example #2 below).</p><p>? Under each question is a comments box. Optionally, you can provide question-specific feedback in this box. This may include a rationalization of your choice, a description of an error within the question or the justification of another answer which was also plausible. In general, any relevant feedback would be useful, and will help in improving this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples</head><p>Here are a few example sentences, categorized into several example types.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Long selected items</head><p>There is no restriction on the length of a select item, so long as there is an expressed sentiment towards it in the sentence (which does not come from within the marked item). Note: It is also a valid choice to select food along with its details description: food from the Italian restaurant near my office, or add the prefix The to the selection (or both). The selection must be a coherent phrase. food from the is not a valid selection. Since these selections all overlap, it is OK to select one of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Sentiment Annotation</head><p>Below are the guidelines for labeling the sentiment of identified target candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General instructions</head><p>In this task you will review a set of sentences, each containing one marked item. Your goal is to determine the sentiment expressed in the sentence towards the marked item.</p><p>Steps 1. Read the sentence carefully.</p><p>2. Identify the sentiment expressed in the sentence towards the marked item, by selecting one of these four options:</p><p>(a) Positive: the expressed sentiment is positive.</p><p>(b) Negative: the expressed sentiment is negative.</p><p>(c) Mixed: the expressed sentiment is both positive and negative.</p><p>(d) None: there is no sentiment expressed towards the item.</p><p>3. If there are no items with a sentiment expressed towards them, proceed to the next sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rules &amp; Tips</head><p>? The sentiment should be expressed towards the marked item, it cannot come from within the marked item (see Example #2 below).</p><p>? A sentence may appear multiple times, each time with one marked item. Different marked items may have different sentiments expressed towards each of them in one sentence (see <ref type="table" target="#tab_4">Example #3 below)</ref> ? Under each question is a comments box. Optionally, you can provide question-specific feedback in this box. This may include a rationalization of your choice, a description of an error within the question or the justification of another answer which was also plausible. In general, any relevant feedback would be useful, and will help in improving this task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Different marked items in one sentence</head><p>Example #3.1: The food was good, but the atmosphere was awful. Answer: Positive The food was good, but the atmosphere was awful. Answer: Negative Example #3.2: The camera has excellent lens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: Positive</head><p>The camera has excellent lens. Answer: Positive Example #3.3: My new camera has excellent lens, but its price is too high. Answer: Mixed Explanation: There is a positive sentiment towards the camera, due to its excellent lens, and also a negative sentiment, because its price is too high, so the correct answer is Mixed.</p><p>My new camera has excellent lens, but its price is too high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: Positive</head><p>My new camera has excellent lens, but its price is too high. Answer: Negative 4. Marked items without a sentiment</p><p>Below are some examples of marked items without an expressed sentiment in the sentence. In cases where there is a expressed sentiment towards other words in the same sentence, it is exemplified as well.</p><p>Example #4.1: Microwave, refrigerator, coffee maker in room. Answer: None Example #4.2: Note that they do not serve beer, you must bring your own. Answer: None Example #4.3: The cons are more annoyances that can be lived with. Answer: None Explanation: While the marked item contains a negative sentiment, there is no sentiment towards the marked item.</p><p>Example #4.4: working with Mac is so much easier, so many cool features. Answer: None working with Mac is so much easier, so many cool features. Answer: Positive working with Mac is so much easier, so many cool features. Answer: Positive Example #4.5: The battery life is excellent-6-7 hours without charging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: None</head><p>The battery life is excellent-6-7 hours without charging. Answer: Positive Example #4.6: I wanted a computer that was quiet, fast, and that had overall great performance. Answer: None 5. "the" can be a part of a marked item I feel a little bit uncomfortable in using the Mac system. Answer: Negative I feel a little bit uncomfortable in using the Mac system. Answer: Negative I feel a little bit uncomfortable in using the Mac system. Answer: None</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Long marked items</head><p>There is no restriction on the length of a marked item, so long as there is an expressed sentiment towards it in the sentence (which does not come from within the marked item).</p><p>The food from the Italian restaurant near my office was very good. Answer: Positive The food from the Italian restaurant near my office was very good. Answer: Positive The food from the Italian restaurant near my office was very good. Answer: None</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Idioms</head><p>A sentiment may be conveyed with an idiom -be sure you understand the meaning of an input sentence before answering. When unsure, look up potential idioms online.</p><p>The laptop's performance was in the middle of the pack, but so is its price. Answer: None Explanation: in the middle of the pack does not convey a positive nor a negative sentiment, and certainly not both (so the answer is not "mixed" as well). <ref type="table" target="#tab_12">Table 6</ref> presents sentences included in YASO, along with the annotated targets and their corresponding sentiments found within each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Annotation Examples</head><p>A target t that has a positive sentiment expressed towards it is marked as [t] P . Similarly [t] N is used for a negative sentiment. For brevity, the examples only show the valid targets annotated within the sentences, hiding any low-confidence annotations or target candidates that were annotated as not having a sentiment in the second annotation phase. As can be seen in the examples, annotated valid targets may overlap, demonstrating the need for the definition of the target clusters. <ref type="table" target="#tab_15">Table 7</ref> further exemplifies sentences in which a cluster containing more than 4 valid targets were detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Detailed Benchmark Results</head><p>In addition to the main benchmark results presented in the paper, <ref type="table">Table 8</ref> shows the precision, recall and F 1 for target extraction and the entire task. For sentiment classification, the same metrics are separately reported for the positive and negative sentiment labels, as well as macro-F 1 over these two classes. <ref type="table">Table 9</ref> presents results similar to <ref type="table" target="#tab_9">Table 5</ref> with another TE evaluation criteria, where a predicted target and a cluster are span-matched if their spans overlap. This is a more relaxed evaluation criteria than the one used in the main results (which consider a predicted target and a cluster as spanmatched if the cluster contains a target with a span equal to the span of the prediction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task:</head><p>TE SC TSA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive Negative</head><p>Dat. System Train P R F 1 P R F 1 P R F 1 mF 1 P R F 1 Y</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Clusters per sentence distribution Figure 3: Per-dataset statistics showing the distributions of: (a) The confidence in the sentiment annotation of each target candidate; (b) The sentiment labels of targets annotated with high-confidence (HC); (c) The number of valid targets within each cluster; (d) The number of clusters in each annotated sentence. The datasets are marked as: SE14-L (L), SE14-R (R), YELP (Y), AMAZON (A), SST (S) and OPINOSIS (O).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>A per-dataset analysis of the detection-phase sentiment labels, showing (a) the distribution of the number of answers that the labels are based on, and (b) how it affects the percentage of correct labels. The datasets are marked as: SE14-L (L), SE14-R (R), YELP (Y), AMAZON (A), SST (S) and OPINOSIS (O).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The annotation error rate per confidence-bin.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Per-dataset annotation statistics: The number</cell></row><row><cell>of annotated sentences (#S) and target candidates an-</cell></row><row><cell>notated within those sentences (#TC); The number of</cell></row><row><cell>targets annotated with high confidence (#HC), and as</cell></row><row><cell>valid targets; (#VT); The number of clusters formed</cell></row><row><cell>from the valid targets (#TC); The average pairwise</cell></row><row><cell>inter-annotator agreement (K). See  ?4.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>further shows many clusters are exclusively present in YASO -they do not overlap any SE14 annotation. A manual analysis of such clus-</figDesc><table><row><cell>% of target candidates</cell><cell>0% 25% 50% 75% 100%</cell><cell>L</cell><cell>R</cell><cell></cell><cell>Y</cell><cell>A</cell><cell cols="2">S</cell><cell>O</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell cols="8">(a) Number of aggregated answers distribution</cell></row><row><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>% correct</cell><cell>60% 80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>L</cell><cell>2</cell><cell>R</cell><cell cols="2">3 # of answers Y A</cell><cell>4</cell><cell>S</cell><cell>O</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell cols="6">(b) Percentage of correct labels</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>A comparison of YASO annotations to labels from the SE14 dataset. The sentiment labels of targets labeled in both datasets may agree (Ag) or disagreee (Dis). Targets exclusively present in one of the datasets (YASO or SE) are further analyzed in ?5.</figDesc><table><row><cell cols="2">Category L R Examples</cell></row><row><cell cols="2">Entities 14 6 Apple, iPhone, Culinaria</cell></row><row><cell cols="2">Product 13 6 laptop, this bar, this place</cell></row><row><cell>Other</cell><cell>10 11 process, decision, choice</cell></row><row><cell cols="2">Indirect 24 11 it, she, this, this one, here</cell></row><row><cell>Error</cell><cell>3 4 -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>A categorization of valid targets in YASO that are not part of SE14, for the laptops (L) and restaurants (R) domains. The categories are detailed in ?5.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>TSA BAT Lap. 27.8 88.0 24.8 34.5 96.3 33.1 8.8 100.0 8.8 57.2 92.2 53.6 Res. 58.0 91.6 54.4 29.3 89.4 25.6 34.9 90.6 31.9 59.1 91.8 55.3</figDesc><table><row><cell>BERT-E2E</cell><cell>Lap. 28.2 91.3 26.5 35.5 97.8 34.5 12.2 97.4 12.0 56.1 94.0 53.4 Res. 52.7 93.4 49.9 28.6 98.0 27.7 9.9 92.5 9.0 50.4 94.3 48.0</cell></row><row><cell>HAST+MCRF</cell><cell>Lap. 16.7 68.3 11.9 21.4 82.0 17.5 2.8 64.9 1.9 34.8 82.2 29.6 Res. 40.7 88.4 36.5 9.4 95.6 9.0 3.1 67.0 2.2 31.6 87.7 28.0</cell></row><row><cell>LCF</cell><cell>Lap. 41.0 72.6 33.3 37.9 85.0 31.9 17.0 80.4 13.7 54.7 91.1 50.6 Res. 48.8 84.8 43.7 36.1 87.1 31.0 16.5 75.7 12.8 55.7 86.5 49.4</cell></row><row><cell>RACL</cell><cell>Lap. 23.0 88.2 20.8 29.0 89.6 25.9 13.2 78.1 10.2 43.2 83.1 37.8 Res. 44.5 87.9 39.9 22.5 88.9 19.7 7.9 86.3 7.0 43.8 85.0 38.4</cell></row><row><cell>Average</cell><cell>Lap. 27.3 81.7 23.5 31.7 90.1 28.6 10.8 84.2 9.3 49.2 88.5 45.0 Res. 48.9 89.2 44.9 25.2 91.8 22.6 14.5 82.4 12.6 48.1 89.1 43.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Benchmark results on YASO with five SOTA systems, trained on data from one SE14 domain (laptops -Lap. or restaurants -Res.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Zhuang Chen and Tieyun Qian. 2020. Relation-aware collaborative learning for unified aspect-based sentiment analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3685-3694. Association for Computational Linguistics.</figDesc><table><row><cell>Conference on Artificial Intelligence, volume 33,</cell><cell>ing of the Association for Computational Linguis-</cell></row><row><cell>pages 6714-6721.</cell><cell>tics, pages 537-546, Florence, Italy. Association for</cell></row><row><cell></cell><cell>Computational Linguistics.</cell></row><row><cell>Xin Li, Lidong Bing, Piji Li, Wai Lam, and Zhimou Yang. 2018. Aspect term extraction with history attention and selective transformation. In Proceed-ings of the Twenty-Seventh International Joint Con-</cell><cell>Binxuan Huang, Yanglan Ou, and Kathleen M. Car-ley. 2018. Aspect level sentiment classification with attention-over-attention neural networks. In Social,</cell></row><row><cell>ference on Artificial Intelligence, IJCAI-18, pages 4194-4200. International Joint Conferences on Ar-tificial Intelligence Organization. Jacob Cohen. 1960. A Coefficient of Agreement for Xin Li, Lidong Bing, Wenxuan Zhang, and Wai Lam. Nominal Scales. Educational and Psychological Measurement, 20(1):37-46. Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent Twitter sentiment clas-2019b. Exploiting BERT for end-to-end aspect-based sentiment analysis. In Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 34-41, Hong Kong, China. Association for Computational Linguistics. sification. In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics (Volume 2: Short Papers), pages 49-54, Baltimore, Maryland. Association for Computational Linguis-tics. Bing Liu. 2012. Sentiment analysis and opinion min-ing. Synthesis lectures on human language technolo-gies, 5(1):1-167.</cell><cell>Cultural, and Behavioral Modeling -11th Interna-tional Conference, SBP-BRiMS 2018, Washington, DC, USA, July 10-13, 2018, Proceedings, volume 10899 of Lecture Notes in Computer Science, pages 197-206. Springer. Qingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, and Min Yang. 2019. A challenge dataset and effec-tive models for aspect-based sentiment analysis. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 6280-6285, Hong Kong, China. Association for Computa-tional Linguistics.</cell></row><row><cell>Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010. Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions. In Proceedings of the 23rd International Conference on Yukun Ma, Haiyun Peng, and E. Cambria. 2018. Tar-geted aspect-based sentiment analysis via embed-ding commonsense knowledge into an attentive lstm. In AAAI.</cell><cell>Akbar Karimi, Leonardo Rossi, Andrea Prati, and Katharina Full. 2020. Adversarial training for aspect-based sentiment analysis with bert. arXiv preprint arXiv:2001.11316.</cell></row><row><cell>Computational Linguistics, pages 340-348. Associ-Yosi Mass, Lili Kotlerman, Shachar Mirkin, Elad ation for Computational Linguistics. Venezian, Gera Witzling, and Noam Slonim. 2018.</cell><cell>Phillip Keung, Yichao Lu, Gy?rgy Szarvas, and Noah A. Smith. 2020. The multilingual Amazon</cell></row><row><cell>What did you mention? a large scale mention detec-Chenggong Gong, Jianfei Yu, and Rui Xia. 2020. Uni-tion benchmark for spoken and written text. fied feature and instance based domain adaptation for aspect-based sentiment analysis. In Proceed-Margaret Mitchell, Jacqui Aguilar, Theresa Wilson, ings of the 2020 Conference on Empirical Methods and Benjamin Van Durme. 2013. Open domain tar-in Natural Language Processing (EMNLP), pages geted sentiment. In Proceedings of the 2013 Con-7035-7045, Online. Association for Computational ference on Empirical Methods in Natural Language Linguistics. Processing, pages 1643-1654, Seattle, Washington,</cell><cell>reviews corpus. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural Language Processing (EMNLP), pages 4563-4568, Online. As-sociation for Computational Linguistics. Roman Klinger and Philipp Cimiano. 2014. The US-AGE review corpus for fine grained multi lingual opinion analysis. In Proceedings of the Ninth In-</cell></row><row><cell>USA. Association for Computational Linguistics. Felix Hamborg, Karsten Donnay, and Bela Gipp. 2021. Towards target-dependent sentiment classification in news articles. In Diversity, Divergence, Dialogue, pages 156-166, Cham. Springer International Pub-Lilja ?vrelid, Petter Maehlum, Jeremy Barnes, and Erik Velldal. 2020. A fine-grained sentiment dataset for</cell><cell>ternational Conference on Language Resources and Evaluation (LREC'14), pages 2211-2218, Reyk-javik, Iceland. European Language Resources Asso-ciation (ELRA).</cell></row><row><cell>lishing. Ruidan He, Wee Sun Lee, H. Ng, and Daniel Dahlmeier. 2018. Effective attention modeling for aspect-level Norwegian. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5025-5033, Marseille, France. European Language Re-sources Association.</cell><cell>J Richard Landis and Gary G Koch. 1977. The mea-surement of observer agreement for categorical data. biometrics, pages 159-174.</cell></row><row><cell>sentiment classification. In COLING. Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2019. An interactive multi-task learn-ing network for end-to-end aspect-based sentiment analysis. In Proceedings of the 57th Annual Meet-Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-ploiting class relationships for sentiment categoriza-tion with respect to rating scales. arXiv preprint cs/0506075.</cell><cell>Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud Aharoni, and Noam Slonim. 2014. Context depen-dent claim detection. In Proceedings of COLING 2014, the 25th International Conference on Compu-tational Linguistics: Technical Papers, pages 1489-</cell></row><row><cell>ing of the Association for Computational Linguis-Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, tics, pages 504-515, Florence, Italy. Association for Computational Linguistics. and Luo Si. 2020. Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. Proceedings of the AAAI Conference on Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embed-Artificial Intelligence, 34(05):8600-8607.</cell><cell>1500. Hao Li and Wei Lu. 2019. Learning explicit and im-plicit structures for targeted sentiment analysis. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the</cell></row><row><cell>dings, convolutional neural networks and incremen-</cell><cell>9th International Joint Conference on Natural Lan-</cell></row><row><cell>tal parsing. To appear. Minh Hieu Phan and Philip O. Ogunbona. 2020. Mod-</cell><cell>guage Processing (EMNLP-IJCNLP), Hong Kong,</cell></row><row><cell>elling context and syntactical features for aspect-</cell><cell>China. Association for Computational Linguistics.</cell></row><row><cell>Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng based sentiment analysis. In Proceedings of the</cell><cell></cell></row><row><cell>Li, and Yiwei Lv. 2019. Open-domain targeted sen-58th Annual Meeting of the Association for Compu-</cell><cell>Xin Li, Lidong Bing, Piji Li, and Wai Lam. 2019a. A</cell></row><row><cell>timent analysis via span-based extraction and classi-tational Linguistics, pages 3211-3220, Online. As-</cell><cell>unified model for opinion target extraction and target</cell></row><row><cell>fication. In Proceedings of the 57th Annual Meet-sociation for Computational Linguistics.</cell><cell>sentiment prediction. In Proceedings of the AAAI</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>uptown I try to stop in as often as possible for the GREAT [cheap [food] P ] P and to pay the friendly [staff] P a visit.</figDesc><table><row><cell cols="2">Input Dataset Sentence</cell></row><row><cell cols="2">SE14-R Although I moved SE14-L A great [college [tool] P ] P !</cell></row><row><cell>OPINOSIS</cell><cell>The [Waitrose supermarket] P has many take out food options .</cell></row><row><cell></cell><cell>[The protective [seal] N ] N was broken when I received this [item] N and a large</cell></row><row><cell>AMAZON</cell><cell>amount of the contents had spilled out of the container into the plastic bag that</cell></row><row><cell></cell><cell>the item was in.</cell></row><row><cell></cell><cell>[The [wait] N ] N was a little longer than what I prefer, but [the [service] P ] P was</cell></row><row><cell>YELP</cell><cell>kind, [the [food] P ] P was incredible, and [the [Phuket Bucket] P ] P was refreshing</cell></row><row><cell></cell><cell>on a warm evening.</cell></row><row><cell>SST</cell><cell>[The Irwins] P emerge unscathed , but [the [fictional [footage] N ] N ] N is uncon-vincing and criminally badly [acted] N .</cell></row><row><cell></cell><cell>For each sentence, the</cell></row><row><cell></cell><cell>examples show item(s) which should be selected,</cell></row><row><cell></cell><cell>and the sentiment expressed towards each such</cell></row><row><cell></cell><cell>item. Further explanations are provided within</cell></row><row><cell></cell><cell>the examples, when needed. Please review the</cell></row><row><cell></cell><cell>examples carefully before starting the task.</cell></row><row><cell></cell><cell>1. Basics</cell></row><row><cell></cell><cell>Example #1.1: The food was good.</cell></row><row><cell></cell><cell>Correct answer: The food was good.</cell></row><row><cell></cell><cell>Explanation: The word good expresses a</cell></row><row><cell></cell><cell>positive sentiment towards food.</cell></row><row><cell></cell><cell>Example #1.2: The food was bad.</cell></row><row><cell></cell><cell>Correct answer: The food was bad.</cell></row><row><cell></cell><cell>Explanation: The word bad expresses a</cell></row><row><cell></cell><cell>negative sentiment towards food.</cell></row><row><cell></cell><cell>Example #1.3: The food was tasty but</cell></row><row><cell></cell><cell>expensive.</cell></row><row><cell></cell><cell>Correct answer: The food was tasty but</cell></row><row><cell></cell><cell>expensive.</cell></row><row><cell></cell><cell>Explanation: tasty expresses a positive</cell></row><row><cell></cell><cell>sentiment, while expensive expresses a nega-</cell></row><row><cell></cell><cell>tive sentiment, so the correct answer is Mixed.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6</head><label>6</label><figDesc>The word excellent expresses a positive sentiment towards lens, while the words too high expresses a negative sentiment towards price. There is a positive sentiment towards the camera, due to its excellent lens, and also a negative sentiment, because its price is too high, so the sentiment towards camera is Mixed. Note: All three items should be selected. Other acceptable selections with a Mixed sentiment are new camera or My new camera.</figDesc><table><row><cell cols="2">: Annotation examples from the various input datasets. A target t that has a positive/negative sentiment</cell></row><row><cell>expressed towards it is marked as [t] P / [t] N .</cell><cell></cell></row><row><cell>Example #1.4: The food was served.</cell><cell>the atmosphere was awful.</cell></row><row><cell>Correct answer: Nothing should be selected,</cell><cell>Correct answer: The food was good, but the</cell></row><row><cell>since there is no sentiment expressed in the</cell><cell>atmosphere was awful.</cell></row><row><cell>sentence.</cell><cell>Explanation: the word good expresses a</cell></row><row><cell></cell><cell>positive sentiment towards food, while the</cell></row><row><cell></cell><cell>word awful expresses a negative sentiment</cell></row><row><cell>2. Sentiment location</cell><cell>towards atmosphere.</cell></row><row><cell></cell><cell>Note: Both items should be selected!</cell></row><row><cell>Example #2.1: I love this great car.</cell><cell></cell></row><row><cell>Correct answer #1: I love this great car. Correct answer #2: I love this great car. Explanation: The word love expresses a positive sentiment towards great car or car.</cell><cell>Example #3.2: The camera has excellent lens. Correct answer: The camera has excellent</cell></row><row><cell>Note: It is OK to select only one of the above options, since they overlap.</cell><cell>lens. Explanation: The word excellent expresses a positive sentiment towards lens. ? An</cell></row><row><cell>Example #2.2: I have a great car. Correct answer: I have a great car. Explanation: The word great expresses a positive sentiment towards car.</cell><cell>excellent lens is a positive thing for a camera to have, thus expressing a positive sentiment towards camera. Note: Both items should be selected!</cell></row><row><cell>Note: Do NOT select the item great car,</cell><cell></cell></row><row><cell>because there is NO sentiment expressed</cell><cell>Example #3.3: My new camera has excellent</cell></row><row><cell>towards great car outside of the phrase great</cell><cell>lens, but its price is too high.</cell></row><row><cell>car itself. The only other information is</cell><cell>Correct answer: My new camera has</cell></row><row><cell>that i have a item, which does not convey a</cell><cell>excellent lens, but its price is too high.</cell></row><row><cell>sentiment towards it.</cell><cell>Explanation:</cell></row><row><cell>3. Multiple selections in one sentence</cell><cell></cell></row><row><cell>Example #3.1: The food was good, but</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Input Dataset Sentence YELP Great [[office staff] P , [[nurse] P practitioner] P and [pediatric doctor] P ] P . AMAZON [Her [[office [routine] P ] P and [morning routine] P ] P ] P are wonderful. OPINOSIS As of today, I am a bit disappointed in [the [[build] N [quality] N ] N of [the [car] N ] N ] N . OPINOSIS [This car] P is nearly perfect when compared to other cars in this class regarding [[interior dimensions] P , [visibility] P , [exterior styling] P ] P , etc .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Examples of sentences in which large target clusters were annotated. There is a positive sentiment (great) expressed towards car outside of the marked item car.</figDesc><table><row><cell>Examples</cell><cell>Answer: Positive</cell></row><row><cell>Here are a few examples, each containing a sentence and a marked item, along with the correct answer and further explanations (when needed). Please review the examples carefully before</cell><cell>Explanation: There is a positive sentiment expressed towards car outside of the marked item car -in the word great and the statement that I love the car.</cell></row><row><cell>starting the task.</cell><cell></cell></row><row><cell></cell><cell>Example #2.3: I have a great car.</cell></row><row><cell>1. Basics</cell><cell>Answer: Positive Explanation:</cell></row><row><cell>Example #1.1: The food was good.</cell><cell></cell></row><row><cell>Answer: Positive</cell><cell></cell></row><row><cell>Example #1.2: The food was bad.</cell><cell></cell></row><row><cell>Answer: Negative</cell><cell></cell></row><row><cell>Example #1.3: The food was tasty but</cell><cell></cell></row><row><cell>expensive.</cell><cell></cell></row><row><cell>Answer: Mixed</cell><cell></cell></row><row><cell>Explanation: tasty expresses a positive</cell><cell></cell></row><row><cell>sentiment, while expensive expresses a</cell><cell></cell></row><row><cell>negative sentiment, so the correct answer is</cell><cell></cell></row><row><cell>Mixed.</cell><cell></cell></row><row><cell>Example #1.4: The food was served.</cell><cell></cell></row><row><cell>Answer: None</cell><cell></cell></row><row><cell>2. Sentiment location</cell><cell></cell></row><row><cell>Example #2.1: I love this great car.</cell><cell></cell></row><row><cell>Answer: Positive</cell><cell></cell></row><row><cell>Explanation: There is a positive sentiment</cell><cell></cell></row><row><cell>expressed towards great car outside of the</cell><cell></cell></row><row><cell>marked item car -in the statement that I love</cell><cell></cell></row><row><cell>the car.</cell><cell></cell></row><row><cell>Example #2.2: I love this great car.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Mixed: a positive and a negative sentiment towards one target, e.g., for car in "a beautiful yet unreliable car".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">github.com/IBM/yaso-tsa</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">www.appen.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">The sum of Ag, Dis and SE inTable 3, subtracting the 8 exclusive SE14 annotations manually identified as errors. 11 github.com/IMPLabUniPr/BERT-for-ABSA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">github.com/yangheng95/LCF-ATEPC 13 github.com/NLPWM-WHU/RACL 14 github.com/lixin4ever/BERT-E2E-ABSA 15 github.com/lixin4ever/HAST 16 github.com/xuuuluuu/ Aspect-Sentiment-Classification</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">github.com/IBM/yaso-tsa</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We wish to thank the anonymous reviewers for their insightful comments, suggestions, and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis in Hindi: Resource creation and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Md Shad Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)<address><addrLine>Portoro?, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2703" to="2709" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MultiBooked: A corpus of Basque and Catalan hotel reviews annotated for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Lambert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International</title>
		<meeting>the Eleventh International</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 5: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orph?e</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?ronique</forename><surname>De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kotelnikov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>Nuria Bel, Salud Mar?a Jim?nez-Zafra, and G?l?en Eryigit; San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2015 task 12: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S15-2082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
	<note>Suresh Manandhar, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adapt or get left behind: Domain adaptation through BERT language model finetuning for aspect-target sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rietzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Engl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4933" to="4941" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Show me your evidence -an automatic method for context dependent evidence detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Dankin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">Alzate</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1050</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A hierarchical model of reviews for aspectbased sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Breslin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="999" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SentiHood: Targeted aspect based sentiment analysis dataset for urban neighbourhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1546" to="1556" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Survey on aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavius</forename><surname>Frasincar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">813</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Attentional encoder network for targeted sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
		</author>
		<idno>abs/1902.09314</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aspect-level sentiment analysis in Czech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Brychc?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Konkol</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-2605</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1035</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Hungarian sentiment corpus manually annotated at aspect level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martina</forename><forename type="middle">Katalin</forename><surname>Szab?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katalin</forename><forename type="middle">Ilona</forename><surname>Simk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Hangya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)<address><addrLine>Portoro?, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2873" to="2878" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective LSTMs for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.374</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4067" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sentence and expression level annotation of opinions in user-generated discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cigdem</forename><surname>Toprak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TDParse: Multi-target-specific sentiment recognition on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaitz</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="483" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06679</idno>
		<title level="m">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Annotating expressions of opinions and emotions in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">0</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BERT post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1242</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2324" to="2335" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aspect sentiment classification with aspect-specific opinion spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3561" to="3567" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Position-aware tagging for aspect sentiment triplet extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.183</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2339" to="2349" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07976</idno>
		<title level="m">Youwei Song, and Ruyang Xu. 2020. A multi-task learning model for chinese-oriented aspect polarity classification and aspect term extraction</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-entity aspect-based sentiment analysis with context, entity and aspect memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lcf: A local context focus mechanism for aspect-based sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuli</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.3390/app9163389</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1253</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">For sentiment classification (SC), the same metrics are separately reported for the positive and negative sentiment labels, as well as macro-F 1 (mF 1 ) over these two classes</title>
	</analytic>
	<monogr>
		<title level="m">Table 8: Detailed benchmark results on YASO with five SOTA systems, trained on data from one SE14 domain (laptops -Lap. or restaurants -Res.)</title>
		<imprint/>
	</monogr>
	<note type="report_type">The datasets</note>
	<note>The reported metrics are precision (P), recall (R) and F 1 for target extraction (TE) and the entire task (TSA). Dat.) are marked as: YELP (Y), AMAZON (A), SST (S) and OPINOSIS (O)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amazon</forename><surname>Sst Opinosis System Train</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Te Sc Tsa Te Sc Tsa Te Sc Tsa Te Sc Tsa Bat</forename><surname>Lap</surname></persName>
		</author>
		<idno>32.3 88.7 29.1 45.1 94.0 42.3 11.5 97.2 11.2 68.9 92.2 64.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert-E2e</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Benchmark results on YASO using overlapping span-matches instead of exact span-matches. This table is similar to Table 5: it presents results from five SOTA systems, trained on data from one SE14 domain (laptops -Lap. or restaurants -Res.). The reported metric is F 1 for target extraction (TE) and the entire task (TSA), and macro-F 1 for sentiment classification</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
