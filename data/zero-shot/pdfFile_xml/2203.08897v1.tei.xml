<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gate-Shift-Fuse for Video Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
						</author>
						<title level="a" type="main">Gate-Shift-Fuse for Video Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action Recognition</term>
					<term>Video Classification</term>
					<term>Spatial Gating</term>
					<term>Channel Fusion !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks are the de facto models for image recognition. However 3D CNNs, the straight forward extension of 2D CNNs for video recognition, have not achieved the same success on standard action recognition benchmarks. One of the main reasons for this reduced performance of 3D CNNs is the increased computational complexity requiring large scale annotated datasets to train them in scale. 3D kernel factorization approaches have been proposed to reduce the complexity of 3D CNNs. Existing kernel factorization approaches follow hand-designed and hard-wired techniques. In this paper we propose Gate-Shift-Fuse (GSF), a novel spatio-temporal feature extraction module which controls interactions in spatio-temporal decomposition and learns to adaptively route features through time and combine them in a data dependent manner. GSF leverages grouped spatial gating to decompose input tensor and channel weighting to fuse the decomposed tensors. GSF can be inserted into existing 2D CNNs to convert them into an efficient and high performing spatio-temporal feature extractor, with negligible parameter and compute overhead. We perform an extensive analysis of GSF using two popular 2D CNN families and achieve state-of-the-art or competitive performance on five standard action recognition benchmarks. Code and models will be made publicly available at https://github.com/swathikirans/GSF</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Understanding human actions in videos is one of the challenging and fundamental problems in computer vision with a wide range of applications from video surveillance to robotics and human-computer interaction. Recently, video action recognition research has witnessed great progress owing to the adoption of Deep Neural Networks. However, such Deep Neural Models are yet to achieve the success of their image recognition counterparts. A key challenge lies in addressing the question of how to better encode the spacetime features.</p><p>Inspired from the improved performance of 2D Convolutional Neural Networks (CNNs) on image recognition tasks, <ref type="bibr" target="#b24">[25]</ref> explores several approaches to extend 2D CNNs for action recognition by considering videos as a set of frames. This includes using a single frame from the video to stacking multiple frames and applying it to a 2D CNN. However it is found that both of these configurations perform similarly, indicating that the learned video representation is not capable of capturing motion information in the video. Two-Stream networks <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b15">[16]</ref> address this problem by stacking optical flow images to capture short-term motion features and incorporate them with the appearance features obtained from a single frame. Such approaches require costly optical flow computations and ignore valuable appearance features from the video. TSN <ref type="bibr" target="#b60">[61]</ref> and ActionVLAD <ref type="bibr" target="#b17">[18]</ref> propose to extract features from multiple frames instead, followed by a late temporal aggregation via mean pooling for improved appearance feature extraction. Such approaches still relied on optical flow images for capturing motion information. The works of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b52">[53]</ref> formulate late temporal aggregation as a sequence learning problem by encoding the frame-? S. Sudhakaran was with FBK, Trento, Italy with this work and is now with Samsung AI Center, Cambridge, UK. E-mail: swathikirans@gmail.com ? S. Escalera is with Universitat de Barcelona and Computer Vision Center, Spain. E-mail: sergio@maia.ub.es ? O. Lanz was with FBK, Trento, Italy with this work and is now with Free University of Bozen-Bolzano, Italy. E-mail: lanz@inf.unibz.it <ref type="figure">Fig. 1</ref>: 3D kernel factorization for spatio-temporal learning in video. Existing approaches decompose into channel-wise (CSN), spatial followed by temporal (S3D, TSM), or grouped spatial and spatio-temporal (GST). In all these, spatial, temporal, and channel-wise interaction is hard-wired. Our Gate-Shift-Fuse (GSF) leverages group spatial gating and fusion (blocks in green) to control interactions in spatialtemporal decomposition. GSF is lightweight and a building block of high performing video feature extractors.</p><p>level features using variants of Recurrent Neural Networks (RNNs). All these approaches perform temporal modelling in a shallow manner. Fine-grained recognition can benefit from deeper temporal modeling. Full-3D CNNs (C3D <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b56">[57]</ref>) process the video in space-time by expanding the kernels of a 2D ConvNet along the temporal dimension. Deep C3Ds are designed to learn powerful representations in the joint spatiotemporal feature space with more parameters (3D kernels) and computations (kernels slide over 3 densely sampled dimensions). In practice however, they may under-perform due to the lack of sufficiently large datasets for training arXiv:2203.08897v1 [cs.CV] <ref type="bibr" target="#b15">16</ref> Mar 2022 them at scale. To cope with these issues arising from the curse of dimension one can narrow down network capacity by design. <ref type="figure">Fig. 1</ref> shows several C3D kernel decomposition approaches proposed for spatio-temporal feature learning in video. A most intuitive approach is to factorize 3D spatiotemporal kernels into 2D spatial plus 1D temporal, resulting in a structural decomposition that disentangles spatial from temporal interactions (P3D <ref type="bibr" target="#b44">[45]</ref>, R(2+1)D <ref type="bibr" target="#b58">[59]</ref>, S3D <ref type="bibr" target="#b70">[71]</ref>). An alternative design is separating channel interactions and spatio-temporal interactions via group convolution (CSN <ref type="bibr" target="#b57">[58]</ref>), or modeling both spatial and spatio-temporal interactions in parallel with 2D and 3D convolution on separated channel groups (GST <ref type="bibr" target="#b36">[37]</ref>). Temporal convolution can be constrained to hard-coded time-shifts that move some of the channels forward in time or backward (TSM <ref type="bibr" target="#b35">[36]</ref>). All these existing approaches learn structured kernels with a hard-wired connectivity and propagation pattern across the network. There is no data dependent decision taken at any point in the network to route features selectively through different branches, for example, group-and-shuffle patterns are fixed by design and learning how to shuffle is combinatorial complexity.</p><p>In this paper we introduce spatial gating and channel fusion in spatial-temporal decomposition of 3D kernels. We implement this concept with Gate-Shift-Fuse (GSF) as shown in <ref type="figure">Fig. 1</ref>. GSF is lightweight and turns a 2D-CNN into a highly efficient spatio-temporal feature extractor. GSF first applies 2D convolution, then decomposes the output tensor using a learnable spatial gating into two tensors: a gated version of it, and its residual. The gated tensor goes through a 1D temporal convolution while its residual is skip-connected to its output. We implement spatial gating as group spatio-temporal convolution with single output plane per group. We use hard-coded time-shift of channel groups instead of learnable temporal convolution. Channel fusion is implemented by generating channel weights via a light weight 2D convolution. The two feature groups, gate-shifted and residual, are then fused by weighted averaging across the channels. With GSF plugged in, a 2D-CNN learns to adaptively route features through time and combine them, at almost no additional parameters and computational overhead. For example, when GSF is plugged into TSN <ref type="bibr" target="#b60">[61]</ref>, an absolute gain of +32 percentage points in accuracy is obtained on Something Something-V1 dataset with just 0.48% additional parameters and 0.58% additional Floating Point Operations (FLOPs).</p><p>The contributions of this paper can be summarized as follows:</p><p>? We propose a novel spatio-temporal feature extraction module that can be plugged into existing 2D CNN architectures with negligible overhead in terms of computations and memory;</p><p>? We perform an extensive ablation analysis of the proposed module to study its effectiveness in video action recognition;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We perform an extensive analysis to study how to effectively integrate the proposed module with two popular 2D CNN architecture families and achieve state-of-the-art or competitive results on five public benchmarks with less parameters and FLOPs com-pared to existing approaches.</p><p>The rest of the paper is organized as follows. Sec. 2 reviews recent works on action recognition. Sec. 3 presents the technical details of Gate-Shift-Fuse (GSF). Experimental results are reported in Sec. 4 and Sec. 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The development of large-scale video datasets such as Kinetics <ref type="bibr" target="#b25">[26]</ref>  <ref type="bibr" target="#b50">[51]</ref>, Moments in Time <ref type="bibr" target="#b41">[42]</ref>, Something Something <ref type="bibr" target="#b18">[19]</ref>, EPIC-Kitchens-100 <ref type="bibr" target="#b4">[5]</ref>, etc., to mention a few, inspired video action recognition research significantly. Such datasets with varying properties, such as requirement of spatial reasoning <ref type="bibr" target="#b25">[26]</ref>, temporal reasoning <ref type="bibr" target="#b18">[19]</ref>, spatiotemporal reasoning <ref type="bibr" target="#b4">[5]</ref>, resulted in the development of a plethora of action recognition approaches. The development of recent large-scale datasets such as HowTo100M <ref type="bibr" target="#b40">[41]</ref>, HVU <ref type="bibr" target="#b5">[6]</ref>, Ego4D <ref type="bibr" target="#b19">[20]</ref>, etc., are expected to further video understanding research beyond simple tasks such as action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Action Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusing appearance and flow. A popular extension of 2D</head><p>CNNs to handle video is the Two-Stream architecture by Simonyan and Zisserman <ref type="bibr" target="#b49">[50]</ref>. Their method consists of two separated CNNs (streams) that are trained to extract features from a sampled RGB video frame paired with the surrounding stack of optical flow images, followed by a late fusion of the prediction scores of both streams. The image stream encodes the appearance information while the optical flow stream encodes the motion information, that are often found to complement each other for action recognition. Several works followed this approach to find a suitable fusion of the streams at various depths <ref type="bibr" target="#b15">[16]</ref> and to explore the use of residual connections <ref type="bibr" target="#b13">[14]</ref> or multiplicative gating functions <ref type="bibr" target="#b14">[15]</ref> between them. These approaches rely on optical flow images for motion information, and a single RGB frame for appearance information, which is limiting when reasoning about the temporal context is required for video understanding.</p><p>Video as a set or sequence of frames. Later, other approaches were developed using multiple RGB frames for video classification. These approaches sparsely sample multiple frames from the video, which are applied to a 2D CNN followed by a late integration of frame-level features using average pooling <ref type="bibr" target="#b60">[61]</ref>, multilayer perceptrons <ref type="bibr" target="#b77">[78]</ref>, recurrent aggregation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b34">[35]</ref>, or attention <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b52">[53]</ref>. To boost performance, most of these approaches also combine video frame sequence with externally computed optical flow. This shows to be helpful, but computationally intensive.</p><p>Modeling short-term temporal dependencies. Other research has investigated the middle ground between late aggregation (of frame features) and early temporal processing (to get optical flow), by modeling short-term dependencies. This includes differencing of intermediate features <ref type="bibr" target="#b42">[43]</ref> and combining Sobel filtering with feature differencing <ref type="bibr" target="#b55">[56]</ref>. Other works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b43">[44]</ref> develop a differentiable network that performs TV-L1 <ref type="bibr" target="#b72">[73]</ref>, a popular optical flow extraction technique. The work of <ref type="bibr" target="#b28">[29]</ref> instead uses a set of fixed filters for extracting motion features, thereby greatly reducing the number of parameters. DMC-Nets <ref type="bibr" target="#b48">[49]</ref> leverage motion vectors in the compressed video to synthesize discriminative motion cues for two-stream action recognition at low computational cost compared to raw flow extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video as a space-time volume.</head><p>Unconstrained modeling and learning of action features is possible when considering video in space-time. Since video can be seen as a temporally dense sampled sequence of images, expanding 2D convolution operation in 2D-CNNs to 3D convolution is a most intuitive approach to spatio-temporal feature learning <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b56">[57]</ref>. The major drawback of 3D CNNs is the huge number of parameters involved. This results in increased computations and the requirement of large scale datasets for pre-training. Carreira and Zisserman <ref type="bibr" target="#b2">[3]</ref> addressed this limitation by inflating video 3D kernels with the 2D weights of a CNN trained for image recognition. Several other approaches focused on reducing the number of parameters by disentangling the spatial and temporal feature extraction operations. P3D <ref type="bibr" target="#b44">[45]</ref> proposes three different choices for separating the spatial and temporal convolutions and develops a 3D-ResNet architecture whose residual units are a sequence of such three modules. R(2+1)D <ref type="bibr" target="#b58">[59]</ref> and S3D-G <ref type="bibr" target="#b70">[71]</ref> also show that a 2D convolution followed by 1D convolution is enough to learn discriminative features for action recognition. CoST <ref type="bibr" target="#b29">[30]</ref> performs 2D convolutions, with shared parameters, along the three orthogonal dimensions of a video sequence. CT-Net <ref type="bibr" target="#b30">[31]</ref> proposes to tensorize the channel dimension of features as a multiplication of K sub-dimensions, followed by 3D separable convolutions along each of the K channel sub-dimensions. This allows the model to gradually increase the spatiotemporal receptive field with reduced computational complexity while maintaining interaction of features across both spatio-temporal and channel dimensions. MultiFiber <ref type="bibr" target="#b3">[4]</ref> uses multiple lightweight networks, the fibers, and multiplexer modules that facilitate information flow using pointwise convolutions across the fibers. CIDC <ref type="bibr" target="#b31">[32]</ref> performs channel independent directional convolution by applying 2D convolution independently on individual feature channels while considering the temporal dimension as channels. Other works involve Neural Architecture Search (NAS) for developing an optimum architecture <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b63">[64]</ref>. X3D <ref type="bibr" target="#b11">[12]</ref> develops a progressive algorithm for expanding the spatio-temporal resolution of inputs, network depth, number of channels, etc., of 3D CNN architectures. The final architecture developed with the search algorithm results in improved performance with less computational complexity compared to hand-designed 3D CNNs.</p><p>Modeling long-term dependencies. Majority of existing approaches rely on the network depth to combine the information present in frames that are further apart in time.</p><p>[66] develops a feature bank that stores features from the entire video which can then be integrated as contextual information to a video recognition model for understanding the present. Non-local <ref type="bibr" target="#b61">[62]</ref> is a plug and play module that computes the features at a specific position in space-time as the weighted sum of features at all other positions, thereby capturing long-range dependencies. Timeception <ref type="bibr" target="#b21">[22]</ref> introduces multi-scale separable temporal convolutions to reason about videos of longer duration. Eidectic 3D LSTM <ref type="bibr" target="#b64">[65]</ref> integrates 3D convolution into RNNs and introduces a gated controlled self-attention module to enable the memory to interact with its past values. The former enables short-term temporal modeling while the latter allows the model to encode long-term temporal information. VideoGraph <ref type="bibr" target="#b22">[23]</ref> constructs a unidirectional graph whose nodes represent the latent action concepts. A novel graph embedding layer is then developed to learn the relationship between these concepts. Zhang et al. <ref type="bibr" target="#b74">[75]</ref> introduces 4D convolution to perform long-term modeling hierarchically. Inspired by the performance improvement obtained by Transformers in Natural Language Processing, Vision Transformer (ViT) <ref type="bibr" target="#b7">[8]</ref> is developed for image recognition. TimeSformer <ref type="bibr" target="#b0">[1]</ref> extends ViT by developing different variants of self-attention schemes for spatio-temporal feature learning. However, even the efficient variant of TimeSformer suffers from increased computational complexity and larger parameter count.</p><p>Multi-scale architectures. Inspired by the effectiveness of Two-stream architectures using RGB and optical flow images, <ref type="bibr" target="#b24">[25]</ref> [13], <ref type="bibr" target="#b10">[11]</ref> developed multi-stream architectures operating on two streams of RGB images. A multi-resolution CNN architecture is developed in <ref type="bibr" target="#b24">[25]</ref> where low resolution frames are processed by one stream and high resolution center crops are processed by the second stream. <ref type="bibr" target="#b10">[11]</ref> proposes to apply alternate frames of high resolution through a compact network in addition to a second stream processing low resolution images. This allows the network to encode features at multiple spatial scales and increase the number of processed frames while maintaining reasonable computational complexity. SlowFast <ref type="bibr" target="#b12">[13]</ref> encodes frames using a dual pathway network. One pathway encodes frames sampled at a slower frame rate while the second encodes frames sampled at a faster frame rate, enabling the network to capture information at multiple temporal scales. These approaches rely on multiple networks to encode information at different scales. To address this issue, TPN <ref type="bibr" target="#b71">[72]</ref> introduces a feature level pyramid network to encode features at multiple hierarchies thereby allowing the network to extract features at different temporal scales. Since the pyramid structure is constructed at feature level, TPN do not require multiple architectures, thereby reducing the number of parameters and computational complexity. MSTI-Net <ref type="bibr" target="#b66">[67]</ref> proposes to split the feature tensors across the channel dimension into four groups and apply convolution operation with different kernel size and aggregate them hierarchically. Varying the kernel size of the convolution operation enables the network to encode spatio-temporal features at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Video Understanding</head><p>Recently, the focus of research is moving to the development of efficient (from a computational point of view) and effective (from a performance point of view) approaches for video understanding. Such approaches perform efficient modeling by (1) reducing redundant computations or (2) developing novel efficient architectures.</p><p>Reducing computations by selective processing. <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b68">[69]</ref> develop a conditional computing approach that decides whether to compute fine resolution features in a data dependent manner, to avoid additional compute overhead. AdaFrame <ref type="bibr" target="#b69">[70]</ref> trains a policy gradient network to determine which frame to process next, thereby skipping processing of non-informative frames in a video. SCSampler <ref type="bibr" target="#b26">[27]</ref> develops a light-weight model for identifying the most salient clip for recognizing the action present in a video. AdaFuse <ref type="bibr" target="#b39">[40]</ref> learns a decision policy to reduce temporal redundancy by removing feature channels that are uninformative and reuse the feature channels from past frames for efficient inference. A reinforcement policy based learning approach for selecting the optimum input frame resolution is developed in <ref type="bibr" target="#b38">[39]</ref>. <ref type="bibr" target="#b1">[2]</ref> follows the approach of knowledge distillation to train a student network that processes a smaller number of frames using a teacher network trained with a larger number of frames. All these approaches are based on existing spatio-temporal feature extractors or architectures that require large memory requirements due to the model size. Even though these approaches reduce the computations in inference time, majority of them suffer from heavy computations during the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient architectures.</head><p>CNNs provide different levels of feature abstractions at different layers of the hierarchy. It has been found that the bottom layer features are less useful for extracting discriminative motion cues <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b80">[81]</ref>. In <ref type="bibr" target="#b54">[55]</ref> it is proposed to apply 1D convolution layers on top of a 2D CNN for video action recognition. The works of <ref type="bibr" target="#b80">[81]</ref> and <ref type="bibr" target="#b70">[71]</ref> show that it is more effective to apply full 3D and separable 3D convolutions at the top layers of a 2D CNN for extracting spatio-temporal features. These approaches resulted in performance improvement over full 3D architectures with less parameters and computations. Static features from individual frames represent scenes and objects and can also provide important cues in identifying the action. This is validated by the improved performance obtained with two-path structures that apply a parallel 2D convolution in addition to the 3D convolution <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b78">[79]</ref>. MiCT <ref type="bibr" target="#b78">[79]</ref> is designed by adding 3D convolution branches in parallel to the 2D convolution branches of a BN-Inception-like CNN. GST <ref type="bibr" target="#b36">[37]</ref> makes use of the idea of grouped convolutions for developing an efficient architecture for action recognition. They separate the features at a hierarchy across the channel dimension and separately perform 2D and 3D convolutions followed by a concatenation operation. In this way, the performance is increased while reducing the number of parameters. MVFNet <ref type="bibr" target="#b67">[68]</ref> extends the idea of GST by replacing the 3D convolution with three 1D convolutions applied to each of the two spatial dimensions and temporal dimension. STM <ref type="bibr" target="#b23">[24]</ref> proposes two parallel blocks for extracting motion features and spatio-temporal features. Their network rely only on 2D and 1D convolutions and feature differencing for encoding motion and spatio-temporal features. TSM <ref type="bibr" target="#b35">[36]</ref> proposes to shift the features across the channel dimension as a way to perform temporal interaction between the features from adjacent frames of a video. This parameterless approach has resulted in similar performance to 3D CNNs. RubiksNet <ref type="bibr" target="#b8">[9]</ref> replaces the convolution operation in 3D CNNs with learnable spatio-temporal shifting. TEA <ref type="bibr" target="#b32">[33]</ref> develops a novel ResNet based architecture that weights channels with motion information while suppressing those without relevant information and that can encode long range temporal information in a hierarchical aggregation fashion. MSNet <ref type="bibr" target="#b27">[28]</ref> develops a plug in module that can estimate the correspondences between adjacent frame level features and convert these into effective motion features that can be injected into the backbone CNN for efficient spatiotemporal modeling.</p><p>In all previous approaches, spatial, temporal, and channel-wise interaction is hard-wired. Here, we propose the Gate-Shift-Fuse (GSF), which control interactions in spatial-temporal decomposition and learns to adaptively route features through time and combine them in a data dependent manner, at almost no additional parameters and computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GATE-SHIFT-FUSE</head><p>We present Gate-Shift-Fuse (GSF), a module capable of converting a 2D CNN into a high performing spatio-temporal feature extractor with minimal overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gate-Shift-Fuse Module</head><p>Several 3D kernel factorization approaches have been developed for efficient video recognition. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the network schematics of some of these approaches. S3D, or R(2+1)D, P3D, decompose 3D convolutions into 2D spatial plus 1D temporal convolutions. TSM replaces 1D temporal shift_fw shift_bw convolution with parameter-free and zero FLOP channelwise temporal shift operations. GST uses group convolution where one group applies 2D spatial and the other 3D spatiotemporal convolution. GST furthermore applies point-wise convolution before and after the block to allow for interactions between spatial and spatio-temporal groups, and for channel reduction and up-sampling. In these modules, the feature flow is hard-wired by design, meaning that features are forwarded from one block to the next without datadependent pooling, gating or routing decision.</p><formula xml:id="formula_0">3 ? 3 sigmoid pool concat tanh 3 ? 3 ? 3 C2D X 1 R 1 Y 1 R 1 Z 1 X Y 1 Y 2 Y 1 shif t F 1 1-F 1 [R 1 ; R 2 ] [Y 1 shif t ; Y 2 shif t ] [Z 1 ; Z 2 ]</formula><p>GSF design, in <ref type="figure" target="#fig_0">Fig. 2</ref>, is inspired by GST and TSM but replaces the hard-wired channel split and concat aggregation with a learnable spatial gating block followed by a learnable fusion block to fuse the two splits of input features. The function of gate block is to selectively route the features through time-shifts to encode temporal information. The fuse block then merge the shifted (temporal) features with the residual (spatial) features. The gate and fuse blocks are learnable and perform gating and fusion operations in a data dependent manner.</p><p>Based on the conceptual design in <ref type="figure" target="#fig_0">Fig. 2</ref>, we instantiate GSF as in <ref type="figure" target="#fig_1">Fig. 3</ref>. We first describe the equations governing the implementation of GSF. Let X be the C ? T ? W ? H shaped input tensor to GSF, where C is the number of channels and W H, T are the spatial and temporal dimensions, respectively. Let X = [X 1 , X 2 ] be the group=2 split of X along the channel dimension,</p><formula xml:id="formula_1">W g = [W g1 , W g2 ] be two 1 ? C /2 ? 3 ? 3 ? 3 shaped gating kernels and W f = [W f1 , W f2 ] be two fusion kernels of shape 1 ? 2 ? 3 ? 3. Then, the GSF output Z = [Z 1 , Z 2 ] is computed as Y 1 = tanh(W g1 * X 1 ) X 1 (1) Y 2 = tanh(W g2 * X 2 ) X 2 (2) R 1 = X 1 ? Y 1 (3) R 2 = X 2 ? Y 2 (4) Y 1 shif t = shift_fw(Y 1 ) (5) Y 2 shif t = shift_bw(Y 2 ) (6) F 1 = ?(W f1 * [?(Y 1 shif t ; R 1 )]) (7) F 2 = ?(W f2 * [?(Y 2 shif t ; R 2 )]) (8) Z 1 = F 1 Y 1 shif t + (1 ? F 1 ) R 1 (9) Z 2 = F 2 Y 2 shif t + (1 ? F 2 ) R 2<label>(10)</label></formula><p>where ' * ' represents convolution, ' ' is Hadamard product, shift_fw, shift_bw is forward, backward temporal shift, ? is sigmoid function, [. ; .] is concatenation operation and ? is spatial average pooling. The output features X obtained from the spatial convolution, inherited from the 2D CNN base model, is applied to the gating module. The gating module splits the features into two groups across the channel dimension and generates two gating planes of shape 1 ? T ? W ? H, each for the two groups. The gating planes are then applied as spatial weight maps to each of the two feature groups to generate a pair of gated features and their residuals. The gated features are then group-shifted forward and backward in time to inject temporal information. The group-shifted features and their corresponding residuals are then applied to the fusion module. The fusion module generates a channel weight map of shape C ?T ?1?1 for each of the two groups. The shifted features and their residuals are then fused via weighted averaging using the channel weight maps. This way, GSF selectively mixes spatial and temporal information through learnable spatial gating and fusion mechanisms. Gating. Gating is implemented with a single spatiotemporal 3D kernel and tanh activation. With a 3D kernel we utilize short-range spatio-temporal information in the gating. tanh provides spatial gating planes with values in the range (?1, +1) and is motivated as follows. When the gating value at a feature location is 0 and that of the timeshifted feature was +1, then a weighted temporal feature averaging is performed at that location. If the gating value of the time-shifted feature was -1 instead, then a weighted temporal feature differencing is performed. Using tanh, the gating can thus learn to apply either of the two modes, location-wise. Fusion. The simplest choice for fusing the time-shifted temporal features with the residual is summation, as done in our preliminary work Gate-Shift Module (GSM) <ref type="bibr" target="#b53">[54]</ref>. However, since different channels of a feature tensor encode different level of information, fusion by summation may be suboptimal. Instead GSF performs weighted averaging on the two feature groups. For this, the group-shifted features and their corresponding residuals are concatenated followed by spatial average pooling operation, resulting in a tensor of shape 2 ? C ? T . Then a 2D convolution is applied on this tensor followed by sigmoid non-linearity to obtain a weight map with shape 1 ? C ? T . This weight map is then reshaped to C ? T ? 1 ? 1 and is used to fuse the timeshifted features and their residuals via weighted averaging.</p><p>When the fusion weight at a feature channel is 0, only the residual features with the spatial information is retained for further processing. On the other hand, if the channel weight is 1, the network will discard the residual spatial features and propagate the time-shifted temporal features. With other weight values, the model mixes the two feature groups selectively by mixing the channels. Parameter count, complexity. The parameter count in the gating is 2 ? (27 ? C /2) = 27 ? C and that of the fusion module is 2?(2?3?3) = 36. The total parameter overhead introduced by GSF is 27?C +36 ? 27?C since C 36 in practice. On the other hand, a typical C3D block such as the 3 ? 1 ? 1 used in S3D <ref type="bibr" target="#b70">[71]</ref>  <ref type="figure" target="#fig_0">(Fig. 2)</ref> has a parameter count of C out ? C ? 3 where C out ? C. Thus the parameter overhead introduced by GSF is far less than that of a C3D block since <ref type="bibr" target="#b26">27</ref> 3 ? C out . The complexity of gating module is H ?W ?T ?27?C while that of the fusion module is C ? T ? 36. The total complexity of GSF is H ? W ? T ? 27 ? C + C ? T ? 18 ? H ? W ? T ? 27 ? C. For a C3D block performing 1D temporal convolution, the complexity is H ? W ? T ? 3 ? C ? C out . Since 27 3 ? C out , one can see that the complexity introduced by GSF is far less than that of a 1D temporal convolution layer. Thus GSF is far more efficient in terms of parameters and computations compared to the temporal convolution used in efficient implementations such as S3D or R(2+1)D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gate-Shift-Fuse Networks</head><p>GSF can be plugged into any 2D CNN architecture to inject temporal information into the features extracted by the backbone CNN. In this work, we choose two widely adopted 2D CNN families, Inception and ResNet, as the backbone CNNs. As mentioned in Sec. 3.1, GSF can be inserted into any layer in the backbone CNN after a 2D convolution. For the Inception family, GSF is applied to the least parametrized branch, i.e., the branch with the least number of convolutions (cf. <ref type="figure" target="#fig_2">Fig.4</ref>). For ResNet based CNN, GSF is applied after the second convolution layer in the bottle neck layer of the backbone (cf. <ref type="figure" target="#fig_3">Fig. 5</ref>). Sec. 4.3 presents the experiments conducted to determine the optimum configuration for applying GSF to the two CNN families considered in this work.</p><p>We train the backbone CNN with GSF using the TSN framework. A fixed number of frames are uniformly sampled from the video and applied to the network. The network outputs a class prediction for each of the frames. The frame level predictions are then averaged to obtain the video level prediction. Training is done using cross-entropy loss computed from the video level prediction and the ground truth video label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>This section presents an extensive set of experiments to evaluate GSF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate Gate-Shift-Fuse (GSF) on five standard action recognition benchmarks, Something Something <ref type="bibr" target="#b18">[19]</ref> (SS-V1 and SS-V2), Kinetics 400 <ref type="bibr" target="#b25">[26]</ref>, Diving48 <ref type="bibr" target="#b33">[34]</ref> and EPIC-Kitchens-100 <ref type="bibr" target="#b4">[5]</ref>. Something-V1 and Something-V2 consists of fine-grained object manipulation actions comprising of 174 categories. Something-V1 consists of 100K videos while Something-V2 is an updated version of Something-V1 with 220K videos. Kinetics 400 is a large scale dataset consisting of 400 action categories with at least 400 clips per action class. Diving48 dataset contains around 18K videos with 48 fine-grained diving categories. EPIC-Kitchens-100 is the largest egocentric action recognition dataset with 90K video clips. Each clip in the dataset is labelled with a verb and noun class from a possible set of 97 verb classes and 300 noun classes. The verb and noun category pairs are combined to form an action class.</p><p>All the datasets considered for evaluating GSF differ considerably in the type of actions present in the videos. Something Something datasets consist of actions that are object agnostic. Thus understanding the actions carried out in the videos require strong temporal reasoning. On the other hand, the videos in EPIC-Kitchens-100 consists of actions that are object specific and requires encoding of the spatio-temporal patterns in the videos for recognizing an action. Both Kinetics 400 and Diving48 datasets contain human actions captured from a third person perspective and require understanding of the temporal dynamics of the human body in the video. While the background of the videos in Kinetics 400 can reveal useful information regarding the action category, the videos in Diving48 are captured with a uniform background thereby requiring encoding of the temporal progression of the actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Training. We train the models end-to-end using Stochastic Gradient Descent (SGD) with momentum 0.9, weight decay 10 ?4 and an initial learning rate of 0.01, for 60 epochs. We use a cosine learning rate schedule with warmup (10 epochs) for adjusting the learning rate. The batch size used is 32 and a dropout of 0.5 is applied to avoid overfitting. 8 or 16 frames sampled from each video is used for predicting the action present in the video. Random scaling, cropping and flipping along with temporal jittering as presented in <ref type="bibr" target="#b60">[61]</ref> are used as data augmentations during training. The backbone CNNs are pretrained on ImageNet. Unless otherwise specified, we use the same setting for training GSFs on all the datasets used in this work. Testing. Similar to contemporary works, we follow two approaches for evaluation: efficiency protocol and accuracy protocol. In the efficiency protocol, a single clip consisting of 8 or 16 frames are uniformly sampled from the video with each frame obtained by cropping the center portion of the video frame. In the accuracy protocol, we sample two clips for all the dataset except for Kinetics 400 from each video and extract three different crops from each frame. Thus a total of 6 clips are sampled from the input video. For Kinetics 400, we sample 10 clips and 3 crops from each video. The predictions of the individual clips are averaged to obtain the final prediction for the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Development</head><p>The model development is performed using SS-V1 dataset. We use 8 frames sampled from the video as input to the    Model design study on GSF-BNInception to analyze recognition accuracy vs number of channels shifted models and use the efficiency protocol for evaluating the testing performance. Inception backbones. We use BNInception and InceptionV3 as the two CNN backbones from the Inception family of models. Since the architectural designs are similar for the two models, we choose BNInception for identifying the optimum model configuration. Inception models have a multi-branch architecture as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We first perform analysis to identify which branch is more suitable for GSF to be plugged into. The results of this analysis are provided in Tab. 1. From the table, one can see that GSF performs the best when GSF is plugged into the branch with the least number of convolution layers.</p><p>As mentioned in Sec. 3.1, temporal information is injected into the features by channel shifting. One can control the amount of temporal information by choosing the number of channels shifted. Shifting all the channels will allow the model to encode more temporal information. However, this is achieved at the expense of spatial information present in the features. There should be an optimum balance between the two. In Tab. 2 we report the performance obtained by varying the number of channels shifted in each layers of the BNInception backbone. It can be seen from the table that shifting all the channels results in the best performance.</p><p>We use the configuration found from these two experiments for constructing GSFs with BNInception and Incep-tionV3 backbones.     <ref type="figure" target="#fig_3">Fig. 5</ref>. Tab. 3 reports the performance of different configurations obtained by applying GSF after different convolution layers present in the block of ResNet50. Following the results from Tab. 3 we apply GSF to after the second convolution of the ResNet block as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Similar to Inception architectures, we also conduct analysis on ResNet by shifting different number of channels. The results are reported in Tab. 4. Unlike with the case of Inception models, the best performance is obtained when only 25% of the channels are shifted. This can be attributed to the fact that shifting all the channels will affect the spatial modelling capability of the network. In the case of Inception based models, the branches without GSF will encode the spatial information. Hence, GSF can be applied to all the channels in the corresponding branch to inject temporal information to the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Analysis</head><p>In this section, we report the ablation analysis performed on the validation set of Something-V1 dataset. In all the experiments, we apply 8 frames as input to the network and report accuracy using the efficiency protocol.</p><p>We first ablate the various components of GSF and validate the design choices. We choose BNInception as the backbone model due to its reduced complexity. The results are reported in Tab. 5. By removing the channel fusion, GSF converges to GSM <ref type="bibr" target="#b53">[54]</ref>, our prior work on top of which GSF is built upon, and underperforms by 0.85%. This validates the importance of the learned channel fusion. We then fix the spatial gating to a fixed value of 1 which results in an accuracy of 45.57%. This is similar to Temporal Shift Module (TSM) <ref type="bibr" target="#b35">[36]</ref> where temporal shifting is applied to all the channels. With a fixed gating of -1, the networks performs   differencing operation between the adjacent frames and results in 45.89%. This is a special case of Temporal Difference Network (TDN) <ref type="bibr" target="#b42">[43]</ref> without the image subnetwork. Finally, we fix the gating to 0 which converges the model to the baseline, TSN, without any temporal interaction between the frame level features, and drops the accuracy by 30%.</p><p>Next we compare the performance improvement obtained by adding different number of GSF to the backbone CNN. Tab. 6 shows the results of this experiment conducted on BNInception backbone. Baseline is the standard Temporal Segment Network (TSN) architecture, with an accuracy of 17.25%. We then applied GSF at the last Inception block of the CNN. This improved the recognition performance by 13%. By adding GSF to the last 5 layers of the backbone, the performance improves by 25%. Thus, a consistent improvement in performance can be observed by plugging in GSF to the backbone layers of the CNN. With GSF added to all the Inception blocks in the CNN, an absolute improvement of 30% is obtained, over the baseline, to reach an accuracy of 48.09%. This huge improvement is obtained only with an overhead of 0.48% and 0.55% in terms of parameters and complexity, respectively.</p><p>We also conducted the above experiment on ResNet-50 backbone. The GSF follows the setting mentioned in Sec. 4.3 and the results are reported in Tab. 7. By plugging in GSF to the last four bottleneck layers of the backbone, a significant improvement of +27% in recognition accuracy is obtained. This is achieved only with a very small overhead in number of parameters and computations. As observed in the case of BNInception backbone, the recognition performance increased consistently by plugging in GSF to all the bottleneck layers of the backbone CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">State-of-the-Art Comparison</head><p>Something-Something. Tab. 8 compares the recognition performance of GSF with state-of-the-art approaches on SS-V1 and SS-V2 datasets. For fair comparison, we compare only with those methods that use RGB frames for feature extraction. We also report the number of frames used by each approach along with the number of parameters and the computational complexity in terms of FLOPs in the table. Since different approaches use different backbone CNNs, we also report the backbones used for fair comparison. The      <ref type="bibr" target="#b4">[5]</ref> we used a smaller batch size of 8 during training. From the table, one can see that GSF model outperforms approaches that uses 3D CNN backbones such as I3D <ref type="bibr" target="#b2">[3]</ref>, SlowFast <ref type="bibr" target="#b12">[13]</ref> and GST <ref type="bibr" target="#b36">[37]</ref> by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion</head><p>We perform further analysis of GSF in this section. The t-SNE plot of features obtained from the output of the layer preceding the final Fully Connected (FC) layer of BNInception backbone is plotted in <ref type="figure">Fig. 6a</ref>. We visualize the 10 action groups defined in <ref type="bibr" target="#b18">[19]</ref> in the plot. From the figure, one can see better separation of the features when   GSF is plugged into the backbone CNN. This allows the model to correctly recognize the action present in the video and results in improved recognition performance compared to the baseline TSN. The action categories that benefited the most by plugging in GSF to the backbone CNN are reported in <ref type="figure">Fig. 7</ref>.</p><p>The Y-axis labels show the true labels (predicted by GSF) and the labels predicted by TSN, separated by "/". From the plot, it can be seen that the baseline TSN confuses actions that are of similar nature but following a different (reversed) order, such as unfolding something and folding something, moving something and something closer to each other and moving something and something away from each other, etc.With GSF modules in the backbone CNN, the model is capable of encoding the temporal ordering of frames to correctly recognize the action. This can further be seen from the visualization of the saliency tubes <ref type="bibr" target="#b51">[52]</ref> for the samples from the most improved classes as shown in <ref type="figure" target="#fig_6">Fig. 9</ref>. From the figure, one can see that GSF predicts the action category by focusing on the relevant frames where the corresponding action is taking place as opposed to the baseline TSN. In <ref type="figure" target="#fig_6">Figs. 9c and 9d</ref>, even though the baseline model is focusing on the relevant spatio-temporal features, the lack of temporal reasoning renders the model to make wrong predictions.</p><p>We analyse the computational complexity and memory requirement for GSFs and state-of-the-art models in <ref type="figure" target="#fig_5">Fig. 8</ref>. X-axis shows the computational complexity in GFLOPs and Y-axis shows the recognition accuracy. The size of the marker indicate the number of parameters present in each model. It can be seen from the plot that GSF outperforms existing approaches in terms of recognition accuracy while maintaining a reasonable compute and memory requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We presented Gate-Shift-Fuse (GSF), a novel spatiotemporal feature extraction module capable of converting a 2D CNN into a high performing spatio-temporal feature extractor with negligible computational overhead. GSF relies on spatial gating and channel fusion to exchange information between neighbouring frames in a data dependent manner. We performed an extensive analysis to study the effectiveness of GSF using two CNN model families on five standard action recognition benchmarks. Compared to other approaches, GSF achieves state-of-the-art or competitive performance on these datasets with far less model complexity, indicating its effectiveness as an efficient spatiotemporal feature extractor. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>C3D decomposition approaches in comparison to GSF schematics. GSF is inspired by GST and TSM but replaces the hard-wired channel split and concat aggregation with learnable spatial gating and fusion blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>GSF implementation with group gating, forwardbackward temporal shift, and fusion. A gate is a single 3D convolution kernel with tanh calibration while fusion consists of a single 2D convolution kernel with sigmoid calibration. Thus very few parameters are added when GSF is used to turn a C2D base model into a spatio-temporal feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>BN-Inception blocks with GSF. Kernel size and stride of conv and pool layers are annotated inside each block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>ResNet bottleneck layer with GSF. The kernel size and stride of conv layers are annotated inside each block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>t-SNE projection of output features obtained from the layer preceeding the final linear layer for (a) TSN baseline and (b) TSN baseline with GSF. Samples from the 10 action groups defined in<ref type="bibr" target="#b18">[19]</ref> are visualized. Action classes with the highest improvement over TSN baseline. X-axis shows the the number of corrected samples for each class. Y-axis labels are in the format true label (GSF)/predicted label (TSN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Accuracy (%) vs complexity (GFLOPs) of state-of-theart approaches on Something-V1 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>plugging something into something but pulling it right out as you remove your hand (c) plugging something into something (d) Saliency tubes generated by TSN (left) and GSM (right) on sample videos taken from the validation set of Something Something-V1 dataset. Action labels are shown as text on columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Model design study done to determine the Inception branch that is most suitable for plugging in GSF.</figDesc><table><row><cell cols="3">% of channels shifted Accuracy (%) Params. FLOPs</cell></row><row><cell>12.5</cell><cell>45.08</cell><cell>10.46M 16.45G</cell></row><row><cell>25</cell><cell>45.44</cell><cell>10.46M 16.46G</cell></row><row><cell>50</cell><cell>46.21</cell><cell>10.47M 16.8G</cell></row><row><cell>75</cell><cell>47.02</cell><cell>10.49M 16.51G</cell></row><row><cell>100</cell><cell>48.09</cell><cell>10.5M 16.44G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Model design analysis done to determine where to plug in GSF on ResNet bottleneck layers.</figDesc><table><row><cell cols="3">% of channels shifted Accuracy (%) Params. FLOPs</cell></row><row><cell>12.5</cell><cell>47.5</cell><cell>23.92M 33.23G</cell></row><row><cell>25</cell><cell>48.36</cell><cell>23.97M 33.4G</cell></row><row><cell>50</cell><cell>47.61</cell><cell>24.08M 33.75G</cell></row><row><cell>75</cell><cell>47.27</cell><cell>24.19M 34.09G</cell></row><row><cell>100</cell><cell>46.26</cell><cell>24.3M 34.43G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table /><note>Model design study on GSF-ResNet-50 to analyze recognition accuracy vs number of channels shifted ResNet backbones. The block diagram of the bottleneck layer of ResNet based CNNs are shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Ablation study on GSF</figDesc><table><row><cell>Model</cell><cell cols="2">Accuracy (%) Params. FLOPs</cell></row><row><cell>BNInception (baseline)</cell><cell>17.25</cell><cell>10.45M 16.43G</cell></row><row><cell>BNInception + 1 GSF</cell><cell>30.39</cell><cell>10.46M 16.44G</cell></row><row><cell>BNInception + 5 GSF</cell><cell>42.43</cell><cell>10.48M 16.46G</cell></row><row><cell>BNInception + 10 GSF</cell><cell>48.09</cell><cell>10.5M 16.53G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 :</head><label>6</label><figDesc>Recognition Accuracy by varying the number of GSF added to BNInception backbone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 :</head><label>7</label><figDesc>Recognition Accuracy by varying the number of GSF added to ResNet50 backbone. GSF outperforms the approach from<ref type="bibr" target="#b37">[38]</ref> by +1.3%. By plugging in the MotionSqueeze module<ref type="bibr" target="#b27">[28]</ref> on the GSF version of ResNet-50, our model surpasses the recently developed state-of-the-art approaches MVF-Net<ref type="bibr" target="#b67">[68]</ref> and CT-Net<ref type="bibr" target="#b30">[31]</ref>, achieving an accuracy of 53.78% on SS-V1. Kinetics400. We compare the performance of GSF with state-of-the-art methods on Tab.9. All two GSF models are trained with 16 frames and the accuracy protocol with 10 clip sampling during inference. As done previously, we split the table into two sections: top section comparing methods that use 2D CNN backbones and bottom section lists the approaches that use 3D CNN backbone. As mentioned in Sec. 4.1, Kinetics400 consists of videos of short duration ( 10s) that do not require strong temporal reasoning. This is reflected in Tab. 9, where the approaches that use 3D CNN for feature extraction performs significantly better than those that use 2D CNNs. Compared to approaches that use 2D CNN, the two GSF models perform competitively with the InceptionV3 based model outperforming its ResNet-50 counterpart by 0.5%.</figDesc><table><row><cell>table is split into two, the first part lists approaches that</cell></row><row><cell>use 2D CNN or efficient implementations of 3D CNN as</cell></row><row><cell>backbone. The second part shows the methods that rely</cell></row><row><cell>on heavier 3D CNNs for spatio-temporal feature extraction.</cell></row><row><cell>Our baseline TSN [61] obtains 17.52% and 32.71% recog-</cell></row><row><cell>nition accuracy on SS-V1 and SS-V2 datasets, respectively.</cell></row><row><cell>Applying GSF to the backbone CNN, an absolute gain</cell></row><row><cell>of +31.79% and +30.2% is obtained on the two datasets.</cell></row><row><cell>This huge improvement is obtained with an increase of</cell></row><row><cell>only +0.48% and +0.55% on the number of parameters</cell></row><row><cell>and FLOPs. Compared to the top performing approaches</cell></row><row><cell>that use 2D CNN, such as TEA [33] and MSNet [28], GSF</cell></row><row><cell>performs competitively. On the other hand, approaches that</cell></row><row><cell>uses 3D CNN such as I3D [3] and Non-local networks [62],</cell></row><row><cell>considerably falls behind GSF. The approach from [38] uses</cell></row><row><cell>a deeper 3D CNN (I3D ResNet-152). When compared to the</cell></row><row><cell>performance obtained with a backbone CNN of equivalent</cell></row><row><cell>depth (ResNet-50),</cell></row></table><note>is initialized with ImageNet pretrained weights. From the table one can see that GSF models outperform the other approaches significantly. One interesting finding is that in contrast to what was observed for Something and Kinetics datasets, the ResNet based backbones result in better perfor- mance than InceptionV3 based model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8 :</head><label>8</label><figDesc>Comparison to state-of-the-art on Something-V1 and Something-V2 datasets.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Accuracy (%)</cell></row><row><cell>bLVNet-TAM [11]</cell><cell>bLResNet-50</cell><cell>73.5</cell></row><row><cell>STM [24]</cell><cell>ResNet-50</cell><cell>73.7</cell></row><row><cell>TSM [36]</cell><cell>ResNet-50</cell><cell>74.1</cell></row><row><cell>Martnez et al. [38]</cell><cell>NA</cell><cell>74.3</cell></row><row><cell>TEA [33]</cell><cell>ResNet-50</cell><cell>76.1</cell></row><row><cell>MFNet [29]</cell><cell>ResNet-50</cell><cell>76.4</cell></row><row><cell>MVFNet [68]</cell><cell>ResNet-50</cell><cell>77.0</cell></row><row><cell>CT-Net [31]</cell><cell>ResNet-50</cell><cell>77.3</cell></row><row><cell>I3D [29]</cell><cell>ResNet-101 + NL</cell><cell>77.7</cell></row><row><cell>Martinez et al. [38]</cell><cell>NA</cell><cell>78.8</cell></row><row><cell>ip-CSN [29]</cell><cell>ResNet-152</cell><cell>79.2</cell></row><row><cell>TPN [29]</cell><cell>ResNet-101</cell><cell>78.9</cell></row><row><cell>SlowFast [29]</cell><cell>ResNet-101 + NL</cell><cell>79.8</cell></row><row><cell>X3D-XXL [29]</cell><cell>-</cell><cell>80.4</cell></row><row><cell>GSF</cell><cell>ResNet-50 InceptionV3</cell><cell>74.74 75.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 9 :</head><label>9</label><figDesc>Comparison to state-of-the-art efficient architectures on Kinetics400.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Accuracy (%) Verb Noun Action</cell></row><row><cell>TSN [61]  *  TRN [78]  *  TSM [36]  *  SlowFast [13]  *</cell><cell>ResNet-50 ResNet-50 ResNet-50 ResNet-50</cell><cell>60.18 46.03 65.88 45.43 67.86 49.01 65.56 50.02</cell><cell>33.19 35.34 38.27 38.54</cell></row><row><cell></cell><cell cols="2">InceptionV3 68.89 51.42</cell><cell>43.11</cell></row><row><cell>GSF</cell><cell>ResNet-50</cell><cell>68.88 52.73</cell><cell>43.84</cell></row><row><cell></cell><cell>ResNet-101</cell><cell>69.06 53.18</cell><cell>44.48</cell></row></table><note>Diving48. We compare the performance of Diving48 dataset on Tab. 11. We use 16 frames to train the GSF model with InceptionV3 backbone and inference is performed following the accuracy protocol. All training settings are the same as mentioned in Sec. 4.2 except for the batch size. For Diving48,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10 :</head><label>10</label><figDesc>Comparison to state-of-the-art on the validation set of EPIC-Kitchens-100.</figDesc><table /><note>* : Results reported from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 11 :</head><label>11</label><figDesc>Comparison to state-of-the-art on Diving48.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We gratefully acknowledge the support from Amazon AWS Machine Learning Research Awards (MLRA) and ICREA under the ICREA Academia programme and NVIDIA AI Technology Centre (NVAITC), EMEA. We acknowledge the CINECA award under the ISCRA initiative, for the availability of high performance computing resources and support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone #Frames Params (M) GFLOPs Accuracy (%) SS-V1 SS-V2 TSN <ref type="bibr" target="#b60">[61]</ref> BN- </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is Space-Time Attention All You Need for Video Understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="page" from="813" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient video classification using fewer frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13256</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Rescaling egocentric vision. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large Scale Holistic Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="593" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="505" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Endto-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2264" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SlowFast Networks for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Something Something&quot; Video Database for Learning and Evaluating Visual Common Sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5842" to="5850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chavis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamburger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07058</idno>
		<title level="m">Around the World in 3,000 Hours of Egocentric Video</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Videograph: Recognizing minutes-long human activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. Worksh</title>
		<meeting>Int. Conf. Comput. Vis. Worksh</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">STM: SpatioTemporal and Motion Encoding for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The Kinetics Human Action Video Dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SCSampler: Sampling Salient Clips from Video for Efficient Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MotionSqueeze: Neural Motion Feature Learning for Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="345" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="387" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative Spatiotemporal Feature Learning for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7872" to="7881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ct-net: Channel tensorization network for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Directional Temporal Modeling for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="275" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TEA: Temporal Excitation and Aggregation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RESOUND: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">VideoL-STM convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Vis. Image Und</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7082" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grouped Spatial-Temporal Aggregation for Efficient Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5512" to="5521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition with spatial-temporal discriminative filter banks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5481" to="5490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ar-net: Adaptive frame resolution for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="86" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Moments in Time Dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal difference networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Wint. Conf. Appl. Comput. Vis</title>
		<meeting>IEEE Wint. Conf. Appl. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9945" to="9953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards Efficient Coarse-to-Fine Networks for Action and Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">AssembleNet++: Assembling Modality Representations via Attention Connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kangaspunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="654" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Assem-bleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DMC-Net: Generating discriminative motion cues for fast compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1268" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10864</idno>
		<title level="m">A short note on the kinetics-700-2020 human action dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Saliency Tubes: Visual Explanations for Spatio-Temporal Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kapidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kalliatakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chrysoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1830" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">LSTA: Long Short-Term Attention for Egocentric Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9954" to="9963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gate-Shift Networks for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1102" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: a fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1390" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Video Classification With Channel-Separated Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video Modeling With Correlation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="349" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">AttentionNas: Spatiotemporal Attention Cell Search for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="449" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Eidetic 3D LSTM: A Model for Video Prediction and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Long-Term Feature Banks for Detailed Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-Scale Spatial-Temporal Integration Convolutional Tube for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="753" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">MVFNet: Multi-View Fusion Network for Efficient Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2943" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A coarse-to-fine framework for resource efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">AdaFrame: Adaptive Frame Selection for Fast Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Temporal Pyramid Network for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Temporal Query Networks for Fine-grained Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4486" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">V4D: 4D Convolutional Neural Networks for Video-level Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Represent</title>
		<meeting>Int. Conf. Learn. Represent</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Maximum-entropy adversarial data augmentation for improved generalization and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08001</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Trajectory Convolution for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inform. Process. Syst</title>
		<meeting>Adv. Neural Inform. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2208" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">MiCT: Mixed 3d/2d convolutional tube for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Approximated Bilinear Modules for Temporal Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3493" to="3502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">ECO: Efficient Convolutional Network for Online Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
