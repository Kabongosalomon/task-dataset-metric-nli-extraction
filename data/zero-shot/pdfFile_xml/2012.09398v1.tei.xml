<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Invariant Teacher and Equivariant Student for Unsupervised 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
							<email>sihengc@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
							<email>maosen_li@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>ya_zhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Invariant Teacher and Equivariant Student for Unsupervised 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel method based on teacher-student learning framework for 3D human pose estimation without any 3D annotation or side information. To solve this unsupervisedlearning problem, the teacher network adopts pose-dictionarybased modeling for regularization to estimate a physically plausible 3D pose. To handle the decomposition ambiguity in the teacher network, we propose a cycle-consistent architecture promoting a 3D rotation-invariant property to train the teacher network. To further improve the estimation accuracy, the student network adopts a novel graph convolution network for flexibility to directly estimate the 3D coordinates. Another cycle-consistent architecture promoting 3D rotationequivariant property is adopted to exploit geometry consistency, together with knowledge distillation from the teacher network to improve the pose estimation performance. We conduct extensive experiments on Human3.6M and MPI-INF-3DHP. Our method reduces the 3D joint prediction error by 11.4% compared to state-of-the-art unsupervised methods and also outperforms many weakly-supervised methods that use side information on Human3.6M. Code will be available at https://github.com/sjtuxcx/ITES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D pose</head><p>Graph-convolution based student network</p><p>Pose-dictionary based teacher network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge distillation loss</head><p>Cycle-consistent loss for 3D rotation-invariance Cycle-consistent loss for 3D rotation-equivariance Graph-convolution based student network</p><p>Pose-dictionary based teacher network Camera view matrix R</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human pose estimation from 2D landmarks receives substantial attention due to its broad applications <ref type="bibr" target="#b20">(Li et al. 2019</ref><ref type="bibr" target="#b21">(Li et al. , 2020a</ref>. Many fully-supervised or weakly-supervised estimating algorithms have been proposed, which require either adequate 3D ground-truth annotations or side information such as unpaired 3D poses <ref type="bibr" target="#b29">(Tung et al. 2017</ref>) and multi-view images <ref type="bibr" target="#b16">(Kocabas, Karagoz, and Akbas 2019)</ref> as supervision. However, obtaining such 3D annotation or side information is time-consuming and prohibitive, which is not feasible for many applicable scenarios. In this paper, we consider the unsupervised-learning setting, which estimates a 3D pose from a given 2D pose without any additional information in either the training or the inference phase.</p><p>Estimating a 3D pose solely from a given 2D pose is an ill-posed problem because there exists an infinite number of 3D skeletons corresponding to the same 2D pose with the perspective projection ambiguity. Some simple self-supervision methods through straightforward reprojection might be underconstrained, resulting in overfitting. Extra regularization or Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  priors of a human body is required to eliminate solutions that are not physically plausible. However, too much regularization tends to limit the flexibility of the model and leads to underfitting. An appropriate trade-off between regularization and flexibility is usually hard to reach, which leads to the regularization-flexibility dilemma.</p><p>To resolve the above dilemma, we propose a teacherstudent framework to balance the regularization and flexibility, which achieves a coarse-to-fine estimation. The proposed pose dictionary-based teacher network emphasizes the regularization, ensuring the feasibility of the pose estimation. The proposed graph-convolution-based student network emphasizes the flexibility, further refining the estimation. To enable unsupervised estimation, both teacher and student networks are trained based on respective cycle-consistent architectures.</p><p>Following <ref type="bibr" target="#b26">(Novotny et al. 2019)</ref>, the teacher network adopts a pose-dictionary-based modeling, where the 3D pose is decomposed into the camera viewpoint and a linear combination of pose atoms in a pose dictionary. However, this pose modeling suffers from decomposition ambiguity <ref type="bibr" target="#b9">(Dai, Li, and He 2014)</ref>, with an infinite number of valid camerapose pairs. To resolve decomposition ambiguity, we propose a novel cycle-consistent architecture promoting the 3D rotation-invariant property to train the teacher network. The 3D rotation-invariant property means that 2D poses from different views of a same 3D skeleton should lead to the same 3D pose and different camera viewpoints, thus making the decomposition unique; see <ref type="figure">Figure 1</ref>(a). In the cycle-consistent architecture, the estimated 3D pose is randomly rotated and reprojected to obtain a new 2D pose, whose estimation is forced to be the same as the original 2D input through a cycle-consistent loss.</p><p>Our student network leverages a trainable skeleton graph structure capturing joints correlation to directly estimate the 3D pose in the input view. The student network receives the knowledge from the teacher and further improves the estimation performance without rigid regularization. Since the student network provides the final estimated 3D pose, which should be adaptive to the input camera view, we propose another cycle-consistent architecture to promote the 3D rotation-equivariant property for the student network. The 3D rotation-equivariance means that the 3D estimation has the same view as the input view; see <ref type="figure">Figure 1</ref>(b). The cycleconsistent architecture provides a self-supervision way by exploiting geometric consistency and enhances the training of the student network.</p><p>Overall, the proposed method is characterized by a rotation-Invariant Teacher and a rotation-Equivariant Student (named ITES thereafter). Different from many traditional teacherstudent frameworks <ref type="bibr" target="#b34">(Zhou et al. 2017a</ref>), our student network refines the coarse teacher estimation result and outperforms the teacher network. It is still worth noting that the student network can not be trained alone without the teacher network because no extra constraints or priors are added into the network to eliminate solutions not physically plausible. The effectiveness of the proposed ITES is validated comprehensively on standard 3D benchmarks. Experimental results show that our approach outperforms the state-of-the-art unsupervised methods on Human3.6M <ref type="bibr" target="#b12">(Ionescu et al. 2013</ref>) and MPI-INF-3DHP <ref type="bibr" target="#b24">(Mehta et al. 2017)</ref>. We furthermore visualize the estimation provided by the proposed ITES, which demonstrates the effectiveness of our approach qualitatively.</p><p>The main contributions of this paper are as follow:</p><p>? We propose an unsupervised teacher-student learning framework, called invariant teacher and equivariant student (ITES), to estimate the 3D human pose from a given 2D pose, without requiring additional annotation or side information.</p><p>? We propose a cycle-consistent architecture to train a posedictionary-based teacher network. This architecture promotes the 3D rotation-invariant property for the estimation.</p><p>? We propose a similar, yet different cycle-consistent architecture exploiting geometry consistency to enhance the training of the graph-convolution-based student network. This architecture promotes the 3D rotation-equivariant property.</p><p>? We conduct extensive experiments on Human3.6M and MPI-INF-3DHP datasets. We show that our ITES outperforms the state-of-the-art unsupervised method by 11.4% (51.4mm vs 58mm) measured by P-MPJPE on Human3.6M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Non-Rigid Structure from Motion</head><p>Non-rigid structure from motion (NRSfM) considers the problem of reconstructing the 3D shapes of non-rigid objects and the viewpoint from 2D points as those objects are moving along with the corresponding shape deforming. NRSfM becomes an ill-posed problem since the number of unknown variables that need to be estimated is more than the number of known equations. To alleviate this problem, extra constraints need to be added. Bregler et al. <ref type="bibr" target="#b4">(Bregler, Hertzmann, and Biermann 2000)</ref> proposed a constraint to model the 3D shapes of objects in a low-rank subspace. Based on this modeling, various constraints about the shapes and the viewpoints are exploited, including the low-rank subspaces in the spatial domain <ref type="bibr" target="#b9">(Dai, Li, and He 2014;</ref><ref type="bibr" target="#b11">Fragkiadaki et al. 2014)</ref>, fixed articulation (Ramakrishna, Kanade, and Sheikh 2012), low-rank subspaces in the temporal domain <ref type="bibr" target="#b1">(Akhter et al. 2009</ref><ref type="bibr" target="#b2">(Akhter et al. , 2010</ref>, the union-of-subspaces <ref type="bibr" target="#b37">(Zhu et al. 2014;</ref><ref type="bibr" target="#b0">Agudo, Pijoan, and Moreno-Noguer 2018)</ref> and block-sparsity <ref type="bibr" target="#b18">(Kong and Lucey 2016)</ref>. Many of these previous approaches consider arbitrary non-rigid objects and deal with small-scale datasets with thousands of images, while we focus on the human body and the proposed method can be applied to handle large-scale datasets with millions of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Weakly-Supervised 3D Pose Estimation</head><p>Weakly-supervised 3D pose estimation does not directly use the corresponding 3D ground truth pose of the input 2D pose or image. Instead, they use a collection of unpaired 3D pose <ref type="bibr" target="#b29">(Tung et al. 2017;</ref><ref type="bibr" target="#b30">Wandt and Rosenhahn 2019;</ref><ref type="bibr" target="#b36">Zhou et al. 2016)</ref>, multi-view supervision <ref type="bibr" target="#b16">(Kocabas, Karagoz, and Akbas 2019;</ref><ref type="bibr" target="#b28">Rhodin et al. 2018;</ref><ref type="bibr" target="#b22">Li et al. 2020b)</ref> or ordinal depth supervision <ref type="bibr" target="#b27">(Pavlakos, Zhou, and Daniilidis 2018)</ref> to alleviate the need for paired 2D-3D annotations. However, those auxiliary supervisions are still expensive to access. In this paper, we propose an unsupervised method that only uses a single 2D pose without auxiliary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Unsupervised 3D Pose Estimation</head><p>Due to the lack of 3D ground truth annotations, extra constraints are needed to solve the unsupervised 3D pose estimation problem. <ref type="bibr" target="#b10">(Drover et al. 2018;</ref><ref type="bibr" target="#b19">Kudo et al. 2018</ref>) introduce implicit constraints based on adversarial training. Following this branch, ) proposes a domain adaptation method with a geometric self-supervision loss. Instead of using adversarial training, <ref type="bibr" target="#b26">(Novotny et al. 2019</ref>) considers the constraints based on low-rank matrix factorization and the canonicalization of 3D shapes. Recently, <ref type="bibr" target="#b31">(Wang, Kong, and Lucey 2019)</ref> proposes a teacher-student framework, where the student is guided by a teacher to alleviate the projection ambiguity. In this work, we also consider a teacher-student framework. Note that <ref type="bibr" target="#b31">(Wang, Kong, and Lucey 2019)</ref> adds a block sparsity constraint into the teacher network structure, while our framework promotes the 3D rotation-invariant property for the teacher network. We also propose a novel graphconvolution-based student network promoting the rotationequivariance comparing to the convolutional-based student network in <ref type="bibr" target="#b31">(Wang, Kong, and Lucey 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Invariant Teacher and Equivariant Student</head><p>In this section, we formulate the task of 3D pose estimation and propose our estimation framework, called invariant teacher and equivariant student (ITES). Let X = [x 1 ? ? ? x N ] ? R 2?N be a 2D pose matrix, where N is the number of body joints and x i ? R 2 is the 2D coordinate of the ith body joint. The root joint (pelvis) is set to the origin. Given this 2D pose matrix X, a 3D pose estimator E(?) is designed to estimate the corresponding 3D pose matrix To train the teacher network, a reprojection loss and a cycle-consistent loss for 3D rotation-invariance is applied. The student network (blue branch) directly estimates the 3D pose in the input view. The student network is trained by knowledge distillation from the teacher network and a cycle-consistent loss for the 3D rotation-equivariance. R and P represent the random rotation and perspective projection operation.</p><formula xml:id="formula_0">Y ? R 3?N ; that is, Y = E(X) ? R 3?N .</formula><p>We apply a perspective camera model as ) by assuming a virtual camera with an intrinsic matrix of I 3 centered at the world origin and fix the distance from skeleton to the camera in the 3D space with a constant t unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>Due to the perspective ambiguity, we need to introduce certain regularization to limit the solution space of an estimator; on the other hand, too much regularization tends to limit the network flexibility, leading to an imprecise result. Thus we consider a teacher-student framework to address this trade-off between regularization and flexibility. The teacher network considers a pose-dictionary-based model limiting the solution space spanned by the pose atoms for regularization. To train this teacher network, we propose a cycle-consistent loss for 3D rotation-invariance, see details in Section 3.2. The student network captures the correlations between body joints and reflects the physical constraints of the human body by leveraging a trainable graph structure. To train this graphconvolution-based student network, we consider knowledge distilling from the teacher network and a cycle-consistent loss for 3D rotation-equivariance; see details in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pose-Dictionary-based Teacher Network</head><p>The specific aim of the teacher network is to recover the corresponding 3D pose and camera-view matrix from a given 2D pose. To achieve this, we consider two features for our design: the pose modeling based on a pose dictionary, which regularizes the solution space of 3D poses, as well as a cycle-consistent estimation architecture, which promotes the rotation-invariant property of 3D pose estimation. Pose modeling. Following the common representation of 3D poses in NRSfM methods <ref type="bibr" target="#b4">(Bregler, Hertzmann, and Biermann 2000)</ref> we model a 3D pose in our teacher network</p><formula xml:id="formula_1">Y t as a linear combination of K pose atoms B k ? R 3?N , k = 1, . . . , K; that is Y t = K k=1 c k B k , where c k is the coefficient of the k-th pose atom. We can rewrite the formula in a matrix representation, Y t = (c ? I 3 )B ? R 3?N where B = [B 1 . . . B K ] ? R 3K?N is a pose dictionary, consisting of K pose atoms; c = [c 1 . . . c K ] ? R K is the coefficient vector and ? denotes the Kronecker product. The 2D pose can be projected from Y t as X = P(RY t ) = P(R(c ? I 3 )B) ? R 2?N ,<label>(1)</label></formula><p>where P is the perspective projection function, which maps the 3D pose on a 2D plane, and R ? SO(3) is the camera view matrix. Network architecture. Motivated by Eq.</p><p>(1), we design a pose-dictionary-based teacher network to solve an inverse problem; that is, given the 2D pose X, we aim to estimate the pose dictionary B, the coefficient c and the camera view matrix R through a neural network.</p><p>The network consists of three parts: a backbone network that extracts features from the input 2D pose, two output heads that produce the coefficients c and the camera view matrix R, respectively, and a pose-generation module that produces the estimated 3D pose. The backbone and two output heads are constructed by fully-connected layers. In the pose-generation module, we multiply a trainable matrix of the pose-dictionary B with the pose coefficients c to generate a 3D pose; that is, Y t = ( c ? I 3 )B ? R 3?N ; see <ref type="figure" target="#fig_3">Figure 3</ref>. Note that B works as trainable parameters shared across different inputs, while c and R are the corresponding outputs of the teacher network.</p><p>To train this network, we consider the following two losses: a self-supervised 2D reprojection loss and a cycle-consistent loss for 3D rotation-invariance.</p><p>Self-supervised 2D reprojection loss. To promote the equality in Eq.(1), the teacher network is learned in a self-  supervised way by minimizing the 2D reprojection loss</p><formula xml:id="formula_2">L REP = 1 N P( R Y t ) ? X 2 F ,<label>(2)</label></formula><p>where ? F indicates the Frobenius norm, Y t is the estimation of the teacher network, the camera view matrix R rotates the estimation 3D pose into the input view, and P is the perspective projection function that projects a 3D pose to a 2D pose. The 2D reprojection loss constrains the 2D projection of the estimated 3D pose close to input 2D pose in a self-supervised manner.</p><p>Cycle-consistent loss for 3D rotation-invariance. According to Eq. (1), one 2D pose can actually be decomposed into multiple pairs of 3D poses and camera views. To be specific, given a 2D pose X, any rotation matrix G ? SO(3) could introduce a new pair of the rotation matrix RG and the 3D pose G ?1 Y t , leading to a new valid decomposition,</p><formula xml:id="formula_3">X = P(RY t ) = P((RG)(G ?1 Y t ))</formula><p>. This decomposition ambiguity causes a problem that there are innumerable solutions to generate 3D human poses from a single 2D pose.</p><p>To address this issue, we propose a cycle-consistent loss promoting 3D rotation-invariance to enable the potentially finite solutions of the teacher network. The intuition is that when we project a 3D pose to various 2D poses along multiple views, the teacher network should estimate the same 3D pose from those projected 2D poses. To achieve this, we rotate the estimated 3D pose Y t by a random rotation matrix R rand ? SO(3), and then project the 3D pose on a 2D plane to obtain a new 2D pose X ? R 2?N . We next input this new 2D pose X to the same teacher network, producing the 3D pose estimation Y t once again; see <ref type="figure" target="#fig_1">Figure 2</ref>. We then minimize the cycle-consistent loss L RIC to restrict Y t to be close to Y t ; that is,</p><formula xml:id="formula_4">L RIC = 1 N F t P(R rand Y t ) ? Y t 2 F ,<label>(3)</label></formula><p>where F t (?) is the teacher network. This consistent 3D pose estimation architecture constrains the teacher network to estimate the canonical 3D pose from various 2D poses. Some previous works also tried to promote the rotationinvariant property. For example, C3DPO <ref type="bibr" target="#b26">(Novotny et al. 2019)</ref> uses an extra canonicalization neural network to recover a random rotation and estimate a canonical Euclidean form of a 3D pose. Compared to those previous methods, the proposed consistent teacher network does not introduce any additional network to handle the decomposition ambiguity, which not only reduces the number of training parameters but also improves empirical performances. Based on the proposed self-supervised 2D projection loss and cycle-consistent loss for 3D rotation-invariance, the teacher network is trained by minimizing L t = ? REP L REP + ? RIC L RIC , where ? REP and ? RIC are two weight hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph-Convolution-based Student Network</head><p>The teacher network with its training architecture makes the estimation problem feasible; however, the learning ability of the teacher network is also limited by linear approximation. To improve the flexibility, we propose a novel graph convolutional network as a student network, which is pose-dictionary free and leverages graphs to model the correlations between body joints. The teacher network estimates the 3D coordinates of the pose because the pose is represented by a linear combination of pose atoms, providing strong regularization. In the student network, we emphasize the flexibility, thus we can simplify the problem to the depth estimation to ensure the reprojection loss is always zero. Inspired by , the student network estimates the depth offset of every joint to the root joint. Given an input 2D pose X, the proposed student network outputs a N -dimensional row vector d = [ d 1 , d 2 , ? ? ? , d N ], whose ith element d i represents the depth offset from the ith body joint to the root joint. Suppose the ith input 2D coordinate is (u i , v i ), utilizing the perspective camera model, the 3D coordinates of the ith body-joint</p><formula xml:id="formula_5">( Y s ) i is then (u i z i , v i z i , d i ), where z i = max(1, t + d i ) and</formula><p>t is the constant distance between the camera and root joint. To achieve the depth estimation, we consider an adaptive graph convolutional network.</p><p>Network structure. The key component of the network is an adaptive graph convolution layer that consists of a physical-graph convolution operation and a nonphysicalgraph convolution operation. Let A be the adjacency matrix that defines the physical connections among body joints and H ? R D?N , H ? R D ?N be the features of body joints before and after the physical-adaptive graph convolution layer.</p><p>For the physical-graph convolution, we leverage the physical constraints of a human body to extract features. Different from the common graph convolution <ref type="bibr" target="#b15">(Kipf and Welling 2016)</ref>, the proposed physical-graph convolution operation introduces a set of trainable edge-weight matrix M = {M d | M d ? R N ?N , d = 1, 2, ? ? ? , D } to adaptively model joints' correlations <ref type="bibr" target="#b33">(Zhao et al. 2019)</ref> and work as</p><formula xml:id="formula_6">H P = D d=1 {? (w d H ? (M d A))} ? R D ?N ,<label>(4)</label></formula><p>where is the concatenate operation along the first dimen-sion, ?(?) is the nonlinear activation function, such as the ReLU function, w d ? R 1?D is the trainable weight vector, ?(?) is the column-wise softmax operation, which normalizes the edge weights and enhances the highest relationships and is an element-wise multiplier. Each edge-weight matrix M d and its associated weight vector w d correspond to one feature channel for the output features of body joints H P . They are trainable and can adaptively adjust the edge weight between each pair of body joints, providing more flexibility to capture the correlations of the body joints. The physical-graph convolution operation relies on the physical connections provided by the human skeleton. To model the nonphysical connections, we further propose a nonphysical-graph convolution operation to capture the correlations between an arbitrary pair of body joints. The proposed nonphysical-graph convolution operation works as</p><formula xml:id="formula_7">H NP = f g (H) ?(H W 1 W 2 H) ? R D ?N ,<label>(5)</label></formula><p>where f (?) and g(?) are two multi-layer perceptrons that operate along the feature dimension and are shared by all the body joints, W 1 ? R D?D and W 2 ? R D?D are two trainable weight matrices and ?(?) is a softmax operation on each column. The N -by-N matrix ?(H W 1 W 2 H) is data-adaptive and models the affinities between all pairs of body points, capturing the nonlocal and nonphysical relationships. Its matrix multiplication with g(H) thus aggregates the features of all the body joints with nonuniform attentions. To integrally capture the features based on physical and nonphysical connections among body joints, we combine two operations and propose an adaptive graph convolutional layer, which is formulated as H = H P + H NP ? R D ?N .</p><p>By using the adaptive graph convolutional layer, we construct the graph-convolution-based student network. <ref type="figure" target="#fig_4">Figure 4</ref> shows a graph convolution block in our student network. The graph-convolution-based student network finally produces the depth d, which can be further converted into 3D coordinates Y s . Compared to many previous graph convolutional networks <ref type="bibr" target="#b15">(Kipf and Welling 2016)</ref>, we adaptively learn a graph structure to capture the correlations among the body joints, which could be beyond the physical connections.</p><p>Knowledge distilling loss. A single graph-convolutionbased student network is hard to estimate a 3D pose as there is no depth supervision for the network training. We thus design a knowledge distillation loss to supervise the student network by the output of the teacher network,</p><formula xml:id="formula_8">L KD = 1 N d ? ( R Y t ) z 2 2 ,<label>(6)</label></formula><p>where the row vector d ? R N is the depth estimated by the student network, R and Y t are the estimated cameraview matrix and the estimated 3D pose output by the teacher network, respectively, and (?) z slices the third dimension, which represents the depths of body joints. Cycle-consistent loss for 3D rotation-equivariance. We use an additional cycle-consistent loss for 3D rotationequivariance similarly to the teacher network to further improve the performance; see Figure2. Note that the student network estimates the depth, which relies on the view of in-put. It thus promotes 3D rotation-equivariance, instead of 3D rotation-invariance as in the teacher network. To be specific, different 2D poses corresponding to a same 3D pose but from different views should produce rotation-equivariant estimation results. Let Y s ? R 3?N denotes the 3D pose estimated by the student network. We rotate 3D pose Y s by a random rotation matrix R rand ? SO(3) and project it to generate a new 2D pose X = P(R rand Y s ). When we input X to the student network, we expect the resulting 3D pose should be close to R rand Y s . The equation of the cycle-consistent loss for 3D rotation-equivariance is thus</p><formula xml:id="formula_9">L REC = 1 N F s P(R rand Y s ) ? R rand Y s 2 F ,<label>(7)</label></formula><p>where F s (?) is the student network. L REC promotes that the information about the random 3D rotation matrix would be preserved in the 3D estimation of the student network. Both the cycle-consistent loss in the teacher network and student network provide self-supervisions to train the networks, but they are significantly different. For the teacher network, the cycle-consistent loss for rotation-invariance handles the perspective projection ambiguity. On the other hand, the cycle-consistent loss for rotation-equivariance ensures the student network adaptive to the camera view and enhances the training. We further present the effects of these two losses in our ablation study; see <ref type="bibr">Sec.4.4</ref>.</p><p>Training of the student network. To train the student network, we freeze the teacher network and update the student network by minimizing both the knowledge distillation loss and the cycle-consistent loss for 3D rotationequivariance. The overall loss for the student network is L s = ? KD L KD + ? REC L REC , where ? KD and ? REC are two weight hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Framework Training</head><p>We consider a two-stage training procedure:</p><p>1. We optimize trainable weights in the pose-dictionarybased teacher network by minimizing L t .</p><p>2. We freeze the teacher network and optimize the graphconvolution-based student network by minimizing L s .</p><p>Finally, we use the estimated 3D pose from the student network as our final output 3D pose; that is, Y = Y s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct extensive experiments to evaluate our model. We show that our ITES outperforms the state-of-the-art method in terms of both quantitive results and qualitative results. We also conduct ablation studies to discuss the effect of various losses, hyperparameters and sub-networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>Human3.6M. Human3.6M <ref type="bibr" target="#b12">(Ionescu et al. 2013</ref>) is a largescale dataset used widely in 3D pose estimation. There are 3.6 million video frames performed by 11 subjects, of which 7 are annotated with 3D poses. It performs 15 classes of actions captured from 4 camera-views. Following the previous works <ref type="bibr" target="#b23">(Martinez et al. 2017)</ref>, we consider the poses of the subjects S1, S5, S6, S7, and S8 for training, and use S9 and S11 for testing. We take two types of 2D poses as inputs: the ground truth 2D annotations (GT) and the 2D poses estimated from images (IMG) by CPN , and we respectively test the model on them. Note that we do not train class-specific model or use the multi-view information. MPI-INF-3DHP. We also use another large scale 3D human pose dataset, MPI-INF-3DHP <ref type="bibr" target="#b24">(Mehta et al. 2017)</ref>, which includes poses in both indoor and outdoor scenes. Following the previous works <ref type="bibr" target="#b14">(Kanazawa et al. 2018;</ref><ref type="bibr" target="#b24">Mehta et al. 2017;</ref><ref type="bibr" target="#b35">Zhou et al. 2017b)</ref>, we train the model on Human3.6M, while we test it on the test set of MPI-INF-3DHP. Metrics. On Human3.6M, we use two quantitative evaluation protocols: 1) the mean per-joint position error (MPJPE), which is the mean Euclidean distance between predicted joints and the ground-truths. For fair comparisons with <ref type="bibr" target="#b26">(Novotny et al. 2019;</ref><ref type="bibr" target="#b19">Kudo et al. 2018)</ref>, we use the same normalization method to scale the estimated poses before calculating MPJPE; and 2) the Procrustes analysis MPJPE (P-MPJPE), which measures the mean error after alignment to the ground truth in terms of translation, rotation, and scale. As for the metrics on MPI-INF-3DHP, we use both the percentage of correct keypoints@150mm (PCK) and the corresponding Area Under Curve (AUC) computed for a range of PCK thresholds. Moreover, we illustrate samples of poses estimated by various algorithms to qualitatively show the effectiveness and rationality of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setups</head><p>We set the distance between the camera and the root joint (pelvis) t as 5 unit. Similar to the network structure proposed in <ref type="bibr" target="#b26">(Novotny et al. 2019)</ref>, the feature-extraction module in the teacher network consists of 6 fully-connected residual layers. Each with 1024/256/1024 neurons. In the teacher network, we set the size of the pose dictionary K to 12. In the student network, we use 8 graph convolution blocks and a residual connection is built across consecutive two blocks. We train the entire framework with the SGD (Bottou 2010) optimizer. In the first training stage, we train the teacher network with the learning rate 0.001 for 40 epoches. In the second training stage, we train the student network for 30 epoches with the learning rate 0.001. The weight parameter is set as ? REP = 5, ? RIC = 1, ? KD = 5, ? REC = 1.    <ref type="bibr" target="#b24">(Mehta et al. 2017)</ref>. The input is the 2D ground truth keypoints following  and Chen et al. are unsupervised pose estimation methods, of which the task scenarios are the same as ITES, while other models use fully supervision or extra side information. <ref type="table" target="#tab_1">Table 1</ref> presents the performance of various models. We report the result of the teacher network after training stage 1 (ITES-T) and the final estimation result of the student network (ITES-TS). We see that i) our method outperforms all the state-of-the-art unsupervised methods and significantly reduces the pose estimation errors by 9.8 mm and 4.6 mm for MPJPE and P-MPJPE in average; ii) we even outperform several weakly-supervised methods that use depth information or multi-view images in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Existing Works</head><p>To test the model's generalization ability, we further train our model on Human3.6M and test it on MPI-INF-3DHP. <ref type="table" target="#tab_2">Table 2</ref> presents the MPJPEs and P-MPJPEs on MPI-INF-3DHP. We see that the proposed ITES method outperforms the state-of-the-art method, Chen et al. , and is close to some fully-supervised method. This reflects that ITES generalizes well to out-of-domain datasets. Qualitative results. <ref type="figure">Figure 5</ref> visualizes the estimation results of C3DPO, one of the state-of-the-art methods, the estimation results of our teacher network (ITES-T) and the final estimation results of our framework (ITES-TS). We see that i) our Input C3DPO</p><p>ITES-T ITES-TS GT <ref type="figure">Figure 5</ref>: Qualitative results on Human3.6M dataset. We present estimations of state-of-the-art method (C3DPO), our teacher network (ITES-T), our entire framework (ITES-TS). method produces more precise estimations than C3DPO; and ii) the 3D poses estimated by the teacher-student framework are better than those by the single teacher network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>Effect of the training losses. First we discuss the effect of training losses we used. <ref type="table" target="#tab_4">Table 3</ref> presents the results. We see that i) The lack of 3D rotation-invariant loss in the teacher network leads to a large estimation error because of the decomposition ambiguity. ii) The student network cannot be trained alone with single cycle-consistent loss for 3D rotationequivariance because of the perspective projection ambiguity. iii) With the knowledge distillation from the teacher network, adding the 3D rotation-equivariant loss will further improve the performance of the student network. Effect of the pose dictionary sizes. The second experiment aims to discuss the effect of different pose dictionary sizes in the teacher network. Figure6 presents the result. We see that either a relatively small or large pose-dictionary size will degrade the performance. A small pose-dictionary size restricts highly on the degree of freedom of the solution space thus cause an inaccurate estimation. A large pose-dictionary size will cause a loose regularization in the teacher network thus prevent the network to learn a set of linearly independent pose atoms. We finally set the pose dictionary size to 12. Effect of various teachers and students. The third experiment aims to evaluate the effectiveness of the proposed posedictionary-based teacher network and graph-convolutionbased student network. We consider two substitutions for the teacher network and the student network, respectively. We  consider C3DPO <ref type="bibr" target="#b26">(Novotny et al. 2019</ref>) as the teacher network and a fully-connected network (ResFC) adopted in <ref type="bibr" target="#b26">(Novotny et al. 2019;</ref><ref type="bibr" target="#b6">Chen et al. 2019)</ref>, as the student network. <ref type="table" target="#tab_7">Table 7</ref> presents the result. Pose-dictionary and AGCN represent our teacher and student network respectively. We see that i) for the teacher network, the proposed pose-dictionary-based network works significantly better than C3DPO; and ii) for the student network, the proposed adaptive graph-convolutionbased network performs better than a commonly used fully connected residual network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Obtaining 3D human pose data acquires physical sensors, which is time-consuming and inconvenient. To alleviate the data bottleneck, we present a novel unsupervised 3D pose estimation approach. We apply a teacher-student framework in which the teacher emphasizes the regularization and the student emphasizes the flexibility. The teacher aims to make the ill-posed estimation problem feasible and the student aims to further improve the performance. Furthermore, we propose two properties for the two networks training by two cycleconsistent architectures: a 3D rotation-invariant property for the teacher for regularization and 3D rotation-equivariant property for the student for further improvement. We showed that this framework achieves the state-of-the-art performance by evaluating on Human3.6M and MPI-INF-3DHP datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Additional Experimental Results</head><p>Effect of different loss weights <ref type="table" target="#tab_6">Table.5 and Table.</ref>6 present the performance of teacher network and student network under different loss weight hyperparameters, respectively. When training the student network, we use the fixed teacher network trained under ? REP = 5, ? RIC = 1. Finally we choose the loss weights of ? REP = 5, ? RIC = 1, ? KD = 5, ? REC = 1. The input is 2D ground truth keypoints and the output is from the teacher network.</p><p>Loss Weights MPJPE P-MPJPE ? REP = 1, ? RIC = 1 144.3 86.1 ? REP = 3, ? RIC = 1 101.5 65.5 ? REP = 5, ? RIC = 1 85.6 54.9 ? REP = 8, ? RIC = 1 92.7 57.7 ? REP = 10, ? RIC = 1 96.7 62.8 <ref type="table">Table 6</ref>: The effect of different loss weights in the student network.</p><p>The input is 2D ground truth keypoints and the output is from the student network, which is also the final estimation result.</p><p>Loss Weights MPJPE P-MPJPE ? KD = 1, ? REC = 1 85.2 54.4 ? KD = 3, ? REC = 1 84.0 53.3 ? KD = 5, ? REC = 1 77.2 51.9 ? KD = 8, ? REC = 1 80.2 52.0 ? KD = 10, ? REC = 1 81.8 52.9</p><p>Effect of different feature extraction modules We discuss the effect of different feature extraction modules in the teacher network. We use fully-connected residual blocks (ResFC) and graph-convolutional blocks (AGCN) same as the student network to extract features for regressing 3D poses and the camera view matrix. <ref type="table">Table.</ref>7 presents the results. Since the teacher network estimates the coefficients and camera views, which are not formed by the graph structure directly, using fully-connected residual blocks is more appropriate to extract features in the teacher network.</p><p>Effect of different student network structures We also make ablations on physical graphs and nonphysical graphs in the student network. 8 present the experiment result. All the    student network is trained under the guidance of the teacher network trained after stage 1(ITES-T).</p><p>Qualitative results on the MPI-INF-3DHP dataset <ref type="figure" target="#fig_7">Fig.7</ref> visualizes the qualitative estimation results on the MPI-INF-3DHP datasets using the model trained on Human3.6M dataset. The ITES represent our estimation results and GT represents the ground truth. Our method ITES performs well on other datasets and outdoor scenes, which reflects our model's generalization ability.</p><p>Model size and inference time We compare our model size and inference time with the only work that releases code in the unsupervised setting, C3DPO <ref type="bibr" target="#b26">(Novotny et al. 2019)</ref>. Models are implemented with Pytorch 1.1.0 and deployed on one GTX-1080Ti GPU. 9 presents the result, our graphconvolution-based student network (ITES-TS) is lighter and faster than C3DPO.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) 3D rotation-invariant property. Different input views result in the same view-invariant 3D estimation. (b) 3D rotationequivariant property. The 3D estimation has the same view as the input views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of ITES. The teacher network (orange branch) estimates the 3D pose and the corresponding camera view matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The pose-dictionary-based teacher network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Adaptive graph convolution block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Quantitative results. To evaluate the effectiveness of the proposed ITES, we first compare our model with state-of-the-art methods on Human3.6M, including Chen &amp; Ramanan (Chen and Ramanan 2017), Martinez et al. (Martinez et al. 2017), IGE-Net (Jack et al. 2019), Ci et al. (Ci et al. 2019), Pavllo et al. (Pavllo et al. 2019), Metha et al. (Mehta et al. 2017), HMR (Kanazawa et al. 2018), SPIN (Kolotouros et al. 2019), 3DInterpreter (Wu et al. 2016), AIGN (Tung et al. 2017), RepNet (Wandt and Rosenhahn 2019), Drover et al. (Drover et al. 2018), Li et al. (Li et al. 2020b), Zhou et al. (Zhou et al. 2017b), Pose-GAN (Kudo et al. 2018), C3DPO (Novotny et al. 2019), Nath et al. (Nath Kundu et al. 2020), Wang et al. (Wang, Kong, and Lucey 2019) and Chen et al. (Chen et al. 2019). Notably, Pose-GAN, C3DPO, Nath et al., Wang et al.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The performance of ITES using different pose dictionary sizes in the teacher network on Human3.6M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results on MPI-INF-3DHP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on Human3.6M. We compare our methods with fully supervised (Fully), Weakly-supervised (Weak) and Unsupervised (Unsup) methods. GT and IMG denote the input data represented by 2D ground truth keypoints and 2D detected keypoints.</figDesc><table><row><cell>Type</cell><cell>Method</cell><cell cols="4">MPJPE GT IMG GT IMG P-MPJPE</cell></row><row><cell cols="2">Fully Chen &amp; Ramanan</cell><cell>-</cell><cell cols="3">114.2 57.5 82.7</cell></row><row><cell></cell><cell>Martinez et al.</cell><cell cols="4">45.5 62.9 37.1 52.1</cell></row><row><cell></cell><cell>IGE-Net</cell><cell cols="4">42.6 66.0 37.7 50.5</cell></row><row><cell></cell><cell>Ci et al.</cell><cell cols="4">36.3 52.7 27.9 42.2</cell></row><row><cell></cell><cell>Pavllo et al.</cell><cell cols="4">37.2 46.8 27.2 36.5</cell></row><row><cell cols="2">Weak 3DInterpreter</cell><cell>-</cell><cell>-</cell><cell cols="2">88.6 98.4</cell></row><row><cell></cell><cell>AIGN</cell><cell>-</cell><cell>-</cell><cell cols="2">79.0 97.2</cell></row><row><cell></cell><cell>RepNet</cell><cell cols="4">50.9 89.9 38.2 65.1</cell></row><row><cell></cell><cell>Drover et al.</cell><cell>-</cell><cell>-</cell><cell cols="2">38.2 64.6</cell></row><row><cell></cell><cell>Li et al.</cell><cell>-</cell><cell>57.0</cell><cell>-</cell><cell>44.1</cell></row><row><cell cols="2">Unsup Pose-GAN</cell><cell cols="2">130.9 173.2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>C3DPO</cell><cell cols="2">95.6 145.0</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Nath et al.</cell><cell>-</cell><cell>-</cell><cell cols="2">63.8 89.4</cell></row><row><cell></cell><cell>Wang et al.</cell><cell>-</cell><cell>86.4</cell><cell>-</cell><cell>62.8</cell></row><row><cell></cell><cell>Chen et al.</cell><cell>-</cell><cell>-</cell><cell>58.0</cell><cell>-</cell></row><row><cell></cell><cell>ITES-T</cell><cell cols="4">85.6 93.7 54.9 62.5</cell></row><row><cell></cell><cell>ITES-TS</cell><cell cols="4">77.2 85.3 51.9 59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Results on MPI-INF-3DHP with the same metric (PCK</cell></row><row><cell>and AUC) as in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effect of various losses component on our frameworktraining on Human3.6M. The input is the 2D ground truth keypoints. Note once we use LKD, the LRP and LREC should also be used to train the teacher network.</figDesc><table><row><cell cols="3">Ablation LREP LRIC LKD LREC MPJPE P-MPJPE</cell></row><row><cell>ITES-T</cell><cell>153.5</cell><cell>120.4</cell></row><row><cell>ITES-T</cell><cell>85.6</cell><cell>54.9</cell></row><row><cell>ITES-TS</cell><cell>192.3</cell><cell>159.7</cell></row><row><cell>ITES-TS</cell><cell>82.4</cell><cell>54.3</cell></row><row><cell>ITES-TS</cell><cell>77.2</cell><cell>51.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effect of the proposed pose-dictionary-based teacher network and graph-convolution-based student network on Human3.6M. We replace the teacher/student network with previous methods. The input is the 2D ground truth keypoints.</figDesc><table><row><cell>Teacher</cell><cell cols="3">Student MPJPE P-MPJPE</cell></row><row><cell>C3DPO</cell><cell>ResFC</cell><cell>93.2</cell><cell>60.0</cell></row><row><cell>C3DPO</cell><cell>AGCN</cell><cell>91.7</cell><cell>59.6</cell></row><row><cell cols="2">Pose-dictionary ResFC</cell><cell>82.3</cell><cell>53.6</cell></row><row><cell cols="2">Pose-dictionary AGCN</cell><cell>77.2</cell><cell>51.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The effect of different loss weights in the teacher network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Discussion on the effect of different feature extraction modules in the teacher network. The input is the 2D ground truth keypoints. The output is from the teacher network.</figDesc><table><row><cell cols="3">Feature Extraction MPJPE P-MPJPE</cell></row><row><cell>AGCN</cell><cell>112.3</cell><cell>68.6</cell></row><row><cell>ResFC</cell><cell>85.9</cell><cell>55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Discussion on the effect of different graph structures in the student network. The input is the 2D ground truth keypoints. The output is from the student network.</figDesc><table><row><cell cols="3">Graph in Studnet network MPJPE P-MPJPE</cell></row><row><cell>Physical only</cell><cell>81.1</cell><cell>52.8</cell></row><row><cell>Nonphysical only</cell><cell>83.3</cell><cell>53.4</cell></row><row><cell>Physical + Nonphysical</cell><cell>77.2</cell><cell>51.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Discussion on the model size and inference time. Both the models are deployed on one GTX-1080TI GPU.</figDesc><table><row><cell cols="6">Methods Model parameters Inference time(ms)</cell></row><row><cell>C3DPO</cell><cell></cell><cell>0.53M</cell><cell></cell><cell>0.17</cell><cell></cell></row><row><cell>ITES-TS</cell><cell></cell><cell>7.27M</cell><cell></cell><cell>0.28</cell><cell></cell></row><row><cell>Input</cell><cell>ITES</cell><cell>GT</cell><cell>Input</cell><cell>ITES</cell><cell>GT</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image collection pop-up: 3d reconstruction and clustering of rigid and non-rigid categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pijoan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nonrigid structure from motion in trajectory space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trajectory space: A dual representation for nonrigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COMPSTAT</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recovering non-rigid 3D shape from image streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Biermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d human pose estima-tion= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple prior-free method for non-rigid structure-from-motion factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grouping-based low-rank trajectory completion and 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">IGE-Net: Inverse graphics energy networks for human pose estimation and single-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prior-less compressible structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised adversarial learning of 3d human pose from 2d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ogaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Odagiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08244</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geometry-Driven Self-Supervised Method for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nath Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<title level="m">Kinematic-Structure-Preserved Representation for Unsupervised 3D Human Pose Estimation. arXiv e-prints arXiv</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">C3dpo: Canonical 3d pose networks for non-rigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distill knowledge from nrsfm for weakly supervised 3d pose learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rocket launching: A universal and efficient framework for training well-performing light net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weaklysupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex non-rigid motion 3d reconstruction by union of subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
